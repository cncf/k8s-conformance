I0118 21:29:31.846265      19 e2e.go:116] Starting e2e run "86437d04-e981-49ed-a4eb-9ac6ca442d05" on Ginkgo node 1
Jan 18 21:29:31.872: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1674077371 - will randomize all specs

Will run 362 of 7067 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Jan 18 21:29:32.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 21:29:32.089: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 18 21:29:32.124: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 18 21:29:32.183: INFO: 24 / 24 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 18 21:29:32.183: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
Jan 18 21:29:32.183: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 18 21:29:32.194: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jan 18 21:29:32.194: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jan 18 21:29:32.194: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'nodelocaldns' (0 seconds elapsed)
Jan 18 21:29:32.195: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'openstack-cinder-csi-nodeplugin' (0 seconds elapsed)
Jan 18 21:29:32.195: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (0 seconds elapsed)
Jan 18 21:29:32.195: INFO: e2e test version: v1.25.4
Jan 18 21:29:32.197: INFO: kube-apiserver version: v1.25.4
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Jan 18 21:29:32.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 21:29:32.204: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.119 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jan 18 21:29:32.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 21:29:32.089: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Jan 18 21:29:32.124: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jan 18 21:29:32.183: INFO: 24 / 24 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jan 18 21:29:32.183: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
    Jan 18 21:29:32.183: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jan 18 21:29:32.194: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Jan 18 21:29:32.194: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Jan 18 21:29:32.194: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'nodelocaldns' (0 seconds elapsed)
    Jan 18 21:29:32.195: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'openstack-cinder-csi-nodeplugin' (0 seconds elapsed)
    Jan 18 21:29:32.195: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (0 seconds elapsed)
    Jan 18 21:29:32.195: INFO: e2e test version: v1.25.4
    Jan 18 21:29:32.197: INFO: kube-apiserver version: v1.25.4
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jan 18 21:29:32.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 21:29:32.204: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:29:32.249
Jan 18 21:29:32.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename services 01/18/23 21:29:32.25
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:29:32.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:29:32.282
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2094 01/18/23 21:29:32.288
STEP: changing the ExternalName service to type=NodePort 01/18/23 21:29:32.294
STEP: creating replication controller externalname-service in namespace services-2094 01/18/23 21:29:32.326
I0118 21:29:32.337582      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2094, replica count: 2
I0118 21:29:35.389539      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 21:29:35.389: INFO: Creating new exec pod
Jan 18 21:29:35.397: INFO: Waiting up to 5m0s for pod "execpodkvhbq" in namespace "services-2094" to be "running"
Jan 18 21:29:35.402: INFO: Pod "execpodkvhbq": Phase="Pending", Reason="", readiness=false. Elapsed: 5.271406ms
Jan 18 21:29:37.409: INFO: Pod "execpodkvhbq": Phase="Running", Reason="", readiness=true. Elapsed: 2.012229275s
Jan 18 21:29:37.410: INFO: Pod "execpodkvhbq" satisfied condition "running"
Jan 18 21:29:38.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-2094 exec execpodkvhbq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 18 21:29:38.681: INFO: stderr: "+ + nc -v -t -w 2 externalname-service 80\necho hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 18 21:29:38.681: INFO: stdout: "externalname-service-qgx4x"
Jan 18 21:29:38.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-2094 exec execpodkvhbq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.5.58 80'
Jan 18 21:29:38.884: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.5.58 80\nConnection to 10.233.5.58 80 port [tcp/http] succeeded!\n"
Jan 18 21:29:38.884: INFO: stdout: "externalname-service-98nvg"
Jan 18 21:29:38.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-2094 exec execpodkvhbq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.168 31502'
Jan 18 21:29:39.079: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.168 31502\nConnection to 192.168.101.168 31502 port [tcp/*] succeeded!\n"
Jan 18 21:29:39.079: INFO: stdout: "externalname-service-qgx4x"
Jan 18 21:29:39.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-2094 exec execpodkvhbq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 31502'
Jan 18 21:29:39.291: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.216 31502\nConnection to 192.168.101.216 31502 port [tcp/*] succeeded!\n"
Jan 18 21:29:39.291: INFO: stdout: "externalname-service-qgx4x"
Jan 18 21:29:39.291: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 21:29:39.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2094" for this suite. 01/18/23 21:29:39.337
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":1,"skipped":31,"failed":0}
------------------------------
• [SLOW TEST] [7.101 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:29:32.249
    Jan 18 21:29:32.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename services 01/18/23 21:29:32.25
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:29:32.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:29:32.282
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-2094 01/18/23 21:29:32.288
    STEP: changing the ExternalName service to type=NodePort 01/18/23 21:29:32.294
    STEP: creating replication controller externalname-service in namespace services-2094 01/18/23 21:29:32.326
    I0118 21:29:32.337582      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2094, replica count: 2
    I0118 21:29:35.389539      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 21:29:35.389: INFO: Creating new exec pod
    Jan 18 21:29:35.397: INFO: Waiting up to 5m0s for pod "execpodkvhbq" in namespace "services-2094" to be "running"
    Jan 18 21:29:35.402: INFO: Pod "execpodkvhbq": Phase="Pending", Reason="", readiness=false. Elapsed: 5.271406ms
    Jan 18 21:29:37.409: INFO: Pod "execpodkvhbq": Phase="Running", Reason="", readiness=true. Elapsed: 2.012229275s
    Jan 18 21:29:37.410: INFO: Pod "execpodkvhbq" satisfied condition "running"
    Jan 18 21:29:38.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-2094 exec execpodkvhbq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 18 21:29:38.681: INFO: stderr: "+ + nc -v -t -w 2 externalname-service 80\necho hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 18 21:29:38.681: INFO: stdout: "externalname-service-qgx4x"
    Jan 18 21:29:38.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-2094 exec execpodkvhbq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.5.58 80'
    Jan 18 21:29:38.884: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.5.58 80\nConnection to 10.233.5.58 80 port [tcp/http] succeeded!\n"
    Jan 18 21:29:38.884: INFO: stdout: "externalname-service-98nvg"
    Jan 18 21:29:38.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-2094 exec execpodkvhbq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.168 31502'
    Jan 18 21:29:39.079: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.168 31502\nConnection to 192.168.101.168 31502 port [tcp/*] succeeded!\n"
    Jan 18 21:29:39.079: INFO: stdout: "externalname-service-qgx4x"
    Jan 18 21:29:39.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-2094 exec execpodkvhbq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 31502'
    Jan 18 21:29:39.291: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.216 31502\nConnection to 192.168.101.216 31502 port [tcp/*] succeeded!\n"
    Jan 18 21:29:39.291: INFO: stdout: "externalname-service-qgx4x"
    Jan 18 21:29:39.291: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 21:29:39.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2094" for this suite. 01/18/23 21:29:39.337
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:29:39.359
Jan 18 21:29:39.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename downward-api 01/18/23 21:29:39.361
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:29:39.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:29:39.389
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 01/18/23 21:29:39.394
Jan 18 21:29:39.404: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0fbb5387-2f14-4675-a22f-22b63d34bd3f" in namespace "downward-api-5066" to be "Succeeded or Failed"
Jan 18 21:29:39.415: INFO: Pod "downwardapi-volume-0fbb5387-2f14-4675-a22f-22b63d34bd3f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.438329ms
Jan 18 21:29:41.421: INFO: Pod "downwardapi-volume-0fbb5387-2f14-4675-a22f-22b63d34bd3f": Phase="Running", Reason="", readiness=true. Elapsed: 2.016726085s
Jan 18 21:29:43.421: INFO: Pod "downwardapi-volume-0fbb5387-2f14-4675-a22f-22b63d34bd3f": Phase="Running", Reason="", readiness=false. Elapsed: 4.016783661s
Jan 18 21:29:45.420: INFO: Pod "downwardapi-volume-0fbb5387-2f14-4675-a22f-22b63d34bd3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015783375s
STEP: Saw pod success 01/18/23 21:29:45.42
Jan 18 21:29:45.421: INFO: Pod "downwardapi-volume-0fbb5387-2f14-4675-a22f-22b63d34bd3f" satisfied condition "Succeeded or Failed"
Jan 18 21:29:45.426: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-0fbb5387-2f14-4675-a22f-22b63d34bd3f container client-container: <nil>
STEP: delete the pod 01/18/23 21:29:45.448
Jan 18 21:29:45.469: INFO: Waiting for pod downwardapi-volume-0fbb5387-2f14-4675-a22f-22b63d34bd3f to disappear
Jan 18 21:29:45.473: INFO: Pod downwardapi-volume-0fbb5387-2f14-4675-a22f-22b63d34bd3f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 21:29:45.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5066" for this suite. 01/18/23 21:29:45.485
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":2,"skipped":96,"failed":0}
------------------------------
• [SLOW TEST] [6.136 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:29:39.359
    Jan 18 21:29:39.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename downward-api 01/18/23 21:29:39.361
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:29:39.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:29:39.389
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 01/18/23 21:29:39.394
    Jan 18 21:29:39.404: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0fbb5387-2f14-4675-a22f-22b63d34bd3f" in namespace "downward-api-5066" to be "Succeeded or Failed"
    Jan 18 21:29:39.415: INFO: Pod "downwardapi-volume-0fbb5387-2f14-4675-a22f-22b63d34bd3f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.438329ms
    Jan 18 21:29:41.421: INFO: Pod "downwardapi-volume-0fbb5387-2f14-4675-a22f-22b63d34bd3f": Phase="Running", Reason="", readiness=true. Elapsed: 2.016726085s
    Jan 18 21:29:43.421: INFO: Pod "downwardapi-volume-0fbb5387-2f14-4675-a22f-22b63d34bd3f": Phase="Running", Reason="", readiness=false. Elapsed: 4.016783661s
    Jan 18 21:29:45.420: INFO: Pod "downwardapi-volume-0fbb5387-2f14-4675-a22f-22b63d34bd3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015783375s
    STEP: Saw pod success 01/18/23 21:29:45.42
    Jan 18 21:29:45.421: INFO: Pod "downwardapi-volume-0fbb5387-2f14-4675-a22f-22b63d34bd3f" satisfied condition "Succeeded or Failed"
    Jan 18 21:29:45.426: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-0fbb5387-2f14-4675-a22f-22b63d34bd3f container client-container: <nil>
    STEP: delete the pod 01/18/23 21:29:45.448
    Jan 18 21:29:45.469: INFO: Waiting for pod downwardapi-volume-0fbb5387-2f14-4675-a22f-22b63d34bd3f to disappear
    Jan 18 21:29:45.473: INFO: Pod downwardapi-volume-0fbb5387-2f14-4675-a22f-22b63d34bd3f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 21:29:45.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5066" for this suite. 01/18/23 21:29:45.485
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:29:45.503
Jan 18 21:29:45.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename secrets 01/18/23 21:29:45.505
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:29:45.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:29:45.549
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-96e3cfc5-2f89-4457-9278-76529527f0e8 01/18/23 21:29:45.553
STEP: Creating a pod to test consume secrets 01/18/23 21:29:45.563
Jan 18 21:29:45.577: INFO: Waiting up to 5m0s for pod "pod-secrets-30460806-51ad-43a8-99ce-d44d0a1f25be" in namespace "secrets-1791" to be "Succeeded or Failed"
Jan 18 21:29:45.608: INFO: Pod "pod-secrets-30460806-51ad-43a8-99ce-d44d0a1f25be": Phase="Pending", Reason="", readiness=false. Elapsed: 30.300373ms
Jan 18 21:29:47.613: INFO: Pod "pod-secrets-30460806-51ad-43a8-99ce-d44d0a1f25be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03596666s
Jan 18 21:29:49.615: INFO: Pod "pod-secrets-30460806-51ad-43a8-99ce-d44d0a1f25be": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038103919s
Jan 18 21:29:51.613: INFO: Pod "pod-secrets-30460806-51ad-43a8-99ce-d44d0a1f25be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035975114s
STEP: Saw pod success 01/18/23 21:29:51.614
Jan 18 21:29:51.614: INFO: Pod "pod-secrets-30460806-51ad-43a8-99ce-d44d0a1f25be" satisfied condition "Succeeded or Failed"
Jan 18 21:29:51.617: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-30460806-51ad-43a8-99ce-d44d0a1f25be container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 21:29:51.625
Jan 18 21:29:51.637: INFO: Waiting for pod pod-secrets-30460806-51ad-43a8-99ce-d44d0a1f25be to disappear
Jan 18 21:29:51.640: INFO: Pod pod-secrets-30460806-51ad-43a8-99ce-d44d0a1f25be no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 18 21:29:51.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1791" for this suite. 01/18/23 21:29:51.646
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":3,"skipped":109,"failed":0}
------------------------------
• [SLOW TEST] [6.150 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:29:45.503
    Jan 18 21:29:45.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename secrets 01/18/23 21:29:45.505
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:29:45.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:29:45.549
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-96e3cfc5-2f89-4457-9278-76529527f0e8 01/18/23 21:29:45.553
    STEP: Creating a pod to test consume secrets 01/18/23 21:29:45.563
    Jan 18 21:29:45.577: INFO: Waiting up to 5m0s for pod "pod-secrets-30460806-51ad-43a8-99ce-d44d0a1f25be" in namespace "secrets-1791" to be "Succeeded or Failed"
    Jan 18 21:29:45.608: INFO: Pod "pod-secrets-30460806-51ad-43a8-99ce-d44d0a1f25be": Phase="Pending", Reason="", readiness=false. Elapsed: 30.300373ms
    Jan 18 21:29:47.613: INFO: Pod "pod-secrets-30460806-51ad-43a8-99ce-d44d0a1f25be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03596666s
    Jan 18 21:29:49.615: INFO: Pod "pod-secrets-30460806-51ad-43a8-99ce-d44d0a1f25be": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038103919s
    Jan 18 21:29:51.613: INFO: Pod "pod-secrets-30460806-51ad-43a8-99ce-d44d0a1f25be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035975114s
    STEP: Saw pod success 01/18/23 21:29:51.614
    Jan 18 21:29:51.614: INFO: Pod "pod-secrets-30460806-51ad-43a8-99ce-d44d0a1f25be" satisfied condition "Succeeded or Failed"
    Jan 18 21:29:51.617: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-30460806-51ad-43a8-99ce-d44d0a1f25be container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 21:29:51.625
    Jan 18 21:29:51.637: INFO: Waiting for pod pod-secrets-30460806-51ad-43a8-99ce-d44d0a1f25be to disappear
    Jan 18 21:29:51.640: INFO: Pod pod-secrets-30460806-51ad-43a8-99ce-d44d0a1f25be no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 21:29:51.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1791" for this suite. 01/18/23 21:29:51.646
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:29:51.669
Jan 18 21:29:51.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename gc 01/18/23 21:29:51.67
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:29:51.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:29:51.703
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jan 18 21:29:51.770: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"3265cb3a-2865-4241-b0db-02ee11f6dcce", Controller:(*bool)(0xc003989222), BlockOwnerDeletion:(*bool)(0xc003989223)}}
Jan 18 21:29:51.855: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"0945ddf1-d6e9-4abe-94f8-1650f05b8c0a", Controller:(*bool)(0xc003989492), BlockOwnerDeletion:(*bool)(0xc003989493)}}
Jan 18 21:29:51.871: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"cf6c305c-0c71-44e8-93c6-360f140b6a47", Controller:(*bool)(0xc0039896e2), BlockOwnerDeletion:(*bool)(0xc0039896e3)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 18 21:29:56.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-503" for this suite. 01/18/23 21:29:56.926
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":4,"skipped":167,"failed":0}
------------------------------
• [SLOW TEST] [5.274 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:29:51.669
    Jan 18 21:29:51.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename gc 01/18/23 21:29:51.67
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:29:51.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:29:51.703
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jan 18 21:29:51.770: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"3265cb3a-2865-4241-b0db-02ee11f6dcce", Controller:(*bool)(0xc003989222), BlockOwnerDeletion:(*bool)(0xc003989223)}}
    Jan 18 21:29:51.855: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"0945ddf1-d6e9-4abe-94f8-1650f05b8c0a", Controller:(*bool)(0xc003989492), BlockOwnerDeletion:(*bool)(0xc003989493)}}
    Jan 18 21:29:51.871: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"cf6c305c-0c71-44e8-93c6-360f140b6a47", Controller:(*bool)(0xc0039896e2), BlockOwnerDeletion:(*bool)(0xc0039896e3)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 18 21:29:56.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-503" for this suite. 01/18/23 21:29:56.926
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:29:56.945
Jan 18 21:29:56.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 21:29:56.947
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:29:56.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:29:56.976
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 01/18/23 21:29:56.981
Jan 18 21:29:56.992: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b2a5a4c4-db57-4ce9-9216-8babf1b61de9" in namespace "projected-3986" to be "Succeeded or Failed"
Jan 18 21:29:56.998: INFO: Pod "downwardapi-volume-b2a5a4c4-db57-4ce9-9216-8babf1b61de9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.735424ms
Jan 18 21:29:59.003: INFO: Pod "downwardapi-volume-b2a5a4c4-db57-4ce9-9216-8babf1b61de9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010674943s
Jan 18 21:30:01.003: INFO: Pod "downwardapi-volume-b2a5a4c4-db57-4ce9-9216-8babf1b61de9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010845937s
STEP: Saw pod success 01/18/23 21:30:01.003
Jan 18 21:30:01.003: INFO: Pod "downwardapi-volume-b2a5a4c4-db57-4ce9-9216-8babf1b61de9" satisfied condition "Succeeded or Failed"
Jan 18 21:30:01.007: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-b2a5a4c4-db57-4ce9-9216-8babf1b61de9 container client-container: <nil>
STEP: delete the pod 01/18/23 21:30:01.016
Jan 18 21:30:01.029: INFO: Waiting for pod downwardapi-volume-b2a5a4c4-db57-4ce9-9216-8babf1b61de9 to disappear
Jan 18 21:30:01.032: INFO: Pod downwardapi-volume-b2a5a4c4-db57-4ce9-9216-8babf1b61de9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 21:30:01.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3986" for this suite. 01/18/23 21:30:01.037
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":5,"skipped":168,"failed":0}
------------------------------
• [4.097 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:29:56.945
    Jan 18 21:29:56.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 21:29:56.947
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:29:56.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:29:56.976
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 01/18/23 21:29:56.981
    Jan 18 21:29:56.992: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b2a5a4c4-db57-4ce9-9216-8babf1b61de9" in namespace "projected-3986" to be "Succeeded or Failed"
    Jan 18 21:29:56.998: INFO: Pod "downwardapi-volume-b2a5a4c4-db57-4ce9-9216-8babf1b61de9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.735424ms
    Jan 18 21:29:59.003: INFO: Pod "downwardapi-volume-b2a5a4c4-db57-4ce9-9216-8babf1b61de9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010674943s
    Jan 18 21:30:01.003: INFO: Pod "downwardapi-volume-b2a5a4c4-db57-4ce9-9216-8babf1b61de9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010845937s
    STEP: Saw pod success 01/18/23 21:30:01.003
    Jan 18 21:30:01.003: INFO: Pod "downwardapi-volume-b2a5a4c4-db57-4ce9-9216-8babf1b61de9" satisfied condition "Succeeded or Failed"
    Jan 18 21:30:01.007: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-b2a5a4c4-db57-4ce9-9216-8babf1b61de9 container client-container: <nil>
    STEP: delete the pod 01/18/23 21:30:01.016
    Jan 18 21:30:01.029: INFO: Waiting for pod downwardapi-volume-b2a5a4c4-db57-4ce9-9216-8babf1b61de9 to disappear
    Jan 18 21:30:01.032: INFO: Pod downwardapi-volume-b2a5a4c4-db57-4ce9-9216-8babf1b61de9 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 21:30:01.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3986" for this suite. 01/18/23 21:30:01.037
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:30:01.044
Jan 18 21:30:01.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename container-runtime 01/18/23 21:30:01.046
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:30:01.063
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:30:01.068
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 01/18/23 21:30:01.072
STEP: wait for the container to reach Succeeded 01/18/23 21:30:01.08
STEP: get the container status 01/18/23 21:30:05.114
STEP: the container should be terminated 01/18/23 21:30:05.119
STEP: the termination message should be set 01/18/23 21:30:05.119
Jan 18 21:30:05.119: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/18/23 21:30:05.12
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 18 21:30:05.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-615" for this suite. 01/18/23 21:30:05.155
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":6,"skipped":174,"failed":0}
------------------------------
• [4.117 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:30:01.044
    Jan 18 21:30:01.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename container-runtime 01/18/23 21:30:01.046
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:30:01.063
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:30:01.068
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 01/18/23 21:30:01.072
    STEP: wait for the container to reach Succeeded 01/18/23 21:30:01.08
    STEP: get the container status 01/18/23 21:30:05.114
    STEP: the container should be terminated 01/18/23 21:30:05.119
    STEP: the termination message should be set 01/18/23 21:30:05.119
    Jan 18 21:30:05.119: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/18/23 21:30:05.12
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 18 21:30:05.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-615" for this suite. 01/18/23 21:30:05.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:30:05.169
Jan 18 21:30:05.169: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename containers 01/18/23 21:30:05.17
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:30:05.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:30:05.207
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 01/18/23 21:30:05.217
Jan 18 21:30:05.229: INFO: Waiting up to 5m0s for pod "client-containers-8f3b8842-314f-49bb-93b8-1435c0552e69" in namespace "containers-67" to be "Succeeded or Failed"
Jan 18 21:30:05.234: INFO: Pod "client-containers-8f3b8842-314f-49bb-93b8-1435c0552e69": Phase="Pending", Reason="", readiness=false. Elapsed: 4.700881ms
Jan 18 21:30:07.241: INFO: Pod "client-containers-8f3b8842-314f-49bb-93b8-1435c0552e69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011610038s
Jan 18 21:30:09.239: INFO: Pod "client-containers-8f3b8842-314f-49bb-93b8-1435c0552e69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008998112s
STEP: Saw pod success 01/18/23 21:30:09.239
Jan 18 21:30:09.239: INFO: Pod "client-containers-8f3b8842-314f-49bb-93b8-1435c0552e69" satisfied condition "Succeeded or Failed"
Jan 18 21:30:09.249: INFO: Trying to get logs from node v1-25-1-18760-w2 pod client-containers-8f3b8842-314f-49bb-93b8-1435c0552e69 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 21:30:09.257
Jan 18 21:30:09.275: INFO: Waiting for pod client-containers-8f3b8842-314f-49bb-93b8-1435c0552e69 to disappear
Jan 18 21:30:09.278: INFO: Pod client-containers-8f3b8842-314f-49bb-93b8-1435c0552e69 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 18 21:30:09.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-67" for this suite. 01/18/23 21:30:09.284
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":7,"skipped":260,"failed":0}
------------------------------
• [4.126 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:30:05.169
    Jan 18 21:30:05.169: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename containers 01/18/23 21:30:05.17
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:30:05.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:30:05.207
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 01/18/23 21:30:05.217
    Jan 18 21:30:05.229: INFO: Waiting up to 5m0s for pod "client-containers-8f3b8842-314f-49bb-93b8-1435c0552e69" in namespace "containers-67" to be "Succeeded or Failed"
    Jan 18 21:30:05.234: INFO: Pod "client-containers-8f3b8842-314f-49bb-93b8-1435c0552e69": Phase="Pending", Reason="", readiness=false. Elapsed: 4.700881ms
    Jan 18 21:30:07.241: INFO: Pod "client-containers-8f3b8842-314f-49bb-93b8-1435c0552e69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011610038s
    Jan 18 21:30:09.239: INFO: Pod "client-containers-8f3b8842-314f-49bb-93b8-1435c0552e69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008998112s
    STEP: Saw pod success 01/18/23 21:30:09.239
    Jan 18 21:30:09.239: INFO: Pod "client-containers-8f3b8842-314f-49bb-93b8-1435c0552e69" satisfied condition "Succeeded or Failed"
    Jan 18 21:30:09.249: INFO: Trying to get logs from node v1-25-1-18760-w2 pod client-containers-8f3b8842-314f-49bb-93b8-1435c0552e69 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 21:30:09.257
    Jan 18 21:30:09.275: INFO: Waiting for pod client-containers-8f3b8842-314f-49bb-93b8-1435c0552e69 to disappear
    Jan 18 21:30:09.278: INFO: Pod client-containers-8f3b8842-314f-49bb-93b8-1435c0552e69 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 18 21:30:09.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-67" for this suite. 01/18/23 21:30:09.284
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:30:09.301
Jan 18 21:30:09.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename daemonsets 01/18/23 21:30:09.304
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:30:09.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:30:09.563
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 01/18/23 21:30:09.592
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 21:30:09.598
Jan 18 21:30:09.605: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:30:09.613: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:30:09.613: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 21:30:10.630: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:30:10.637: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:30:10.637: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 21:30:11.624: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:30:11.631: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 21:30:11.631: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
Jan 18 21:30:12.624: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:30:12.629: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 21:30:12.629: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 01/18/23 21:30:12.633
STEP: DeleteCollection of the DaemonSets 01/18/23 21:30:12.64
STEP: Verify that ReplicaSets have been deleted 01/18/23 21:30:12.653
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Jan 18 21:30:12.935: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"133938"},"items":null}

Jan 18 21:30:13.026: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"133939"},"items":[{"metadata":{"name":"daemon-set-psdrb","generateName":"daemon-set-","namespace":"daemonsets-6404","uid":"e7d14d46-928f-48cc-bd63-325def6946c6","resourceVersion":"133938","creationTimestamp":"2023-01-18T21:30:09Z","deletionTimestamp":"2023-01-18T21:30:42Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"dffafd99c40d96993ff0c0bbc978cf8ad87c566f0f6db2ecef46ba950ca884f7","cni.projectcalico.org/podIP":"10.233.68.49/32","cni.projectcalico.org/podIPs":"10.233.68.49/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"8478aec5-55be-4d69-9f83-d2e6d4e9fdcb","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-18T21:30:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8478aec5-55be-4d69-9f83-d2e6d4e9fdcb\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-18T21:30:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-18T21:30:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-cftfd","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-cftfd","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"v1-25-1-18760-w2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["v1-25-1-18760-w2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T21:30:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T21:30:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T21:30:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T21:30:09Z"}],"hostIP":"192.168.101.216","podIP":"10.233.68.49","podIPs":[{"ip":"10.233.68.49"}],"startTime":"2023-01-18T21:30:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-18T21:30:10Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://2ad3972c8b4cb21414319cbc3c7081561e9841320b6ce30f53d034e9ca6ff666","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-wnbls","generateName":"daemon-set-","namespace":"daemonsets-6404","uid":"1a020a3e-a820-4944-9568-8533ecb8e67f","resourceVersion":"133939","creationTimestamp":"2023-01-18T21:30:09Z","deletionTimestamp":"2023-01-18T21:30:42Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"6c71bcd2d8a44a20b8b6fb4b49fdcd477ee773aee645baaaeb909e8eddb02a29","cni.projectcalico.org/podIP":"10.233.78.201/32","cni.projectcalico.org/podIPs":"10.233.78.201/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"8478aec5-55be-4d69-9f83-d2e6d4e9fdcb","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-18T21:30:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8478aec5-55be-4d69-9f83-d2e6d4e9fdcb\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-18T21:30:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-18T21:30:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.201\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-jxrl7","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-jxrl7","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"v1-25-1-18760-w","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["v1-25-1-18760-w"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T21:30:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T21:30:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T21:30:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T21:30:09Z"}],"hostIP":"192.168.101.168","podIP":"10.233.78.201","podIPs":[{"ip":"10.233.78.201"}],"startTime":"2023-01-18T21:30:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-18T21:30:10Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://ff741686351ff0ff7d242a40f83e750eaa038281ae68308ba8488b89378e1281","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 18 21:30:13.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6404" for this suite. 01/18/23 21:30:13.065
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":8,"skipped":261,"failed":0}
------------------------------
• [3.782 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:30:09.301
    Jan 18 21:30:09.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename daemonsets 01/18/23 21:30:09.304
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:30:09.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:30:09.563
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 01/18/23 21:30:09.592
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 21:30:09.598
    Jan 18 21:30:09.605: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:30:09.613: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:30:09.613: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 21:30:10.630: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:30:10.637: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:30:10.637: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 21:30:11.624: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:30:11.631: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 21:30:11.631: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
    Jan 18 21:30:12.624: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:30:12.629: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 21:30:12.629: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 01/18/23 21:30:12.633
    STEP: DeleteCollection of the DaemonSets 01/18/23 21:30:12.64
    STEP: Verify that ReplicaSets have been deleted 01/18/23 21:30:12.653
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Jan 18 21:30:12.935: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"133938"},"items":null}

    Jan 18 21:30:13.026: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"133939"},"items":[{"metadata":{"name":"daemon-set-psdrb","generateName":"daemon-set-","namespace":"daemonsets-6404","uid":"e7d14d46-928f-48cc-bd63-325def6946c6","resourceVersion":"133938","creationTimestamp":"2023-01-18T21:30:09Z","deletionTimestamp":"2023-01-18T21:30:42Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"dffafd99c40d96993ff0c0bbc978cf8ad87c566f0f6db2ecef46ba950ca884f7","cni.projectcalico.org/podIP":"10.233.68.49/32","cni.projectcalico.org/podIPs":"10.233.68.49/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"8478aec5-55be-4d69-9f83-d2e6d4e9fdcb","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-18T21:30:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8478aec5-55be-4d69-9f83-d2e6d4e9fdcb\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-18T21:30:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-18T21:30:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-cftfd","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-cftfd","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"v1-25-1-18760-w2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["v1-25-1-18760-w2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T21:30:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T21:30:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T21:30:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T21:30:09Z"}],"hostIP":"192.168.101.216","podIP":"10.233.68.49","podIPs":[{"ip":"10.233.68.49"}],"startTime":"2023-01-18T21:30:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-18T21:30:10Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://2ad3972c8b4cb21414319cbc3c7081561e9841320b6ce30f53d034e9ca6ff666","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-wnbls","generateName":"daemon-set-","namespace":"daemonsets-6404","uid":"1a020a3e-a820-4944-9568-8533ecb8e67f","resourceVersion":"133939","creationTimestamp":"2023-01-18T21:30:09Z","deletionTimestamp":"2023-01-18T21:30:42Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"6c71bcd2d8a44a20b8b6fb4b49fdcd477ee773aee645baaaeb909e8eddb02a29","cni.projectcalico.org/podIP":"10.233.78.201/32","cni.projectcalico.org/podIPs":"10.233.78.201/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"8478aec5-55be-4d69-9f83-d2e6d4e9fdcb","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-18T21:30:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8478aec5-55be-4d69-9f83-d2e6d4e9fdcb\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-18T21:30:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-18T21:30:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.201\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-jxrl7","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-jxrl7","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"v1-25-1-18760-w","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["v1-25-1-18760-w"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T21:30:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T21:30:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T21:30:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T21:30:09Z"}],"hostIP":"192.168.101.168","podIP":"10.233.78.201","podIPs":[{"ip":"10.233.78.201"}],"startTime":"2023-01-18T21:30:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-18T21:30:10Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://ff741686351ff0ff7d242a40f83e750eaa038281ae68308ba8488b89378e1281","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 21:30:13.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-6404" for this suite. 01/18/23 21:30:13.065
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:30:13.088
Jan 18 21:30:13.089: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename pod-network-test 01/18/23 21:30:13.09
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:30:13.119
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:30:13.126
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-9518 01/18/23 21:30:13.133
STEP: creating a selector 01/18/23 21:30:13.134
STEP: Creating the service pods in kubernetes 01/18/23 21:30:13.134
Jan 18 21:30:13.134: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 18 21:30:13.181: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9518" to be "running and ready"
Jan 18 21:30:13.191: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.712312ms
Jan 18 21:30:13.191: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:30:15.199: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017765945s
Jan 18 21:30:15.199: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:30:17.197: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.015935531s
Jan 18 21:30:17.197: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:30:19.199: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017696181s
Jan 18 21:30:19.199: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:30:21.199: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.018046867s
Jan 18 21:30:21.199: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:30:23.197: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015962582s
Jan 18 21:30:23.197: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:30:25.197: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015959792s
Jan 18 21:30:25.197: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:30:27.198: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.017633065s
Jan 18 21:30:27.199: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:30:29.200: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.019242798s
Jan 18 21:30:29.200: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:30:31.199: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.018194899s
Jan 18 21:30:31.199: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:30:33.197: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.016301161s
Jan 18 21:30:33.198: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:30:35.197: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.016397481s
Jan 18 21:30:35.197: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 18 21:30:35.198: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 18 21:30:35.203: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9518" to be "running and ready"
Jan 18 21:30:35.208: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.960763ms
Jan 18 21:30:35.209: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 18 21:30:35.209: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/18/23 21:30:35.213
Jan 18 21:30:35.219: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9518" to be "running"
Jan 18 21:30:35.225: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.874654ms
Jan 18 21:30:37.232: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013115452s
Jan 18 21:30:37.233: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 18 21:30:37.237: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 18 21:30:37.237: INFO: Breadth first check of 10.233.78.202 on host 192.168.101.168...
Jan 18 21:30:37.240: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.68.29:9080/dial?request=hostname&protocol=udp&host=10.233.78.202&port=8081&tries=1'] Namespace:pod-network-test-9518 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:30:37.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 21:30:37.242: INFO: ExecWithOptions: Clientset creation
Jan 18 21:30:37.242: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9518/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.68.29%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.78.202%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 21:30:37.347: INFO: Waiting for responses: map[]
Jan 18 21:30:37.347: INFO: reached 10.233.78.202 after 0/1 tries
Jan 18 21:30:37.347: INFO: Breadth first check of 10.233.68.53 on host 192.168.101.216...
Jan 18 21:30:37.352: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.68.29:9080/dial?request=hostname&protocol=udp&host=10.233.68.53&port=8081&tries=1'] Namespace:pod-network-test-9518 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:30:37.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 21:30:37.353: INFO: ExecWithOptions: Clientset creation
Jan 18 21:30:37.353: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9518/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.68.29%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.68.53%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 21:30:37.444: INFO: Waiting for responses: map[]
Jan 18 21:30:37.444: INFO: reached 10.233.68.53 after 0/1 tries
Jan 18 21:30:37.444: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 18 21:30:37.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9518" for this suite. 01/18/23 21:30:37.45
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":9,"skipped":263,"failed":0}
------------------------------
• [SLOW TEST] [24.371 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:30:13.088
    Jan 18 21:30:13.089: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename pod-network-test 01/18/23 21:30:13.09
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:30:13.119
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:30:13.126
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-9518 01/18/23 21:30:13.133
    STEP: creating a selector 01/18/23 21:30:13.134
    STEP: Creating the service pods in kubernetes 01/18/23 21:30:13.134
    Jan 18 21:30:13.134: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 18 21:30:13.181: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9518" to be "running and ready"
    Jan 18 21:30:13.191: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.712312ms
    Jan 18 21:30:13.191: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:30:15.199: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017765945s
    Jan 18 21:30:15.199: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:30:17.197: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.015935531s
    Jan 18 21:30:17.197: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:30:19.199: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017696181s
    Jan 18 21:30:19.199: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:30:21.199: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.018046867s
    Jan 18 21:30:21.199: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:30:23.197: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015962582s
    Jan 18 21:30:23.197: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:30:25.197: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015959792s
    Jan 18 21:30:25.197: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:30:27.198: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.017633065s
    Jan 18 21:30:27.199: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:30:29.200: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.019242798s
    Jan 18 21:30:29.200: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:30:31.199: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.018194899s
    Jan 18 21:30:31.199: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:30:33.197: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.016301161s
    Jan 18 21:30:33.198: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:30:35.197: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.016397481s
    Jan 18 21:30:35.197: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 18 21:30:35.198: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 18 21:30:35.203: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9518" to be "running and ready"
    Jan 18 21:30:35.208: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.960763ms
    Jan 18 21:30:35.209: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 18 21:30:35.209: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/18/23 21:30:35.213
    Jan 18 21:30:35.219: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9518" to be "running"
    Jan 18 21:30:35.225: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.874654ms
    Jan 18 21:30:37.232: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013115452s
    Jan 18 21:30:37.233: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 18 21:30:37.237: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 18 21:30:37.237: INFO: Breadth first check of 10.233.78.202 on host 192.168.101.168...
    Jan 18 21:30:37.240: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.68.29:9080/dial?request=hostname&protocol=udp&host=10.233.78.202&port=8081&tries=1'] Namespace:pod-network-test-9518 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:30:37.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 21:30:37.242: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:30:37.242: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9518/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.68.29%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.78.202%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 18 21:30:37.347: INFO: Waiting for responses: map[]
    Jan 18 21:30:37.347: INFO: reached 10.233.78.202 after 0/1 tries
    Jan 18 21:30:37.347: INFO: Breadth first check of 10.233.68.53 on host 192.168.101.216...
    Jan 18 21:30:37.352: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.68.29:9080/dial?request=hostname&protocol=udp&host=10.233.68.53&port=8081&tries=1'] Namespace:pod-network-test-9518 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:30:37.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 21:30:37.353: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:30:37.353: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9518/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.68.29%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.68.53%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 18 21:30:37.444: INFO: Waiting for responses: map[]
    Jan 18 21:30:37.444: INFO: reached 10.233.68.53 after 0/1 tries
    Jan 18 21:30:37.444: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 18 21:30:37.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-9518" for this suite. 01/18/23 21:30:37.45
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:30:37.462
Jan 18 21:30:37.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename disruption 01/18/23 21:30:37.463
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:30:37.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:30:37.499
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:30:37.506
Jan 18 21:30:37.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename disruption-2 01/18/23 21:30:37.509
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:30:37.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:30:37.543
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 01/18/23 21:30:37.552
STEP: Waiting for the pdb to be processed 01/18/23 21:30:39.574
STEP: Waiting for the pdb to be processed 01/18/23 21:30:39.599
STEP: listing a collection of PDBs across all namespaces 01/18/23 21:30:41.609
STEP: listing a collection of PDBs in namespace disruption-2265 01/18/23 21:30:41.614
STEP: deleting a collection of PDBs 01/18/23 21:30:41.619
STEP: Waiting for the PDB collection to be deleted 01/18/23 21:30:41.632
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Jan 18 21:30:41.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-7966" for this suite. 01/18/23 21:30:41.642
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 18 21:30:41.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2265" for this suite. 01/18/23 21:30:41.654
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":10,"skipped":268,"failed":0}
------------------------------
• [4.202 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:30:37.462
    Jan 18 21:30:37.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename disruption 01/18/23 21:30:37.463
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:30:37.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:30:37.499
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:30:37.506
    Jan 18 21:30:37.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename disruption-2 01/18/23 21:30:37.509
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:30:37.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:30:37.543
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 01/18/23 21:30:37.552
    STEP: Waiting for the pdb to be processed 01/18/23 21:30:39.574
    STEP: Waiting for the pdb to be processed 01/18/23 21:30:39.599
    STEP: listing a collection of PDBs across all namespaces 01/18/23 21:30:41.609
    STEP: listing a collection of PDBs in namespace disruption-2265 01/18/23 21:30:41.614
    STEP: deleting a collection of PDBs 01/18/23 21:30:41.619
    STEP: Waiting for the PDB collection to be deleted 01/18/23 21:30:41.632
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Jan 18 21:30:41.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-7966" for this suite. 01/18/23 21:30:41.642
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 18 21:30:41.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2265" for this suite. 01/18/23 21:30:41.654
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:30:41.689
Jan 18 21:30:41.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename runtimeclass 01/18/23 21:30:41.692
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:30:41.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:30:41.725
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 01/18/23 21:30:41.731
STEP: getting /apis/node.k8s.io 01/18/23 21:30:41.735
STEP: getting /apis/node.k8s.io/v1 01/18/23 21:30:41.737
STEP: creating 01/18/23 21:30:41.739
STEP: watching 01/18/23 21:30:41.756
Jan 18 21:30:41.756: INFO: starting watch
STEP: getting 01/18/23 21:30:41.766
STEP: listing 01/18/23 21:30:41.771
STEP: patching 01/18/23 21:30:41.777
STEP: updating 01/18/23 21:30:41.789
Jan 18 21:30:41.796: INFO: waiting for watch events with expected annotations
STEP: deleting 01/18/23 21:30:41.797
STEP: deleting a collection 01/18/23 21:30:41.812
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 18 21:30:41.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5685" for this suite. 01/18/23 21:30:41.841
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":11,"skipped":283,"failed":0}
------------------------------
• [0.159 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:30:41.689
    Jan 18 21:30:41.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename runtimeclass 01/18/23 21:30:41.692
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:30:41.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:30:41.725
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 01/18/23 21:30:41.731
    STEP: getting /apis/node.k8s.io 01/18/23 21:30:41.735
    STEP: getting /apis/node.k8s.io/v1 01/18/23 21:30:41.737
    STEP: creating 01/18/23 21:30:41.739
    STEP: watching 01/18/23 21:30:41.756
    Jan 18 21:30:41.756: INFO: starting watch
    STEP: getting 01/18/23 21:30:41.766
    STEP: listing 01/18/23 21:30:41.771
    STEP: patching 01/18/23 21:30:41.777
    STEP: updating 01/18/23 21:30:41.789
    Jan 18 21:30:41.796: INFO: waiting for watch events with expected annotations
    STEP: deleting 01/18/23 21:30:41.797
    STEP: deleting a collection 01/18/23 21:30:41.812
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 18 21:30:41.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-5685" for this suite. 01/18/23 21:30:41.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:30:41.851
Jan 18 21:30:41.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename emptydir 01/18/23 21:30:41.853
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:30:41.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:30:41.878
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 01/18/23 21:30:41.884
Jan 18 21:30:41.896: INFO: Waiting up to 5m0s for pod "pod-ae328a22-7651-4552-b8b7-15cd3fc32b5b" in namespace "emptydir-2044" to be "Succeeded or Failed"
Jan 18 21:30:41.902: INFO: Pod "pod-ae328a22-7651-4552-b8b7-15cd3fc32b5b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.51342ms
Jan 18 21:30:43.924: INFO: Pod "pod-ae328a22-7651-4552-b8b7-15cd3fc32b5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027130934s
Jan 18 21:30:45.909: INFO: Pod "pod-ae328a22-7651-4552-b8b7-15cd3fc32b5b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012503948s
Jan 18 21:30:47.909: INFO: Pod "pod-ae328a22-7651-4552-b8b7-15cd3fc32b5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012465856s
STEP: Saw pod success 01/18/23 21:30:47.909
Jan 18 21:30:47.910: INFO: Pod "pod-ae328a22-7651-4552-b8b7-15cd3fc32b5b" satisfied condition "Succeeded or Failed"
Jan 18 21:30:47.915: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-ae328a22-7651-4552-b8b7-15cd3fc32b5b container test-container: <nil>
STEP: delete the pod 01/18/23 21:30:47.922
Jan 18 21:30:47.936: INFO: Waiting for pod pod-ae328a22-7651-4552-b8b7-15cd3fc32b5b to disappear
Jan 18 21:30:47.939: INFO: Pod pod-ae328a22-7651-4552-b8b7-15cd3fc32b5b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 21:30:47.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2044" for this suite. 01/18/23 21:30:47.945
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":12,"skipped":292,"failed":0}
------------------------------
• [SLOW TEST] [6.100 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:30:41.851
    Jan 18 21:30:41.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename emptydir 01/18/23 21:30:41.853
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:30:41.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:30:41.878
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/18/23 21:30:41.884
    Jan 18 21:30:41.896: INFO: Waiting up to 5m0s for pod "pod-ae328a22-7651-4552-b8b7-15cd3fc32b5b" in namespace "emptydir-2044" to be "Succeeded or Failed"
    Jan 18 21:30:41.902: INFO: Pod "pod-ae328a22-7651-4552-b8b7-15cd3fc32b5b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.51342ms
    Jan 18 21:30:43.924: INFO: Pod "pod-ae328a22-7651-4552-b8b7-15cd3fc32b5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027130934s
    Jan 18 21:30:45.909: INFO: Pod "pod-ae328a22-7651-4552-b8b7-15cd3fc32b5b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012503948s
    Jan 18 21:30:47.909: INFO: Pod "pod-ae328a22-7651-4552-b8b7-15cd3fc32b5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012465856s
    STEP: Saw pod success 01/18/23 21:30:47.909
    Jan 18 21:30:47.910: INFO: Pod "pod-ae328a22-7651-4552-b8b7-15cd3fc32b5b" satisfied condition "Succeeded or Failed"
    Jan 18 21:30:47.915: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-ae328a22-7651-4552-b8b7-15cd3fc32b5b container test-container: <nil>
    STEP: delete the pod 01/18/23 21:30:47.922
    Jan 18 21:30:47.936: INFO: Waiting for pod pod-ae328a22-7651-4552-b8b7-15cd3fc32b5b to disappear
    Jan 18 21:30:47.939: INFO: Pod pod-ae328a22-7651-4552-b8b7-15cd3fc32b5b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 21:30:47.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2044" for this suite. 01/18/23 21:30:47.945
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:30:47.953
Jan 18 21:30:47.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename container-runtime 01/18/23 21:30:47.954
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:30:47.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:30:47.987
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/18/23 21:30:48.001
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/18/23 21:31:08.13
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/18/23 21:31:08.135
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/18/23 21:31:08.145
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/18/23 21:31:08.145
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/18/23 21:31:08.187
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/18/23 21:31:11.213
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/18/23 21:31:13.231
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/18/23 21:31:13.242
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/18/23 21:31:13.243
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/18/23 21:31:13.299
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/18/23 21:31:14.321
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/18/23 21:31:17.35
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/18/23 21:31:17.357
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/18/23 21:31:17.357
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 18 21:31:17.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4906" for this suite. 01/18/23 21:31:17.397
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":13,"skipped":295,"failed":0}
------------------------------
• [SLOW TEST] [29.455 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:30:47.953
    Jan 18 21:30:47.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename container-runtime 01/18/23 21:30:47.954
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:30:47.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:30:47.987
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/18/23 21:30:48.001
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/18/23 21:31:08.13
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/18/23 21:31:08.135
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/18/23 21:31:08.145
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/18/23 21:31:08.145
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/18/23 21:31:08.187
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/18/23 21:31:11.213
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/18/23 21:31:13.231
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/18/23 21:31:13.242
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/18/23 21:31:13.243
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/18/23 21:31:13.299
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/18/23 21:31:14.321
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/18/23 21:31:17.35
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/18/23 21:31:17.357
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/18/23 21:31:17.357
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 18 21:31:17.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-4906" for this suite. 01/18/23 21:31:17.397
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:31:17.433
Jan 18 21:31:17.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 21:31:17.435
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:31:17.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:31:17.466
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/18/23 21:31:17.475
Jan 18 21:31:17.486: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4674" to be "running and ready"
Jan 18 21:31:17.495: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.333259ms
Jan 18 21:31:17.495: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:31:19.505: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.018768825s
Jan 18 21:31:19.506: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 18 21:31:19.506: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 01/18/23 21:31:19.517
Jan 18 21:31:19.533: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-4674" to be "running and ready"
Jan 18 21:31:19.540: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.803937ms
Jan 18 21:31:19.540: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:31:21.546: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012886463s
Jan 18 21:31:21.546: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jan 18 21:31:21.546: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/18/23 21:31:21.551
STEP: delete the pod with lifecycle hook 01/18/23 21:31:21.56
Jan 18 21:31:21.597: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 18 21:31:21.604: INFO: Pod pod-with-poststart-http-hook still exists
Jan 18 21:31:23.606: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 18 21:31:23.613: INFO: Pod pod-with-poststart-http-hook still exists
Jan 18 21:31:25.606: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 18 21:31:25.611: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 18 21:31:25.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4674" for this suite. 01/18/23 21:31:25.617
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":14,"skipped":328,"failed":0}
------------------------------
• [SLOW TEST] [8.194 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:31:17.433
    Jan 18 21:31:17.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 21:31:17.435
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:31:17.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:31:17.466
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/18/23 21:31:17.475
    Jan 18 21:31:17.486: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4674" to be "running and ready"
    Jan 18 21:31:17.495: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.333259ms
    Jan 18 21:31:17.495: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:31:19.505: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.018768825s
    Jan 18 21:31:19.506: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 18 21:31:19.506: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 01/18/23 21:31:19.517
    Jan 18 21:31:19.533: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-4674" to be "running and ready"
    Jan 18 21:31:19.540: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.803937ms
    Jan 18 21:31:19.540: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:31:21.546: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012886463s
    Jan 18 21:31:21.546: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jan 18 21:31:21.546: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/18/23 21:31:21.551
    STEP: delete the pod with lifecycle hook 01/18/23 21:31:21.56
    Jan 18 21:31:21.597: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 18 21:31:21.604: INFO: Pod pod-with-poststart-http-hook still exists
    Jan 18 21:31:23.606: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 18 21:31:23.613: INFO: Pod pod-with-poststart-http-hook still exists
    Jan 18 21:31:25.606: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 18 21:31:25.611: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 18 21:31:25.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-4674" for this suite. 01/18/23 21:31:25.617
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:31:25.632
Jan 18 21:31:25.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename sysctl 01/18/23 21:31:25.634
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:31:25.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:31:25.658
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/18/23 21:31:25.664
STEP: Watching for error events or started pod 01/18/23 21:31:25.677
STEP: Waiting for pod completion 01/18/23 21:31:27.682
Jan 18 21:31:27.682: INFO: Waiting up to 3m0s for pod "sysctl-3fd88449-d778-4017-a411-2f15246d4b91" in namespace "sysctl-3135" to be "completed"
Jan 18 21:31:27.686: INFO: Pod "sysctl-3fd88449-d778-4017-a411-2f15246d4b91": Phase="Pending", Reason="", readiness=false. Elapsed: 3.568369ms
Jan 18 21:31:29.692: INFO: Pod "sysctl-3fd88449-d778-4017-a411-2f15246d4b91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009169103s
Jan 18 21:31:29.692: INFO: Pod "sysctl-3fd88449-d778-4017-a411-2f15246d4b91" satisfied condition "completed"
STEP: Checking that the pod succeeded 01/18/23 21:31:29.696
STEP: Getting logs from the pod 01/18/23 21:31:29.697
STEP: Checking that the sysctl is actually updated 01/18/23 21:31:29.717
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 18 21:31:29.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-3135" for this suite. 01/18/23 21:31:29.723
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":15,"skipped":361,"failed":0}
------------------------------
• [4.099 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:31:25.632
    Jan 18 21:31:25.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename sysctl 01/18/23 21:31:25.634
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:31:25.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:31:25.658
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/18/23 21:31:25.664
    STEP: Watching for error events or started pod 01/18/23 21:31:25.677
    STEP: Waiting for pod completion 01/18/23 21:31:27.682
    Jan 18 21:31:27.682: INFO: Waiting up to 3m0s for pod "sysctl-3fd88449-d778-4017-a411-2f15246d4b91" in namespace "sysctl-3135" to be "completed"
    Jan 18 21:31:27.686: INFO: Pod "sysctl-3fd88449-d778-4017-a411-2f15246d4b91": Phase="Pending", Reason="", readiness=false. Elapsed: 3.568369ms
    Jan 18 21:31:29.692: INFO: Pod "sysctl-3fd88449-d778-4017-a411-2f15246d4b91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009169103s
    Jan 18 21:31:29.692: INFO: Pod "sysctl-3fd88449-d778-4017-a411-2f15246d4b91" satisfied condition "completed"
    STEP: Checking that the pod succeeded 01/18/23 21:31:29.696
    STEP: Getting logs from the pod 01/18/23 21:31:29.697
    STEP: Checking that the sysctl is actually updated 01/18/23 21:31:29.717
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 18 21:31:29.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-3135" for this suite. 01/18/23 21:31:29.723
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:31:29.747
Jan 18 21:31:29.748: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename secrets 01/18/23 21:31:29.749
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:31:29.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:31:29.781
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-daf01c64-0dd4-4c76-b90a-364a961cbf9c 01/18/23 21:31:29.787
STEP: Creating a pod to test consume secrets 01/18/23 21:31:29.801
Jan 18 21:31:29.814: INFO: Waiting up to 5m0s for pod "pod-secrets-80db4c57-f33b-4a3c-a21e-c5fc092dd960" in namespace "secrets-4750" to be "Succeeded or Failed"
Jan 18 21:31:29.819: INFO: Pod "pod-secrets-80db4c57-f33b-4a3c-a21e-c5fc092dd960": Phase="Pending", Reason="", readiness=false. Elapsed: 5.716513ms
Jan 18 21:31:31.826: INFO: Pod "pod-secrets-80db4c57-f33b-4a3c-a21e-c5fc092dd960": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012127525s
Jan 18 21:31:33.847: INFO: Pod "pod-secrets-80db4c57-f33b-4a3c-a21e-c5fc092dd960": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033722127s
STEP: Saw pod success 01/18/23 21:31:33.848
Jan 18 21:31:33.848: INFO: Pod "pod-secrets-80db4c57-f33b-4a3c-a21e-c5fc092dd960" satisfied condition "Succeeded or Failed"
Jan 18 21:31:33.855: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-80db4c57-f33b-4a3c-a21e-c5fc092dd960 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 21:31:33.865
Jan 18 21:31:33.879: INFO: Waiting for pod pod-secrets-80db4c57-f33b-4a3c-a21e-c5fc092dd960 to disappear
Jan 18 21:31:33.884: INFO: Pod pod-secrets-80db4c57-f33b-4a3c-a21e-c5fc092dd960 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 18 21:31:33.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4750" for this suite. 01/18/23 21:31:33.889
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":16,"skipped":400,"failed":0}
------------------------------
• [4.150 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:31:29.747
    Jan 18 21:31:29.748: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename secrets 01/18/23 21:31:29.749
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:31:29.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:31:29.781
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-daf01c64-0dd4-4c76-b90a-364a961cbf9c 01/18/23 21:31:29.787
    STEP: Creating a pod to test consume secrets 01/18/23 21:31:29.801
    Jan 18 21:31:29.814: INFO: Waiting up to 5m0s for pod "pod-secrets-80db4c57-f33b-4a3c-a21e-c5fc092dd960" in namespace "secrets-4750" to be "Succeeded or Failed"
    Jan 18 21:31:29.819: INFO: Pod "pod-secrets-80db4c57-f33b-4a3c-a21e-c5fc092dd960": Phase="Pending", Reason="", readiness=false. Elapsed: 5.716513ms
    Jan 18 21:31:31.826: INFO: Pod "pod-secrets-80db4c57-f33b-4a3c-a21e-c5fc092dd960": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012127525s
    Jan 18 21:31:33.847: INFO: Pod "pod-secrets-80db4c57-f33b-4a3c-a21e-c5fc092dd960": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033722127s
    STEP: Saw pod success 01/18/23 21:31:33.848
    Jan 18 21:31:33.848: INFO: Pod "pod-secrets-80db4c57-f33b-4a3c-a21e-c5fc092dd960" satisfied condition "Succeeded or Failed"
    Jan 18 21:31:33.855: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-80db4c57-f33b-4a3c-a21e-c5fc092dd960 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 21:31:33.865
    Jan 18 21:31:33.879: INFO: Waiting for pod pod-secrets-80db4c57-f33b-4a3c-a21e-c5fc092dd960 to disappear
    Jan 18 21:31:33.884: INFO: Pod pod-secrets-80db4c57-f33b-4a3c-a21e-c5fc092dd960 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 21:31:33.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4750" for this suite. 01/18/23 21:31:33.889
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:31:33.905
Jan 18 21:31:33.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename webhook 01/18/23 21:31:33.906
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:31:33.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:31:33.93
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 21:31:33.961
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:31:34.657
STEP: Deploying the webhook pod 01/18/23 21:31:34.665
STEP: Wait for the deployment to be ready 01/18/23 21:31:34.68
Jan 18 21:31:34.692: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/18/23 21:31:36.732
STEP: Verifying the service has paired with the endpoint 01/18/23 21:31:36.769
Jan 18 21:31:37.769: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 01/18/23 21:31:37.775
STEP: Creating a custom resource definition that should be denied by the webhook 01/18/23 21:31:37.795
Jan 18 21:31:37.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 21:31:37.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-540" for this suite. 01/18/23 21:31:37.829
STEP: Destroying namespace "webhook-540-markers" for this suite. 01/18/23 21:31:37.839
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":17,"skipped":460,"failed":0}
------------------------------
• [3.997 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:31:33.905
    Jan 18 21:31:33.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename webhook 01/18/23 21:31:33.906
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:31:33.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:31:33.93
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 21:31:33.961
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:31:34.657
    STEP: Deploying the webhook pod 01/18/23 21:31:34.665
    STEP: Wait for the deployment to be ready 01/18/23 21:31:34.68
    Jan 18 21:31:34.692: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/18/23 21:31:36.732
    STEP: Verifying the service has paired with the endpoint 01/18/23 21:31:36.769
    Jan 18 21:31:37.769: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 01/18/23 21:31:37.775
    STEP: Creating a custom resource definition that should be denied by the webhook 01/18/23 21:31:37.795
    Jan 18 21:31:37.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 21:31:37.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-540" for this suite. 01/18/23 21:31:37.829
    STEP: Destroying namespace "webhook-540-markers" for this suite. 01/18/23 21:31:37.839
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:31:37.912
Jan 18 21:31:37.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename resourcequota 01/18/23 21:31:37.913
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:31:37.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:31:37.945
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 01/18/23 21:31:37.951
STEP: Ensuring ResourceQuota status is calculated 01/18/23 21:31:37.96
STEP: Creating a ResourceQuota with not terminating scope 01/18/23 21:31:39.968
STEP: Ensuring ResourceQuota status is calculated 01/18/23 21:31:39.974
STEP: Creating a long running pod 01/18/23 21:31:41.98
STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/18/23 21:31:41.996
STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/18/23 21:31:44.003
STEP: Deleting the pod 01/18/23 21:31:46.013
STEP: Ensuring resource quota status released the pod usage 01/18/23 21:31:46.041
STEP: Creating a terminating pod 01/18/23 21:31:48.049
STEP: Ensuring resource quota with terminating scope captures the pod usage 01/18/23 21:31:48.065
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/18/23 21:31:50.071
STEP: Deleting the pod 01/18/23 21:31:52.078
STEP: Ensuring resource quota status released the pod usage 01/18/23 21:31:52.099
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 21:31:54.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1565" for this suite. 01/18/23 21:31:54.114
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":18,"skipped":476,"failed":0}
------------------------------
• [SLOW TEST] [16.211 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:31:37.912
    Jan 18 21:31:37.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename resourcequota 01/18/23 21:31:37.913
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:31:37.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:31:37.945
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 01/18/23 21:31:37.951
    STEP: Ensuring ResourceQuota status is calculated 01/18/23 21:31:37.96
    STEP: Creating a ResourceQuota with not terminating scope 01/18/23 21:31:39.968
    STEP: Ensuring ResourceQuota status is calculated 01/18/23 21:31:39.974
    STEP: Creating a long running pod 01/18/23 21:31:41.98
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/18/23 21:31:41.996
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/18/23 21:31:44.003
    STEP: Deleting the pod 01/18/23 21:31:46.013
    STEP: Ensuring resource quota status released the pod usage 01/18/23 21:31:46.041
    STEP: Creating a terminating pod 01/18/23 21:31:48.049
    STEP: Ensuring resource quota with terminating scope captures the pod usage 01/18/23 21:31:48.065
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/18/23 21:31:50.071
    STEP: Deleting the pod 01/18/23 21:31:52.078
    STEP: Ensuring resource quota status released the pod usage 01/18/23 21:31:52.099
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 21:31:54.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1565" for this suite. 01/18/23 21:31:54.114
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:31:54.123
Jan 18 21:31:54.124: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename container-probe 01/18/23 21:31:54.125
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:31:54.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:31:54.161
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-d9baa71b-4be3-4664-ac82-03f5cc3f8282 in namespace container-probe-5440 01/18/23 21:31:54.171
Jan 18 21:31:54.181: INFO: Waiting up to 5m0s for pod "test-webserver-d9baa71b-4be3-4664-ac82-03f5cc3f8282" in namespace "container-probe-5440" to be "not pending"
Jan 18 21:31:54.192: INFO: Pod "test-webserver-d9baa71b-4be3-4664-ac82-03f5cc3f8282": Phase="Pending", Reason="", readiness=false. Elapsed: 11.149367ms
Jan 18 21:31:56.197: INFO: Pod "test-webserver-d9baa71b-4be3-4664-ac82-03f5cc3f8282": Phase="Running", Reason="", readiness=true. Elapsed: 2.01631578s
Jan 18 21:31:56.197: INFO: Pod "test-webserver-d9baa71b-4be3-4664-ac82-03f5cc3f8282" satisfied condition "not pending"
Jan 18 21:31:56.197: INFO: Started pod test-webserver-d9baa71b-4be3-4664-ac82-03f5cc3f8282 in namespace container-probe-5440
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 21:31:56.198
Jan 18 21:31:56.203: INFO: Initial restart count of pod test-webserver-d9baa71b-4be3-4664-ac82-03f5cc3f8282 is 0
STEP: deleting the pod 01/18/23 21:35:56.954
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 18 21:35:56.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5440" for this suite. 01/18/23 21:35:56.988
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":19,"skipped":484,"failed":0}
------------------------------
• [SLOW TEST] [242.871 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:31:54.123
    Jan 18 21:31:54.124: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename container-probe 01/18/23 21:31:54.125
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:31:54.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:31:54.161
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-d9baa71b-4be3-4664-ac82-03f5cc3f8282 in namespace container-probe-5440 01/18/23 21:31:54.171
    Jan 18 21:31:54.181: INFO: Waiting up to 5m0s for pod "test-webserver-d9baa71b-4be3-4664-ac82-03f5cc3f8282" in namespace "container-probe-5440" to be "not pending"
    Jan 18 21:31:54.192: INFO: Pod "test-webserver-d9baa71b-4be3-4664-ac82-03f5cc3f8282": Phase="Pending", Reason="", readiness=false. Elapsed: 11.149367ms
    Jan 18 21:31:56.197: INFO: Pod "test-webserver-d9baa71b-4be3-4664-ac82-03f5cc3f8282": Phase="Running", Reason="", readiness=true. Elapsed: 2.01631578s
    Jan 18 21:31:56.197: INFO: Pod "test-webserver-d9baa71b-4be3-4664-ac82-03f5cc3f8282" satisfied condition "not pending"
    Jan 18 21:31:56.197: INFO: Started pod test-webserver-d9baa71b-4be3-4664-ac82-03f5cc3f8282 in namespace container-probe-5440
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 21:31:56.198
    Jan 18 21:31:56.203: INFO: Initial restart count of pod test-webserver-d9baa71b-4be3-4664-ac82-03f5cc3f8282 is 0
    STEP: deleting the pod 01/18/23 21:35:56.954
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 18 21:35:56.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5440" for this suite. 01/18/23 21:35:56.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:35:57.011
Jan 18 21:35:57.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename disruption 01/18/23 21:35:57.015
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:35:57.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:35:57.045
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 01/18/23 21:35:57.053
STEP: Waiting for the pdb to be processed 01/18/23 21:35:57.062
STEP: First trying to evict a pod which shouldn't be evictable 01/18/23 21:35:59.081
STEP: Waiting for all pods to be running 01/18/23 21:35:59.082
Jan 18 21:35:59.087: INFO: pods: 0 < 3
Jan 18 21:36:01.168: INFO: running pods: 1 < 3
STEP: locating a running pod 01/18/23 21:36:03.094
STEP: Updating the pdb to allow a pod to be evicted 01/18/23 21:36:03.11
STEP: Waiting for the pdb to be processed 01/18/23 21:36:03.121
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/18/23 21:36:05.137
STEP: Waiting for all pods to be running 01/18/23 21:36:05.138
STEP: Waiting for the pdb to observed all healthy pods 01/18/23 21:36:05.144
STEP: Patching the pdb to disallow a pod to be evicted 01/18/23 21:36:05.182
STEP: Waiting for the pdb to be processed 01/18/23 21:36:05.228
STEP: Waiting for all pods to be running 01/18/23 21:36:07.247
Jan 18 21:36:07.253: INFO: running pods: 2 < 3
STEP: locating a running pod 01/18/23 21:36:09.259
STEP: Deleting the pdb to allow a pod to be evicted 01/18/23 21:36:09.271
STEP: Waiting for the pdb to be deleted 01/18/23 21:36:09.278
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/18/23 21:36:09.286
STEP: Waiting for all pods to be running 01/18/23 21:36:09.286
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 18 21:36:09.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9322" for this suite. 01/18/23 21:36:09.333
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":20,"skipped":498,"failed":0}
------------------------------
• [SLOW TEST] [12.344 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:35:57.011
    Jan 18 21:35:57.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename disruption 01/18/23 21:35:57.015
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:35:57.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:35:57.045
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 01/18/23 21:35:57.053
    STEP: Waiting for the pdb to be processed 01/18/23 21:35:57.062
    STEP: First trying to evict a pod which shouldn't be evictable 01/18/23 21:35:59.081
    STEP: Waiting for all pods to be running 01/18/23 21:35:59.082
    Jan 18 21:35:59.087: INFO: pods: 0 < 3
    Jan 18 21:36:01.168: INFO: running pods: 1 < 3
    STEP: locating a running pod 01/18/23 21:36:03.094
    STEP: Updating the pdb to allow a pod to be evicted 01/18/23 21:36:03.11
    STEP: Waiting for the pdb to be processed 01/18/23 21:36:03.121
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/18/23 21:36:05.137
    STEP: Waiting for all pods to be running 01/18/23 21:36:05.138
    STEP: Waiting for the pdb to observed all healthy pods 01/18/23 21:36:05.144
    STEP: Patching the pdb to disallow a pod to be evicted 01/18/23 21:36:05.182
    STEP: Waiting for the pdb to be processed 01/18/23 21:36:05.228
    STEP: Waiting for all pods to be running 01/18/23 21:36:07.247
    Jan 18 21:36:07.253: INFO: running pods: 2 < 3
    STEP: locating a running pod 01/18/23 21:36:09.259
    STEP: Deleting the pdb to allow a pod to be evicted 01/18/23 21:36:09.271
    STEP: Waiting for the pdb to be deleted 01/18/23 21:36:09.278
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/18/23 21:36:09.286
    STEP: Waiting for all pods to be running 01/18/23 21:36:09.286
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 18 21:36:09.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-9322" for this suite. 01/18/23 21:36:09.333
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:36:09.357
Jan 18 21:36:09.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename downward-api 01/18/23 21:36:09.359
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:36:09.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:36:09.382
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 01/18/23 21:36:09.396
Jan 18 21:36:09.410: INFO: Waiting up to 5m0s for pod "annotationupdate6f617e68-25cb-4925-ac1f-364652c33a03" in namespace "downward-api-8755" to be "running and ready"
Jan 18 21:36:09.424: INFO: Pod "annotationupdate6f617e68-25cb-4925-ac1f-364652c33a03": Phase="Pending", Reason="", readiness=false. Elapsed: 13.699398ms
Jan 18 21:36:09.425: INFO: The phase of Pod annotationupdate6f617e68-25cb-4925-ac1f-364652c33a03 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:36:11.431: INFO: Pod "annotationupdate6f617e68-25cb-4925-ac1f-364652c33a03": Phase="Running", Reason="", readiness=true. Elapsed: 2.020091879s
Jan 18 21:36:11.431: INFO: The phase of Pod annotationupdate6f617e68-25cb-4925-ac1f-364652c33a03 is Running (Ready = true)
Jan 18 21:36:11.431: INFO: Pod "annotationupdate6f617e68-25cb-4925-ac1f-364652c33a03" satisfied condition "running and ready"
Jan 18 21:36:11.977: INFO: Successfully updated pod "annotationupdate6f617e68-25cb-4925-ac1f-364652c33a03"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 21:36:13.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8755" for this suite. 01/18/23 21:36:14.003
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":21,"skipped":505,"failed":0}
------------------------------
• [4.655 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:36:09.357
    Jan 18 21:36:09.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename downward-api 01/18/23 21:36:09.359
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:36:09.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:36:09.382
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 01/18/23 21:36:09.396
    Jan 18 21:36:09.410: INFO: Waiting up to 5m0s for pod "annotationupdate6f617e68-25cb-4925-ac1f-364652c33a03" in namespace "downward-api-8755" to be "running and ready"
    Jan 18 21:36:09.424: INFO: Pod "annotationupdate6f617e68-25cb-4925-ac1f-364652c33a03": Phase="Pending", Reason="", readiness=false. Elapsed: 13.699398ms
    Jan 18 21:36:09.425: INFO: The phase of Pod annotationupdate6f617e68-25cb-4925-ac1f-364652c33a03 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:36:11.431: INFO: Pod "annotationupdate6f617e68-25cb-4925-ac1f-364652c33a03": Phase="Running", Reason="", readiness=true. Elapsed: 2.020091879s
    Jan 18 21:36:11.431: INFO: The phase of Pod annotationupdate6f617e68-25cb-4925-ac1f-364652c33a03 is Running (Ready = true)
    Jan 18 21:36:11.431: INFO: Pod "annotationupdate6f617e68-25cb-4925-ac1f-364652c33a03" satisfied condition "running and ready"
    Jan 18 21:36:11.977: INFO: Successfully updated pod "annotationupdate6f617e68-25cb-4925-ac1f-364652c33a03"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 21:36:13.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8755" for this suite. 01/18/23 21:36:14.003
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:36:14.023
Jan 18 21:36:14.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 21:36:14.025
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:36:14.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:36:14.056
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-0e52e50e-b700-438f-b18a-3ad6f565de17 01/18/23 21:36:14.062
STEP: Creating a pod to test consume configMaps 01/18/23 21:36:14.071
Jan 18 21:36:14.084: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5d18783a-3727-48cc-ac29-82f8d2c16d89" in namespace "projected-5419" to be "Succeeded or Failed"
Jan 18 21:36:14.101: INFO: Pod "pod-projected-configmaps-5d18783a-3727-48cc-ac29-82f8d2c16d89": Phase="Pending", Reason="", readiness=false. Elapsed: 16.495718ms
Jan 18 21:36:16.109: INFO: Pod "pod-projected-configmaps-5d18783a-3727-48cc-ac29-82f8d2c16d89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024771894s
Jan 18 21:36:18.109: INFO: Pod "pod-projected-configmaps-5d18783a-3727-48cc-ac29-82f8d2c16d89": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024362956s
Jan 18 21:36:20.109: INFO: Pod "pod-projected-configmaps-5d18783a-3727-48cc-ac29-82f8d2c16d89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023824649s
STEP: Saw pod success 01/18/23 21:36:20.109
Jan 18 21:36:20.110: INFO: Pod "pod-projected-configmaps-5d18783a-3727-48cc-ac29-82f8d2c16d89" satisfied condition "Succeeded or Failed"
Jan 18 21:36:20.114: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-5d18783a-3727-48cc-ac29-82f8d2c16d89 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 21:36:20.122
Jan 18 21:36:20.136: INFO: Waiting for pod pod-projected-configmaps-5d18783a-3727-48cc-ac29-82f8d2c16d89 to disappear
Jan 18 21:36:20.166: INFO: Pod pod-projected-configmaps-5d18783a-3727-48cc-ac29-82f8d2c16d89 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 18 21:36:20.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5419" for this suite. 01/18/23 21:36:20.179
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":22,"skipped":531,"failed":0}
------------------------------
• [SLOW TEST] [6.164 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:36:14.023
    Jan 18 21:36:14.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 21:36:14.025
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:36:14.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:36:14.056
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-0e52e50e-b700-438f-b18a-3ad6f565de17 01/18/23 21:36:14.062
    STEP: Creating a pod to test consume configMaps 01/18/23 21:36:14.071
    Jan 18 21:36:14.084: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5d18783a-3727-48cc-ac29-82f8d2c16d89" in namespace "projected-5419" to be "Succeeded or Failed"
    Jan 18 21:36:14.101: INFO: Pod "pod-projected-configmaps-5d18783a-3727-48cc-ac29-82f8d2c16d89": Phase="Pending", Reason="", readiness=false. Elapsed: 16.495718ms
    Jan 18 21:36:16.109: INFO: Pod "pod-projected-configmaps-5d18783a-3727-48cc-ac29-82f8d2c16d89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024771894s
    Jan 18 21:36:18.109: INFO: Pod "pod-projected-configmaps-5d18783a-3727-48cc-ac29-82f8d2c16d89": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024362956s
    Jan 18 21:36:20.109: INFO: Pod "pod-projected-configmaps-5d18783a-3727-48cc-ac29-82f8d2c16d89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023824649s
    STEP: Saw pod success 01/18/23 21:36:20.109
    Jan 18 21:36:20.110: INFO: Pod "pod-projected-configmaps-5d18783a-3727-48cc-ac29-82f8d2c16d89" satisfied condition "Succeeded or Failed"
    Jan 18 21:36:20.114: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-5d18783a-3727-48cc-ac29-82f8d2c16d89 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 21:36:20.122
    Jan 18 21:36:20.136: INFO: Waiting for pod pod-projected-configmaps-5d18783a-3727-48cc-ac29-82f8d2c16d89 to disappear
    Jan 18 21:36:20.166: INFO: Pod pod-projected-configmaps-5d18783a-3727-48cc-ac29-82f8d2c16d89 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 18 21:36:20.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5419" for this suite. 01/18/23 21:36:20.179
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:36:20.188
Jan 18 21:36:20.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename init-container 01/18/23 21:36:20.19
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:36:20.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:36:20.211
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 01/18/23 21:36:20.218
Jan 18 21:36:20.218: INFO: PodSpec: initContainers in spec.initContainers
Jan 18 21:37:03.574: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-1df8fb62-687d-4295-841a-1ddec17233d9", GenerateName:"", Namespace:"init-container-5691", SelfLink:"", UID:"7b2af232-d9b2-4b4a-941e-476f519aec05", ResourceVersion:"136212", Generation:0, CreationTimestamp:time.Date(2023, time.January, 18, 21, 36, 20, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"218642988"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"b4088f5a76993697018ab3350ad2978cbf837336257caa093c3533aa8e161513", "cni.projectcalico.org/podIP":"10.233.68.43/32", "cni.projectcalico.org/podIPs":"10.233.68.43/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 21, 36, 20, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000ba9278), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 21, 36, 20, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000ba92a8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 21, 37, 3, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000ba92f0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-m6wl7", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc001b0b460), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-m6wl7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-m6wl7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-m6wl7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002d6b9d8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"v1-25-1-18760-w2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0005b6850), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002d6ba60)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002d6ba80)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002d6ba88), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002d6ba8c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001061ca0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 21, 36, 20, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 21, 36, 20, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 21, 36, 20, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 21, 36, 20, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.101.216", PodIP:"10.233.68.43", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.68.43"}}, StartTime:time.Date(2023, time.January, 18, 21, 36, 20, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0005b6930)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0005b69a0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://257a3cd24b6828fd921ba6fee3b305986b12d97d7a0459128d2a39e57b1888cd", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001b0b4e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001b0b4c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc002d6bb0f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 18 21:37:03.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5691" for this suite. 01/18/23 21:37:03.597
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":23,"skipped":532,"failed":0}
------------------------------
• [SLOW TEST] [43.416 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:36:20.188
    Jan 18 21:36:20.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename init-container 01/18/23 21:36:20.19
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:36:20.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:36:20.211
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 01/18/23 21:36:20.218
    Jan 18 21:36:20.218: INFO: PodSpec: initContainers in spec.initContainers
    Jan 18 21:37:03.574: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-1df8fb62-687d-4295-841a-1ddec17233d9", GenerateName:"", Namespace:"init-container-5691", SelfLink:"", UID:"7b2af232-d9b2-4b4a-941e-476f519aec05", ResourceVersion:"136212", Generation:0, CreationTimestamp:time.Date(2023, time.January, 18, 21, 36, 20, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"218642988"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"b4088f5a76993697018ab3350ad2978cbf837336257caa093c3533aa8e161513", "cni.projectcalico.org/podIP":"10.233.68.43/32", "cni.projectcalico.org/podIPs":"10.233.68.43/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 21, 36, 20, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000ba9278), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 21, 36, 20, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000ba92a8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 21, 37, 3, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000ba92f0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-m6wl7", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc001b0b460), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-m6wl7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-m6wl7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-m6wl7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002d6b9d8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"v1-25-1-18760-w2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0005b6850), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002d6ba60)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002d6ba80)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002d6ba88), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002d6ba8c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001061ca0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 21, 36, 20, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 21, 36, 20, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 21, 36, 20, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 21, 36, 20, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.101.216", PodIP:"10.233.68.43", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.68.43"}}, StartTime:time.Date(2023, time.January, 18, 21, 36, 20, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0005b6930)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0005b69a0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://257a3cd24b6828fd921ba6fee3b305986b12d97d7a0459128d2a39e57b1888cd", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001b0b4e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001b0b4c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc002d6bb0f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 18 21:37:03.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-5691" for this suite. 01/18/23 21:37:03.597
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:37:03.61
Jan 18 21:37:03.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename limitrange 01/18/23 21:37:03.612
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:37:03.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:37:03.634
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 01/18/23 21:37:03.639
STEP: Setting up watch 01/18/23 21:37:03.639
STEP: Submitting a LimitRange 01/18/23 21:37:03.742
STEP: Verifying LimitRange creation was observed 01/18/23 21:37:03.756
STEP: Fetching the LimitRange to ensure it has proper values 01/18/23 21:37:03.757
Jan 18 21:37:03.766: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 18 21:37:03.766: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 01/18/23 21:37:03.766
STEP: Ensuring Pod has resource requirements applied from LimitRange 01/18/23 21:37:03.774
Jan 18 21:37:03.781: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 18 21:37:03.781: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 01/18/23 21:37:03.782
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/18/23 21:37:03.79
Jan 18 21:37:03.794: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan 18 21:37:03.794: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 01/18/23 21:37:03.794
STEP: Failing to create a Pod with more than max resources 01/18/23 21:37:03.798
STEP: Updating a LimitRange 01/18/23 21:37:03.802
STEP: Verifying LimitRange updating is effective 01/18/23 21:37:03.815
STEP: Creating a Pod with less than former min resources 01/18/23 21:37:05.822
STEP: Failing to create a Pod with more than max resources 01/18/23 21:37:05.831
STEP: Deleting a LimitRange 01/18/23 21:37:05.841
STEP: Verifying the LimitRange was deleted 01/18/23 21:37:05.856
Jan 18 21:37:10.861: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 01/18/23 21:37:10.861
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Jan 18 21:37:10.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-148" for this suite. 01/18/23 21:37:10.876
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":24,"skipped":540,"failed":0}
------------------------------
• [SLOW TEST] [7.282 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:37:03.61
    Jan 18 21:37:03.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename limitrange 01/18/23 21:37:03.612
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:37:03.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:37:03.634
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 01/18/23 21:37:03.639
    STEP: Setting up watch 01/18/23 21:37:03.639
    STEP: Submitting a LimitRange 01/18/23 21:37:03.742
    STEP: Verifying LimitRange creation was observed 01/18/23 21:37:03.756
    STEP: Fetching the LimitRange to ensure it has proper values 01/18/23 21:37:03.757
    Jan 18 21:37:03.766: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 18 21:37:03.766: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 01/18/23 21:37:03.766
    STEP: Ensuring Pod has resource requirements applied from LimitRange 01/18/23 21:37:03.774
    Jan 18 21:37:03.781: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 18 21:37:03.781: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 01/18/23 21:37:03.782
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/18/23 21:37:03.79
    Jan 18 21:37:03.794: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jan 18 21:37:03.794: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 01/18/23 21:37:03.794
    STEP: Failing to create a Pod with more than max resources 01/18/23 21:37:03.798
    STEP: Updating a LimitRange 01/18/23 21:37:03.802
    STEP: Verifying LimitRange updating is effective 01/18/23 21:37:03.815
    STEP: Creating a Pod with less than former min resources 01/18/23 21:37:05.822
    STEP: Failing to create a Pod with more than max resources 01/18/23 21:37:05.831
    STEP: Deleting a LimitRange 01/18/23 21:37:05.841
    STEP: Verifying the LimitRange was deleted 01/18/23 21:37:05.856
    Jan 18 21:37:10.861: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 01/18/23 21:37:10.861
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Jan 18 21:37:10.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-148" for this suite. 01/18/23 21:37:10.876
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:37:10.894
Jan 18 21:37:10.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename webhook 01/18/23 21:37:10.897
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:37:10.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:37:10.923
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 21:37:10.955
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:37:11.519
STEP: Deploying the webhook pod 01/18/23 21:37:11.528
STEP: Wait for the deployment to be ready 01/18/23 21:37:11.54
Jan 18 21:37:11.571: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 18 21:37:13.598: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 37, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 37, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 37, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 37, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/18/23 21:37:15.605
STEP: Verifying the service has paired with the endpoint 01/18/23 21:37:15.619
Jan 18 21:37:16.620: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 01/18/23 21:37:16.627
STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 21:37:16.647
STEP: Updating a validating webhook configuration's rules to not include the create operation 01/18/23 21:37:16.661
STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 21:37:16.673
STEP: Patching a validating webhook configuration's rules to include the create operation 01/18/23 21:37:16.685
STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 21:37:16.694
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 21:37:16.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4740" for this suite. 01/18/23 21:37:16.714
STEP: Destroying namespace "webhook-4740-markers" for this suite. 01/18/23 21:37:16.72
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":25,"skipped":542,"failed":0}
------------------------------
• [SLOW TEST] [5.899 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:37:10.894
    Jan 18 21:37:10.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename webhook 01/18/23 21:37:10.897
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:37:10.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:37:10.923
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 21:37:10.955
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:37:11.519
    STEP: Deploying the webhook pod 01/18/23 21:37:11.528
    STEP: Wait for the deployment to be ready 01/18/23 21:37:11.54
    Jan 18 21:37:11.571: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 18 21:37:13.598: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 37, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 37, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 37, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 37, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/18/23 21:37:15.605
    STEP: Verifying the service has paired with the endpoint 01/18/23 21:37:15.619
    Jan 18 21:37:16.620: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 01/18/23 21:37:16.627
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 21:37:16.647
    STEP: Updating a validating webhook configuration's rules to not include the create operation 01/18/23 21:37:16.661
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 21:37:16.673
    STEP: Patching a validating webhook configuration's rules to include the create operation 01/18/23 21:37:16.685
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 21:37:16.694
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 21:37:16.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4740" for this suite. 01/18/23 21:37:16.714
    STEP: Destroying namespace "webhook-4740-markers" for this suite. 01/18/23 21:37:16.72
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:37:16.796
Jan 18 21:37:16.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename emptydir 01/18/23 21:37:16.798
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:37:17.059
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:37:17.071
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 01/18/23 21:37:17.077
Jan 18 21:37:17.088: INFO: Waiting up to 5m0s for pod "pod-fab818f7-88e8-4601-8d01-828ec1a0c817" in namespace "emptydir-3070" to be "Succeeded or Failed"
Jan 18 21:37:17.095: INFO: Pod "pod-fab818f7-88e8-4601-8d01-828ec1a0c817": Phase="Pending", Reason="", readiness=false. Elapsed: 6.722445ms
Jan 18 21:37:19.112: INFO: Pod "pod-fab818f7-88e8-4601-8d01-828ec1a0c817": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022917191s
Jan 18 21:37:21.101: INFO: Pod "pod-fab818f7-88e8-4601-8d01-828ec1a0c817": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012628065s
STEP: Saw pod success 01/18/23 21:37:21.101
Jan 18 21:37:21.102: INFO: Pod "pod-fab818f7-88e8-4601-8d01-828ec1a0c817" satisfied condition "Succeeded or Failed"
Jan 18 21:37:21.106: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-fab818f7-88e8-4601-8d01-828ec1a0c817 container test-container: <nil>
STEP: delete the pod 01/18/23 21:37:21.115
Jan 18 21:37:21.129: INFO: Waiting for pod pod-fab818f7-88e8-4601-8d01-828ec1a0c817 to disappear
Jan 18 21:37:21.133: INFO: Pod pod-fab818f7-88e8-4601-8d01-828ec1a0c817 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 21:37:21.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3070" for this suite. 01/18/23 21:37:21.138
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":26,"skipped":553,"failed":0}
------------------------------
• [4.351 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:37:16.796
    Jan 18 21:37:16.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename emptydir 01/18/23 21:37:16.798
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:37:17.059
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:37:17.071
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 01/18/23 21:37:17.077
    Jan 18 21:37:17.088: INFO: Waiting up to 5m0s for pod "pod-fab818f7-88e8-4601-8d01-828ec1a0c817" in namespace "emptydir-3070" to be "Succeeded or Failed"
    Jan 18 21:37:17.095: INFO: Pod "pod-fab818f7-88e8-4601-8d01-828ec1a0c817": Phase="Pending", Reason="", readiness=false. Elapsed: 6.722445ms
    Jan 18 21:37:19.112: INFO: Pod "pod-fab818f7-88e8-4601-8d01-828ec1a0c817": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022917191s
    Jan 18 21:37:21.101: INFO: Pod "pod-fab818f7-88e8-4601-8d01-828ec1a0c817": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012628065s
    STEP: Saw pod success 01/18/23 21:37:21.101
    Jan 18 21:37:21.102: INFO: Pod "pod-fab818f7-88e8-4601-8d01-828ec1a0c817" satisfied condition "Succeeded or Failed"
    Jan 18 21:37:21.106: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-fab818f7-88e8-4601-8d01-828ec1a0c817 container test-container: <nil>
    STEP: delete the pod 01/18/23 21:37:21.115
    Jan 18 21:37:21.129: INFO: Waiting for pod pod-fab818f7-88e8-4601-8d01-828ec1a0c817 to disappear
    Jan 18 21:37:21.133: INFO: Pod pod-fab818f7-88e8-4601-8d01-828ec1a0c817 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 21:37:21.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3070" for this suite. 01/18/23 21:37:21.138
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:37:21.156
Jan 18 21:37:21.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename kubectl 01/18/23 21:37:21.157
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:37:21.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:37:21.192
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 01/18/23 21:37:21.198
Jan 18 21:37:21.199: INFO: namespace kubectl-4343
Jan 18 21:37:21.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4343 create -f -'
Jan 18 21:37:22.865: INFO: stderr: ""
Jan 18 21:37:22.865: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/18/23 21:37:22.865
Jan 18 21:37:23.872: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 21:37:23.872: INFO: Found 0 / 1
Jan 18 21:37:24.871: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 21:37:24.872: INFO: Found 1 / 1
Jan 18 21:37:24.872: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 18 21:37:24.886: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 21:37:24.886: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 18 21:37:24.886: INFO: wait on agnhost-primary startup in kubectl-4343 
Jan 18 21:37:24.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4343 logs agnhost-primary-m8h6w agnhost-primary'
Jan 18 21:37:25.009: INFO: stderr: ""
Jan 18 21:37:25.009: INFO: stdout: "Paused\n"
STEP: exposing RC 01/18/23 21:37:25.009
Jan 18 21:37:25.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4343 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan 18 21:37:25.151: INFO: stderr: ""
Jan 18 21:37:25.151: INFO: stdout: "service/rm2 exposed\n"
Jan 18 21:37:25.161: INFO: Service rm2 in namespace kubectl-4343 found.
STEP: exposing service 01/18/23 21:37:27.173
Jan 18 21:37:27.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4343 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan 18 21:37:27.302: INFO: stderr: ""
Jan 18 21:37:27.303: INFO: stdout: "service/rm3 exposed\n"
Jan 18 21:37:27.306: INFO: Service rm3 in namespace kubectl-4343 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 21:37:29.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4343" for this suite. 01/18/23 21:37:29.329
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":27,"skipped":600,"failed":0}
------------------------------
• [SLOW TEST] [8.180 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:37:21.156
    Jan 18 21:37:21.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename kubectl 01/18/23 21:37:21.157
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:37:21.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:37:21.192
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 01/18/23 21:37:21.198
    Jan 18 21:37:21.199: INFO: namespace kubectl-4343
    Jan 18 21:37:21.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4343 create -f -'
    Jan 18 21:37:22.865: INFO: stderr: ""
    Jan 18 21:37:22.865: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/18/23 21:37:22.865
    Jan 18 21:37:23.872: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 21:37:23.872: INFO: Found 0 / 1
    Jan 18 21:37:24.871: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 21:37:24.872: INFO: Found 1 / 1
    Jan 18 21:37:24.872: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 18 21:37:24.886: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 21:37:24.886: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 18 21:37:24.886: INFO: wait on agnhost-primary startup in kubectl-4343 
    Jan 18 21:37:24.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4343 logs agnhost-primary-m8h6w agnhost-primary'
    Jan 18 21:37:25.009: INFO: stderr: ""
    Jan 18 21:37:25.009: INFO: stdout: "Paused\n"
    STEP: exposing RC 01/18/23 21:37:25.009
    Jan 18 21:37:25.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4343 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jan 18 21:37:25.151: INFO: stderr: ""
    Jan 18 21:37:25.151: INFO: stdout: "service/rm2 exposed\n"
    Jan 18 21:37:25.161: INFO: Service rm2 in namespace kubectl-4343 found.
    STEP: exposing service 01/18/23 21:37:27.173
    Jan 18 21:37:27.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4343 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jan 18 21:37:27.302: INFO: stderr: ""
    Jan 18 21:37:27.303: INFO: stdout: "service/rm3 exposed\n"
    Jan 18 21:37:27.306: INFO: Service rm3 in namespace kubectl-4343 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 21:37:29.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4343" for this suite. 01/18/23 21:37:29.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:37:29.341
Jan 18 21:37:29.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename resourcequota 01/18/23 21:37:29.342
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:37:29.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:37:29.369
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 01/18/23 21:37:29.374
STEP: Getting a ResourceQuota 01/18/23 21:37:29.38
STEP: Updating a ResourceQuota 01/18/23 21:37:29.384
STEP: Verifying a ResourceQuota was modified 01/18/23 21:37:29.402
STEP: Deleting a ResourceQuota 01/18/23 21:37:29.406
STEP: Verifying the deleted ResourceQuota 01/18/23 21:37:29.418
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 21:37:29.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2094" for this suite. 01/18/23 21:37:29.428
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":28,"skipped":621,"failed":0}
------------------------------
• [0.094 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:37:29.341
    Jan 18 21:37:29.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename resourcequota 01/18/23 21:37:29.342
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:37:29.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:37:29.369
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 01/18/23 21:37:29.374
    STEP: Getting a ResourceQuota 01/18/23 21:37:29.38
    STEP: Updating a ResourceQuota 01/18/23 21:37:29.384
    STEP: Verifying a ResourceQuota was modified 01/18/23 21:37:29.402
    STEP: Deleting a ResourceQuota 01/18/23 21:37:29.406
    STEP: Verifying the deleted ResourceQuota 01/18/23 21:37:29.418
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 21:37:29.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2094" for this suite. 01/18/23 21:37:29.428
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:37:29.444
Jan 18 21:37:29.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename csistoragecapacity 01/18/23 21:37:29.445
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:37:29.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:37:29.472
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 01/18/23 21:37:29.478
STEP: getting /apis/storage.k8s.io 01/18/23 21:37:29.482
STEP: getting /apis/storage.k8s.io/v1 01/18/23 21:37:29.484
STEP: creating 01/18/23 21:37:29.486
STEP: watching 01/18/23 21:37:29.509
Jan 18 21:37:29.510: INFO: starting watch
STEP: getting 01/18/23 21:37:29.525
STEP: listing in namespace 01/18/23 21:37:29.529
STEP: listing across namespaces 01/18/23 21:37:29.533
STEP: patching 01/18/23 21:37:29.537
STEP: updating 01/18/23 21:37:29.544
Jan 18 21:37:29.551: INFO: waiting for watch events with expected annotations in namespace
Jan 18 21:37:29.551: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 01/18/23 21:37:29.552
STEP: deleting a collection 01/18/23 21:37:29.566
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Jan 18 21:37:29.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-4234" for this suite. 01/18/23 21:37:29.591
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":29,"skipped":654,"failed":0}
------------------------------
• [0.158 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:37:29.444
    Jan 18 21:37:29.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename csistoragecapacity 01/18/23 21:37:29.445
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:37:29.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:37:29.472
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 01/18/23 21:37:29.478
    STEP: getting /apis/storage.k8s.io 01/18/23 21:37:29.482
    STEP: getting /apis/storage.k8s.io/v1 01/18/23 21:37:29.484
    STEP: creating 01/18/23 21:37:29.486
    STEP: watching 01/18/23 21:37:29.509
    Jan 18 21:37:29.510: INFO: starting watch
    STEP: getting 01/18/23 21:37:29.525
    STEP: listing in namespace 01/18/23 21:37:29.529
    STEP: listing across namespaces 01/18/23 21:37:29.533
    STEP: patching 01/18/23 21:37:29.537
    STEP: updating 01/18/23 21:37:29.544
    Jan 18 21:37:29.551: INFO: waiting for watch events with expected annotations in namespace
    Jan 18 21:37:29.551: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 01/18/23 21:37:29.552
    STEP: deleting a collection 01/18/23 21:37:29.566
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Jan 18 21:37:29.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-4234" for this suite. 01/18/23 21:37:29.591
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:37:29.603
Jan 18 21:37:29.604: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename cronjob 01/18/23 21:37:29.608
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:37:29.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:37:29.651
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 01/18/23 21:37:29.658
STEP: creating 01/18/23 21:37:29.659
STEP: getting 01/18/23 21:37:29.667
STEP: listing 01/18/23 21:37:29.671
STEP: watching 01/18/23 21:37:29.676
Jan 18 21:37:29.676: INFO: starting watch
STEP: cluster-wide listing 01/18/23 21:37:29.679
STEP: cluster-wide watching 01/18/23 21:37:29.684
Jan 18 21:37:29.684: INFO: starting watch
STEP: patching 01/18/23 21:37:29.686
STEP: updating 01/18/23 21:37:29.695
Jan 18 21:37:29.706: INFO: waiting for watch events with expected annotations
Jan 18 21:37:29.706: INFO: saw patched and updated annotations
STEP: patching /status 01/18/23 21:37:29.706
STEP: updating /status 01/18/23 21:37:29.717
STEP: get /status 01/18/23 21:37:29.727
STEP: deleting 01/18/23 21:37:29.732
STEP: deleting a collection 01/18/23 21:37:29.749
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 18 21:37:29.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5342" for this suite. 01/18/23 21:37:29.767
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":30,"skipped":676,"failed":0}
------------------------------
• [0.176 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:37:29.603
    Jan 18 21:37:29.604: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename cronjob 01/18/23 21:37:29.608
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:37:29.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:37:29.651
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 01/18/23 21:37:29.658
    STEP: creating 01/18/23 21:37:29.659
    STEP: getting 01/18/23 21:37:29.667
    STEP: listing 01/18/23 21:37:29.671
    STEP: watching 01/18/23 21:37:29.676
    Jan 18 21:37:29.676: INFO: starting watch
    STEP: cluster-wide listing 01/18/23 21:37:29.679
    STEP: cluster-wide watching 01/18/23 21:37:29.684
    Jan 18 21:37:29.684: INFO: starting watch
    STEP: patching 01/18/23 21:37:29.686
    STEP: updating 01/18/23 21:37:29.695
    Jan 18 21:37:29.706: INFO: waiting for watch events with expected annotations
    Jan 18 21:37:29.706: INFO: saw patched and updated annotations
    STEP: patching /status 01/18/23 21:37:29.706
    STEP: updating /status 01/18/23 21:37:29.717
    STEP: get /status 01/18/23 21:37:29.727
    STEP: deleting 01/18/23 21:37:29.732
    STEP: deleting a collection 01/18/23 21:37:29.749
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 18 21:37:29.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-5342" for this suite. 01/18/23 21:37:29.767
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:37:29.782
Jan 18 21:37:29.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 21:37:29.784
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:37:29.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:37:29.825
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-9fca7461-a671-4c75-8936-e9c967e9cef7 01/18/23 21:37:29.833
STEP: Creating a pod to test consume secrets 01/18/23 21:37:29.849
Jan 18 21:37:29.867: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-acf88192-9379-4a24-8414-0ac60182ba0c" in namespace "projected-6020" to be "Succeeded or Failed"
Jan 18 21:37:29.877: INFO: Pod "pod-projected-secrets-acf88192-9379-4a24-8414-0ac60182ba0c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.514021ms
Jan 18 21:37:31.883: INFO: Pod "pod-projected-secrets-acf88192-9379-4a24-8414-0ac60182ba0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016646923s
Jan 18 21:37:33.885: INFO: Pod "pod-projected-secrets-acf88192-9379-4a24-8414-0ac60182ba0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018493009s
STEP: Saw pod success 01/18/23 21:37:33.885
Jan 18 21:37:33.886: INFO: Pod "pod-projected-secrets-acf88192-9379-4a24-8414-0ac60182ba0c" satisfied condition "Succeeded or Failed"
Jan 18 21:37:33.891: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-secrets-acf88192-9379-4a24-8414-0ac60182ba0c container projected-secret-volume-test: <nil>
STEP: delete the pod 01/18/23 21:37:33.899
Jan 18 21:37:33.923: INFO: Waiting for pod pod-projected-secrets-acf88192-9379-4a24-8414-0ac60182ba0c to disappear
Jan 18 21:37:33.927: INFO: Pod pod-projected-secrets-acf88192-9379-4a24-8414-0ac60182ba0c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 18 21:37:33.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6020" for this suite. 01/18/23 21:37:33.933
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":31,"skipped":678,"failed":0}
------------------------------
• [4.159 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:37:29.782
    Jan 18 21:37:29.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 21:37:29.784
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:37:29.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:37:29.825
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-9fca7461-a671-4c75-8936-e9c967e9cef7 01/18/23 21:37:29.833
    STEP: Creating a pod to test consume secrets 01/18/23 21:37:29.849
    Jan 18 21:37:29.867: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-acf88192-9379-4a24-8414-0ac60182ba0c" in namespace "projected-6020" to be "Succeeded or Failed"
    Jan 18 21:37:29.877: INFO: Pod "pod-projected-secrets-acf88192-9379-4a24-8414-0ac60182ba0c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.514021ms
    Jan 18 21:37:31.883: INFO: Pod "pod-projected-secrets-acf88192-9379-4a24-8414-0ac60182ba0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016646923s
    Jan 18 21:37:33.885: INFO: Pod "pod-projected-secrets-acf88192-9379-4a24-8414-0ac60182ba0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018493009s
    STEP: Saw pod success 01/18/23 21:37:33.885
    Jan 18 21:37:33.886: INFO: Pod "pod-projected-secrets-acf88192-9379-4a24-8414-0ac60182ba0c" satisfied condition "Succeeded or Failed"
    Jan 18 21:37:33.891: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-secrets-acf88192-9379-4a24-8414-0ac60182ba0c container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 21:37:33.899
    Jan 18 21:37:33.923: INFO: Waiting for pod pod-projected-secrets-acf88192-9379-4a24-8414-0ac60182ba0c to disappear
    Jan 18 21:37:33.927: INFO: Pod pod-projected-secrets-acf88192-9379-4a24-8414-0ac60182ba0c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 18 21:37:33.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6020" for this suite. 01/18/23 21:37:33.933
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:37:33.943
Jan 18 21:37:33.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename emptydir 01/18/23 21:37:33.945
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:37:33.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:37:33.968
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/18/23 21:37:33.972
Jan 18 21:37:33.980: INFO: Waiting up to 5m0s for pod "pod-783b4fad-ec51-4046-a527-6a10c1b24aae" in namespace "emptydir-9565" to be "Succeeded or Failed"
Jan 18 21:37:33.985: INFO: Pod "pod-783b4fad-ec51-4046-a527-6a10c1b24aae": Phase="Pending", Reason="", readiness=false. Elapsed: 5.188173ms
Jan 18 21:37:35.991: INFO: Pod "pod-783b4fad-ec51-4046-a527-6a10c1b24aae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011340451s
Jan 18 21:37:37.991: INFO: Pod "pod-783b4fad-ec51-4046-a527-6a10c1b24aae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01095548s
STEP: Saw pod success 01/18/23 21:37:37.991
Jan 18 21:37:37.991: INFO: Pod "pod-783b4fad-ec51-4046-a527-6a10c1b24aae" satisfied condition "Succeeded or Failed"
Jan 18 21:37:38.007: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-783b4fad-ec51-4046-a527-6a10c1b24aae container test-container: <nil>
STEP: delete the pod 01/18/23 21:37:38.016
Jan 18 21:37:38.029: INFO: Waiting for pod pod-783b4fad-ec51-4046-a527-6a10c1b24aae to disappear
Jan 18 21:37:38.033: INFO: Pod pod-783b4fad-ec51-4046-a527-6a10c1b24aae no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 21:37:38.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9565" for this suite. 01/18/23 21:37:38.038
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":32,"skipped":679,"failed":0}
------------------------------
• [4.101 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:37:33.943
    Jan 18 21:37:33.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename emptydir 01/18/23 21:37:33.945
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:37:33.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:37:33.968
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/18/23 21:37:33.972
    Jan 18 21:37:33.980: INFO: Waiting up to 5m0s for pod "pod-783b4fad-ec51-4046-a527-6a10c1b24aae" in namespace "emptydir-9565" to be "Succeeded or Failed"
    Jan 18 21:37:33.985: INFO: Pod "pod-783b4fad-ec51-4046-a527-6a10c1b24aae": Phase="Pending", Reason="", readiness=false. Elapsed: 5.188173ms
    Jan 18 21:37:35.991: INFO: Pod "pod-783b4fad-ec51-4046-a527-6a10c1b24aae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011340451s
    Jan 18 21:37:37.991: INFO: Pod "pod-783b4fad-ec51-4046-a527-6a10c1b24aae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01095548s
    STEP: Saw pod success 01/18/23 21:37:37.991
    Jan 18 21:37:37.991: INFO: Pod "pod-783b4fad-ec51-4046-a527-6a10c1b24aae" satisfied condition "Succeeded or Failed"
    Jan 18 21:37:38.007: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-783b4fad-ec51-4046-a527-6a10c1b24aae container test-container: <nil>
    STEP: delete the pod 01/18/23 21:37:38.016
    Jan 18 21:37:38.029: INFO: Waiting for pod pod-783b4fad-ec51-4046-a527-6a10c1b24aae to disappear
    Jan 18 21:37:38.033: INFO: Pod pod-783b4fad-ec51-4046-a527-6a10c1b24aae no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 21:37:38.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9565" for this suite. 01/18/23 21:37:38.038
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:37:38.054
Jan 18 21:37:38.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename webhook 01/18/23 21:37:38.055
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:37:38.077
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:37:38.081
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 21:37:38.1
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:37:38.736
STEP: Deploying the webhook pod 01/18/23 21:37:38.745
STEP: Wait for the deployment to be ready 01/18/23 21:37:38.759
Jan 18 21:37:38.775: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 21:37:40.797
STEP: Verifying the service has paired with the endpoint 01/18/23 21:37:40.824
Jan 18 21:37:41.824: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/18/23 21:37:41.83
STEP: create a pod that should be updated by the webhook 01/18/23 21:37:41.853
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 21:37:41.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3767" for this suite. 01/18/23 21:37:41.9
STEP: Destroying namespace "webhook-3767-markers" for this suite. 01/18/23 21:37:41.91
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":33,"skipped":713,"failed":0}
------------------------------
• [4.068 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:37:38.054
    Jan 18 21:37:38.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename webhook 01/18/23 21:37:38.055
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:37:38.077
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:37:38.081
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 21:37:38.1
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:37:38.736
    STEP: Deploying the webhook pod 01/18/23 21:37:38.745
    STEP: Wait for the deployment to be ready 01/18/23 21:37:38.759
    Jan 18 21:37:38.775: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 21:37:40.797
    STEP: Verifying the service has paired with the endpoint 01/18/23 21:37:40.824
    Jan 18 21:37:41.824: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/18/23 21:37:41.83
    STEP: create a pod that should be updated by the webhook 01/18/23 21:37:41.853
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 21:37:41.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3767" for this suite. 01/18/23 21:37:41.9
    STEP: Destroying namespace "webhook-3767-markers" for this suite. 01/18/23 21:37:41.91
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:37:42.159
Jan 18 21:37:42.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename webhook 01/18/23 21:37:42.161
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:37:42.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:37:42.192
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 21:37:42.225
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:37:42.671
STEP: Deploying the webhook pod 01/18/23 21:37:42.682
STEP: Wait for the deployment to be ready 01/18/23 21:37:42.694
Jan 18 21:37:42.721: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 18 21:37:44.740: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 37, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 37, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 37, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 37, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/18/23 21:37:46.746
STEP: Verifying the service has paired with the endpoint 01/18/23 21:37:46.774
Jan 18 21:37:47.775: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 01/18/23 21:37:47.853
STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 21:37:47.902
STEP: Deleting the collection of validation webhooks 01/18/23 21:37:47.949
STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 21:37:48.011
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 21:37:48.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-728" for this suite. 01/18/23 21:37:48.028
STEP: Destroying namespace "webhook-728-markers" for this suite. 01/18/23 21:37:48.036
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":34,"skipped":801,"failed":0}
------------------------------
• [SLOW TEST] [5.968 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:37:42.159
    Jan 18 21:37:42.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename webhook 01/18/23 21:37:42.161
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:37:42.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:37:42.192
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 21:37:42.225
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:37:42.671
    STEP: Deploying the webhook pod 01/18/23 21:37:42.682
    STEP: Wait for the deployment to be ready 01/18/23 21:37:42.694
    Jan 18 21:37:42.721: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 18 21:37:44.740: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 37, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 37, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 37, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 37, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/18/23 21:37:46.746
    STEP: Verifying the service has paired with the endpoint 01/18/23 21:37:46.774
    Jan 18 21:37:47.775: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 01/18/23 21:37:47.853
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 21:37:47.902
    STEP: Deleting the collection of validation webhooks 01/18/23 21:37:47.949
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 21:37:48.011
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 21:37:48.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-728" for this suite. 01/18/23 21:37:48.028
    STEP: Destroying namespace "webhook-728-markers" for this suite. 01/18/23 21:37:48.036
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:37:48.158
Jan 18 21:37:48.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename emptydir 01/18/23 21:37:48.162
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:37:48.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:37:48.199
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 01/18/23 21:37:48.215
Jan 18 21:37:48.242: INFO: Waiting up to 5m0s for pod "pod-f816280a-e20a-4c4b-b120-776aaaf89ae4" in namespace "emptydir-4590" to be "Succeeded or Failed"
Jan 18 21:37:48.259: INFO: Pod "pod-f816280a-e20a-4c4b-b120-776aaaf89ae4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.925465ms
Jan 18 21:37:50.266: INFO: Pod "pod-f816280a-e20a-4c4b-b120-776aaaf89ae4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023353884s
Jan 18 21:37:52.265: INFO: Pod "pod-f816280a-e20a-4c4b-b120-776aaaf89ae4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02302814s
STEP: Saw pod success 01/18/23 21:37:52.266
Jan 18 21:37:52.266: INFO: Pod "pod-f816280a-e20a-4c4b-b120-776aaaf89ae4" satisfied condition "Succeeded or Failed"
Jan 18 21:37:52.271: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-f816280a-e20a-4c4b-b120-776aaaf89ae4 container test-container: <nil>
STEP: delete the pod 01/18/23 21:37:52.278
Jan 18 21:37:52.290: INFO: Waiting for pod pod-f816280a-e20a-4c4b-b120-776aaaf89ae4 to disappear
Jan 18 21:37:52.294: INFO: Pod pod-f816280a-e20a-4c4b-b120-776aaaf89ae4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 21:37:52.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4590" for this suite. 01/18/23 21:37:52.3
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":35,"skipped":820,"failed":0}
------------------------------
• [4.148 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:37:48.158
    Jan 18 21:37:48.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename emptydir 01/18/23 21:37:48.162
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:37:48.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:37:48.199
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/18/23 21:37:48.215
    Jan 18 21:37:48.242: INFO: Waiting up to 5m0s for pod "pod-f816280a-e20a-4c4b-b120-776aaaf89ae4" in namespace "emptydir-4590" to be "Succeeded or Failed"
    Jan 18 21:37:48.259: INFO: Pod "pod-f816280a-e20a-4c4b-b120-776aaaf89ae4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.925465ms
    Jan 18 21:37:50.266: INFO: Pod "pod-f816280a-e20a-4c4b-b120-776aaaf89ae4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023353884s
    Jan 18 21:37:52.265: INFO: Pod "pod-f816280a-e20a-4c4b-b120-776aaaf89ae4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02302814s
    STEP: Saw pod success 01/18/23 21:37:52.266
    Jan 18 21:37:52.266: INFO: Pod "pod-f816280a-e20a-4c4b-b120-776aaaf89ae4" satisfied condition "Succeeded or Failed"
    Jan 18 21:37:52.271: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-f816280a-e20a-4c4b-b120-776aaaf89ae4 container test-container: <nil>
    STEP: delete the pod 01/18/23 21:37:52.278
    Jan 18 21:37:52.290: INFO: Waiting for pod pod-f816280a-e20a-4c4b-b120-776aaaf89ae4 to disappear
    Jan 18 21:37:52.294: INFO: Pod pod-f816280a-e20a-4c4b-b120-776aaaf89ae4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 21:37:52.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4590" for this suite. 01/18/23 21:37:52.3
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:37:52.317
Jan 18 21:37:52.317: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename daemonsets 01/18/23 21:37:52.319
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:37:52.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:37:52.344
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Jan 18 21:37:52.374: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 01/18/23 21:37:52.38
Jan 18 21:37:52.385: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:37:52.385: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 01/18/23 21:37:52.385
Jan 18 21:37:52.423: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:37:52.423: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 21:37:53.434: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:37:53.434: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 21:37:54.428: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 21:37:54.429: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 01/18/23 21:37:54.434
Jan 18 21:37:54.462: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 21:37:54.462: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jan 18 21:37:55.469: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:37:55.469: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/18/23 21:37:55.469
Jan 18 21:37:55.485: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:37:55.485: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 21:37:56.506: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:37:56.506: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 21:37:57.490: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:37:57.490: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 21:37:58.490: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:37:58.491: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 21:37:59.491: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 21:37:59.491: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/18/23 21:37:59.5
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7955, will wait for the garbage collector to delete the pods 01/18/23 21:37:59.5
Jan 18 21:37:59.563: INFO: Deleting DaemonSet.extensions daemon-set took: 8.09917ms
Jan 18 21:37:59.664: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.709124ms
Jan 18 21:38:02.169: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:38:02.169: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 21:38:02.172: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"137008"},"items":null}

Jan 18 21:38:02.176: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"137008"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 18 21:38:02.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7955" for this suite. 01/18/23 21:38:02.215
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":36,"skipped":838,"failed":0}
------------------------------
• [SLOW TEST] [9.915 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:37:52.317
    Jan 18 21:37:52.317: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename daemonsets 01/18/23 21:37:52.319
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:37:52.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:37:52.344
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Jan 18 21:37:52.374: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 01/18/23 21:37:52.38
    Jan 18 21:37:52.385: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:37:52.385: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 01/18/23 21:37:52.385
    Jan 18 21:37:52.423: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:37:52.423: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 21:37:53.434: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:37:53.434: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 21:37:54.428: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 21:37:54.429: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 01/18/23 21:37:54.434
    Jan 18 21:37:54.462: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 21:37:54.462: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Jan 18 21:37:55.469: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:37:55.469: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/18/23 21:37:55.469
    Jan 18 21:37:55.485: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:37:55.485: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 21:37:56.506: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:37:56.506: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 21:37:57.490: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:37:57.490: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 21:37:58.490: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:37:58.491: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 21:37:59.491: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 21:37:59.491: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 21:37:59.5
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7955, will wait for the garbage collector to delete the pods 01/18/23 21:37:59.5
    Jan 18 21:37:59.563: INFO: Deleting DaemonSet.extensions daemon-set took: 8.09917ms
    Jan 18 21:37:59.664: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.709124ms
    Jan 18 21:38:02.169: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:38:02.169: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 21:38:02.172: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"137008"},"items":null}

    Jan 18 21:38:02.176: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"137008"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 21:38:02.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7955" for this suite. 01/18/23 21:38:02.215
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:38:02.239
Jan 18 21:38:02.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename replication-controller 01/18/23 21:38:02.241
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:38:02.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:38:02.277
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 01/18/23 21:38:02.29
STEP: waiting for RC to be added 01/18/23 21:38:02.302
STEP: waiting for available Replicas 01/18/23 21:38:02.302
STEP: patching ReplicationController 01/18/23 21:38:03.866
STEP: waiting for RC to be modified 01/18/23 21:38:03.883
STEP: patching ReplicationController status 01/18/23 21:38:03.884
STEP: waiting for RC to be modified 01/18/23 21:38:03.891
STEP: waiting for available Replicas 01/18/23 21:38:03.892
STEP: fetching ReplicationController status 01/18/23 21:38:03.9
STEP: patching ReplicationController scale 01/18/23 21:38:03.905
STEP: waiting for RC to be modified 01/18/23 21:38:03.912
STEP: waiting for ReplicationController's scale to be the max amount 01/18/23 21:38:03.912
STEP: fetching ReplicationController; ensuring that it's patched 01/18/23 21:38:05.106
STEP: updating ReplicationController status 01/18/23 21:38:05.12
STEP: waiting for RC to be modified 01/18/23 21:38:05.13
STEP: listing all ReplicationControllers 01/18/23 21:38:05.131
STEP: checking that ReplicationController has expected values 01/18/23 21:38:05.141
STEP: deleting ReplicationControllers by collection 01/18/23 21:38:05.141
STEP: waiting for ReplicationController to have a DELETED watchEvent 01/18/23 21:38:05.16
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 18 21:38:05.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2518" for this suite. 01/18/23 21:38:05.246
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":37,"skipped":844,"failed":0}
------------------------------
• [3.019 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:38:02.239
    Jan 18 21:38:02.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename replication-controller 01/18/23 21:38:02.241
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:38:02.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:38:02.277
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 01/18/23 21:38:02.29
    STEP: waiting for RC to be added 01/18/23 21:38:02.302
    STEP: waiting for available Replicas 01/18/23 21:38:02.302
    STEP: patching ReplicationController 01/18/23 21:38:03.866
    STEP: waiting for RC to be modified 01/18/23 21:38:03.883
    STEP: patching ReplicationController status 01/18/23 21:38:03.884
    STEP: waiting for RC to be modified 01/18/23 21:38:03.891
    STEP: waiting for available Replicas 01/18/23 21:38:03.892
    STEP: fetching ReplicationController status 01/18/23 21:38:03.9
    STEP: patching ReplicationController scale 01/18/23 21:38:03.905
    STEP: waiting for RC to be modified 01/18/23 21:38:03.912
    STEP: waiting for ReplicationController's scale to be the max amount 01/18/23 21:38:03.912
    STEP: fetching ReplicationController; ensuring that it's patched 01/18/23 21:38:05.106
    STEP: updating ReplicationController status 01/18/23 21:38:05.12
    STEP: waiting for RC to be modified 01/18/23 21:38:05.13
    STEP: listing all ReplicationControllers 01/18/23 21:38:05.131
    STEP: checking that ReplicationController has expected values 01/18/23 21:38:05.141
    STEP: deleting ReplicationControllers by collection 01/18/23 21:38:05.141
    STEP: waiting for ReplicationController to have a DELETED watchEvent 01/18/23 21:38:05.16
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 18 21:38:05.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-2518" for this suite. 01/18/23 21:38:05.246
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:38:05.27
Jan 18 21:38:05.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename kubelet-test 01/18/23 21:38:05.272
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:38:05.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:38:05.312
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 18 21:38:05.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1361" for this suite. 01/18/23 21:38:05.37
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":38,"skipped":897,"failed":0}
------------------------------
• [0.114 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:38:05.27
    Jan 18 21:38:05.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename kubelet-test 01/18/23 21:38:05.272
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:38:05.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:38:05.312
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 18 21:38:05.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-1361" for this suite. 01/18/23 21:38:05.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:38:05.392
Jan 18 21:38:05.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 21:38:05.393
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:38:05.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:38:05.426
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 01/18/23 21:38:05.43
Jan 18 21:38:05.440: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d86806e-554c-4e84-8a6f-bebf840b9d2d" in namespace "projected-7950" to be "Succeeded or Failed"
Jan 18 21:38:05.476: INFO: Pod "downwardapi-volume-4d86806e-554c-4e84-8a6f-bebf840b9d2d": Phase="Pending", Reason="", readiness=false. Elapsed: 35.881297ms
Jan 18 21:38:07.482: INFO: Pod "downwardapi-volume-4d86806e-554c-4e84-8a6f-bebf840b9d2d": Phase="Running", Reason="", readiness=true. Elapsed: 2.041714653s
Jan 18 21:38:09.483: INFO: Pod "downwardapi-volume-4d86806e-554c-4e84-8a6f-bebf840b9d2d": Phase="Running", Reason="", readiness=false. Elapsed: 4.0428072s
Jan 18 21:38:11.482: INFO: Pod "downwardapi-volume-4d86806e-554c-4e84-8a6f-bebf840b9d2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041990059s
STEP: Saw pod success 01/18/23 21:38:11.482
Jan 18 21:38:11.482: INFO: Pod "downwardapi-volume-4d86806e-554c-4e84-8a6f-bebf840b9d2d" satisfied condition "Succeeded or Failed"
Jan 18 21:38:11.486: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-4d86806e-554c-4e84-8a6f-bebf840b9d2d container client-container: <nil>
STEP: delete the pod 01/18/23 21:38:11.494
Jan 18 21:38:11.536: INFO: Waiting for pod downwardapi-volume-4d86806e-554c-4e84-8a6f-bebf840b9d2d to disappear
Jan 18 21:38:11.557: INFO: Pod downwardapi-volume-4d86806e-554c-4e84-8a6f-bebf840b9d2d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 21:38:11.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7950" for this suite. 01/18/23 21:38:11.562
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":39,"skipped":926,"failed":0}
------------------------------
• [SLOW TEST] [6.183 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:38:05.392
    Jan 18 21:38:05.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 21:38:05.393
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:38:05.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:38:05.426
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 01/18/23 21:38:05.43
    Jan 18 21:38:05.440: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d86806e-554c-4e84-8a6f-bebf840b9d2d" in namespace "projected-7950" to be "Succeeded or Failed"
    Jan 18 21:38:05.476: INFO: Pod "downwardapi-volume-4d86806e-554c-4e84-8a6f-bebf840b9d2d": Phase="Pending", Reason="", readiness=false. Elapsed: 35.881297ms
    Jan 18 21:38:07.482: INFO: Pod "downwardapi-volume-4d86806e-554c-4e84-8a6f-bebf840b9d2d": Phase="Running", Reason="", readiness=true. Elapsed: 2.041714653s
    Jan 18 21:38:09.483: INFO: Pod "downwardapi-volume-4d86806e-554c-4e84-8a6f-bebf840b9d2d": Phase="Running", Reason="", readiness=false. Elapsed: 4.0428072s
    Jan 18 21:38:11.482: INFO: Pod "downwardapi-volume-4d86806e-554c-4e84-8a6f-bebf840b9d2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041990059s
    STEP: Saw pod success 01/18/23 21:38:11.482
    Jan 18 21:38:11.482: INFO: Pod "downwardapi-volume-4d86806e-554c-4e84-8a6f-bebf840b9d2d" satisfied condition "Succeeded or Failed"
    Jan 18 21:38:11.486: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-4d86806e-554c-4e84-8a6f-bebf840b9d2d container client-container: <nil>
    STEP: delete the pod 01/18/23 21:38:11.494
    Jan 18 21:38:11.536: INFO: Waiting for pod downwardapi-volume-4d86806e-554c-4e84-8a6f-bebf840b9d2d to disappear
    Jan 18 21:38:11.557: INFO: Pod downwardapi-volume-4d86806e-554c-4e84-8a6f-bebf840b9d2d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 21:38:11.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7950" for this suite. 01/18/23 21:38:11.562
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:38:11.588
Jan 18 21:38:11.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename emptydir-wrapper 01/18/23 21:38:11.59
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:38:11.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:38:11.611
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jan 18 21:38:11.635: INFO: Waiting up to 5m0s for pod "pod-secrets-bb8ded86-1ae2-4bfd-9196-de96f3e3a8a6" in namespace "emptydir-wrapper-8311" to be "running and ready"
Jan 18 21:38:11.648: INFO: Pod "pod-secrets-bb8ded86-1ae2-4bfd-9196-de96f3e3a8a6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.43865ms
Jan 18 21:38:11.648: INFO: The phase of Pod pod-secrets-bb8ded86-1ae2-4bfd-9196-de96f3e3a8a6 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:38:13.653: INFO: Pod "pod-secrets-bb8ded86-1ae2-4bfd-9196-de96f3e3a8a6": Phase="Running", Reason="", readiness=true. Elapsed: 2.017490108s
Jan 18 21:38:13.653: INFO: The phase of Pod pod-secrets-bb8ded86-1ae2-4bfd-9196-de96f3e3a8a6 is Running (Ready = true)
Jan 18 21:38:13.653: INFO: Pod "pod-secrets-bb8ded86-1ae2-4bfd-9196-de96f3e3a8a6" satisfied condition "running and ready"
STEP: Cleaning up the secret 01/18/23 21:38:13.657
STEP: Cleaning up the configmap 01/18/23 21:38:13.663
STEP: Cleaning up the pod 01/18/23 21:38:13.673
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jan 18 21:38:13.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8311" for this suite. 01/18/23 21:38:13.699
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":40,"skipped":957,"failed":0}
------------------------------
• [2.120 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:38:11.588
    Jan 18 21:38:11.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename emptydir-wrapper 01/18/23 21:38:11.59
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:38:11.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:38:11.611
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jan 18 21:38:11.635: INFO: Waiting up to 5m0s for pod "pod-secrets-bb8ded86-1ae2-4bfd-9196-de96f3e3a8a6" in namespace "emptydir-wrapper-8311" to be "running and ready"
    Jan 18 21:38:11.648: INFO: Pod "pod-secrets-bb8ded86-1ae2-4bfd-9196-de96f3e3a8a6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.43865ms
    Jan 18 21:38:11.648: INFO: The phase of Pod pod-secrets-bb8ded86-1ae2-4bfd-9196-de96f3e3a8a6 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:38:13.653: INFO: Pod "pod-secrets-bb8ded86-1ae2-4bfd-9196-de96f3e3a8a6": Phase="Running", Reason="", readiness=true. Elapsed: 2.017490108s
    Jan 18 21:38:13.653: INFO: The phase of Pod pod-secrets-bb8ded86-1ae2-4bfd-9196-de96f3e3a8a6 is Running (Ready = true)
    Jan 18 21:38:13.653: INFO: Pod "pod-secrets-bb8ded86-1ae2-4bfd-9196-de96f3e3a8a6" satisfied condition "running and ready"
    STEP: Cleaning up the secret 01/18/23 21:38:13.657
    STEP: Cleaning up the configmap 01/18/23 21:38:13.663
    STEP: Cleaning up the pod 01/18/23 21:38:13.673
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jan 18 21:38:13.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-8311" for this suite. 01/18/23 21:38:13.699
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:38:13.715
Jan 18 21:38:13.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename resourcequota 01/18/23 21:38:13.717
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:38:13.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:38:13.738
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 01/18/23 21:38:13.743
STEP: Creating a ResourceQuota 01/18/23 21:38:18.748
STEP: Ensuring resource quota status is calculated 01/18/23 21:38:18.753
STEP: Creating a ReplicaSet 01/18/23 21:38:20.759
STEP: Ensuring resource quota status captures replicaset creation 01/18/23 21:38:20.778
STEP: Deleting a ReplicaSet 01/18/23 21:38:22.785
STEP: Ensuring resource quota status released usage 01/18/23 21:38:22.793
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 21:38:24.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5075" for this suite. 01/18/23 21:38:24.807
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":41,"skipped":961,"failed":0}
------------------------------
• [SLOW TEST] [11.102 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:38:13.715
    Jan 18 21:38:13.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename resourcequota 01/18/23 21:38:13.717
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:38:13.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:38:13.738
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 01/18/23 21:38:13.743
    STEP: Creating a ResourceQuota 01/18/23 21:38:18.748
    STEP: Ensuring resource quota status is calculated 01/18/23 21:38:18.753
    STEP: Creating a ReplicaSet 01/18/23 21:38:20.759
    STEP: Ensuring resource quota status captures replicaset creation 01/18/23 21:38:20.778
    STEP: Deleting a ReplicaSet 01/18/23 21:38:22.785
    STEP: Ensuring resource quota status released usage 01/18/23 21:38:22.793
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 21:38:24.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5075" for this suite. 01/18/23 21:38:24.807
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:38:24.839
Jan 18 21:38:24.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename kubelet-test 01/18/23 21:38:24.841
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:38:24.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:38:24.89
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 01/18/23 21:38:24.91
Jan 18 21:38:24.911: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases90eed025-f335-463e-ab82-c0c099a721d7" in namespace "kubelet-test-2916" to be "completed"
Jan 18 21:38:24.920: INFO: Pod "agnhost-host-aliases90eed025-f335-463e-ab82-c0c099a721d7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.215699ms
Jan 18 21:38:26.926: INFO: Pod "agnhost-host-aliases90eed025-f335-463e-ab82-c0c099a721d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015033078s
Jan 18 21:38:28.927: INFO: Pod "agnhost-host-aliases90eed025-f335-463e-ab82-c0c099a721d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016400875s
Jan 18 21:38:30.926: INFO: Pod "agnhost-host-aliases90eed025-f335-463e-ab82-c0c099a721d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015663438s
Jan 18 21:38:30.927: INFO: Pod "agnhost-host-aliases90eed025-f335-463e-ab82-c0c099a721d7" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 18 21:38:30.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2916" for this suite. 01/18/23 21:38:30.949
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":42,"skipped":1032,"failed":0}
------------------------------
• [SLOW TEST] [6.118 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:38:24.839
    Jan 18 21:38:24.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename kubelet-test 01/18/23 21:38:24.841
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:38:24.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:38:24.89
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 01/18/23 21:38:24.91
    Jan 18 21:38:24.911: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases90eed025-f335-463e-ab82-c0c099a721d7" in namespace "kubelet-test-2916" to be "completed"
    Jan 18 21:38:24.920: INFO: Pod "agnhost-host-aliases90eed025-f335-463e-ab82-c0c099a721d7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.215699ms
    Jan 18 21:38:26.926: INFO: Pod "agnhost-host-aliases90eed025-f335-463e-ab82-c0c099a721d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015033078s
    Jan 18 21:38:28.927: INFO: Pod "agnhost-host-aliases90eed025-f335-463e-ab82-c0c099a721d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016400875s
    Jan 18 21:38:30.926: INFO: Pod "agnhost-host-aliases90eed025-f335-463e-ab82-c0c099a721d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015663438s
    Jan 18 21:38:30.927: INFO: Pod "agnhost-host-aliases90eed025-f335-463e-ab82-c0c099a721d7" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 18 21:38:30.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-2916" for this suite. 01/18/23 21:38:30.949
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:38:30.961
Jan 18 21:38:30.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename statefulset 01/18/23 21:38:30.963
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:38:30.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:38:31.008
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2951 01/18/23 21:38:31.016
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 01/18/23 21:38:31.025
STEP: Creating pod with conflicting port in namespace statefulset-2951 01/18/23 21:38:31.042
STEP: Waiting until pod test-pod will start running in namespace statefulset-2951 01/18/23 21:38:31.062
Jan 18 21:38:31.062: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-2951" to be "running"
Jan 18 21:38:31.072: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.992798ms
Jan 18 21:38:33.078: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016155976s
Jan 18 21:38:33.079: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-2951 01/18/23 21:38:33.079
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2951 01/18/23 21:38:33.092
Jan 18 21:38:33.124: INFO: Observed stateful pod in namespace: statefulset-2951, name: ss-0, uid: bf0be61c-2bf3-49a3-94b3-83bd3f25bf86, status phase: Pending. Waiting for statefulset controller to delete.
Jan 18 21:38:33.157: INFO: Observed stateful pod in namespace: statefulset-2951, name: ss-0, uid: bf0be61c-2bf3-49a3-94b3-83bd3f25bf86, status phase: Failed. Waiting for statefulset controller to delete.
Jan 18 21:38:33.169: INFO: Observed stateful pod in namespace: statefulset-2951, name: ss-0, uid: bf0be61c-2bf3-49a3-94b3-83bd3f25bf86, status phase: Failed. Waiting for statefulset controller to delete.
Jan 18 21:38:33.175: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2951
STEP: Removing pod with conflicting port in namespace statefulset-2951 01/18/23 21:38:33.175
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2951 and will be in running state 01/18/23 21:38:33.206
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 21:38:35.255: INFO: Deleting all statefulset in ns statefulset-2951
Jan 18 21:38:35.264: INFO: Scaling statefulset ss to 0
Jan 18 21:38:45.326: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 21:38:45.330: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 18 21:38:45.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2951" for this suite. 01/18/23 21:38:45.387
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":43,"skipped":1041,"failed":0}
------------------------------
• [SLOW TEST] [14.438 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:38:30.961
    Jan 18 21:38:30.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename statefulset 01/18/23 21:38:30.963
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:38:30.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:38:31.008
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2951 01/18/23 21:38:31.016
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 01/18/23 21:38:31.025
    STEP: Creating pod with conflicting port in namespace statefulset-2951 01/18/23 21:38:31.042
    STEP: Waiting until pod test-pod will start running in namespace statefulset-2951 01/18/23 21:38:31.062
    Jan 18 21:38:31.062: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-2951" to be "running"
    Jan 18 21:38:31.072: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.992798ms
    Jan 18 21:38:33.078: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016155976s
    Jan 18 21:38:33.079: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-2951 01/18/23 21:38:33.079
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2951 01/18/23 21:38:33.092
    Jan 18 21:38:33.124: INFO: Observed stateful pod in namespace: statefulset-2951, name: ss-0, uid: bf0be61c-2bf3-49a3-94b3-83bd3f25bf86, status phase: Pending. Waiting for statefulset controller to delete.
    Jan 18 21:38:33.157: INFO: Observed stateful pod in namespace: statefulset-2951, name: ss-0, uid: bf0be61c-2bf3-49a3-94b3-83bd3f25bf86, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 18 21:38:33.169: INFO: Observed stateful pod in namespace: statefulset-2951, name: ss-0, uid: bf0be61c-2bf3-49a3-94b3-83bd3f25bf86, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 18 21:38:33.175: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2951
    STEP: Removing pod with conflicting port in namespace statefulset-2951 01/18/23 21:38:33.175
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2951 and will be in running state 01/18/23 21:38:33.206
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 18 21:38:35.255: INFO: Deleting all statefulset in ns statefulset-2951
    Jan 18 21:38:35.264: INFO: Scaling statefulset ss to 0
    Jan 18 21:38:45.326: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 21:38:45.330: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 18 21:38:45.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2951" for this suite. 01/18/23 21:38:45.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:38:45.413
Jan 18 21:38:45.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename replication-controller 01/18/23 21:38:45.416
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:38:45.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:38:45.454
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-deb3e5d1-b68f-4669-8507-9e6233d2c40f 01/18/23 21:38:45.46
Jan 18 21:38:45.472: INFO: Pod name my-hostname-basic-deb3e5d1-b68f-4669-8507-9e6233d2c40f: Found 0 pods out of 1
Jan 18 21:38:50.491: INFO: Pod name my-hostname-basic-deb3e5d1-b68f-4669-8507-9e6233d2c40f: Found 1 pods out of 1
Jan 18 21:38:50.491: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-deb3e5d1-b68f-4669-8507-9e6233d2c40f" are running
Jan 18 21:38:50.491: INFO: Waiting up to 5m0s for pod "my-hostname-basic-deb3e5d1-b68f-4669-8507-9e6233d2c40f-z9z87" in namespace "replication-controller-1234" to be "running"
Jan 18 21:38:50.515: INFO: Pod "my-hostname-basic-deb3e5d1-b68f-4669-8507-9e6233d2c40f-z9z87": Phase="Running", Reason="", readiness=true. Elapsed: 23.434559ms
Jan 18 21:38:50.515: INFO: Pod "my-hostname-basic-deb3e5d1-b68f-4669-8507-9e6233d2c40f-z9z87" satisfied condition "running"
Jan 18 21:38:50.515: INFO: Pod "my-hostname-basic-deb3e5d1-b68f-4669-8507-9e6233d2c40f-z9z87" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 21:38:45 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 21:38:47 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 21:38:47 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 21:38:45 +0000 UTC Reason: Message:}])
Jan 18 21:38:50.515: INFO: Trying to dial the pod
Jan 18 21:38:55.534: INFO: Controller my-hostname-basic-deb3e5d1-b68f-4669-8507-9e6233d2c40f: Got expected result from replica 1 [my-hostname-basic-deb3e5d1-b68f-4669-8507-9e6233d2c40f-z9z87]: "my-hostname-basic-deb3e5d1-b68f-4669-8507-9e6233d2c40f-z9z87", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 18 21:38:55.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1234" for this suite. 01/18/23 21:38:55.542
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":44,"skipped":1081,"failed":0}
------------------------------
• [SLOW TEST] [10.138 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:38:45.413
    Jan 18 21:38:45.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename replication-controller 01/18/23 21:38:45.416
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:38:45.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:38:45.454
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-deb3e5d1-b68f-4669-8507-9e6233d2c40f 01/18/23 21:38:45.46
    Jan 18 21:38:45.472: INFO: Pod name my-hostname-basic-deb3e5d1-b68f-4669-8507-9e6233d2c40f: Found 0 pods out of 1
    Jan 18 21:38:50.491: INFO: Pod name my-hostname-basic-deb3e5d1-b68f-4669-8507-9e6233d2c40f: Found 1 pods out of 1
    Jan 18 21:38:50.491: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-deb3e5d1-b68f-4669-8507-9e6233d2c40f" are running
    Jan 18 21:38:50.491: INFO: Waiting up to 5m0s for pod "my-hostname-basic-deb3e5d1-b68f-4669-8507-9e6233d2c40f-z9z87" in namespace "replication-controller-1234" to be "running"
    Jan 18 21:38:50.515: INFO: Pod "my-hostname-basic-deb3e5d1-b68f-4669-8507-9e6233d2c40f-z9z87": Phase="Running", Reason="", readiness=true. Elapsed: 23.434559ms
    Jan 18 21:38:50.515: INFO: Pod "my-hostname-basic-deb3e5d1-b68f-4669-8507-9e6233d2c40f-z9z87" satisfied condition "running"
    Jan 18 21:38:50.515: INFO: Pod "my-hostname-basic-deb3e5d1-b68f-4669-8507-9e6233d2c40f-z9z87" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 21:38:45 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 21:38:47 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 21:38:47 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 21:38:45 +0000 UTC Reason: Message:}])
    Jan 18 21:38:50.515: INFO: Trying to dial the pod
    Jan 18 21:38:55.534: INFO: Controller my-hostname-basic-deb3e5d1-b68f-4669-8507-9e6233d2c40f: Got expected result from replica 1 [my-hostname-basic-deb3e5d1-b68f-4669-8507-9e6233d2c40f-z9z87]: "my-hostname-basic-deb3e5d1-b68f-4669-8507-9e6233d2c40f-z9z87", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 18 21:38:55.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-1234" for this suite. 01/18/23 21:38:55.542
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:38:55.554
Jan 18 21:38:55.554: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 21:38:55.555
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:38:55.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:38:55.591
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 01/18/23 21:38:55.596
Jan 18 21:38:55.627: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9f6ae5e4-145e-4851-8ef5-7b6e7f4a3291" in namespace "projected-3111" to be "Succeeded or Failed"
Jan 18 21:38:55.639: INFO: Pod "downwardapi-volume-9f6ae5e4-145e-4851-8ef5-7b6e7f4a3291": Phase="Pending", Reason="", readiness=false. Elapsed: 11.239697ms
Jan 18 21:38:57.646: INFO: Pod "downwardapi-volume-9f6ae5e4-145e-4851-8ef5-7b6e7f4a3291": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018144487s
Jan 18 21:38:59.645: INFO: Pod "downwardapi-volume-9f6ae5e4-145e-4851-8ef5-7b6e7f4a3291": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01793094s
STEP: Saw pod success 01/18/23 21:38:59.646
Jan 18 21:38:59.647: INFO: Pod "downwardapi-volume-9f6ae5e4-145e-4851-8ef5-7b6e7f4a3291" satisfied condition "Succeeded or Failed"
Jan 18 21:38:59.652: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-9f6ae5e4-145e-4851-8ef5-7b6e7f4a3291 container client-container: <nil>
STEP: delete the pod 01/18/23 21:38:59.676
Jan 18 21:38:59.700: INFO: Waiting for pod downwardapi-volume-9f6ae5e4-145e-4851-8ef5-7b6e7f4a3291 to disappear
Jan 18 21:38:59.705: INFO: Pod downwardapi-volume-9f6ae5e4-145e-4851-8ef5-7b6e7f4a3291 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 21:38:59.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3111" for this suite. 01/18/23 21:38:59.711
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":45,"skipped":1081,"failed":0}
------------------------------
• [4.165 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:38:55.554
    Jan 18 21:38:55.554: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 21:38:55.555
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:38:55.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:38:55.591
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 01/18/23 21:38:55.596
    Jan 18 21:38:55.627: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9f6ae5e4-145e-4851-8ef5-7b6e7f4a3291" in namespace "projected-3111" to be "Succeeded or Failed"
    Jan 18 21:38:55.639: INFO: Pod "downwardapi-volume-9f6ae5e4-145e-4851-8ef5-7b6e7f4a3291": Phase="Pending", Reason="", readiness=false. Elapsed: 11.239697ms
    Jan 18 21:38:57.646: INFO: Pod "downwardapi-volume-9f6ae5e4-145e-4851-8ef5-7b6e7f4a3291": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018144487s
    Jan 18 21:38:59.645: INFO: Pod "downwardapi-volume-9f6ae5e4-145e-4851-8ef5-7b6e7f4a3291": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01793094s
    STEP: Saw pod success 01/18/23 21:38:59.646
    Jan 18 21:38:59.647: INFO: Pod "downwardapi-volume-9f6ae5e4-145e-4851-8ef5-7b6e7f4a3291" satisfied condition "Succeeded or Failed"
    Jan 18 21:38:59.652: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-9f6ae5e4-145e-4851-8ef5-7b6e7f4a3291 container client-container: <nil>
    STEP: delete the pod 01/18/23 21:38:59.676
    Jan 18 21:38:59.700: INFO: Waiting for pod downwardapi-volume-9f6ae5e4-145e-4851-8ef5-7b6e7f4a3291 to disappear
    Jan 18 21:38:59.705: INFO: Pod downwardapi-volume-9f6ae5e4-145e-4851-8ef5-7b6e7f4a3291 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 21:38:59.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3111" for this suite. 01/18/23 21:38:59.711
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:38:59.719
Jan 18 21:38:59.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 21:38:59.723
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:38:59.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:38:59.757
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Jan 18 21:38:59.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 21:39:05.098
Jan 18 21:39:05.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-8224 --namespace=crd-publish-openapi-8224 create -f -'
Jan 18 21:39:06.325: INFO: stderr: ""
Jan 18 21:39:06.325: INFO: stdout: "e2e-test-crd-publish-openapi-1914-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 18 21:39:06.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-8224 --namespace=crd-publish-openapi-8224 delete e2e-test-crd-publish-openapi-1914-crds test-cr'
Jan 18 21:39:06.431: INFO: stderr: ""
Jan 18 21:39:06.431: INFO: stdout: "e2e-test-crd-publish-openapi-1914-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 18 21:39:06.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-8224 --namespace=crd-publish-openapi-8224 apply -f -'
Jan 18 21:39:06.778: INFO: stderr: ""
Jan 18 21:39:06.778: INFO: stdout: "e2e-test-crd-publish-openapi-1914-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 18 21:39:06.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-8224 --namespace=crd-publish-openapi-8224 delete e2e-test-crd-publish-openapi-1914-crds test-cr'
Jan 18 21:39:06.889: INFO: stderr: ""
Jan 18 21:39:06.889: INFO: stdout: "e2e-test-crd-publish-openapi-1914-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 01/18/23 21:39:06.889
Jan 18 21:39:06.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-8224 explain e2e-test-crd-publish-openapi-1914-crds'
Jan 18 21:39:07.224: INFO: stderr: ""
Jan 18 21:39:07.224: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1914-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 21:39:12.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8224" for this suite. 01/18/23 21:39:12.454
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":46,"skipped":1084,"failed":0}
------------------------------
• [SLOW TEST] [12.742 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:38:59.719
    Jan 18 21:38:59.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 21:38:59.723
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:38:59.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:38:59.757
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Jan 18 21:38:59.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 21:39:05.098
    Jan 18 21:39:05.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-8224 --namespace=crd-publish-openapi-8224 create -f -'
    Jan 18 21:39:06.325: INFO: stderr: ""
    Jan 18 21:39:06.325: INFO: stdout: "e2e-test-crd-publish-openapi-1914-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 18 21:39:06.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-8224 --namespace=crd-publish-openapi-8224 delete e2e-test-crd-publish-openapi-1914-crds test-cr'
    Jan 18 21:39:06.431: INFO: stderr: ""
    Jan 18 21:39:06.431: INFO: stdout: "e2e-test-crd-publish-openapi-1914-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jan 18 21:39:06.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-8224 --namespace=crd-publish-openapi-8224 apply -f -'
    Jan 18 21:39:06.778: INFO: stderr: ""
    Jan 18 21:39:06.778: INFO: stdout: "e2e-test-crd-publish-openapi-1914-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 18 21:39:06.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-8224 --namespace=crd-publish-openapi-8224 delete e2e-test-crd-publish-openapi-1914-crds test-cr'
    Jan 18 21:39:06.889: INFO: stderr: ""
    Jan 18 21:39:06.889: INFO: stdout: "e2e-test-crd-publish-openapi-1914-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 01/18/23 21:39:06.889
    Jan 18 21:39:06.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-8224 explain e2e-test-crd-publish-openapi-1914-crds'
    Jan 18 21:39:07.224: INFO: stderr: ""
    Jan 18 21:39:07.224: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1914-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 21:39:12.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8224" for this suite. 01/18/23 21:39:12.454
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:39:12.466
Jan 18 21:39:12.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename watch 01/18/23 21:39:12.467
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:39:12.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:39:12.518
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 01/18/23 21:39:12.522
STEP: starting a background goroutine to produce watch events 01/18/23 21:39:12.526
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/18/23 21:39:12.526
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 18 21:39:15.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6775" for this suite. 01/18/23 21:39:15.323
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":47,"skipped":1103,"failed":0}
------------------------------
• [2.910 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:39:12.466
    Jan 18 21:39:12.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename watch 01/18/23 21:39:12.467
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:39:12.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:39:12.518
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 01/18/23 21:39:12.522
    STEP: starting a background goroutine to produce watch events 01/18/23 21:39:12.526
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/18/23 21:39:12.526
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 18 21:39:15.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-6775" for this suite. 01/18/23 21:39:15.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:39:15.384
Jan 18 21:39:15.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename downward-api 01/18/23 21:39:15.386
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:39:15.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:39:15.41
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 01/18/23 21:39:15.415
Jan 18 21:39:15.425: INFO: Waiting up to 5m0s for pod "downwardapi-volume-add8734a-176d-4ec6-a283-36c5f71fee0d" in namespace "downward-api-5281" to be "Succeeded or Failed"
Jan 18 21:39:15.429: INFO: Pod "downwardapi-volume-add8734a-176d-4ec6-a283-36c5f71fee0d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.524189ms
Jan 18 21:39:17.436: INFO: Pod "downwardapi-volume-add8734a-176d-4ec6-a283-36c5f71fee0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010254119s
Jan 18 21:39:19.444: INFO: Pod "downwardapi-volume-add8734a-176d-4ec6-a283-36c5f71fee0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018306864s
STEP: Saw pod success 01/18/23 21:39:19.444
Jan 18 21:39:19.445: INFO: Pod "downwardapi-volume-add8734a-176d-4ec6-a283-36c5f71fee0d" satisfied condition "Succeeded or Failed"
Jan 18 21:39:19.449: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-add8734a-176d-4ec6-a283-36c5f71fee0d container client-container: <nil>
STEP: delete the pod 01/18/23 21:39:19.457
Jan 18 21:39:19.470: INFO: Waiting for pod downwardapi-volume-add8734a-176d-4ec6-a283-36c5f71fee0d to disappear
Jan 18 21:39:19.474: INFO: Pod downwardapi-volume-add8734a-176d-4ec6-a283-36c5f71fee0d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 21:39:19.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5281" for this suite. 01/18/23 21:39:19.479
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":48,"skipped":1120,"failed":0}
------------------------------
• [4.100 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:39:15.384
    Jan 18 21:39:15.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename downward-api 01/18/23 21:39:15.386
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:39:15.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:39:15.41
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 01/18/23 21:39:15.415
    Jan 18 21:39:15.425: INFO: Waiting up to 5m0s for pod "downwardapi-volume-add8734a-176d-4ec6-a283-36c5f71fee0d" in namespace "downward-api-5281" to be "Succeeded or Failed"
    Jan 18 21:39:15.429: INFO: Pod "downwardapi-volume-add8734a-176d-4ec6-a283-36c5f71fee0d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.524189ms
    Jan 18 21:39:17.436: INFO: Pod "downwardapi-volume-add8734a-176d-4ec6-a283-36c5f71fee0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010254119s
    Jan 18 21:39:19.444: INFO: Pod "downwardapi-volume-add8734a-176d-4ec6-a283-36c5f71fee0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018306864s
    STEP: Saw pod success 01/18/23 21:39:19.444
    Jan 18 21:39:19.445: INFO: Pod "downwardapi-volume-add8734a-176d-4ec6-a283-36c5f71fee0d" satisfied condition "Succeeded or Failed"
    Jan 18 21:39:19.449: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-add8734a-176d-4ec6-a283-36c5f71fee0d container client-container: <nil>
    STEP: delete the pod 01/18/23 21:39:19.457
    Jan 18 21:39:19.470: INFO: Waiting for pod downwardapi-volume-add8734a-176d-4ec6-a283-36c5f71fee0d to disappear
    Jan 18 21:39:19.474: INFO: Pod downwardapi-volume-add8734a-176d-4ec6-a283-36c5f71fee0d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 21:39:19.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5281" for this suite. 01/18/23 21:39:19.479
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:39:19.485
Jan 18 21:39:19.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename daemonsets 01/18/23 21:39:19.486
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:39:19.51
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:39:19.517
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Jan 18 21:39:19.551: INFO: Create a RollingUpdate DaemonSet
Jan 18 21:39:19.561: INFO: Check that daemon pods launch on every node of the cluster
Jan 18 21:39:19.568: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:39:19.582: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:39:19.582: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 21:39:20.716: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:39:20.724: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:39:20.724: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 21:39:21.592: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:39:21.597: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 21:39:21.597: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Jan 18 21:39:21.597: INFO: Update the DaemonSet to trigger a rollout
Jan 18 21:39:21.611: INFO: Updating DaemonSet daemon-set
Jan 18 21:39:24.647: INFO: Roll back the DaemonSet before rollout is complete
Jan 18 21:39:24.667: INFO: Updating DaemonSet daemon-set
Jan 18 21:39:24.668: INFO: Make sure DaemonSet rollback is complete
Jan 18 21:39:24.677: INFO: Wrong image for pod: daemon-set-86d5f. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Jan 18 21:39:24.677: INFO: Pod daemon-set-86d5f is not available
Jan 18 21:39:24.687: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:39:25.700: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:39:26.700: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:39:27.705: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:39:28.694: INFO: Pod daemon-set-s9lsn is not available
Jan 18 21:39:28.699: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/18/23 21:39:28.707
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8282, will wait for the garbage collector to delete the pods 01/18/23 21:39:28.708
Jan 18 21:39:28.770: INFO: Deleting DaemonSet.extensions daemon-set took: 7.229888ms
Jan 18 21:39:28.871: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.987832ms
Jan 18 21:39:31.475: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:39:31.475: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 21:39:31.479: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"138009"},"items":null}

Jan 18 21:39:31.483: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"138009"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 18 21:39:31.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8282" for this suite. 01/18/23 21:39:31.504
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":49,"skipped":1123,"failed":0}
------------------------------
• [SLOW TEST] [12.032 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:39:19.485
    Jan 18 21:39:19.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename daemonsets 01/18/23 21:39:19.486
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:39:19.51
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:39:19.517
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Jan 18 21:39:19.551: INFO: Create a RollingUpdate DaemonSet
    Jan 18 21:39:19.561: INFO: Check that daemon pods launch on every node of the cluster
    Jan 18 21:39:19.568: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:39:19.582: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:39:19.582: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 21:39:20.716: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:39:20.724: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:39:20.724: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 21:39:21.592: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:39:21.597: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 21:39:21.597: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Jan 18 21:39:21.597: INFO: Update the DaemonSet to trigger a rollout
    Jan 18 21:39:21.611: INFO: Updating DaemonSet daemon-set
    Jan 18 21:39:24.647: INFO: Roll back the DaemonSet before rollout is complete
    Jan 18 21:39:24.667: INFO: Updating DaemonSet daemon-set
    Jan 18 21:39:24.668: INFO: Make sure DaemonSet rollback is complete
    Jan 18 21:39:24.677: INFO: Wrong image for pod: daemon-set-86d5f. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Jan 18 21:39:24.677: INFO: Pod daemon-set-86d5f is not available
    Jan 18 21:39:24.687: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:39:25.700: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:39:26.700: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:39:27.705: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:39:28.694: INFO: Pod daemon-set-s9lsn is not available
    Jan 18 21:39:28.699: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 21:39:28.707
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8282, will wait for the garbage collector to delete the pods 01/18/23 21:39:28.708
    Jan 18 21:39:28.770: INFO: Deleting DaemonSet.extensions daemon-set took: 7.229888ms
    Jan 18 21:39:28.871: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.987832ms
    Jan 18 21:39:31.475: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:39:31.475: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 21:39:31.479: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"138009"},"items":null}

    Jan 18 21:39:31.483: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"138009"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 21:39:31.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-8282" for this suite. 01/18/23 21:39:31.504
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:39:31.523
Jan 18 21:39:31.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 21:39:31.525
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:39:31.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:39:31.775
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-cd8cd9eb-32d6-49b1-a0dc-5211e3da0be8 01/18/23 21:39:31.781
STEP: Creating a pod to test consume configMaps 01/18/23 21:39:31.787
Jan 18 21:39:31.799: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3797fd0d-d540-49d3-90ee-4e854987d5b5" in namespace "projected-6812" to be "Succeeded or Failed"
Jan 18 21:39:31.807: INFO: Pod "pod-projected-configmaps-3797fd0d-d540-49d3-90ee-4e854987d5b5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.710381ms
Jan 18 21:39:33.815: INFO: Pod "pod-projected-configmaps-3797fd0d-d540-49d3-90ee-4e854987d5b5": Phase="Running", Reason="", readiness=true. Elapsed: 2.016073035s
Jan 18 21:39:35.816: INFO: Pod "pod-projected-configmaps-3797fd0d-d540-49d3-90ee-4e854987d5b5": Phase="Running", Reason="", readiness=false. Elapsed: 4.017372178s
Jan 18 21:39:37.814: INFO: Pod "pod-projected-configmaps-3797fd0d-d540-49d3-90ee-4e854987d5b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015726705s
STEP: Saw pod success 01/18/23 21:39:37.815
Jan 18 21:39:37.815: INFO: Pod "pod-projected-configmaps-3797fd0d-d540-49d3-90ee-4e854987d5b5" satisfied condition "Succeeded or Failed"
Jan 18 21:39:37.820: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-3797fd0d-d540-49d3-90ee-4e854987d5b5 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 21:39:37.827
Jan 18 21:39:37.841: INFO: Waiting for pod pod-projected-configmaps-3797fd0d-d540-49d3-90ee-4e854987d5b5 to disappear
Jan 18 21:39:37.844: INFO: Pod pod-projected-configmaps-3797fd0d-d540-49d3-90ee-4e854987d5b5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 18 21:39:37.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6812" for this suite. 01/18/23 21:39:37.849
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":50,"skipped":1134,"failed":0}
------------------------------
• [SLOW TEST] [6.332 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:39:31.523
    Jan 18 21:39:31.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 21:39:31.525
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:39:31.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:39:31.775
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-cd8cd9eb-32d6-49b1-a0dc-5211e3da0be8 01/18/23 21:39:31.781
    STEP: Creating a pod to test consume configMaps 01/18/23 21:39:31.787
    Jan 18 21:39:31.799: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3797fd0d-d540-49d3-90ee-4e854987d5b5" in namespace "projected-6812" to be "Succeeded or Failed"
    Jan 18 21:39:31.807: INFO: Pod "pod-projected-configmaps-3797fd0d-d540-49d3-90ee-4e854987d5b5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.710381ms
    Jan 18 21:39:33.815: INFO: Pod "pod-projected-configmaps-3797fd0d-d540-49d3-90ee-4e854987d5b5": Phase="Running", Reason="", readiness=true. Elapsed: 2.016073035s
    Jan 18 21:39:35.816: INFO: Pod "pod-projected-configmaps-3797fd0d-d540-49d3-90ee-4e854987d5b5": Phase="Running", Reason="", readiness=false. Elapsed: 4.017372178s
    Jan 18 21:39:37.814: INFO: Pod "pod-projected-configmaps-3797fd0d-d540-49d3-90ee-4e854987d5b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015726705s
    STEP: Saw pod success 01/18/23 21:39:37.815
    Jan 18 21:39:37.815: INFO: Pod "pod-projected-configmaps-3797fd0d-d540-49d3-90ee-4e854987d5b5" satisfied condition "Succeeded or Failed"
    Jan 18 21:39:37.820: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-3797fd0d-d540-49d3-90ee-4e854987d5b5 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 21:39:37.827
    Jan 18 21:39:37.841: INFO: Waiting for pod pod-projected-configmaps-3797fd0d-d540-49d3-90ee-4e854987d5b5 to disappear
    Jan 18 21:39:37.844: INFO: Pod pod-projected-configmaps-3797fd0d-d540-49d3-90ee-4e854987d5b5 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 18 21:39:37.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6812" for this suite. 01/18/23 21:39:37.849
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:39:37.865
Jan 18 21:39:37.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename configmap 01/18/23 21:39:37.867
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:39:37.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:39:37.899
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-21f9d199-9032-40ea-80df-79274ff597bf 01/18/23 21:39:37.904
STEP: Creating a pod to test consume configMaps 01/18/23 21:39:37.912
Jan 18 21:39:37.921: INFO: Waiting up to 5m0s for pod "pod-configmaps-b84ce523-8409-4d6d-99c0-ffe80e3ed475" in namespace "configmap-1639" to be "Succeeded or Failed"
Jan 18 21:39:37.926: INFO: Pod "pod-configmaps-b84ce523-8409-4d6d-99c0-ffe80e3ed475": Phase="Pending", Reason="", readiness=false. Elapsed: 5.082333ms
Jan 18 21:39:39.933: INFO: Pod "pod-configmaps-b84ce523-8409-4d6d-99c0-ffe80e3ed475": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011740067s
Jan 18 21:39:41.933: INFO: Pod "pod-configmaps-b84ce523-8409-4d6d-99c0-ffe80e3ed475": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011675476s
STEP: Saw pod success 01/18/23 21:39:41.933
Jan 18 21:39:41.934: INFO: Pod "pod-configmaps-b84ce523-8409-4d6d-99c0-ffe80e3ed475" satisfied condition "Succeeded or Failed"
Jan 18 21:39:41.938: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-b84ce523-8409-4d6d-99c0-ffe80e3ed475 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 21:39:41.949
Jan 18 21:39:41.962: INFO: Waiting for pod pod-configmaps-b84ce523-8409-4d6d-99c0-ffe80e3ed475 to disappear
Jan 18 21:39:41.967: INFO: Pod pod-configmaps-b84ce523-8409-4d6d-99c0-ffe80e3ed475 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 21:39:41.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1639" for this suite. 01/18/23 21:39:41.975
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":51,"skipped":1185,"failed":0}
------------------------------
• [4.118 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:39:37.865
    Jan 18 21:39:37.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename configmap 01/18/23 21:39:37.867
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:39:37.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:39:37.899
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-21f9d199-9032-40ea-80df-79274ff597bf 01/18/23 21:39:37.904
    STEP: Creating a pod to test consume configMaps 01/18/23 21:39:37.912
    Jan 18 21:39:37.921: INFO: Waiting up to 5m0s for pod "pod-configmaps-b84ce523-8409-4d6d-99c0-ffe80e3ed475" in namespace "configmap-1639" to be "Succeeded or Failed"
    Jan 18 21:39:37.926: INFO: Pod "pod-configmaps-b84ce523-8409-4d6d-99c0-ffe80e3ed475": Phase="Pending", Reason="", readiness=false. Elapsed: 5.082333ms
    Jan 18 21:39:39.933: INFO: Pod "pod-configmaps-b84ce523-8409-4d6d-99c0-ffe80e3ed475": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011740067s
    Jan 18 21:39:41.933: INFO: Pod "pod-configmaps-b84ce523-8409-4d6d-99c0-ffe80e3ed475": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011675476s
    STEP: Saw pod success 01/18/23 21:39:41.933
    Jan 18 21:39:41.934: INFO: Pod "pod-configmaps-b84ce523-8409-4d6d-99c0-ffe80e3ed475" satisfied condition "Succeeded or Failed"
    Jan 18 21:39:41.938: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-b84ce523-8409-4d6d-99c0-ffe80e3ed475 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 21:39:41.949
    Jan 18 21:39:41.962: INFO: Waiting for pod pod-configmaps-b84ce523-8409-4d6d-99c0-ffe80e3ed475 to disappear
    Jan 18 21:39:41.967: INFO: Pod pod-configmaps-b84ce523-8409-4d6d-99c0-ffe80e3ed475 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 21:39:41.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1639" for this suite. 01/18/23 21:39:41.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:39:41.985
Jan 18 21:39:41.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 21:39:41.987
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:39:42.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:39:42.018
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-b4148a7e-3e7d-4146-aae2-0005f367eb6b 01/18/23 21:39:42.029
STEP: Creating the pod 01/18/23 21:39:42.035
Jan 18 21:39:42.048: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-718a7a4d-8ee3-4917-afc3-d352bdd63d84" in namespace "projected-6233" to be "running and ready"
Jan 18 21:39:42.240: INFO: Pod "pod-projected-configmaps-718a7a4d-8ee3-4917-afc3-d352bdd63d84": Phase="Pending", Reason="", readiness=false. Elapsed: 191.716723ms
Jan 18 21:39:42.240: INFO: The phase of Pod pod-projected-configmaps-718a7a4d-8ee3-4917-afc3-d352bdd63d84 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:39:44.249: INFO: Pod "pod-projected-configmaps-718a7a4d-8ee3-4917-afc3-d352bdd63d84": Phase="Running", Reason="", readiness=true. Elapsed: 2.200526071s
Jan 18 21:39:44.249: INFO: The phase of Pod pod-projected-configmaps-718a7a4d-8ee3-4917-afc3-d352bdd63d84 is Running (Ready = true)
Jan 18 21:39:44.249: INFO: Pod "pod-projected-configmaps-718a7a4d-8ee3-4917-afc3-d352bdd63d84" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-b4148a7e-3e7d-4146-aae2-0005f367eb6b 01/18/23 21:39:44.265
STEP: waiting to observe update in volume 01/18/23 21:39:44.27
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 18 21:39:46.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6233" for this suite. 01/18/23 21:39:46.292
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":52,"skipped":1205,"failed":0}
------------------------------
• [4.314 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:39:41.985
    Jan 18 21:39:41.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 21:39:41.987
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:39:42.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:39:42.018
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-b4148a7e-3e7d-4146-aae2-0005f367eb6b 01/18/23 21:39:42.029
    STEP: Creating the pod 01/18/23 21:39:42.035
    Jan 18 21:39:42.048: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-718a7a4d-8ee3-4917-afc3-d352bdd63d84" in namespace "projected-6233" to be "running and ready"
    Jan 18 21:39:42.240: INFO: Pod "pod-projected-configmaps-718a7a4d-8ee3-4917-afc3-d352bdd63d84": Phase="Pending", Reason="", readiness=false. Elapsed: 191.716723ms
    Jan 18 21:39:42.240: INFO: The phase of Pod pod-projected-configmaps-718a7a4d-8ee3-4917-afc3-d352bdd63d84 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:39:44.249: INFO: Pod "pod-projected-configmaps-718a7a4d-8ee3-4917-afc3-d352bdd63d84": Phase="Running", Reason="", readiness=true. Elapsed: 2.200526071s
    Jan 18 21:39:44.249: INFO: The phase of Pod pod-projected-configmaps-718a7a4d-8ee3-4917-afc3-d352bdd63d84 is Running (Ready = true)
    Jan 18 21:39:44.249: INFO: Pod "pod-projected-configmaps-718a7a4d-8ee3-4917-afc3-d352bdd63d84" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-b4148a7e-3e7d-4146-aae2-0005f367eb6b 01/18/23 21:39:44.265
    STEP: waiting to observe update in volume 01/18/23 21:39:44.27
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 18 21:39:46.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6233" for this suite. 01/18/23 21:39:46.292
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:39:46.303
Jan 18 21:39:46.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename webhook 01/18/23 21:39:46.305
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:39:46.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:39:46.36
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 21:39:46.402
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:39:47.116
STEP: Deploying the webhook pod 01/18/23 21:39:47.124
STEP: Wait for the deployment to be ready 01/18/23 21:39:47.139
Jan 18 21:39:47.153: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 21:39:49.166
STEP: Verifying the service has paired with the endpoint 01/18/23 21:39:49.179
Jan 18 21:39:50.179: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 01/18/23 21:39:50.254
STEP: Creating a configMap that should be mutated 01/18/23 21:39:50.279
STEP: Deleting the collection of validation webhooks 01/18/23 21:39:50.334
STEP: Creating a configMap that should not be mutated 01/18/23 21:39:50.398
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 21:39:50.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9168" for this suite. 01/18/23 21:39:50.415
STEP: Destroying namespace "webhook-9168-markers" for this suite. 01/18/23 21:39:50.421
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":53,"skipped":1232,"failed":0}
------------------------------
• [4.185 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:39:46.303
    Jan 18 21:39:46.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename webhook 01/18/23 21:39:46.305
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:39:46.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:39:46.36
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 21:39:46.402
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:39:47.116
    STEP: Deploying the webhook pod 01/18/23 21:39:47.124
    STEP: Wait for the deployment to be ready 01/18/23 21:39:47.139
    Jan 18 21:39:47.153: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 21:39:49.166
    STEP: Verifying the service has paired with the endpoint 01/18/23 21:39:49.179
    Jan 18 21:39:50.179: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 01/18/23 21:39:50.254
    STEP: Creating a configMap that should be mutated 01/18/23 21:39:50.279
    STEP: Deleting the collection of validation webhooks 01/18/23 21:39:50.334
    STEP: Creating a configMap that should not be mutated 01/18/23 21:39:50.398
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 21:39:50.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9168" for this suite. 01/18/23 21:39:50.415
    STEP: Destroying namespace "webhook-9168-markers" for this suite. 01/18/23 21:39:50.421
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:39:50.517
Jan 18 21:39:50.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename configmap 01/18/23 21:39:50.52
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:39:50.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:39:50.549
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-cf880e47-4ea2-453b-b298-44c1aff1cf47 01/18/23 21:39:50.555
STEP: Creating a pod to test consume configMaps 01/18/23 21:39:50.562
Jan 18 21:39:50.581: INFO: Waiting up to 5m0s for pod "pod-configmaps-72eac1a8-58d0-4ffe-91cc-1ade3b44adb7" in namespace "configmap-9563" to be "Succeeded or Failed"
Jan 18 21:39:50.586: INFO: Pod "pod-configmaps-72eac1a8-58d0-4ffe-91cc-1ade3b44adb7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.504132ms
Jan 18 21:39:52.593: INFO: Pod "pod-configmaps-72eac1a8-58d0-4ffe-91cc-1ade3b44adb7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011468625s
Jan 18 21:39:54.665: INFO: Pod "pod-configmaps-72eac1a8-58d0-4ffe-91cc-1ade3b44adb7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.08333531s
STEP: Saw pod success 01/18/23 21:39:54.665
Jan 18 21:39:54.665: INFO: Pod "pod-configmaps-72eac1a8-58d0-4ffe-91cc-1ade3b44adb7" satisfied condition "Succeeded or Failed"
Jan 18 21:39:54.746: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-72eac1a8-58d0-4ffe-91cc-1ade3b44adb7 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 21:39:54.818
Jan 18 21:39:54.837: INFO: Waiting for pod pod-configmaps-72eac1a8-58d0-4ffe-91cc-1ade3b44adb7 to disappear
Jan 18 21:39:54.844: INFO: Pod pod-configmaps-72eac1a8-58d0-4ffe-91cc-1ade3b44adb7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 21:39:54.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9563" for this suite. 01/18/23 21:39:54.85
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":54,"skipped":1242,"failed":0}
------------------------------
• [4.339 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:39:50.517
    Jan 18 21:39:50.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename configmap 01/18/23 21:39:50.52
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:39:50.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:39:50.549
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-cf880e47-4ea2-453b-b298-44c1aff1cf47 01/18/23 21:39:50.555
    STEP: Creating a pod to test consume configMaps 01/18/23 21:39:50.562
    Jan 18 21:39:50.581: INFO: Waiting up to 5m0s for pod "pod-configmaps-72eac1a8-58d0-4ffe-91cc-1ade3b44adb7" in namespace "configmap-9563" to be "Succeeded or Failed"
    Jan 18 21:39:50.586: INFO: Pod "pod-configmaps-72eac1a8-58d0-4ffe-91cc-1ade3b44adb7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.504132ms
    Jan 18 21:39:52.593: INFO: Pod "pod-configmaps-72eac1a8-58d0-4ffe-91cc-1ade3b44adb7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011468625s
    Jan 18 21:39:54.665: INFO: Pod "pod-configmaps-72eac1a8-58d0-4ffe-91cc-1ade3b44adb7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.08333531s
    STEP: Saw pod success 01/18/23 21:39:54.665
    Jan 18 21:39:54.665: INFO: Pod "pod-configmaps-72eac1a8-58d0-4ffe-91cc-1ade3b44adb7" satisfied condition "Succeeded or Failed"
    Jan 18 21:39:54.746: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-72eac1a8-58d0-4ffe-91cc-1ade3b44adb7 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 21:39:54.818
    Jan 18 21:39:54.837: INFO: Waiting for pod pod-configmaps-72eac1a8-58d0-4ffe-91cc-1ade3b44adb7 to disappear
    Jan 18 21:39:54.844: INFO: Pod pod-configmaps-72eac1a8-58d0-4ffe-91cc-1ade3b44adb7 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 21:39:54.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9563" for this suite. 01/18/23 21:39:54.85
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:39:54.862
Jan 18 21:39:54.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 21:39:54.863
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:39:54.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:39:54.89
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 01/18/23 21:39:54.932
Jan 18 21:39:54.942: INFO: Waiting up to 5m0s for pod "downwardapi-volume-31838d09-f666-4c47-991a-5bd665343b92" in namespace "projected-4661" to be "Succeeded or Failed"
Jan 18 21:39:54.950: INFO: Pod "downwardapi-volume-31838d09-f666-4c47-991a-5bd665343b92": Phase="Pending", Reason="", readiness=false. Elapsed: 7.304741ms
Jan 18 21:39:56.956: INFO: Pod "downwardapi-volume-31838d09-f666-4c47-991a-5bd665343b92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01318116s
Jan 18 21:39:58.958: INFO: Pod "downwardapi-volume-31838d09-f666-4c47-991a-5bd665343b92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015550097s
STEP: Saw pod success 01/18/23 21:39:58.958
Jan 18 21:39:58.959: INFO: Pod "downwardapi-volume-31838d09-f666-4c47-991a-5bd665343b92" satisfied condition "Succeeded or Failed"
Jan 18 21:39:58.964: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-31838d09-f666-4c47-991a-5bd665343b92 container client-container: <nil>
STEP: delete the pod 01/18/23 21:39:58.972
Jan 18 21:39:58.997: INFO: Waiting for pod downwardapi-volume-31838d09-f666-4c47-991a-5bd665343b92 to disappear
Jan 18 21:39:59.006: INFO: Pod downwardapi-volume-31838d09-f666-4c47-991a-5bd665343b92 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 21:39:59.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4661" for this suite. 01/18/23 21:39:59.014
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":55,"skipped":1266,"failed":0}
------------------------------
• [4.164 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:39:54.862
    Jan 18 21:39:54.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 21:39:54.863
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:39:54.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:39:54.89
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 01/18/23 21:39:54.932
    Jan 18 21:39:54.942: INFO: Waiting up to 5m0s for pod "downwardapi-volume-31838d09-f666-4c47-991a-5bd665343b92" in namespace "projected-4661" to be "Succeeded or Failed"
    Jan 18 21:39:54.950: INFO: Pod "downwardapi-volume-31838d09-f666-4c47-991a-5bd665343b92": Phase="Pending", Reason="", readiness=false. Elapsed: 7.304741ms
    Jan 18 21:39:56.956: INFO: Pod "downwardapi-volume-31838d09-f666-4c47-991a-5bd665343b92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01318116s
    Jan 18 21:39:58.958: INFO: Pod "downwardapi-volume-31838d09-f666-4c47-991a-5bd665343b92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015550097s
    STEP: Saw pod success 01/18/23 21:39:58.958
    Jan 18 21:39:58.959: INFO: Pod "downwardapi-volume-31838d09-f666-4c47-991a-5bd665343b92" satisfied condition "Succeeded or Failed"
    Jan 18 21:39:58.964: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-31838d09-f666-4c47-991a-5bd665343b92 container client-container: <nil>
    STEP: delete the pod 01/18/23 21:39:58.972
    Jan 18 21:39:58.997: INFO: Waiting for pod downwardapi-volume-31838d09-f666-4c47-991a-5bd665343b92 to disappear
    Jan 18 21:39:59.006: INFO: Pod downwardapi-volume-31838d09-f666-4c47-991a-5bd665343b92 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 21:39:59.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4661" for this suite. 01/18/23 21:39:59.014
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:39:59.027
Jan 18 21:39:59.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename container-probe 01/18/23 21:39:59.031
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:39:59.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:39:59.053
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Jan 18 21:39:59.078: INFO: Waiting up to 5m0s for pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99" in namespace "container-probe-3639" to be "running and ready"
Jan 18 21:39:59.100: INFO: Pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99": Phase="Pending", Reason="", readiness=false. Elapsed: 21.609315ms
Jan 18 21:39:59.100: INFO: The phase of Pod test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:40:01.106: INFO: Pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99": Phase="Running", Reason="", readiness=false. Elapsed: 2.027328625s
Jan 18 21:40:01.106: INFO: The phase of Pod test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99 is Running (Ready = false)
Jan 18 21:40:03.105: INFO: Pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99": Phase="Running", Reason="", readiness=false. Elapsed: 4.027143s
Jan 18 21:40:03.106: INFO: The phase of Pod test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99 is Running (Ready = false)
Jan 18 21:40:05.110: INFO: Pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99": Phase="Running", Reason="", readiness=false. Elapsed: 6.031414824s
Jan 18 21:40:05.110: INFO: The phase of Pod test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99 is Running (Ready = false)
Jan 18 21:40:07.107: INFO: Pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99": Phase="Running", Reason="", readiness=false. Elapsed: 8.029132879s
Jan 18 21:40:07.108: INFO: The phase of Pod test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99 is Running (Ready = false)
Jan 18 21:40:09.105: INFO: Pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99": Phase="Running", Reason="", readiness=false. Elapsed: 10.026932851s
Jan 18 21:40:09.105: INFO: The phase of Pod test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99 is Running (Ready = false)
Jan 18 21:40:11.106: INFO: Pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99": Phase="Running", Reason="", readiness=false. Elapsed: 12.027194388s
Jan 18 21:40:11.106: INFO: The phase of Pod test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99 is Running (Ready = false)
Jan 18 21:40:13.106: INFO: Pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99": Phase="Running", Reason="", readiness=false. Elapsed: 14.027651385s
Jan 18 21:40:13.106: INFO: The phase of Pod test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99 is Running (Ready = false)
Jan 18 21:40:15.108: INFO: Pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99": Phase="Running", Reason="", readiness=false. Elapsed: 16.029719297s
Jan 18 21:40:15.108: INFO: The phase of Pod test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99 is Running (Ready = false)
Jan 18 21:40:17.106: INFO: Pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99": Phase="Running", Reason="", readiness=false. Elapsed: 18.027722411s
Jan 18 21:40:17.107: INFO: The phase of Pod test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99 is Running (Ready = false)
Jan 18 21:40:19.106: INFO: Pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99": Phase="Running", Reason="", readiness=false. Elapsed: 20.027480396s
Jan 18 21:40:19.106: INFO: The phase of Pod test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99 is Running (Ready = false)
Jan 18 21:40:21.107: INFO: Pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99": Phase="Running", Reason="", readiness=true. Elapsed: 22.029044718s
Jan 18 21:40:21.108: INFO: The phase of Pod test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99 is Running (Ready = true)
Jan 18 21:40:21.108: INFO: Pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99" satisfied condition "running and ready"
Jan 18 21:40:21.115: INFO: Container started at 2023-01-18 21:40:00 +0000 UTC, pod became ready at 2023-01-18 21:40:19 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 18 21:40:21.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3639" for this suite. 01/18/23 21:40:21.122
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":56,"skipped":1266,"failed":0}
------------------------------
• [SLOW TEST] [22.103 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:39:59.027
    Jan 18 21:39:59.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename container-probe 01/18/23 21:39:59.031
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:39:59.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:39:59.053
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Jan 18 21:39:59.078: INFO: Waiting up to 5m0s for pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99" in namespace "container-probe-3639" to be "running and ready"
    Jan 18 21:39:59.100: INFO: Pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99": Phase="Pending", Reason="", readiness=false. Elapsed: 21.609315ms
    Jan 18 21:39:59.100: INFO: The phase of Pod test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:40:01.106: INFO: Pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99": Phase="Running", Reason="", readiness=false. Elapsed: 2.027328625s
    Jan 18 21:40:01.106: INFO: The phase of Pod test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99 is Running (Ready = false)
    Jan 18 21:40:03.105: INFO: Pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99": Phase="Running", Reason="", readiness=false. Elapsed: 4.027143s
    Jan 18 21:40:03.106: INFO: The phase of Pod test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99 is Running (Ready = false)
    Jan 18 21:40:05.110: INFO: Pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99": Phase="Running", Reason="", readiness=false. Elapsed: 6.031414824s
    Jan 18 21:40:05.110: INFO: The phase of Pod test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99 is Running (Ready = false)
    Jan 18 21:40:07.107: INFO: Pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99": Phase="Running", Reason="", readiness=false. Elapsed: 8.029132879s
    Jan 18 21:40:07.108: INFO: The phase of Pod test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99 is Running (Ready = false)
    Jan 18 21:40:09.105: INFO: Pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99": Phase="Running", Reason="", readiness=false. Elapsed: 10.026932851s
    Jan 18 21:40:09.105: INFO: The phase of Pod test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99 is Running (Ready = false)
    Jan 18 21:40:11.106: INFO: Pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99": Phase="Running", Reason="", readiness=false. Elapsed: 12.027194388s
    Jan 18 21:40:11.106: INFO: The phase of Pod test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99 is Running (Ready = false)
    Jan 18 21:40:13.106: INFO: Pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99": Phase="Running", Reason="", readiness=false. Elapsed: 14.027651385s
    Jan 18 21:40:13.106: INFO: The phase of Pod test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99 is Running (Ready = false)
    Jan 18 21:40:15.108: INFO: Pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99": Phase="Running", Reason="", readiness=false. Elapsed: 16.029719297s
    Jan 18 21:40:15.108: INFO: The phase of Pod test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99 is Running (Ready = false)
    Jan 18 21:40:17.106: INFO: Pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99": Phase="Running", Reason="", readiness=false. Elapsed: 18.027722411s
    Jan 18 21:40:17.107: INFO: The phase of Pod test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99 is Running (Ready = false)
    Jan 18 21:40:19.106: INFO: Pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99": Phase="Running", Reason="", readiness=false. Elapsed: 20.027480396s
    Jan 18 21:40:19.106: INFO: The phase of Pod test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99 is Running (Ready = false)
    Jan 18 21:40:21.107: INFO: Pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99": Phase="Running", Reason="", readiness=true. Elapsed: 22.029044718s
    Jan 18 21:40:21.108: INFO: The phase of Pod test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99 is Running (Ready = true)
    Jan 18 21:40:21.108: INFO: Pod "test-webserver-71984526-ccdc-4050-9315-b9ad48bbcb99" satisfied condition "running and ready"
    Jan 18 21:40:21.115: INFO: Container started at 2023-01-18 21:40:00 +0000 UTC, pod became ready at 2023-01-18 21:40:19 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 18 21:40:21.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-3639" for this suite. 01/18/23 21:40:21.122
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:40:21.133
Jan 18 21:40:21.133: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename downward-api 01/18/23 21:40:21.135
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:40:21.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:40:21.159
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 01/18/23 21:40:21.164
Jan 18 21:40:21.174: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ed54d437-5559-428c-9f8c-c4e17ac2efa2" in namespace "downward-api-1164" to be "Succeeded or Failed"
Jan 18 21:40:21.181: INFO: Pod "downwardapi-volume-ed54d437-5559-428c-9f8c-c4e17ac2efa2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.681104ms
Jan 18 21:40:23.189: INFO: Pod "downwardapi-volume-ed54d437-5559-428c-9f8c-c4e17ac2efa2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014148768s
Jan 18 21:40:25.192: INFO: Pod "downwardapi-volume-ed54d437-5559-428c-9f8c-c4e17ac2efa2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017584769s
STEP: Saw pod success 01/18/23 21:40:25.192
Jan 18 21:40:25.193: INFO: Pod "downwardapi-volume-ed54d437-5559-428c-9f8c-c4e17ac2efa2" satisfied condition "Succeeded or Failed"
Jan 18 21:40:25.197: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-ed54d437-5559-428c-9f8c-c4e17ac2efa2 container client-container: <nil>
STEP: delete the pod 01/18/23 21:40:25.205
Jan 18 21:40:25.222: INFO: Waiting for pod downwardapi-volume-ed54d437-5559-428c-9f8c-c4e17ac2efa2 to disappear
Jan 18 21:40:25.227: INFO: Pod downwardapi-volume-ed54d437-5559-428c-9f8c-c4e17ac2efa2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 21:40:25.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1164" for this suite. 01/18/23 21:40:25.233
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":57,"skipped":1274,"failed":0}
------------------------------
• [4.109 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:40:21.133
    Jan 18 21:40:21.133: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename downward-api 01/18/23 21:40:21.135
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:40:21.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:40:21.159
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 01/18/23 21:40:21.164
    Jan 18 21:40:21.174: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ed54d437-5559-428c-9f8c-c4e17ac2efa2" in namespace "downward-api-1164" to be "Succeeded or Failed"
    Jan 18 21:40:21.181: INFO: Pod "downwardapi-volume-ed54d437-5559-428c-9f8c-c4e17ac2efa2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.681104ms
    Jan 18 21:40:23.189: INFO: Pod "downwardapi-volume-ed54d437-5559-428c-9f8c-c4e17ac2efa2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014148768s
    Jan 18 21:40:25.192: INFO: Pod "downwardapi-volume-ed54d437-5559-428c-9f8c-c4e17ac2efa2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017584769s
    STEP: Saw pod success 01/18/23 21:40:25.192
    Jan 18 21:40:25.193: INFO: Pod "downwardapi-volume-ed54d437-5559-428c-9f8c-c4e17ac2efa2" satisfied condition "Succeeded or Failed"
    Jan 18 21:40:25.197: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-ed54d437-5559-428c-9f8c-c4e17ac2efa2 container client-container: <nil>
    STEP: delete the pod 01/18/23 21:40:25.205
    Jan 18 21:40:25.222: INFO: Waiting for pod downwardapi-volume-ed54d437-5559-428c-9f8c-c4e17ac2efa2 to disappear
    Jan 18 21:40:25.227: INFO: Pod downwardapi-volume-ed54d437-5559-428c-9f8c-c4e17ac2efa2 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 21:40:25.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1164" for this suite. 01/18/23 21:40:25.233
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:40:25.248
Jan 18 21:40:25.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename dns 01/18/23 21:40:25.25
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:40:25.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:40:25.294
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/18/23 21:40:25.298
Jan 18 21:40:25.310: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-24  c295c6e3-5d0d-471b-9004-69edc8f99c8a 138532 0 2023-01-18 21:40:25 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-18 21:40:25 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sdkxt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sdkxt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:40:25.312: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-24" to be "running and ready"
Jan 18 21:40:25.327: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 10.194468ms
Jan 18 21:40:25.327: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:40:27.333: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.015636165s
Jan 18 21:40:27.333: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jan 18 21:40:27.333: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 01/18/23 21:40:27.333
Jan 18 21:40:27.333: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-24 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:40:27.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 21:40:27.334: INFO: ExecWithOptions: Clientset creation
Jan 18 21:40:27.335: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-24/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 01/18/23 21:40:27.484
Jan 18 21:40:27.484: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-24 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:40:27.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 21:40:27.485: INFO: ExecWithOptions: Clientset creation
Jan 18 21:40:27.485: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-24/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 21:40:27.603: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 18 21:40:27.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-24" for this suite. 01/18/23 21:40:27.635
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":58,"skipped":1277,"failed":0}
------------------------------
• [2.399 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:40:25.248
    Jan 18 21:40:25.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename dns 01/18/23 21:40:25.25
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:40:25.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:40:25.294
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/18/23 21:40:25.298
    Jan 18 21:40:25.310: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-24  c295c6e3-5d0d-471b-9004-69edc8f99c8a 138532 0 2023-01-18 21:40:25 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-18 21:40:25 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sdkxt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sdkxt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:40:25.312: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-24" to be "running and ready"
    Jan 18 21:40:25.327: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 10.194468ms
    Jan 18 21:40:25.327: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:40:27.333: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.015636165s
    Jan 18 21:40:27.333: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jan 18 21:40:27.333: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 01/18/23 21:40:27.333
    Jan 18 21:40:27.333: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-24 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:40:27.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 21:40:27.334: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:40:27.335: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-24/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 01/18/23 21:40:27.484
    Jan 18 21:40:27.484: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-24 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:40:27.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 21:40:27.485: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:40:27.485: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-24/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 18 21:40:27.603: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 18 21:40:27.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-24" for this suite. 01/18/23 21:40:27.635
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:40:27.649
Jan 18 21:40:27.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename gc 01/18/23 21:40:27.651
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:40:27.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:40:27.675
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 01/18/23 21:40:27.68
STEP: Wait for the Deployment to create new ReplicaSet 01/18/23 21:40:27.687
STEP: delete the deployment 01/18/23 21:40:28.2
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/18/23 21:40:28.207
STEP: Gathering metrics 01/18/23 21:40:28.739
Jan 18 21:40:28.792: INFO: Waiting up to 5m0s for pod "kube-controller-manager-v1-25-1-18760-m" in namespace "kube-system" to be "running and ready"
Jan 18 21:40:28.808: INFO: Pod "kube-controller-manager-v1-25-1-18760-m": Phase="Running", Reason="", readiness=true. Elapsed: 15.004937ms
Jan 18 21:40:28.808: INFO: The phase of Pod kube-controller-manager-v1-25-1-18760-m is Running (Ready = true)
Jan 18 21:40:28.808: INFO: Pod "kube-controller-manager-v1-25-1-18760-m" satisfied condition "running and ready"
Jan 18 21:40:28.956: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 18 21:40:28.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4878" for this suite. 01/18/23 21:40:28.963
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":59,"skipped":1318,"failed":0}
------------------------------
• [1.324 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:40:27.649
    Jan 18 21:40:27.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename gc 01/18/23 21:40:27.651
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:40:27.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:40:27.675
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 01/18/23 21:40:27.68
    STEP: Wait for the Deployment to create new ReplicaSet 01/18/23 21:40:27.687
    STEP: delete the deployment 01/18/23 21:40:28.2
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/18/23 21:40:28.207
    STEP: Gathering metrics 01/18/23 21:40:28.739
    Jan 18 21:40:28.792: INFO: Waiting up to 5m0s for pod "kube-controller-manager-v1-25-1-18760-m" in namespace "kube-system" to be "running and ready"
    Jan 18 21:40:28.808: INFO: Pod "kube-controller-manager-v1-25-1-18760-m": Phase="Running", Reason="", readiness=true. Elapsed: 15.004937ms
    Jan 18 21:40:28.808: INFO: The phase of Pod kube-controller-manager-v1-25-1-18760-m is Running (Ready = true)
    Jan 18 21:40:28.808: INFO: Pod "kube-controller-manager-v1-25-1-18760-m" satisfied condition "running and ready"
    Jan 18 21:40:28.956: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 18 21:40:28.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4878" for this suite. 01/18/23 21:40:28.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:40:28.981
Jan 18 21:40:28.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename kubectl 01/18/23 21:40:28.985
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:40:29.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:40:29.019
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 01/18/23 21:40:29.026
Jan 18 21:40:29.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4482 create -f -'
Jan 18 21:40:30.432: INFO: stderr: ""
Jan 18 21:40:30.432: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 21:40:30.432
Jan 18 21:40:30.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4482 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 21:40:30.598: INFO: stderr: ""
Jan 18 21:40:30.598: INFO: stdout: "update-demo-nautilus-f727p update-demo-nautilus-zclp2 "
Jan 18 21:40:30.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4482 get pods update-demo-nautilus-f727p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 21:40:30.714: INFO: stderr: ""
Jan 18 21:40:30.714: INFO: stdout: ""
Jan 18 21:40:30.714: INFO: update-demo-nautilus-f727p is created but not running
Jan 18 21:40:35.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4482 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 21:40:35.828: INFO: stderr: ""
Jan 18 21:40:35.828: INFO: stdout: "update-demo-nautilus-f727p update-demo-nautilus-zclp2 "
Jan 18 21:40:35.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4482 get pods update-demo-nautilus-f727p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 21:40:35.938: INFO: stderr: ""
Jan 18 21:40:35.938: INFO: stdout: ""
Jan 18 21:40:35.938: INFO: update-demo-nautilus-f727p is created but not running
Jan 18 21:40:40.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4482 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 21:40:41.089: INFO: stderr: ""
Jan 18 21:40:41.089: INFO: stdout: "update-demo-nautilus-f727p update-demo-nautilus-zclp2 "
Jan 18 21:40:41.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4482 get pods update-demo-nautilus-f727p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 21:40:41.209: INFO: stderr: ""
Jan 18 21:40:41.209: INFO: stdout: "true"
Jan 18 21:40:41.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4482 get pods update-demo-nautilus-f727p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 21:40:41.333: INFO: stderr: ""
Jan 18 21:40:41.333: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 18 21:40:41.333: INFO: validating pod update-demo-nautilus-f727p
Jan 18 21:40:41.346: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 21:40:41.346: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 21:40:41.346: INFO: update-demo-nautilus-f727p is verified up and running
Jan 18 21:40:41.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4482 get pods update-demo-nautilus-zclp2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 21:40:41.503: INFO: stderr: ""
Jan 18 21:40:41.503: INFO: stdout: "true"
Jan 18 21:40:41.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4482 get pods update-demo-nautilus-zclp2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 21:40:41.645: INFO: stderr: ""
Jan 18 21:40:41.645: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 18 21:40:41.645: INFO: validating pod update-demo-nautilus-zclp2
Jan 18 21:40:41.653: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 21:40:41.653: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 21:40:41.653: INFO: update-demo-nautilus-zclp2 is verified up and running
STEP: using delete to clean up resources 01/18/23 21:40:41.653
Jan 18 21:40:41.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4482 delete --grace-period=0 --force -f -'
Jan 18 21:40:41.798: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 21:40:41.798: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 18 21:40:41.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4482 get rc,svc -l name=update-demo --no-headers'
Jan 18 21:40:41.989: INFO: stderr: "No resources found in kubectl-4482 namespace.\n"
Jan 18 21:40:41.989: INFO: stdout: ""
Jan 18 21:40:41.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4482 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 18 21:40:42.134: INFO: stderr: ""
Jan 18 21:40:42.134: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 21:40:42.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4482" for this suite. 01/18/23 21:40:42.14
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":60,"skipped":1323,"failed":0}
------------------------------
• [SLOW TEST] [13.166 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:40:28.981
    Jan 18 21:40:28.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename kubectl 01/18/23 21:40:28.985
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:40:29.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:40:29.019
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 01/18/23 21:40:29.026
    Jan 18 21:40:29.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4482 create -f -'
    Jan 18 21:40:30.432: INFO: stderr: ""
    Jan 18 21:40:30.432: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 21:40:30.432
    Jan 18 21:40:30.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4482 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 21:40:30.598: INFO: stderr: ""
    Jan 18 21:40:30.598: INFO: stdout: "update-demo-nautilus-f727p update-demo-nautilus-zclp2 "
    Jan 18 21:40:30.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4482 get pods update-demo-nautilus-f727p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 21:40:30.714: INFO: stderr: ""
    Jan 18 21:40:30.714: INFO: stdout: ""
    Jan 18 21:40:30.714: INFO: update-demo-nautilus-f727p is created but not running
    Jan 18 21:40:35.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4482 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 21:40:35.828: INFO: stderr: ""
    Jan 18 21:40:35.828: INFO: stdout: "update-demo-nautilus-f727p update-demo-nautilus-zclp2 "
    Jan 18 21:40:35.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4482 get pods update-demo-nautilus-f727p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 21:40:35.938: INFO: stderr: ""
    Jan 18 21:40:35.938: INFO: stdout: ""
    Jan 18 21:40:35.938: INFO: update-demo-nautilus-f727p is created but not running
    Jan 18 21:40:40.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4482 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 21:40:41.089: INFO: stderr: ""
    Jan 18 21:40:41.089: INFO: stdout: "update-demo-nautilus-f727p update-demo-nautilus-zclp2 "
    Jan 18 21:40:41.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4482 get pods update-demo-nautilus-f727p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 21:40:41.209: INFO: stderr: ""
    Jan 18 21:40:41.209: INFO: stdout: "true"
    Jan 18 21:40:41.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4482 get pods update-demo-nautilus-f727p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 21:40:41.333: INFO: stderr: ""
    Jan 18 21:40:41.333: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 18 21:40:41.333: INFO: validating pod update-demo-nautilus-f727p
    Jan 18 21:40:41.346: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 21:40:41.346: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 21:40:41.346: INFO: update-demo-nautilus-f727p is verified up and running
    Jan 18 21:40:41.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4482 get pods update-demo-nautilus-zclp2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 21:40:41.503: INFO: stderr: ""
    Jan 18 21:40:41.503: INFO: stdout: "true"
    Jan 18 21:40:41.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4482 get pods update-demo-nautilus-zclp2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 21:40:41.645: INFO: stderr: ""
    Jan 18 21:40:41.645: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 18 21:40:41.645: INFO: validating pod update-demo-nautilus-zclp2
    Jan 18 21:40:41.653: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 21:40:41.653: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 21:40:41.653: INFO: update-demo-nautilus-zclp2 is verified up and running
    STEP: using delete to clean up resources 01/18/23 21:40:41.653
    Jan 18 21:40:41.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4482 delete --grace-period=0 --force -f -'
    Jan 18 21:40:41.798: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 21:40:41.798: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 18 21:40:41.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4482 get rc,svc -l name=update-demo --no-headers'
    Jan 18 21:40:41.989: INFO: stderr: "No resources found in kubectl-4482 namespace.\n"
    Jan 18 21:40:41.989: INFO: stdout: ""
    Jan 18 21:40:41.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4482 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 18 21:40:42.134: INFO: stderr: ""
    Jan 18 21:40:42.134: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 21:40:42.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4482" for this suite. 01/18/23 21:40:42.14
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:40:42.153
Jan 18 21:40:42.153: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename svcaccounts 01/18/23 21:40:42.155
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:40:42.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:40:42.178
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Jan 18 21:40:42.210: INFO: created pod pod-service-account-defaultsa
Jan 18 21:40:42.210: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 18 21:40:42.223: INFO: created pod pod-service-account-mountsa
Jan 18 21:40:42.223: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 18 21:40:42.317: INFO: created pod pod-service-account-nomountsa
Jan 18 21:40:42.317: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 18 21:40:42.350: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 18 21:40:42.350: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 18 21:40:42.393: INFO: created pod pod-service-account-mountsa-mountspec
Jan 18 21:40:42.393: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 18 21:40:42.425: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 18 21:40:42.425: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 18 21:40:42.438: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 18 21:40:42.438: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 18 21:40:42.475: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 18 21:40:42.475: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 18 21:40:42.504: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 18 21:40:42.504: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 18 21:40:42.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3492" for this suite. 01/18/23 21:40:42.583
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":61,"skipped":1379,"failed":0}
------------------------------
• [0.438 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:40:42.153
    Jan 18 21:40:42.153: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 21:40:42.155
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:40:42.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:40:42.178
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Jan 18 21:40:42.210: INFO: created pod pod-service-account-defaultsa
    Jan 18 21:40:42.210: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jan 18 21:40:42.223: INFO: created pod pod-service-account-mountsa
    Jan 18 21:40:42.223: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jan 18 21:40:42.317: INFO: created pod pod-service-account-nomountsa
    Jan 18 21:40:42.317: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jan 18 21:40:42.350: INFO: created pod pod-service-account-defaultsa-mountspec
    Jan 18 21:40:42.350: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jan 18 21:40:42.393: INFO: created pod pod-service-account-mountsa-mountspec
    Jan 18 21:40:42.393: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jan 18 21:40:42.425: INFO: created pod pod-service-account-nomountsa-mountspec
    Jan 18 21:40:42.425: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jan 18 21:40:42.438: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jan 18 21:40:42.438: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jan 18 21:40:42.475: INFO: created pod pod-service-account-mountsa-nomountspec
    Jan 18 21:40:42.475: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jan 18 21:40:42.504: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jan 18 21:40:42.504: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 18 21:40:42.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-3492" for this suite. 01/18/23 21:40:42.583
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:40:42.592
Jan 18 21:40:42.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename deployment 01/18/23 21:40:42.623
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:40:42.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:40:42.684
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 01/18/23 21:40:42.699
Jan 18 21:40:42.699: INFO: Creating simple deployment test-deployment-lpgvv
Jan 18 21:40:42.728: INFO: deployment "test-deployment-lpgvv" doesn't have the required revision set
Jan 18 21:40:44.866: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-lpgvv-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:40:46.874: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-lpgvv-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:40:48.907: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-lpgvv-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:40:50.871: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-lpgvv-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status 01/18/23 21:40:52.877
Jan 18 21:40:52.882: INFO: Deployment test-deployment-lpgvv has Conditions: [{Available True 2023-01-18 21:40:51 +0000 UTC 2023-01-18 21:40:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-18 21:40:51 +0000 UTC 2023-01-18 21:40:42 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lpgvv-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 01/18/23 21:40:52.882
Jan 18 21:40:52.898: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 40, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 40, 51, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 40, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-lpgvv-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 01/18/23 21:40:52.898
Jan 18 21:40:52.905: INFO: Observed &Deployment event: ADDED
Jan 18 21:40:52.905: INFO: Observed Deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:40:42 +0000 UTC 2023-01-18 21:40:42 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lpgvv-777898ffcc"}
Jan 18 21:40:52.905: INFO: Observed &Deployment event: MODIFIED
Jan 18 21:40:52.905: INFO: Observed Deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:40:42 +0000 UTC 2023-01-18 21:40:42 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lpgvv-777898ffcc"}
Jan 18 21:40:52.906: INFO: Observed Deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 21:40:42 +0000 UTC 2023-01-18 21:40:42 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 18 21:40:52.906: INFO: Observed &Deployment event: MODIFIED
Jan 18 21:40:52.906: INFO: Observed Deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 21:40:42 +0000 UTC 2023-01-18 21:40:42 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 18 21:40:52.906: INFO: Observed Deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:40:42 +0000 UTC 2023-01-18 21:40:42 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-lpgvv-777898ffcc" is progressing.}
Jan 18 21:40:52.907: INFO: Observed &Deployment event: MODIFIED
Jan 18 21:40:52.907: INFO: Observed Deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 21:40:51 +0000 UTC 2023-01-18 21:40:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 18 21:40:52.907: INFO: Observed Deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:40:51 +0000 UTC 2023-01-18 21:40:42 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lpgvv-777898ffcc" has successfully progressed.}
Jan 18 21:40:52.907: INFO: Observed &Deployment event: MODIFIED
Jan 18 21:40:52.907: INFO: Observed Deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 21:40:51 +0000 UTC 2023-01-18 21:40:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 18 21:40:52.907: INFO: Observed Deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:40:51 +0000 UTC 2023-01-18 21:40:42 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lpgvv-777898ffcc" has successfully progressed.}
Jan 18 21:40:52.907: INFO: Found Deployment test-deployment-lpgvv in namespace deployment-8341 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 18 21:40:52.907: INFO: Deployment test-deployment-lpgvv has an updated status
STEP: patching the Statefulset Status 01/18/23 21:40:52.907
Jan 18 21:40:52.908: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 18 21:40:52.917: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 01/18/23 21:40:52.917
Jan 18 21:40:52.921: INFO: Observed &Deployment event: ADDED
Jan 18 21:40:52.921: INFO: Observed deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:40:42 +0000 UTC 2023-01-18 21:40:42 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lpgvv-777898ffcc"}
Jan 18 21:40:52.921: INFO: Observed &Deployment event: MODIFIED
Jan 18 21:40:52.921: INFO: Observed deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:40:42 +0000 UTC 2023-01-18 21:40:42 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lpgvv-777898ffcc"}
Jan 18 21:40:52.921: INFO: Observed deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 21:40:42 +0000 UTC 2023-01-18 21:40:42 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 18 21:40:52.921: INFO: Observed &Deployment event: MODIFIED
Jan 18 21:40:52.921: INFO: Observed deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 21:40:42 +0000 UTC 2023-01-18 21:40:42 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 18 21:40:52.921: INFO: Observed deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:40:42 +0000 UTC 2023-01-18 21:40:42 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-lpgvv-777898ffcc" is progressing.}
Jan 18 21:40:52.922: INFO: Observed &Deployment event: MODIFIED
Jan 18 21:40:52.922: INFO: Observed deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 21:40:51 +0000 UTC 2023-01-18 21:40:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 18 21:40:52.922: INFO: Observed deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:40:51 +0000 UTC 2023-01-18 21:40:42 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lpgvv-777898ffcc" has successfully progressed.}
Jan 18 21:40:52.922: INFO: Observed &Deployment event: MODIFIED
Jan 18 21:40:52.923: INFO: Observed deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 21:40:51 +0000 UTC 2023-01-18 21:40:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 18 21:40:52.923: INFO: Observed deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:40:51 +0000 UTC 2023-01-18 21:40:42 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lpgvv-777898ffcc" has successfully progressed.}
Jan 18 21:40:52.923: INFO: Observed deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 18 21:40:52.923: INFO: Observed &Deployment event: MODIFIED
Jan 18 21:40:52.923: INFO: Found deployment test-deployment-lpgvv in namespace deployment-8341 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan 18 21:40:52.924: INFO: Deployment test-deployment-lpgvv has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 21:40:53.137: INFO: Deployment "test-deployment-lpgvv":
&Deployment{ObjectMeta:{test-deployment-lpgvv  deployment-8341  cd339e3e-29d8-42cb-8a58-c4658f861839 138951 1 2023-01-18 21:40:42 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-18 21:40:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-18 21:40:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-18 21:40:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004eada88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-lpgvv-777898ffcc",LastUpdateTime:2023-01-18 21:40:52 +0000 UTC,LastTransitionTime:2023-01-18 21:40:52 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 18 21:40:53.143: INFO: New ReplicaSet "test-deployment-lpgvv-777898ffcc" of Deployment "test-deployment-lpgvv":
&ReplicaSet{ObjectMeta:{test-deployment-lpgvv-777898ffcc  deployment-8341  9127c128-efea-4f11-9ca6-815e72201ecd 138942 1 2023-01-18 21:40:42 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-lpgvv cd339e3e-29d8-42cb-8a58-c4658f861839 0xc004eade90 0xc004eade91}] [] [{kube-controller-manager Update apps/v1 2023-01-18 21:40:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cd339e3e-29d8-42cb-8a58-c4658f861839\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 21:40:51 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004eadf38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 21:40:53.150: INFO: Pod "test-deployment-lpgvv-777898ffcc-w9mkv" is available:
&Pod{ObjectMeta:{test-deployment-lpgvv-777898ffcc-w9mkv test-deployment-lpgvv-777898ffcc- deployment-8341  355fe072-9fe2-4ebf-ba31-56a1029d6497 138941 0 2023-01-18 21:40:42 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:59503f9caa5bd72d7c6ed65deaf1058d0742d1c19598bee565bfd602af65776e cni.projectcalico.org/podIP:10.233.68.75/32 cni.projectcalico.org/podIPs:10.233.68.75/32] [{apps/v1 ReplicaSet test-deployment-lpgvv-777898ffcc 9127c128-efea-4f11-9ca6-815e72201ecd 0xc005568320 0xc005568321}] [] [{kube-controller-manager Update v1 2023-01-18 21:40:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9127c128-efea-4f11-9ca6-815e72201ecd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 21:40:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 21:40:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.75\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4sck7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4sck7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:40:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:40:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:40:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:40:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.75,StartTime:2023-01-18 21:40:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:40:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://aa662276d9e80385aa23ea85c65ff67c1f828fc2f76650c35c37cdbfd7d43126,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.75,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 18 21:40:53.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8341" for this suite. 01/18/23 21:40:53.165
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":62,"skipped":1380,"failed":0}
------------------------------
• [SLOW TEST] [10.580 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:40:42.592
    Jan 18 21:40:42.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename deployment 01/18/23 21:40:42.623
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:40:42.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:40:42.684
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 01/18/23 21:40:42.699
    Jan 18 21:40:42.699: INFO: Creating simple deployment test-deployment-lpgvv
    Jan 18 21:40:42.728: INFO: deployment "test-deployment-lpgvv" doesn't have the required revision set
    Jan 18 21:40:44.866: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-lpgvv-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:40:46.874: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-lpgvv-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:40:48.907: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-lpgvv-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:40:50.871: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-lpgvv-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Getting /status 01/18/23 21:40:52.877
    Jan 18 21:40:52.882: INFO: Deployment test-deployment-lpgvv has Conditions: [{Available True 2023-01-18 21:40:51 +0000 UTC 2023-01-18 21:40:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-18 21:40:51 +0000 UTC 2023-01-18 21:40:42 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lpgvv-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 01/18/23 21:40:52.882
    Jan 18 21:40:52.898: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 40, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 40, 51, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 40, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 40, 42, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-lpgvv-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 01/18/23 21:40:52.898
    Jan 18 21:40:52.905: INFO: Observed &Deployment event: ADDED
    Jan 18 21:40:52.905: INFO: Observed Deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:40:42 +0000 UTC 2023-01-18 21:40:42 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lpgvv-777898ffcc"}
    Jan 18 21:40:52.905: INFO: Observed &Deployment event: MODIFIED
    Jan 18 21:40:52.905: INFO: Observed Deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:40:42 +0000 UTC 2023-01-18 21:40:42 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lpgvv-777898ffcc"}
    Jan 18 21:40:52.906: INFO: Observed Deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 21:40:42 +0000 UTC 2023-01-18 21:40:42 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 18 21:40:52.906: INFO: Observed &Deployment event: MODIFIED
    Jan 18 21:40:52.906: INFO: Observed Deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 21:40:42 +0000 UTC 2023-01-18 21:40:42 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 18 21:40:52.906: INFO: Observed Deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:40:42 +0000 UTC 2023-01-18 21:40:42 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-lpgvv-777898ffcc" is progressing.}
    Jan 18 21:40:52.907: INFO: Observed &Deployment event: MODIFIED
    Jan 18 21:40:52.907: INFO: Observed Deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 21:40:51 +0000 UTC 2023-01-18 21:40:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 18 21:40:52.907: INFO: Observed Deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:40:51 +0000 UTC 2023-01-18 21:40:42 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lpgvv-777898ffcc" has successfully progressed.}
    Jan 18 21:40:52.907: INFO: Observed &Deployment event: MODIFIED
    Jan 18 21:40:52.907: INFO: Observed Deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 21:40:51 +0000 UTC 2023-01-18 21:40:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 18 21:40:52.907: INFO: Observed Deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:40:51 +0000 UTC 2023-01-18 21:40:42 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lpgvv-777898ffcc" has successfully progressed.}
    Jan 18 21:40:52.907: INFO: Found Deployment test-deployment-lpgvv in namespace deployment-8341 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 18 21:40:52.907: INFO: Deployment test-deployment-lpgvv has an updated status
    STEP: patching the Statefulset Status 01/18/23 21:40:52.907
    Jan 18 21:40:52.908: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 18 21:40:52.917: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 01/18/23 21:40:52.917
    Jan 18 21:40:52.921: INFO: Observed &Deployment event: ADDED
    Jan 18 21:40:52.921: INFO: Observed deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:40:42 +0000 UTC 2023-01-18 21:40:42 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lpgvv-777898ffcc"}
    Jan 18 21:40:52.921: INFO: Observed &Deployment event: MODIFIED
    Jan 18 21:40:52.921: INFO: Observed deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:40:42 +0000 UTC 2023-01-18 21:40:42 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lpgvv-777898ffcc"}
    Jan 18 21:40:52.921: INFO: Observed deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 21:40:42 +0000 UTC 2023-01-18 21:40:42 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 18 21:40:52.921: INFO: Observed &Deployment event: MODIFIED
    Jan 18 21:40:52.921: INFO: Observed deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 21:40:42 +0000 UTC 2023-01-18 21:40:42 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 18 21:40:52.921: INFO: Observed deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:40:42 +0000 UTC 2023-01-18 21:40:42 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-lpgvv-777898ffcc" is progressing.}
    Jan 18 21:40:52.922: INFO: Observed &Deployment event: MODIFIED
    Jan 18 21:40:52.922: INFO: Observed deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 21:40:51 +0000 UTC 2023-01-18 21:40:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 18 21:40:52.922: INFO: Observed deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:40:51 +0000 UTC 2023-01-18 21:40:42 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lpgvv-777898ffcc" has successfully progressed.}
    Jan 18 21:40:52.922: INFO: Observed &Deployment event: MODIFIED
    Jan 18 21:40:52.923: INFO: Observed deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 21:40:51 +0000 UTC 2023-01-18 21:40:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 18 21:40:52.923: INFO: Observed deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:40:51 +0000 UTC 2023-01-18 21:40:42 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lpgvv-777898ffcc" has successfully progressed.}
    Jan 18 21:40:52.923: INFO: Observed deployment test-deployment-lpgvv in namespace deployment-8341 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 18 21:40:52.923: INFO: Observed &Deployment event: MODIFIED
    Jan 18 21:40:52.923: INFO: Found deployment test-deployment-lpgvv in namespace deployment-8341 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jan 18 21:40:52.924: INFO: Deployment test-deployment-lpgvv has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 21:40:53.137: INFO: Deployment "test-deployment-lpgvv":
    &Deployment{ObjectMeta:{test-deployment-lpgvv  deployment-8341  cd339e3e-29d8-42cb-8a58-c4658f861839 138951 1 2023-01-18 21:40:42 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-18 21:40:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-18 21:40:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-18 21:40:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004eada88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-lpgvv-777898ffcc",LastUpdateTime:2023-01-18 21:40:52 +0000 UTC,LastTransitionTime:2023-01-18 21:40:52 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 18 21:40:53.143: INFO: New ReplicaSet "test-deployment-lpgvv-777898ffcc" of Deployment "test-deployment-lpgvv":
    &ReplicaSet{ObjectMeta:{test-deployment-lpgvv-777898ffcc  deployment-8341  9127c128-efea-4f11-9ca6-815e72201ecd 138942 1 2023-01-18 21:40:42 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-lpgvv cd339e3e-29d8-42cb-8a58-c4658f861839 0xc004eade90 0xc004eade91}] [] [{kube-controller-manager Update apps/v1 2023-01-18 21:40:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cd339e3e-29d8-42cb-8a58-c4658f861839\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 21:40:51 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004eadf38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 21:40:53.150: INFO: Pod "test-deployment-lpgvv-777898ffcc-w9mkv" is available:
    &Pod{ObjectMeta:{test-deployment-lpgvv-777898ffcc-w9mkv test-deployment-lpgvv-777898ffcc- deployment-8341  355fe072-9fe2-4ebf-ba31-56a1029d6497 138941 0 2023-01-18 21:40:42 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:59503f9caa5bd72d7c6ed65deaf1058d0742d1c19598bee565bfd602af65776e cni.projectcalico.org/podIP:10.233.68.75/32 cni.projectcalico.org/podIPs:10.233.68.75/32] [{apps/v1 ReplicaSet test-deployment-lpgvv-777898ffcc 9127c128-efea-4f11-9ca6-815e72201ecd 0xc005568320 0xc005568321}] [] [{kube-controller-manager Update v1 2023-01-18 21:40:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9127c128-efea-4f11-9ca6-815e72201ecd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 21:40:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 21:40:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.75\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4sck7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4sck7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:40:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:40:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:40:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:40:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.75,StartTime:2023-01-18 21:40:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:40:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://aa662276d9e80385aa23ea85c65ff67c1f828fc2f76650c35c37cdbfd7d43126,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.75,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 18 21:40:53.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-8341" for this suite. 01/18/23 21:40:53.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:40:53.175
Jan 18 21:40:53.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename replicaset 01/18/23 21:40:53.177
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:40:53.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:40:53.203
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/18/23 21:40:53.209
Jan 18 21:40:53.222: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 18 21:40:58.231: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 21:40:58.231
STEP: getting scale subresource 01/18/23 21:40:58.231
STEP: updating a scale subresource 01/18/23 21:40:58.236
STEP: verifying the replicaset Spec.Replicas was modified 01/18/23 21:40:58.286
STEP: Patch a scale subresource 01/18/23 21:40:58.295
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 18 21:40:58.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8143" for this suite. 01/18/23 21:40:58.436
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":63,"skipped":1407,"failed":0}
------------------------------
• [SLOW TEST] [5.319 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:40:53.175
    Jan 18 21:40:53.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename replicaset 01/18/23 21:40:53.177
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:40:53.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:40:53.203
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/18/23 21:40:53.209
    Jan 18 21:40:53.222: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 18 21:40:58.231: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 21:40:58.231
    STEP: getting scale subresource 01/18/23 21:40:58.231
    STEP: updating a scale subresource 01/18/23 21:40:58.236
    STEP: verifying the replicaset Spec.Replicas was modified 01/18/23 21:40:58.286
    STEP: Patch a scale subresource 01/18/23 21:40:58.295
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 18 21:40:58.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-8143" for this suite. 01/18/23 21:40:58.436
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:40:58.505
Jan 18 21:40:58.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename disruption 01/18/23 21:40:58.508
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:40:58.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:40:58.572
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 01/18/23 21:40:58.585
STEP: Updating PodDisruptionBudget status 01/18/23 21:41:00.606
STEP: Waiting for all pods to be running 01/18/23 21:41:00.616
Jan 18 21:41:00.623: INFO: running pods: 0 < 1
STEP: locating a running pod 01/18/23 21:41:02.629
STEP: Waiting for the pdb to be processed 01/18/23 21:41:02.647
STEP: Patching PodDisruptionBudget status 01/18/23 21:41:02.665
STEP: Waiting for the pdb to be processed 01/18/23 21:41:02.678
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 18 21:41:02.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2424" for this suite. 01/18/23 21:41:02.692
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":64,"skipped":1499,"failed":0}
------------------------------
• [4.198 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:40:58.505
    Jan 18 21:40:58.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename disruption 01/18/23 21:40:58.508
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:40:58.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:40:58.572
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 01/18/23 21:40:58.585
    STEP: Updating PodDisruptionBudget status 01/18/23 21:41:00.606
    STEP: Waiting for all pods to be running 01/18/23 21:41:00.616
    Jan 18 21:41:00.623: INFO: running pods: 0 < 1
    STEP: locating a running pod 01/18/23 21:41:02.629
    STEP: Waiting for the pdb to be processed 01/18/23 21:41:02.647
    STEP: Patching PodDisruptionBudget status 01/18/23 21:41:02.665
    STEP: Waiting for the pdb to be processed 01/18/23 21:41:02.678
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 18 21:41:02.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2424" for this suite. 01/18/23 21:41:02.692
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:41:02.703
Jan 18 21:41:02.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename namespaces 01/18/23 21:41:02.705
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:02.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:02.732
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 01/18/23 21:41:02.74
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:02.769
STEP: Creating a pod in the namespace 01/18/23 21:41:02.774
STEP: Waiting for the pod to have running status 01/18/23 21:41:02.783
Jan 18 21:41:02.783: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-9369" to be "running"
Jan 18 21:41:02.788: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.471915ms
Jan 18 21:41:04.793: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009660746s
Jan 18 21:41:04.794: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 01/18/23 21:41:04.794
STEP: Waiting for the namespace to be removed. 01/18/23 21:41:04.802
STEP: Recreating the namespace 01/18/23 21:41:15.808
STEP: Verifying there are no pods in the namespace 01/18/23 21:41:15.838
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 18 21:41:15.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9209" for this suite. 01/18/23 21:41:15.852
STEP: Destroying namespace "nsdeletetest-9369" for this suite. 01/18/23 21:41:15.859
Jan 18 21:41:15.877: INFO: Namespace nsdeletetest-9369 was already deleted
STEP: Destroying namespace "nsdeletetest-3475" for this suite. 01/18/23 21:41:15.877
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":65,"skipped":1499,"failed":0}
------------------------------
• [SLOW TEST] [13.181 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:41:02.703
    Jan 18 21:41:02.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename namespaces 01/18/23 21:41:02.705
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:02.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:02.732
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 01/18/23 21:41:02.74
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:02.769
    STEP: Creating a pod in the namespace 01/18/23 21:41:02.774
    STEP: Waiting for the pod to have running status 01/18/23 21:41:02.783
    Jan 18 21:41:02.783: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-9369" to be "running"
    Jan 18 21:41:02.788: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.471915ms
    Jan 18 21:41:04.793: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009660746s
    Jan 18 21:41:04.794: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 01/18/23 21:41:04.794
    STEP: Waiting for the namespace to be removed. 01/18/23 21:41:04.802
    STEP: Recreating the namespace 01/18/23 21:41:15.808
    STEP: Verifying there are no pods in the namespace 01/18/23 21:41:15.838
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 21:41:15.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-9209" for this suite. 01/18/23 21:41:15.852
    STEP: Destroying namespace "nsdeletetest-9369" for this suite. 01/18/23 21:41:15.859
    Jan 18 21:41:15.877: INFO: Namespace nsdeletetest-9369 was already deleted
    STEP: Destroying namespace "nsdeletetest-3475" for this suite. 01/18/23 21:41:15.877
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:41:15.895
Jan 18 21:41:15.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename configmap 01/18/23 21:41:15.897
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:15.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:15.936
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-d93718ad-4540-4937-a4d9-006c36104775 01/18/23 21:41:15.942
STEP: Creating a pod to test consume configMaps 01/18/23 21:41:15.948
Jan 18 21:41:15.962: INFO: Waiting up to 5m0s for pod "pod-configmaps-66d99880-b085-4d80-b562-2a852c171bfa" in namespace "configmap-96" to be "Succeeded or Failed"
Jan 18 21:41:15.967: INFO: Pod "pod-configmaps-66d99880-b085-4d80-b562-2a852c171bfa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.892491ms
Jan 18 21:41:17.975: INFO: Pod "pod-configmaps-66d99880-b085-4d80-b562-2a852c171bfa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012667921s
Jan 18 21:41:19.977: INFO: Pod "pod-configmaps-66d99880-b085-4d80-b562-2a852c171bfa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01561015s
Jan 18 21:41:21.972: INFO: Pod "pod-configmaps-66d99880-b085-4d80-b562-2a852c171bfa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010332816s
STEP: Saw pod success 01/18/23 21:41:21.972
Jan 18 21:41:21.973: INFO: Pod "pod-configmaps-66d99880-b085-4d80-b562-2a852c171bfa" satisfied condition "Succeeded or Failed"
Jan 18 21:41:21.979: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-66d99880-b085-4d80-b562-2a852c171bfa container configmap-volume-test: <nil>
STEP: delete the pod 01/18/23 21:41:21.999
Jan 18 21:41:22.019: INFO: Waiting for pod pod-configmaps-66d99880-b085-4d80-b562-2a852c171bfa to disappear
Jan 18 21:41:22.024: INFO: Pod pod-configmaps-66d99880-b085-4d80-b562-2a852c171bfa no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 21:41:22.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-96" for this suite. 01/18/23 21:41:22.034
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":66,"skipped":1517,"failed":0}
------------------------------
• [SLOW TEST] [6.148 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:41:15.895
    Jan 18 21:41:15.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename configmap 01/18/23 21:41:15.897
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:15.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:15.936
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-d93718ad-4540-4937-a4d9-006c36104775 01/18/23 21:41:15.942
    STEP: Creating a pod to test consume configMaps 01/18/23 21:41:15.948
    Jan 18 21:41:15.962: INFO: Waiting up to 5m0s for pod "pod-configmaps-66d99880-b085-4d80-b562-2a852c171bfa" in namespace "configmap-96" to be "Succeeded or Failed"
    Jan 18 21:41:15.967: INFO: Pod "pod-configmaps-66d99880-b085-4d80-b562-2a852c171bfa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.892491ms
    Jan 18 21:41:17.975: INFO: Pod "pod-configmaps-66d99880-b085-4d80-b562-2a852c171bfa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012667921s
    Jan 18 21:41:19.977: INFO: Pod "pod-configmaps-66d99880-b085-4d80-b562-2a852c171bfa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01561015s
    Jan 18 21:41:21.972: INFO: Pod "pod-configmaps-66d99880-b085-4d80-b562-2a852c171bfa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010332816s
    STEP: Saw pod success 01/18/23 21:41:21.972
    Jan 18 21:41:21.973: INFO: Pod "pod-configmaps-66d99880-b085-4d80-b562-2a852c171bfa" satisfied condition "Succeeded or Failed"
    Jan 18 21:41:21.979: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-66d99880-b085-4d80-b562-2a852c171bfa container configmap-volume-test: <nil>
    STEP: delete the pod 01/18/23 21:41:21.999
    Jan 18 21:41:22.019: INFO: Waiting for pod pod-configmaps-66d99880-b085-4d80-b562-2a852c171bfa to disappear
    Jan 18 21:41:22.024: INFO: Pod pod-configmaps-66d99880-b085-4d80-b562-2a852c171bfa no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 21:41:22.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-96" for this suite. 01/18/23 21:41:22.034
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:41:22.045
Jan 18 21:41:22.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 21:41:22.049
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:22.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:22.078
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-b55e9ef1-2719-4199-897a-e8627cab881f 01/18/23 21:41:22.083
STEP: Creating a pod to test consume configMaps 01/18/23 21:41:22.09
Jan 18 21:41:22.099: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-01ff6d28-61f4-47eb-aeec-26a85c00d848" in namespace "projected-3739" to be "Succeeded or Failed"
Jan 18 21:41:22.102: INFO: Pod "pod-projected-configmaps-01ff6d28-61f4-47eb-aeec-26a85c00d848": Phase="Pending", Reason="", readiness=false. Elapsed: 2.928731ms
Jan 18 21:41:24.109: INFO: Pod "pod-projected-configmaps-01ff6d28-61f4-47eb-aeec-26a85c00d848": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009839965s
Jan 18 21:41:26.115: INFO: Pod "pod-projected-configmaps-01ff6d28-61f4-47eb-aeec-26a85c00d848": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015934348s
STEP: Saw pod success 01/18/23 21:41:26.116
Jan 18 21:41:26.117: INFO: Pod "pod-projected-configmaps-01ff6d28-61f4-47eb-aeec-26a85c00d848" satisfied condition "Succeeded or Failed"
Jan 18 21:41:26.121: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-01ff6d28-61f4-47eb-aeec-26a85c00d848 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 21:41:26.133
Jan 18 21:41:26.173: INFO: Waiting for pod pod-projected-configmaps-01ff6d28-61f4-47eb-aeec-26a85c00d848 to disappear
Jan 18 21:41:26.179: INFO: Pod pod-projected-configmaps-01ff6d28-61f4-47eb-aeec-26a85c00d848 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 18 21:41:26.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3739" for this suite. 01/18/23 21:41:26.196
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":67,"skipped":1530,"failed":0}
------------------------------
• [4.171 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:41:22.045
    Jan 18 21:41:22.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 21:41:22.049
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:22.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:22.078
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-b55e9ef1-2719-4199-897a-e8627cab881f 01/18/23 21:41:22.083
    STEP: Creating a pod to test consume configMaps 01/18/23 21:41:22.09
    Jan 18 21:41:22.099: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-01ff6d28-61f4-47eb-aeec-26a85c00d848" in namespace "projected-3739" to be "Succeeded or Failed"
    Jan 18 21:41:22.102: INFO: Pod "pod-projected-configmaps-01ff6d28-61f4-47eb-aeec-26a85c00d848": Phase="Pending", Reason="", readiness=false. Elapsed: 2.928731ms
    Jan 18 21:41:24.109: INFO: Pod "pod-projected-configmaps-01ff6d28-61f4-47eb-aeec-26a85c00d848": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009839965s
    Jan 18 21:41:26.115: INFO: Pod "pod-projected-configmaps-01ff6d28-61f4-47eb-aeec-26a85c00d848": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015934348s
    STEP: Saw pod success 01/18/23 21:41:26.116
    Jan 18 21:41:26.117: INFO: Pod "pod-projected-configmaps-01ff6d28-61f4-47eb-aeec-26a85c00d848" satisfied condition "Succeeded or Failed"
    Jan 18 21:41:26.121: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-01ff6d28-61f4-47eb-aeec-26a85c00d848 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 21:41:26.133
    Jan 18 21:41:26.173: INFO: Waiting for pod pod-projected-configmaps-01ff6d28-61f4-47eb-aeec-26a85c00d848 to disappear
    Jan 18 21:41:26.179: INFO: Pod pod-projected-configmaps-01ff6d28-61f4-47eb-aeec-26a85c00d848 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 18 21:41:26.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3739" for this suite. 01/18/23 21:41:26.196
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:41:26.216
Jan 18 21:41:26.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename services 01/18/23 21:41:26.218
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:26.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:26.248
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-3576 01/18/23 21:41:26.253
STEP: creating replication controller nodeport-test in namespace services-3576 01/18/23 21:41:26.299
I0118 21:41:26.315470      19 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-3576, replica count: 2
I0118 21:41:29.369128      19 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 21:41:29.369: INFO: Creating new exec pod
Jan 18 21:41:29.386: INFO: Waiting up to 5m0s for pod "execpodl6xz7" in namespace "services-3576" to be "running"
Jan 18 21:41:29.399: INFO: Pod "execpodl6xz7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.235129ms
Jan 18 21:41:31.406: INFO: Pod "execpodl6xz7": Phase="Running", Reason="", readiness=true. Elapsed: 2.019442671s
Jan 18 21:41:31.406: INFO: Pod "execpodl6xz7" satisfied condition "running"
Jan 18 21:41:32.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3576 exec execpodl6xz7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 18 21:41:32.626: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 18 21:41:32.626: INFO: stdout: "nodeport-test-zmkm4"
Jan 18 21:41:32.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3576 exec execpodl6xz7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.25.144 80'
Jan 18 21:41:32.812: INFO: stderr: "+ nc -v -t -w 2 10.233.25.144 80\n+ echo hostName\nConnection to 10.233.25.144 80 port [tcp/http] succeeded!\n"
Jan 18 21:41:32.812: INFO: stdout: "nodeport-test-2lrgl"
Jan 18 21:41:32.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3576 exec execpodl6xz7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.168 31469'
Jan 18 21:41:33.025: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.168 31469\nConnection to 192.168.101.168 31469 port [tcp/*] succeeded!\n"
Jan 18 21:41:33.025: INFO: stdout: ""
Jan 18 21:41:34.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3576 exec execpodl6xz7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.168 31469'
Jan 18 21:41:34.219: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.168 31469\nConnection to 192.168.101.168 31469 port [tcp/*] succeeded!\n"
Jan 18 21:41:34.219: INFO: stdout: "nodeport-test-2lrgl"
Jan 18 21:41:34.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3576 exec execpodl6xz7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 31469'
Jan 18 21:41:34.394: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.216 31469\nConnection to 192.168.101.216 31469 port [tcp/*] succeeded!\n"
Jan 18 21:41:34.394: INFO: stdout: ""
Jan 18 21:41:35.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3576 exec execpodl6xz7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 31469'
Jan 18 21:41:35.583: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.216 31469\nConnection to 192.168.101.216 31469 port [tcp/*] succeeded!\n"
Jan 18 21:41:35.583: INFO: stdout: "nodeport-test-2lrgl"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 21:41:35.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3576" for this suite. 01/18/23 21:41:35.588
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":68,"skipped":1534,"failed":0}
------------------------------
• [SLOW TEST] [9.382 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:41:26.216
    Jan 18 21:41:26.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename services 01/18/23 21:41:26.218
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:26.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:26.248
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-3576 01/18/23 21:41:26.253
    STEP: creating replication controller nodeport-test in namespace services-3576 01/18/23 21:41:26.299
    I0118 21:41:26.315470      19 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-3576, replica count: 2
    I0118 21:41:29.369128      19 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 21:41:29.369: INFO: Creating new exec pod
    Jan 18 21:41:29.386: INFO: Waiting up to 5m0s for pod "execpodl6xz7" in namespace "services-3576" to be "running"
    Jan 18 21:41:29.399: INFO: Pod "execpodl6xz7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.235129ms
    Jan 18 21:41:31.406: INFO: Pod "execpodl6xz7": Phase="Running", Reason="", readiness=true. Elapsed: 2.019442671s
    Jan 18 21:41:31.406: INFO: Pod "execpodl6xz7" satisfied condition "running"
    Jan 18 21:41:32.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3576 exec execpodl6xz7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 18 21:41:32.626: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 18 21:41:32.626: INFO: stdout: "nodeport-test-zmkm4"
    Jan 18 21:41:32.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3576 exec execpodl6xz7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.25.144 80'
    Jan 18 21:41:32.812: INFO: stderr: "+ nc -v -t -w 2 10.233.25.144 80\n+ echo hostName\nConnection to 10.233.25.144 80 port [tcp/http] succeeded!\n"
    Jan 18 21:41:32.812: INFO: stdout: "nodeport-test-2lrgl"
    Jan 18 21:41:32.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3576 exec execpodl6xz7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.168 31469'
    Jan 18 21:41:33.025: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.168 31469\nConnection to 192.168.101.168 31469 port [tcp/*] succeeded!\n"
    Jan 18 21:41:33.025: INFO: stdout: ""
    Jan 18 21:41:34.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3576 exec execpodl6xz7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.168 31469'
    Jan 18 21:41:34.219: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.168 31469\nConnection to 192.168.101.168 31469 port [tcp/*] succeeded!\n"
    Jan 18 21:41:34.219: INFO: stdout: "nodeport-test-2lrgl"
    Jan 18 21:41:34.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3576 exec execpodl6xz7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 31469'
    Jan 18 21:41:34.394: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.216 31469\nConnection to 192.168.101.216 31469 port [tcp/*] succeeded!\n"
    Jan 18 21:41:34.394: INFO: stdout: ""
    Jan 18 21:41:35.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3576 exec execpodl6xz7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 31469'
    Jan 18 21:41:35.583: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.216 31469\nConnection to 192.168.101.216 31469 port [tcp/*] succeeded!\n"
    Jan 18 21:41:35.583: INFO: stdout: "nodeport-test-2lrgl"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 21:41:35.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3576" for this suite. 01/18/23 21:41:35.588
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:41:35.599
Jan 18 21:41:35.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename downward-api 01/18/23 21:41:35.6
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:35.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:35.676
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 01/18/23 21:41:35.681
Jan 18 21:41:35.691: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d584d672-8453-4463-9a1d-041404e7c025" in namespace "downward-api-796" to be "Succeeded or Failed"
Jan 18 21:41:35.702: INFO: Pod "downwardapi-volume-d584d672-8453-4463-9a1d-041404e7c025": Phase="Pending", Reason="", readiness=false. Elapsed: 11.016233ms
Jan 18 21:41:37.708: INFO: Pod "downwardapi-volume-d584d672-8453-4463-9a1d-041404e7c025": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016759683s
Jan 18 21:41:39.708: INFO: Pod "downwardapi-volume-d584d672-8453-4463-9a1d-041404e7c025": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016739398s
STEP: Saw pod success 01/18/23 21:41:39.708
Jan 18 21:41:39.709: INFO: Pod "downwardapi-volume-d584d672-8453-4463-9a1d-041404e7c025" satisfied condition "Succeeded or Failed"
Jan 18 21:41:39.714: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-d584d672-8453-4463-9a1d-041404e7c025 container client-container: <nil>
STEP: delete the pod 01/18/23 21:41:39.722
Jan 18 21:41:39.736: INFO: Waiting for pod downwardapi-volume-d584d672-8453-4463-9a1d-041404e7c025 to disappear
Jan 18 21:41:39.743: INFO: Pod downwardapi-volume-d584d672-8453-4463-9a1d-041404e7c025 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 21:41:39.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-796" for this suite. 01/18/23 21:41:39.747
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":69,"skipped":1537,"failed":0}
------------------------------
• [4.156 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:41:35.599
    Jan 18 21:41:35.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename downward-api 01/18/23 21:41:35.6
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:35.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:35.676
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 01/18/23 21:41:35.681
    Jan 18 21:41:35.691: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d584d672-8453-4463-9a1d-041404e7c025" in namespace "downward-api-796" to be "Succeeded or Failed"
    Jan 18 21:41:35.702: INFO: Pod "downwardapi-volume-d584d672-8453-4463-9a1d-041404e7c025": Phase="Pending", Reason="", readiness=false. Elapsed: 11.016233ms
    Jan 18 21:41:37.708: INFO: Pod "downwardapi-volume-d584d672-8453-4463-9a1d-041404e7c025": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016759683s
    Jan 18 21:41:39.708: INFO: Pod "downwardapi-volume-d584d672-8453-4463-9a1d-041404e7c025": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016739398s
    STEP: Saw pod success 01/18/23 21:41:39.708
    Jan 18 21:41:39.709: INFO: Pod "downwardapi-volume-d584d672-8453-4463-9a1d-041404e7c025" satisfied condition "Succeeded or Failed"
    Jan 18 21:41:39.714: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-d584d672-8453-4463-9a1d-041404e7c025 container client-container: <nil>
    STEP: delete the pod 01/18/23 21:41:39.722
    Jan 18 21:41:39.736: INFO: Waiting for pod downwardapi-volume-d584d672-8453-4463-9a1d-041404e7c025 to disappear
    Jan 18 21:41:39.743: INFO: Pod downwardapi-volume-d584d672-8453-4463-9a1d-041404e7c025 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 21:41:39.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-796" for this suite. 01/18/23 21:41:39.747
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:41:39.765
Jan 18 21:41:39.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename configmap 01/18/23 21:41:39.766
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:39.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:39.79
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-4463b542-2eb2-4b26-918b-30e9a36793c9 01/18/23 21:41:39.801
STEP: Creating configMap with name cm-test-opt-upd-e9e89633-8850-4d25-aceb-e77a3defe9fb 01/18/23 21:41:39.807
STEP: Creating the pod 01/18/23 21:41:39.812
Jan 18 21:41:39.822: INFO: Waiting up to 5m0s for pod "pod-configmaps-fa37391d-a5c9-42dc-880d-443d500e5f62" in namespace "configmap-877" to be "running and ready"
Jan 18 21:41:39.830: INFO: Pod "pod-configmaps-fa37391d-a5c9-42dc-880d-443d500e5f62": Phase="Pending", Reason="", readiness=false. Elapsed: 8.230796ms
Jan 18 21:41:39.830: INFO: The phase of Pod pod-configmaps-fa37391d-a5c9-42dc-880d-443d500e5f62 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:41:41.840: INFO: Pod "pod-configmaps-fa37391d-a5c9-42dc-880d-443d500e5f62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018038959s
Jan 18 21:41:41.840: INFO: The phase of Pod pod-configmaps-fa37391d-a5c9-42dc-880d-443d500e5f62 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:41:43.871: INFO: Pod "pod-configmaps-fa37391d-a5c9-42dc-880d-443d500e5f62": Phase="Running", Reason="", readiness=true. Elapsed: 4.048920735s
Jan 18 21:41:43.871: INFO: The phase of Pod pod-configmaps-fa37391d-a5c9-42dc-880d-443d500e5f62 is Running (Ready = true)
Jan 18 21:41:43.871: INFO: Pod "pod-configmaps-fa37391d-a5c9-42dc-880d-443d500e5f62" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-4463b542-2eb2-4b26-918b-30e9a36793c9 01/18/23 21:41:43.902
STEP: Updating configmap cm-test-opt-upd-e9e89633-8850-4d25-aceb-e77a3defe9fb 01/18/23 21:41:43.909
STEP: Creating configMap with name cm-test-opt-create-390c21a2-ea9d-4da1-a716-cdf7b90866a8 01/18/23 21:41:43.915
STEP: waiting to observe update in volume 01/18/23 21:41:43.921
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 21:42:52.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-877" for this suite. 01/18/23 21:42:52.354
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":70,"skipped":1545,"failed":0}
------------------------------
• [SLOW TEST] [72.598 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:41:39.765
    Jan 18 21:41:39.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename configmap 01/18/23 21:41:39.766
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:39.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:39.79
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-4463b542-2eb2-4b26-918b-30e9a36793c9 01/18/23 21:41:39.801
    STEP: Creating configMap with name cm-test-opt-upd-e9e89633-8850-4d25-aceb-e77a3defe9fb 01/18/23 21:41:39.807
    STEP: Creating the pod 01/18/23 21:41:39.812
    Jan 18 21:41:39.822: INFO: Waiting up to 5m0s for pod "pod-configmaps-fa37391d-a5c9-42dc-880d-443d500e5f62" in namespace "configmap-877" to be "running and ready"
    Jan 18 21:41:39.830: INFO: Pod "pod-configmaps-fa37391d-a5c9-42dc-880d-443d500e5f62": Phase="Pending", Reason="", readiness=false. Elapsed: 8.230796ms
    Jan 18 21:41:39.830: INFO: The phase of Pod pod-configmaps-fa37391d-a5c9-42dc-880d-443d500e5f62 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:41:41.840: INFO: Pod "pod-configmaps-fa37391d-a5c9-42dc-880d-443d500e5f62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018038959s
    Jan 18 21:41:41.840: INFO: The phase of Pod pod-configmaps-fa37391d-a5c9-42dc-880d-443d500e5f62 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:41:43.871: INFO: Pod "pod-configmaps-fa37391d-a5c9-42dc-880d-443d500e5f62": Phase="Running", Reason="", readiness=true. Elapsed: 4.048920735s
    Jan 18 21:41:43.871: INFO: The phase of Pod pod-configmaps-fa37391d-a5c9-42dc-880d-443d500e5f62 is Running (Ready = true)
    Jan 18 21:41:43.871: INFO: Pod "pod-configmaps-fa37391d-a5c9-42dc-880d-443d500e5f62" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-4463b542-2eb2-4b26-918b-30e9a36793c9 01/18/23 21:41:43.902
    STEP: Updating configmap cm-test-opt-upd-e9e89633-8850-4d25-aceb-e77a3defe9fb 01/18/23 21:41:43.909
    STEP: Creating configMap with name cm-test-opt-create-390c21a2-ea9d-4da1-a716-cdf7b90866a8 01/18/23 21:41:43.915
    STEP: waiting to observe update in volume 01/18/23 21:41:43.921
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 21:42:52.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-877" for this suite. 01/18/23 21:42:52.354
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:42:52.367
Jan 18 21:42:52.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename configmap 01/18/23 21:42:52.369
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:42:52.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:42:52.395
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 21:42:52.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6543" for this suite. 01/18/23 21:42:52.444
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":71,"skipped":1547,"failed":0}
------------------------------
• [0.084 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:42:52.367
    Jan 18 21:42:52.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename configmap 01/18/23 21:42:52.369
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:42:52.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:42:52.395
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 21:42:52.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6543" for this suite. 01/18/23 21:42:52.444
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:42:52.465
Jan 18 21:42:52.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename webhook 01/18/23 21:42:52.467
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:42:52.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:42:52.493
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 21:42:52.515
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:42:53.305
STEP: Deploying the webhook pod 01/18/23 21:42:53.317
STEP: Wait for the deployment to be ready 01/18/23 21:42:53.332
Jan 18 21:42:53.340: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 18 21:42:55.355: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 42, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 42, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 42, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 42, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/18/23 21:42:57.36
STEP: Verifying the service has paired with the endpoint 01/18/23 21:42:57.391
Jan 18 21:42:58.391: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Jan 18 21:42:58.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7568-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 21:42:58.914
STEP: Creating a custom resource that should be mutated by the webhook 01/18/23 21:42:58.935
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 21:43:01.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-987" for this suite. 01/18/23 21:43:01.551
STEP: Destroying namespace "webhook-987-markers" for this suite. 01/18/23 21:43:01.561
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":72,"skipped":1615,"failed":0}
------------------------------
• [SLOW TEST] [9.219 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:42:52.465
    Jan 18 21:42:52.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename webhook 01/18/23 21:42:52.467
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:42:52.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:42:52.493
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 21:42:52.515
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:42:53.305
    STEP: Deploying the webhook pod 01/18/23 21:42:53.317
    STEP: Wait for the deployment to be ready 01/18/23 21:42:53.332
    Jan 18 21:42:53.340: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 18 21:42:55.355: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 42, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 42, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 42, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 42, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/18/23 21:42:57.36
    STEP: Verifying the service has paired with the endpoint 01/18/23 21:42:57.391
    Jan 18 21:42:58.391: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Jan 18 21:42:58.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7568-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 21:42:58.914
    STEP: Creating a custom resource that should be mutated by the webhook 01/18/23 21:42:58.935
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 21:43:01.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-987" for this suite. 01/18/23 21:43:01.551
    STEP: Destroying namespace "webhook-987-markers" for this suite. 01/18/23 21:43:01.561
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:43:01.688
Jan 18 21:43:01.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename daemonsets 01/18/23 21:43:01.691
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:43:01.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:43:01.841
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 01/18/23 21:43:01.887
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 21:43:01.894
Jan 18 21:43:01.915: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:43:01.941: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:43:01.941: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 21:43:02.947: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:43:02.951: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:43:02.951: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 21:43:03.956: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:43:03.969: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 21:43:03.970: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/18/23 21:43:03.974
Jan 18 21:43:04.019: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:43:04.039: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 21:43:04.040: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 21:43:05.046: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:43:05.058: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 21:43:05.058: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 21:43:06.046: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:43:06.052: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 21:43:06.052: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 01/18/23 21:43:06.052
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/18/23 21:43:06.062
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6745, will wait for the garbage collector to delete the pods 01/18/23 21:43:06.062
Jan 18 21:43:06.128: INFO: Deleting DaemonSet.extensions daemon-set took: 8.291887ms
Jan 18 21:43:06.229: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.449604ms
Jan 18 21:43:08.435: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:43:08.435: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 21:43:08.439: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"140068"},"items":null}

Jan 18 21:43:08.443: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"140068"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 18 21:43:08.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6745" for this suite. 01/18/23 21:43:08.463
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":73,"skipped":1642,"failed":0}
------------------------------
• [SLOW TEST] [6.783 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:43:01.688
    Jan 18 21:43:01.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename daemonsets 01/18/23 21:43:01.691
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:43:01.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:43:01.841
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 01/18/23 21:43:01.887
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 21:43:01.894
    Jan 18 21:43:01.915: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:43:01.941: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:43:01.941: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 21:43:02.947: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:43:02.951: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:43:02.951: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 21:43:03.956: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:43:03.969: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 21:43:03.970: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/18/23 21:43:03.974
    Jan 18 21:43:04.019: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:43:04.039: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 21:43:04.040: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 21:43:05.046: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:43:05.058: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 21:43:05.058: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 21:43:06.046: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:43:06.052: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 21:43:06.052: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 01/18/23 21:43:06.052
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 21:43:06.062
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6745, will wait for the garbage collector to delete the pods 01/18/23 21:43:06.062
    Jan 18 21:43:06.128: INFO: Deleting DaemonSet.extensions daemon-set took: 8.291887ms
    Jan 18 21:43:06.229: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.449604ms
    Jan 18 21:43:08.435: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:43:08.435: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 21:43:08.439: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"140068"},"items":null}

    Jan 18 21:43:08.443: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"140068"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 21:43:08.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-6745" for this suite. 01/18/23 21:43:08.463
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:43:08.484
Jan 18 21:43:08.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename secrets 01/18/23 21:43:08.485
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:43:08.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:43:08.51
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-9de3b3de-d41c-4ffc-9d52-c01a7cc41a9e 01/18/23 21:43:08.515
STEP: Creating a pod to test consume secrets 01/18/23 21:43:08.521
Jan 18 21:43:08.532: INFO: Waiting up to 5m0s for pod "pod-secrets-20d00781-095b-431d-a84f-894a6ab15ba3" in namespace "secrets-697" to be "Succeeded or Failed"
Jan 18 21:43:08.537: INFO: Pod "pod-secrets-20d00781-095b-431d-a84f-894a6ab15ba3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.612563ms
Jan 18 21:43:10.544: INFO: Pod "pod-secrets-20d00781-095b-431d-a84f-894a6ab15ba3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010895558s
Jan 18 21:43:12.544: INFO: Pod "pod-secrets-20d00781-095b-431d-a84f-894a6ab15ba3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011387961s
STEP: Saw pod success 01/18/23 21:43:12.544
Jan 18 21:43:12.545: INFO: Pod "pod-secrets-20d00781-095b-431d-a84f-894a6ab15ba3" satisfied condition "Succeeded or Failed"
Jan 18 21:43:12.549: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-20d00781-095b-431d-a84f-894a6ab15ba3 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 21:43:12.564
Jan 18 21:43:12.583: INFO: Waiting for pod pod-secrets-20d00781-095b-431d-a84f-894a6ab15ba3 to disappear
Jan 18 21:43:12.587: INFO: Pod pod-secrets-20d00781-095b-431d-a84f-894a6ab15ba3 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 18 21:43:12.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-697" for this suite. 01/18/23 21:43:12.601
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":74,"skipped":1690,"failed":0}
------------------------------
• [4.131 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:43:08.484
    Jan 18 21:43:08.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename secrets 01/18/23 21:43:08.485
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:43:08.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:43:08.51
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-9de3b3de-d41c-4ffc-9d52-c01a7cc41a9e 01/18/23 21:43:08.515
    STEP: Creating a pod to test consume secrets 01/18/23 21:43:08.521
    Jan 18 21:43:08.532: INFO: Waiting up to 5m0s for pod "pod-secrets-20d00781-095b-431d-a84f-894a6ab15ba3" in namespace "secrets-697" to be "Succeeded or Failed"
    Jan 18 21:43:08.537: INFO: Pod "pod-secrets-20d00781-095b-431d-a84f-894a6ab15ba3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.612563ms
    Jan 18 21:43:10.544: INFO: Pod "pod-secrets-20d00781-095b-431d-a84f-894a6ab15ba3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010895558s
    Jan 18 21:43:12.544: INFO: Pod "pod-secrets-20d00781-095b-431d-a84f-894a6ab15ba3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011387961s
    STEP: Saw pod success 01/18/23 21:43:12.544
    Jan 18 21:43:12.545: INFO: Pod "pod-secrets-20d00781-095b-431d-a84f-894a6ab15ba3" satisfied condition "Succeeded or Failed"
    Jan 18 21:43:12.549: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-20d00781-095b-431d-a84f-894a6ab15ba3 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 21:43:12.564
    Jan 18 21:43:12.583: INFO: Waiting for pod pod-secrets-20d00781-095b-431d-a84f-894a6ab15ba3 to disappear
    Jan 18 21:43:12.587: INFO: Pod pod-secrets-20d00781-095b-431d-a84f-894a6ab15ba3 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 21:43:12.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-697" for this suite. 01/18/23 21:43:12.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:43:12.617
Jan 18 21:43:12.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename secrets 01/18/23 21:43:12.619
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:43:12.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:43:12.643
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 18 21:43:12.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5699" for this suite. 01/18/23 21:43:12.717
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":75,"skipped":1697,"failed":0}
------------------------------
• [0.108 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:43:12.617
    Jan 18 21:43:12.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename secrets 01/18/23 21:43:12.619
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:43:12.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:43:12.643
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 21:43:12.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5699" for this suite. 01/18/23 21:43:12.717
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:43:12.729
Jan 18 21:43:12.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename statefulset 01/18/23 21:43:12.731
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:43:12.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:43:12.757
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2467 01/18/23 21:43:12.76
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-2467 01/18/23 21:43:12.772
Jan 18 21:43:12.786: INFO: Found 0 stateful pods, waiting for 1
Jan 18 21:43:22.793: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 01/18/23 21:43:22.801
STEP: Getting /status 01/18/23 21:43:22.811
Jan 18 21:43:22.818: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 01/18/23 21:43:22.818
Jan 18 21:43:22.830: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 01/18/23 21:43:22.83
Jan 18 21:43:22.833: INFO: Observed &StatefulSet event: ADDED
Jan 18 21:43:22.833: INFO: Found Statefulset ss in namespace statefulset-2467 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 18 21:43:22.833: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 01/18/23 21:43:22.833
Jan 18 21:43:22.833: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 18 21:43:22.840: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 01/18/23 21:43:22.841
Jan 18 21:43:22.846: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 21:43:22.847: INFO: Deleting all statefulset in ns statefulset-2467
Jan 18 21:43:22.865: INFO: Scaling statefulset ss to 0
Jan 18 21:43:32.890: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 21:43:32.896: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 18 21:43:32.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2467" for this suite. 01/18/23 21:43:32.92
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":76,"skipped":1699,"failed":0}
------------------------------
• [SLOW TEST] [20.201 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:43:12.729
    Jan 18 21:43:12.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename statefulset 01/18/23 21:43:12.731
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:43:12.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:43:12.757
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2467 01/18/23 21:43:12.76
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-2467 01/18/23 21:43:12.772
    Jan 18 21:43:12.786: INFO: Found 0 stateful pods, waiting for 1
    Jan 18 21:43:22.793: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 01/18/23 21:43:22.801
    STEP: Getting /status 01/18/23 21:43:22.811
    Jan 18 21:43:22.818: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 01/18/23 21:43:22.818
    Jan 18 21:43:22.830: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 01/18/23 21:43:22.83
    Jan 18 21:43:22.833: INFO: Observed &StatefulSet event: ADDED
    Jan 18 21:43:22.833: INFO: Found Statefulset ss in namespace statefulset-2467 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 18 21:43:22.833: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 01/18/23 21:43:22.833
    Jan 18 21:43:22.833: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 18 21:43:22.840: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 01/18/23 21:43:22.841
    Jan 18 21:43:22.846: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 18 21:43:22.847: INFO: Deleting all statefulset in ns statefulset-2467
    Jan 18 21:43:22.865: INFO: Scaling statefulset ss to 0
    Jan 18 21:43:32.890: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 21:43:32.896: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 18 21:43:32.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2467" for this suite. 01/18/23 21:43:32.92
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:43:32.941
Jan 18 21:43:32.941: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 21:43:32.943
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:43:32.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:43:32.965
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Jan 18 21:43:32.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 21:43:36.037
Jan 18 21:43:36.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-6403 --namespace=crd-publish-openapi-6403 create -f -'
Jan 18 21:43:37.418: INFO: stderr: ""
Jan 18 21:43:37.418: INFO: stdout: "e2e-test-crd-publish-openapi-8136-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 18 21:43:37.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-6403 --namespace=crd-publish-openapi-6403 delete e2e-test-crd-publish-openapi-8136-crds test-cr'
Jan 18 21:43:37.536: INFO: stderr: ""
Jan 18 21:43:37.536: INFO: stdout: "e2e-test-crd-publish-openapi-8136-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 18 21:43:37.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-6403 --namespace=crd-publish-openapi-6403 apply -f -'
Jan 18 21:43:37.888: INFO: stderr: ""
Jan 18 21:43:37.888: INFO: stdout: "e2e-test-crd-publish-openapi-8136-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 18 21:43:37.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-6403 --namespace=crd-publish-openapi-6403 delete e2e-test-crd-publish-openapi-8136-crds test-cr'
Jan 18 21:43:37.991: INFO: stderr: ""
Jan 18 21:43:37.991: INFO: stdout: "e2e-test-crd-publish-openapi-8136-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/18/23 21:43:37.991
Jan 18 21:43:37.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-6403 explain e2e-test-crd-publish-openapi-8136-crds'
Jan 18 21:43:38.314: INFO: stderr: ""
Jan 18 21:43:38.314: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8136-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 21:43:43.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6403" for this suite. 01/18/23 21:43:43.361
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":77,"skipped":1724,"failed":0}
------------------------------
• [SLOW TEST] [10.426 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:43:32.941
    Jan 18 21:43:32.941: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 21:43:32.943
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:43:32.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:43:32.965
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Jan 18 21:43:32.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 21:43:36.037
    Jan 18 21:43:36.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-6403 --namespace=crd-publish-openapi-6403 create -f -'
    Jan 18 21:43:37.418: INFO: stderr: ""
    Jan 18 21:43:37.418: INFO: stdout: "e2e-test-crd-publish-openapi-8136-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 18 21:43:37.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-6403 --namespace=crd-publish-openapi-6403 delete e2e-test-crd-publish-openapi-8136-crds test-cr'
    Jan 18 21:43:37.536: INFO: stderr: ""
    Jan 18 21:43:37.536: INFO: stdout: "e2e-test-crd-publish-openapi-8136-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jan 18 21:43:37.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-6403 --namespace=crd-publish-openapi-6403 apply -f -'
    Jan 18 21:43:37.888: INFO: stderr: ""
    Jan 18 21:43:37.888: INFO: stdout: "e2e-test-crd-publish-openapi-8136-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 18 21:43:37.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-6403 --namespace=crd-publish-openapi-6403 delete e2e-test-crd-publish-openapi-8136-crds test-cr'
    Jan 18 21:43:37.991: INFO: stderr: ""
    Jan 18 21:43:37.991: INFO: stdout: "e2e-test-crd-publish-openapi-8136-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/18/23 21:43:37.991
    Jan 18 21:43:37.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-6403 explain e2e-test-crd-publish-openapi-8136-crds'
    Jan 18 21:43:38.314: INFO: stderr: ""
    Jan 18 21:43:38.314: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8136-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 21:43:43.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6403" for this suite. 01/18/23 21:43:43.361
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:43:43.371
Jan 18 21:43:43.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 21:43:43.373
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:43:43.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:43:43.394
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 01/18/23 21:43:43.405
Jan 18 21:43:43.421: INFO: Waiting up to 5m0s for pod "downwardapi-volume-25db7a2b-57e8-428a-b042-959deef9733d" in namespace "projected-5508" to be "Succeeded or Failed"
Jan 18 21:43:43.436: INFO: Pod "downwardapi-volume-25db7a2b-57e8-428a-b042-959deef9733d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.595257ms
Jan 18 21:43:45.443: INFO: Pod "downwardapi-volume-25db7a2b-57e8-428a-b042-959deef9733d": Phase="Running", Reason="", readiness=true. Elapsed: 2.021972507s
Jan 18 21:43:47.443: INFO: Pod "downwardapi-volume-25db7a2b-57e8-428a-b042-959deef9733d": Phase="Running", Reason="", readiness=false. Elapsed: 4.021918521s
Jan 18 21:43:49.444: INFO: Pod "downwardapi-volume-25db7a2b-57e8-428a-b042-959deef9733d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023576807s
STEP: Saw pod success 01/18/23 21:43:49.445
Jan 18 21:43:49.445: INFO: Pod "downwardapi-volume-25db7a2b-57e8-428a-b042-959deef9733d" satisfied condition "Succeeded or Failed"
Jan 18 21:43:49.452: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-25db7a2b-57e8-428a-b042-959deef9733d container client-container: <nil>
STEP: delete the pod 01/18/23 21:43:49.462
Jan 18 21:43:49.482: INFO: Waiting for pod downwardapi-volume-25db7a2b-57e8-428a-b042-959deef9733d to disappear
Jan 18 21:43:49.486: INFO: Pod downwardapi-volume-25db7a2b-57e8-428a-b042-959deef9733d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 21:43:49.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5508" for this suite. 01/18/23 21:43:49.494
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":78,"skipped":1734,"failed":0}
------------------------------
• [SLOW TEST] [6.130 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:43:43.371
    Jan 18 21:43:43.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 21:43:43.373
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:43:43.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:43:43.394
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 01/18/23 21:43:43.405
    Jan 18 21:43:43.421: INFO: Waiting up to 5m0s for pod "downwardapi-volume-25db7a2b-57e8-428a-b042-959deef9733d" in namespace "projected-5508" to be "Succeeded or Failed"
    Jan 18 21:43:43.436: INFO: Pod "downwardapi-volume-25db7a2b-57e8-428a-b042-959deef9733d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.595257ms
    Jan 18 21:43:45.443: INFO: Pod "downwardapi-volume-25db7a2b-57e8-428a-b042-959deef9733d": Phase="Running", Reason="", readiness=true. Elapsed: 2.021972507s
    Jan 18 21:43:47.443: INFO: Pod "downwardapi-volume-25db7a2b-57e8-428a-b042-959deef9733d": Phase="Running", Reason="", readiness=false. Elapsed: 4.021918521s
    Jan 18 21:43:49.444: INFO: Pod "downwardapi-volume-25db7a2b-57e8-428a-b042-959deef9733d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023576807s
    STEP: Saw pod success 01/18/23 21:43:49.445
    Jan 18 21:43:49.445: INFO: Pod "downwardapi-volume-25db7a2b-57e8-428a-b042-959deef9733d" satisfied condition "Succeeded or Failed"
    Jan 18 21:43:49.452: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-25db7a2b-57e8-428a-b042-959deef9733d container client-container: <nil>
    STEP: delete the pod 01/18/23 21:43:49.462
    Jan 18 21:43:49.482: INFO: Waiting for pod downwardapi-volume-25db7a2b-57e8-428a-b042-959deef9733d to disappear
    Jan 18 21:43:49.486: INFO: Pod downwardapi-volume-25db7a2b-57e8-428a-b042-959deef9733d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 21:43:49.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5508" for this suite. 01/18/23 21:43:49.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:43:49.504
Jan 18 21:43:49.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 21:43:49.506
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:43:49.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:43:49.528
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jan 18 21:43:49.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 21:43:52.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6191" for this suite. 01/18/23 21:43:52.913
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":79,"skipped":1741,"failed":0}
------------------------------
• [3.418 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:43:49.504
    Jan 18 21:43:49.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 21:43:49.506
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:43:49.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:43:49.528
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jan 18 21:43:49.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 21:43:52.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-6191" for this suite. 01/18/23 21:43:52.913
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:43:52.924
Jan 18 21:43:52.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename pod-network-test 01/18/23 21:43:52.927
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:43:52.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:43:52.961
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-9208 01/18/23 21:43:52.965
STEP: creating a selector 01/18/23 21:43:52.966
STEP: Creating the service pods in kubernetes 01/18/23 21:43:52.966
Jan 18 21:43:52.966: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 18 21:43:53.002: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9208" to be "running and ready"
Jan 18 21:43:53.027: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 24.819588ms
Jan 18 21:43:53.027: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:43:55.033: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.031227784s
Jan 18 21:43:55.033: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:43:57.034: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.032075536s
Jan 18 21:43:57.034: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:43:59.033: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.030936382s
Jan 18 21:43:59.033: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:44:01.033: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.030954497s
Jan 18 21:44:01.033: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:44:03.033: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.031063218s
Jan 18 21:44:03.033: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:44:05.032: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.029794985s
Jan 18 21:44:05.032: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:44:07.033: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.030858733s
Jan 18 21:44:07.033: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:44:09.032: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.029545342s
Jan 18 21:44:09.032: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:44:11.032: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.030024943s
Jan 18 21:44:11.032: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:44:13.032: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.030219011s
Jan 18 21:44:13.033: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:44:15.037: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.035065552s
Jan 18 21:44:15.037: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 18 21:44:15.037: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 18 21:44:15.041: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9208" to be "running and ready"
Jan 18 21:44:15.045: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.929004ms
Jan 18 21:44:15.045: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 18 21:44:15.045: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/18/23 21:44:15.05
Jan 18 21:44:15.063: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9208" to be "running"
Jan 18 21:44:15.075: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.045095ms
Jan 18 21:44:17.082: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.019324064s
Jan 18 21:44:17.082: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 18 21:44:17.087: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 18 21:44:17.087: INFO: Breadth first check of 10.233.78.214 on host 192.168.101.168...
Jan 18 21:44:17.091: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.68.90:9080/dial?request=hostname&protocol=http&host=10.233.78.214&port=8083&tries=1'] Namespace:pod-network-test-9208 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:44:17.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 21:44:17.093: INFO: ExecWithOptions: Clientset creation
Jan 18 21:44:17.093: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9208/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.68.90%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.78.214%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 21:44:17.214: INFO: Waiting for responses: map[]
Jan 18 21:44:17.214: INFO: reached 10.233.78.214 after 0/1 tries
Jan 18 21:44:17.214: INFO: Breadth first check of 10.233.68.88 on host 192.168.101.216...
Jan 18 21:44:17.227: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.68.90:9080/dial?request=hostname&protocol=http&host=10.233.68.88&port=8083&tries=1'] Namespace:pod-network-test-9208 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:44:17.227: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 21:44:17.229: INFO: ExecWithOptions: Clientset creation
Jan 18 21:44:17.229: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9208/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.68.90%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.68.88%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 21:44:17.316: INFO: Waiting for responses: map[]
Jan 18 21:44:17.316: INFO: reached 10.233.68.88 after 0/1 tries
Jan 18 21:44:17.316: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 18 21:44:17.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9208" for this suite. 01/18/23 21:44:17.322
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":80,"skipped":1747,"failed":0}
------------------------------
• [SLOW TEST] [24.408 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:43:52.924
    Jan 18 21:43:52.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename pod-network-test 01/18/23 21:43:52.927
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:43:52.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:43:52.961
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-9208 01/18/23 21:43:52.965
    STEP: creating a selector 01/18/23 21:43:52.966
    STEP: Creating the service pods in kubernetes 01/18/23 21:43:52.966
    Jan 18 21:43:52.966: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 18 21:43:53.002: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9208" to be "running and ready"
    Jan 18 21:43:53.027: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 24.819588ms
    Jan 18 21:43:53.027: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:43:55.033: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.031227784s
    Jan 18 21:43:55.033: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:43:57.034: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.032075536s
    Jan 18 21:43:57.034: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:43:59.033: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.030936382s
    Jan 18 21:43:59.033: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:44:01.033: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.030954497s
    Jan 18 21:44:01.033: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:44:03.033: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.031063218s
    Jan 18 21:44:03.033: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:44:05.032: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.029794985s
    Jan 18 21:44:05.032: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:44:07.033: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.030858733s
    Jan 18 21:44:07.033: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:44:09.032: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.029545342s
    Jan 18 21:44:09.032: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:44:11.032: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.030024943s
    Jan 18 21:44:11.032: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:44:13.032: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.030219011s
    Jan 18 21:44:13.033: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:44:15.037: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.035065552s
    Jan 18 21:44:15.037: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 18 21:44:15.037: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 18 21:44:15.041: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9208" to be "running and ready"
    Jan 18 21:44:15.045: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.929004ms
    Jan 18 21:44:15.045: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 18 21:44:15.045: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/18/23 21:44:15.05
    Jan 18 21:44:15.063: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9208" to be "running"
    Jan 18 21:44:15.075: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.045095ms
    Jan 18 21:44:17.082: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.019324064s
    Jan 18 21:44:17.082: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 18 21:44:17.087: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 18 21:44:17.087: INFO: Breadth first check of 10.233.78.214 on host 192.168.101.168...
    Jan 18 21:44:17.091: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.68.90:9080/dial?request=hostname&protocol=http&host=10.233.78.214&port=8083&tries=1'] Namespace:pod-network-test-9208 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:44:17.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 21:44:17.093: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:44:17.093: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9208/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.68.90%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.78.214%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 18 21:44:17.214: INFO: Waiting for responses: map[]
    Jan 18 21:44:17.214: INFO: reached 10.233.78.214 after 0/1 tries
    Jan 18 21:44:17.214: INFO: Breadth first check of 10.233.68.88 on host 192.168.101.216...
    Jan 18 21:44:17.227: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.68.90:9080/dial?request=hostname&protocol=http&host=10.233.68.88&port=8083&tries=1'] Namespace:pod-network-test-9208 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:44:17.227: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 21:44:17.229: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:44:17.229: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9208/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.68.90%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.68.88%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 18 21:44:17.316: INFO: Waiting for responses: map[]
    Jan 18 21:44:17.316: INFO: reached 10.233.68.88 after 0/1 tries
    Jan 18 21:44:17.316: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 18 21:44:17.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-9208" for this suite. 01/18/23 21:44:17.322
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:44:17.333
Jan 18 21:44:17.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 21:44:17.336
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:44:17.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:44:17.365
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 01/18/23 21:44:17.37
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/18/23 21:44:17.372
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/18/23 21:44:17.372
STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/18/23 21:44:17.372
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/18/23 21:44:17.374
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/18/23 21:44:17.375
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/18/23 21:44:17.377
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 21:44:17.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1509" for this suite. 01/18/23 21:44:17.385
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":81,"skipped":1747,"failed":0}
------------------------------
• [0.059 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:44:17.333
    Jan 18 21:44:17.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 21:44:17.336
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:44:17.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:44:17.365
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 01/18/23 21:44:17.37
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/18/23 21:44:17.372
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/18/23 21:44:17.372
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/18/23 21:44:17.372
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/18/23 21:44:17.374
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/18/23 21:44:17.375
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/18/23 21:44:17.377
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 21:44:17.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-1509" for this suite. 01/18/23 21:44:17.385
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:44:17.397
Jan 18 21:44:17.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename pods 01/18/23 21:44:17.398
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:44:17.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:44:17.425
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Jan 18 21:44:17.442: INFO: Waiting up to 5m0s for pod "server-envvars-247ab444-a10f-4418-abe0-a643acdc4fa1" in namespace "pods-4861" to be "running and ready"
Jan 18 21:44:17.456: INFO: Pod "server-envvars-247ab444-a10f-4418-abe0-a643acdc4fa1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.961468ms
Jan 18 21:44:17.456: INFO: The phase of Pod server-envvars-247ab444-a10f-4418-abe0-a643acdc4fa1 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:44:19.462: INFO: Pod "server-envvars-247ab444-a10f-4418-abe0-a643acdc4fa1": Phase="Running", Reason="", readiness=true. Elapsed: 2.019573837s
Jan 18 21:44:19.463: INFO: The phase of Pod server-envvars-247ab444-a10f-4418-abe0-a643acdc4fa1 is Running (Ready = true)
Jan 18 21:44:19.463: INFO: Pod "server-envvars-247ab444-a10f-4418-abe0-a643acdc4fa1" satisfied condition "running and ready"
Jan 18 21:44:19.508: INFO: Waiting up to 5m0s for pod "client-envvars-9962fe11-8f74-4929-b724-7928495ee47c" in namespace "pods-4861" to be "Succeeded or Failed"
Jan 18 21:44:19.518: INFO: Pod "client-envvars-9962fe11-8f74-4929-b724-7928495ee47c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.44926ms
Jan 18 21:44:21.523: INFO: Pod "client-envvars-9962fe11-8f74-4929-b724-7928495ee47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014585429s
Jan 18 21:44:23.523: INFO: Pod "client-envvars-9962fe11-8f74-4929-b724-7928495ee47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01522589s
STEP: Saw pod success 01/18/23 21:44:23.523
Jan 18 21:44:23.524: INFO: Pod "client-envvars-9962fe11-8f74-4929-b724-7928495ee47c" satisfied condition "Succeeded or Failed"
Jan 18 21:44:23.528: INFO: Trying to get logs from node v1-25-1-18760-w2 pod client-envvars-9962fe11-8f74-4929-b724-7928495ee47c container env3cont: <nil>
STEP: delete the pod 01/18/23 21:44:23.536
Jan 18 21:44:23.549: INFO: Waiting for pod client-envvars-9962fe11-8f74-4929-b724-7928495ee47c to disappear
Jan 18 21:44:23.554: INFO: Pod client-envvars-9962fe11-8f74-4929-b724-7928495ee47c no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 21:44:23.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4861" for this suite. 01/18/23 21:44:23.56
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":82,"skipped":1773,"failed":0}
------------------------------
• [SLOW TEST] [6.182 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:44:17.397
    Jan 18 21:44:17.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename pods 01/18/23 21:44:17.398
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:44:17.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:44:17.425
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Jan 18 21:44:17.442: INFO: Waiting up to 5m0s for pod "server-envvars-247ab444-a10f-4418-abe0-a643acdc4fa1" in namespace "pods-4861" to be "running and ready"
    Jan 18 21:44:17.456: INFO: Pod "server-envvars-247ab444-a10f-4418-abe0-a643acdc4fa1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.961468ms
    Jan 18 21:44:17.456: INFO: The phase of Pod server-envvars-247ab444-a10f-4418-abe0-a643acdc4fa1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:44:19.462: INFO: Pod "server-envvars-247ab444-a10f-4418-abe0-a643acdc4fa1": Phase="Running", Reason="", readiness=true. Elapsed: 2.019573837s
    Jan 18 21:44:19.463: INFO: The phase of Pod server-envvars-247ab444-a10f-4418-abe0-a643acdc4fa1 is Running (Ready = true)
    Jan 18 21:44:19.463: INFO: Pod "server-envvars-247ab444-a10f-4418-abe0-a643acdc4fa1" satisfied condition "running and ready"
    Jan 18 21:44:19.508: INFO: Waiting up to 5m0s for pod "client-envvars-9962fe11-8f74-4929-b724-7928495ee47c" in namespace "pods-4861" to be "Succeeded or Failed"
    Jan 18 21:44:19.518: INFO: Pod "client-envvars-9962fe11-8f74-4929-b724-7928495ee47c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.44926ms
    Jan 18 21:44:21.523: INFO: Pod "client-envvars-9962fe11-8f74-4929-b724-7928495ee47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014585429s
    Jan 18 21:44:23.523: INFO: Pod "client-envvars-9962fe11-8f74-4929-b724-7928495ee47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01522589s
    STEP: Saw pod success 01/18/23 21:44:23.523
    Jan 18 21:44:23.524: INFO: Pod "client-envvars-9962fe11-8f74-4929-b724-7928495ee47c" satisfied condition "Succeeded or Failed"
    Jan 18 21:44:23.528: INFO: Trying to get logs from node v1-25-1-18760-w2 pod client-envvars-9962fe11-8f74-4929-b724-7928495ee47c container env3cont: <nil>
    STEP: delete the pod 01/18/23 21:44:23.536
    Jan 18 21:44:23.549: INFO: Waiting for pod client-envvars-9962fe11-8f74-4929-b724-7928495ee47c to disappear
    Jan 18 21:44:23.554: INFO: Pod client-envvars-9962fe11-8f74-4929-b724-7928495ee47c no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 21:44:23.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4861" for this suite. 01/18/23 21:44:23.56
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:44:23.584
Jan 18 21:44:23.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename kubectl 01/18/23 21:44:23.587
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:44:23.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:44:23.634
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/18/23 21:44:23.639
Jan 18 21:44:23.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-8679 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 18 21:44:23.772: INFO: stderr: ""
Jan 18 21:44:23.772: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 01/18/23 21:44:23.772
Jan 18 21:44:23.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-8679 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Jan 18 21:44:25.608: INFO: stderr: ""
Jan 18 21:44:25.608: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/18/23 21:44:25.608
Jan 18 21:44:25.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-8679 delete pods e2e-test-httpd-pod'
Jan 18 21:44:27.824: INFO: stderr: ""
Jan 18 21:44:27.824: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 21:44:27.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8679" for this suite. 01/18/23 21:44:27.831
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":83,"skipped":1776,"failed":0}
------------------------------
• [4.257 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:44:23.584
    Jan 18 21:44:23.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename kubectl 01/18/23 21:44:23.587
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:44:23.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:44:23.634
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/18/23 21:44:23.639
    Jan 18 21:44:23.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-8679 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 18 21:44:23.772: INFO: stderr: ""
    Jan 18 21:44:23.772: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 01/18/23 21:44:23.772
    Jan 18 21:44:23.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-8679 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Jan 18 21:44:25.608: INFO: stderr: ""
    Jan 18 21:44:25.608: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/18/23 21:44:25.608
    Jan 18 21:44:25.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-8679 delete pods e2e-test-httpd-pod'
    Jan 18 21:44:27.824: INFO: stderr: ""
    Jan 18 21:44:27.824: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 21:44:27.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8679" for this suite. 01/18/23 21:44:27.831
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:44:27.846
Jan 18 21:44:27.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename secrets 01/18/23 21:44:27.849
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:44:27.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:44:27.872
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-81418157-bc21-4255-9cf3-21886d8364fd 01/18/23 21:44:27.876
STEP: Creating a pod to test consume secrets 01/18/23 21:44:27.881
Jan 18 21:44:27.890: INFO: Waiting up to 5m0s for pod "pod-secrets-b2ed6dd5-cc1c-419a-b664-f0ecf5ca9876" in namespace "secrets-7884" to be "Succeeded or Failed"
Jan 18 21:44:27.896: INFO: Pod "pod-secrets-b2ed6dd5-cc1c-419a-b664-f0ecf5ca9876": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027782ms
Jan 18 21:44:29.901: INFO: Pod "pod-secrets-b2ed6dd5-cc1c-419a-b664-f0ecf5ca9876": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010701617s
Jan 18 21:44:31.906: INFO: Pod "pod-secrets-b2ed6dd5-cc1c-419a-b664-f0ecf5ca9876": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016470685s
STEP: Saw pod success 01/18/23 21:44:31.906
Jan 18 21:44:31.907: INFO: Pod "pod-secrets-b2ed6dd5-cc1c-419a-b664-f0ecf5ca9876" satisfied condition "Succeeded or Failed"
Jan 18 21:44:31.911: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-b2ed6dd5-cc1c-419a-b664-f0ecf5ca9876 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 21:44:31.918
Jan 18 21:44:31.931: INFO: Waiting for pod pod-secrets-b2ed6dd5-cc1c-419a-b664-f0ecf5ca9876 to disappear
Jan 18 21:44:31.936: INFO: Pod pod-secrets-b2ed6dd5-cc1c-419a-b664-f0ecf5ca9876 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 18 21:44:31.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7884" for this suite. 01/18/23 21:44:31.942
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":84,"skipped":1787,"failed":0}
------------------------------
• [4.105 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:44:27.846
    Jan 18 21:44:27.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename secrets 01/18/23 21:44:27.849
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:44:27.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:44:27.872
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-81418157-bc21-4255-9cf3-21886d8364fd 01/18/23 21:44:27.876
    STEP: Creating a pod to test consume secrets 01/18/23 21:44:27.881
    Jan 18 21:44:27.890: INFO: Waiting up to 5m0s for pod "pod-secrets-b2ed6dd5-cc1c-419a-b664-f0ecf5ca9876" in namespace "secrets-7884" to be "Succeeded or Failed"
    Jan 18 21:44:27.896: INFO: Pod "pod-secrets-b2ed6dd5-cc1c-419a-b664-f0ecf5ca9876": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027782ms
    Jan 18 21:44:29.901: INFO: Pod "pod-secrets-b2ed6dd5-cc1c-419a-b664-f0ecf5ca9876": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010701617s
    Jan 18 21:44:31.906: INFO: Pod "pod-secrets-b2ed6dd5-cc1c-419a-b664-f0ecf5ca9876": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016470685s
    STEP: Saw pod success 01/18/23 21:44:31.906
    Jan 18 21:44:31.907: INFO: Pod "pod-secrets-b2ed6dd5-cc1c-419a-b664-f0ecf5ca9876" satisfied condition "Succeeded or Failed"
    Jan 18 21:44:31.911: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-b2ed6dd5-cc1c-419a-b664-f0ecf5ca9876 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 21:44:31.918
    Jan 18 21:44:31.931: INFO: Waiting for pod pod-secrets-b2ed6dd5-cc1c-419a-b664-f0ecf5ca9876 to disappear
    Jan 18 21:44:31.936: INFO: Pod pod-secrets-b2ed6dd5-cc1c-419a-b664-f0ecf5ca9876 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 21:44:31.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7884" for this suite. 01/18/23 21:44:31.942
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:44:31.959
Jan 18 21:44:31.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename cronjob 01/18/23 21:44:31.961
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:44:31.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:44:31.985
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 01/18/23 21:44:31.99
STEP: Ensuring more than one job is running at a time 01/18/23 21:44:31.997
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/18/23 21:46:02.006
STEP: Removing cronjob 01/18/23 21:46:02.014
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 18 21:46:02.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8712" for this suite. 01/18/23 21:46:02.029
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":85,"skipped":1799,"failed":0}
------------------------------
• [SLOW TEST] [90.085 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:44:31.959
    Jan 18 21:44:31.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename cronjob 01/18/23 21:44:31.961
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:44:31.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:44:31.985
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 01/18/23 21:44:31.99
    STEP: Ensuring more than one job is running at a time 01/18/23 21:44:31.997
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/18/23 21:46:02.006
    STEP: Removing cronjob 01/18/23 21:46:02.014
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 18 21:46:02.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-8712" for this suite. 01/18/23 21:46:02.029
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:46:02.049
Jan 18 21:46:02.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename svcaccounts 01/18/23 21:46:02.051
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:46:02.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:46:02.137
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Jan 18 21:46:02.168: INFO: created pod
Jan 18 21:46:02.168: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-5195" to be "Succeeded or Failed"
Jan 18 21:46:02.174: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.946759ms
Jan 18 21:46:04.180: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011233346s
Jan 18 21:46:06.183: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014161886s
Jan 18 21:46:08.181: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012367759s
STEP: Saw pod success 01/18/23 21:46:08.181
Jan 18 21:46:08.182: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan 18 21:46:38.182: INFO: polling logs
Jan 18 21:46:38.207: INFO: Pod logs: 
I0118 21:46:03.572583       1 log.go:195] OK: Got token
I0118 21:46:03.572658       1 log.go:195] validating with in-cluster discovery
I0118 21:46:03.573880       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0118 21:46:03.573969       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5195:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674078962, NotBefore:1674078362, IssuedAt:1674078362, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5195", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"acc56d89-2fa4-43ed-98d2-44f574d99b59"}}}
I0118 21:46:03.611135       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0118 21:46:03.623001       1 log.go:195] OK: Validated signature on JWT
I0118 21:46:03.623430       1 log.go:195] OK: Got valid claims from token!
I0118 21:46:03.623582       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5195:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674078962, NotBefore:1674078362, IssuedAt:1674078362, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5195", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"acc56d89-2fa4-43ed-98d2-44f574d99b59"}}}

Jan 18 21:46:38.207: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 18 21:46:38.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5195" for this suite. 01/18/23 21:46:38.235
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":86,"skipped":1823,"failed":0}
------------------------------
• [SLOW TEST] [36.193 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:46:02.049
    Jan 18 21:46:02.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 21:46:02.051
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:46:02.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:46:02.137
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Jan 18 21:46:02.168: INFO: created pod
    Jan 18 21:46:02.168: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-5195" to be "Succeeded or Failed"
    Jan 18 21:46:02.174: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.946759ms
    Jan 18 21:46:04.180: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011233346s
    Jan 18 21:46:06.183: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014161886s
    Jan 18 21:46:08.181: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012367759s
    STEP: Saw pod success 01/18/23 21:46:08.181
    Jan 18 21:46:08.182: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jan 18 21:46:38.182: INFO: polling logs
    Jan 18 21:46:38.207: INFO: Pod logs: 
    I0118 21:46:03.572583       1 log.go:195] OK: Got token
    I0118 21:46:03.572658       1 log.go:195] validating with in-cluster discovery
    I0118 21:46:03.573880       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0118 21:46:03.573969       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5195:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674078962, NotBefore:1674078362, IssuedAt:1674078362, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5195", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"acc56d89-2fa4-43ed-98d2-44f574d99b59"}}}
    I0118 21:46:03.611135       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0118 21:46:03.623001       1 log.go:195] OK: Validated signature on JWT
    I0118 21:46:03.623430       1 log.go:195] OK: Got valid claims from token!
    I0118 21:46:03.623582       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5195:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674078962, NotBefore:1674078362, IssuedAt:1674078362, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5195", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"acc56d89-2fa4-43ed-98d2-44f574d99b59"}}}

    Jan 18 21:46:38.207: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 18 21:46:38.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5195" for this suite. 01/18/23 21:46:38.235
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:46:38.244
Jan 18 21:46:38.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename services 01/18/23 21:46:38.247
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:46:38.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:46:38.286
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-6773 01/18/23 21:46:38.29
Jan 18 21:46:38.301: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-6773" to be "running and ready"
Jan 18 21:46:38.307: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 6.819208ms
Jan 18 21:46:38.308: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:46:40.313: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.012555366s
Jan 18 21:46:40.314: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 18 21:46:40.314: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Jan 18 21:46:40.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-6773 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 18 21:46:40.541: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 18 21:46:40.541: INFO: stdout: "ipvs"
Jan 18 21:46:40.541: INFO: proxyMode: ipvs
Jan 18 21:46:40.558: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 18 21:46:40.562: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-6773 01/18/23 21:46:40.563
STEP: creating replication controller affinity-clusterip-timeout in namespace services-6773 01/18/23 21:46:40.583
I0118 21:46:40.601218      19 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-6773, replica count: 3
I0118 21:46:43.652720      19 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 21:46:43.665: INFO: Creating new exec pod
Jan 18 21:46:43.672: INFO: Waiting up to 5m0s for pod "execpod-affinity2zl9t" in namespace "services-6773" to be "running"
Jan 18 21:46:43.685: INFO: Pod "execpod-affinity2zl9t": Phase="Pending", Reason="", readiness=false. Elapsed: 12.587458ms
Jan 18 21:46:45.690: INFO: Pod "execpod-affinity2zl9t": Phase="Running", Reason="", readiness=true. Elapsed: 2.017787309s
Jan 18 21:46:45.690: INFO: Pod "execpod-affinity2zl9t" satisfied condition "running"
Jan 18 21:46:46.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-6773 exec execpod-affinity2zl9t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jan 18 21:46:46.919: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jan 18 21:46:46.919: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 21:46:46.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-6773 exec execpod-affinity2zl9t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.42.92 80'
Jan 18 21:46:47.130: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.42.92 80\nConnection to 10.233.42.92 80 port [tcp/http] succeeded!\n"
Jan 18 21:46:47.130: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 21:46:47.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-6773 exec execpod-affinity2zl9t -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.42.92:80/ ; done'
Jan 18 21:46:47.470: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n"
Jan 18 21:46:47.470: INFO: stdout: "\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh"
Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
Jan 18 21:46:47.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-6773 exec execpod-affinity2zl9t -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.42.92:80/'
Jan 18 21:46:47.665: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n"
Jan 18 21:46:47.665: INFO: stdout: "affinity-clusterip-timeout-gwhvh"
Jan 18 21:48:57.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-6773 exec execpod-affinity2zl9t -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.42.92:80/'
Jan 18 21:48:57.913: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n"
Jan 18 21:48:57.913: INFO: stdout: "affinity-clusterip-timeout-dz4xs"
Jan 18 21:48:57.913: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-6773, will wait for the garbage collector to delete the pods 01/18/23 21:48:57.948
Jan 18 21:48:58.027: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 9.285398ms
Jan 18 21:48:58.139: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 111.836809ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 21:49:00.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6773" for this suite. 01/18/23 21:49:00.978
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":87,"skipped":1833,"failed":0}
------------------------------
• [SLOW TEST] [142.758 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:46:38.244
    Jan 18 21:46:38.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename services 01/18/23 21:46:38.247
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:46:38.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:46:38.286
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-6773 01/18/23 21:46:38.29
    Jan 18 21:46:38.301: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-6773" to be "running and ready"
    Jan 18 21:46:38.307: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 6.819208ms
    Jan 18 21:46:38.308: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:46:40.313: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.012555366s
    Jan 18 21:46:40.314: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Jan 18 21:46:40.314: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Jan 18 21:46:40.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-6773 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Jan 18 21:46:40.541: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Jan 18 21:46:40.541: INFO: stdout: "ipvs"
    Jan 18 21:46:40.541: INFO: proxyMode: ipvs
    Jan 18 21:46:40.558: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Jan 18 21:46:40.562: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-6773 01/18/23 21:46:40.563
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-6773 01/18/23 21:46:40.583
    I0118 21:46:40.601218      19 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-6773, replica count: 3
    I0118 21:46:43.652720      19 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 21:46:43.665: INFO: Creating new exec pod
    Jan 18 21:46:43.672: INFO: Waiting up to 5m0s for pod "execpod-affinity2zl9t" in namespace "services-6773" to be "running"
    Jan 18 21:46:43.685: INFO: Pod "execpod-affinity2zl9t": Phase="Pending", Reason="", readiness=false. Elapsed: 12.587458ms
    Jan 18 21:46:45.690: INFO: Pod "execpod-affinity2zl9t": Phase="Running", Reason="", readiness=true. Elapsed: 2.017787309s
    Jan 18 21:46:45.690: INFO: Pod "execpod-affinity2zl9t" satisfied condition "running"
    Jan 18 21:46:46.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-6773 exec execpod-affinity2zl9t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Jan 18 21:46:46.919: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Jan 18 21:46:46.919: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 21:46:46.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-6773 exec execpod-affinity2zl9t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.42.92 80'
    Jan 18 21:46:47.130: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.42.92 80\nConnection to 10.233.42.92 80 port [tcp/http] succeeded!\n"
    Jan 18 21:46:47.130: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 21:46:47.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-6773 exec execpod-affinity2zl9t -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.42.92:80/ ; done'
    Jan 18 21:46:47.470: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n"
    Jan 18 21:46:47.470: INFO: stdout: "\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh\naffinity-clusterip-timeout-gwhvh"
    Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
    Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
    Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
    Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
    Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
    Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
    Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
    Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
    Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
    Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
    Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
    Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
    Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
    Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
    Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
    Jan 18 21:46:47.470: INFO: Received response from host: affinity-clusterip-timeout-gwhvh
    Jan 18 21:46:47.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-6773 exec execpod-affinity2zl9t -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.42.92:80/'
    Jan 18 21:46:47.665: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n"
    Jan 18 21:46:47.665: INFO: stdout: "affinity-clusterip-timeout-gwhvh"
    Jan 18 21:48:57.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-6773 exec execpod-affinity2zl9t -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.42.92:80/'
    Jan 18 21:48:57.913: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.42.92:80/\n"
    Jan 18 21:48:57.913: INFO: stdout: "affinity-clusterip-timeout-dz4xs"
    Jan 18 21:48:57.913: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-6773, will wait for the garbage collector to delete the pods 01/18/23 21:48:57.948
    Jan 18 21:48:58.027: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 9.285398ms
    Jan 18 21:48:58.139: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 111.836809ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 21:49:00.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6773" for this suite. 01/18/23 21:49:00.978
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:49:01.005
Jan 18 21:49:01.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename pods 01/18/23 21:49:01.009
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:49:01.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:49:01.044
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 01/18/23 21:49:01.055
Jan 18 21:49:01.070: INFO: Waiting up to 5m0s for pod "pod-xj4wz" in namespace "pods-1548" to be "running"
Jan 18 21:49:01.075: INFO: Pod "pod-xj4wz": Phase="Pending", Reason="", readiness=false. Elapsed: 4.742455ms
Jan 18 21:49:03.081: INFO: Pod "pod-xj4wz": Phase="Running", Reason="", readiness=true. Elapsed: 2.010801541s
Jan 18 21:49:03.081: INFO: Pod "pod-xj4wz" satisfied condition "running"
STEP: patching /status 01/18/23 21:49:03.081
Jan 18 21:49:03.099: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 21:49:03.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1548" for this suite. 01/18/23 21:49:03.111
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":88,"skipped":1841,"failed":0}
------------------------------
• [2.114 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:49:01.005
    Jan 18 21:49:01.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename pods 01/18/23 21:49:01.009
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:49:01.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:49:01.044
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 01/18/23 21:49:01.055
    Jan 18 21:49:01.070: INFO: Waiting up to 5m0s for pod "pod-xj4wz" in namespace "pods-1548" to be "running"
    Jan 18 21:49:01.075: INFO: Pod "pod-xj4wz": Phase="Pending", Reason="", readiness=false. Elapsed: 4.742455ms
    Jan 18 21:49:03.081: INFO: Pod "pod-xj4wz": Phase="Running", Reason="", readiness=true. Elapsed: 2.010801541s
    Jan 18 21:49:03.081: INFO: Pod "pod-xj4wz" satisfied condition "running"
    STEP: patching /status 01/18/23 21:49:03.081
    Jan 18 21:49:03.099: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 21:49:03.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1548" for this suite. 01/18/23 21:49:03.111
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:49:03.12
Jan 18 21:49:03.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename security-context 01/18/23 21:49:03.122
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:49:03.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:49:03.15
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/18/23 21:49:03.155
Jan 18 21:49:03.170: INFO: Waiting up to 5m0s for pod "security-context-e264b047-ca58-4258-aa82-205997656942" in namespace "security-context-4314" to be "Succeeded or Failed"
Jan 18 21:49:03.173: INFO: Pod "security-context-e264b047-ca58-4258-aa82-205997656942": Phase="Pending", Reason="", readiness=false. Elapsed: 3.154684ms
Jan 18 21:49:05.177: INFO: Pod "security-context-e264b047-ca58-4258-aa82-205997656942": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007841576s
Jan 18 21:49:07.179: INFO: Pod "security-context-e264b047-ca58-4258-aa82-205997656942": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009675844s
STEP: Saw pod success 01/18/23 21:49:07.179
Jan 18 21:49:07.180: INFO: Pod "security-context-e264b047-ca58-4258-aa82-205997656942" satisfied condition "Succeeded or Failed"
Jan 18 21:49:07.188: INFO: Trying to get logs from node v1-25-1-18760-w2 pod security-context-e264b047-ca58-4258-aa82-205997656942 container test-container: <nil>
STEP: delete the pod 01/18/23 21:49:07.206
Jan 18 21:49:07.219: INFO: Waiting for pod security-context-e264b047-ca58-4258-aa82-205997656942 to disappear
Jan 18 21:49:07.222: INFO: Pod security-context-e264b047-ca58-4258-aa82-205997656942 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 18 21:49:07.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-4314" for this suite. 01/18/23 21:49:07.227
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":89,"skipped":1849,"failed":0}
------------------------------
• [4.113 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:49:03.12
    Jan 18 21:49:03.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename security-context 01/18/23 21:49:03.122
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:49:03.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:49:03.15
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/18/23 21:49:03.155
    Jan 18 21:49:03.170: INFO: Waiting up to 5m0s for pod "security-context-e264b047-ca58-4258-aa82-205997656942" in namespace "security-context-4314" to be "Succeeded or Failed"
    Jan 18 21:49:03.173: INFO: Pod "security-context-e264b047-ca58-4258-aa82-205997656942": Phase="Pending", Reason="", readiness=false. Elapsed: 3.154684ms
    Jan 18 21:49:05.177: INFO: Pod "security-context-e264b047-ca58-4258-aa82-205997656942": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007841576s
    Jan 18 21:49:07.179: INFO: Pod "security-context-e264b047-ca58-4258-aa82-205997656942": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009675844s
    STEP: Saw pod success 01/18/23 21:49:07.179
    Jan 18 21:49:07.180: INFO: Pod "security-context-e264b047-ca58-4258-aa82-205997656942" satisfied condition "Succeeded or Failed"
    Jan 18 21:49:07.188: INFO: Trying to get logs from node v1-25-1-18760-w2 pod security-context-e264b047-ca58-4258-aa82-205997656942 container test-container: <nil>
    STEP: delete the pod 01/18/23 21:49:07.206
    Jan 18 21:49:07.219: INFO: Waiting for pod security-context-e264b047-ca58-4258-aa82-205997656942 to disappear
    Jan 18 21:49:07.222: INFO: Pod security-context-e264b047-ca58-4258-aa82-205997656942 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 18 21:49:07.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-4314" for this suite. 01/18/23 21:49:07.227
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:49:07.234
Jan 18 21:49:07.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename emptydir 01/18/23 21:49:07.235
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:49:07.259
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:49:07.265
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/18/23 21:49:07.27
Jan 18 21:49:07.283: INFO: Waiting up to 5m0s for pod "pod-50ecaf5e-a51c-45e3-8104-f5bbd0b95afa" in namespace "emptydir-3003" to be "Succeeded or Failed"
Jan 18 21:49:07.297: INFO: Pod "pod-50ecaf5e-a51c-45e3-8104-f5bbd0b95afa": Phase="Pending", Reason="", readiness=false. Elapsed: 14.003786ms
Jan 18 21:49:09.306: INFO: Pod "pod-50ecaf5e-a51c-45e3-8104-f5bbd0b95afa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023022215s
Jan 18 21:49:11.305: INFO: Pod "pod-50ecaf5e-a51c-45e3-8104-f5bbd0b95afa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022457937s
STEP: Saw pod success 01/18/23 21:49:11.306
Jan 18 21:49:11.306: INFO: Pod "pod-50ecaf5e-a51c-45e3-8104-f5bbd0b95afa" satisfied condition "Succeeded or Failed"
Jan 18 21:49:11.311: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-50ecaf5e-a51c-45e3-8104-f5bbd0b95afa container test-container: <nil>
STEP: delete the pod 01/18/23 21:49:11.324
Jan 18 21:49:11.339: INFO: Waiting for pod pod-50ecaf5e-a51c-45e3-8104-f5bbd0b95afa to disappear
Jan 18 21:49:11.344: INFO: Pod pod-50ecaf5e-a51c-45e3-8104-f5bbd0b95afa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 21:49:11.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3003" for this suite. 01/18/23 21:49:11.35
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":90,"skipped":1857,"failed":0}
------------------------------
• [4.124 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:49:07.234
    Jan 18 21:49:07.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename emptydir 01/18/23 21:49:07.235
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:49:07.259
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:49:07.265
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/18/23 21:49:07.27
    Jan 18 21:49:07.283: INFO: Waiting up to 5m0s for pod "pod-50ecaf5e-a51c-45e3-8104-f5bbd0b95afa" in namespace "emptydir-3003" to be "Succeeded or Failed"
    Jan 18 21:49:07.297: INFO: Pod "pod-50ecaf5e-a51c-45e3-8104-f5bbd0b95afa": Phase="Pending", Reason="", readiness=false. Elapsed: 14.003786ms
    Jan 18 21:49:09.306: INFO: Pod "pod-50ecaf5e-a51c-45e3-8104-f5bbd0b95afa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023022215s
    Jan 18 21:49:11.305: INFO: Pod "pod-50ecaf5e-a51c-45e3-8104-f5bbd0b95afa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022457937s
    STEP: Saw pod success 01/18/23 21:49:11.306
    Jan 18 21:49:11.306: INFO: Pod "pod-50ecaf5e-a51c-45e3-8104-f5bbd0b95afa" satisfied condition "Succeeded or Failed"
    Jan 18 21:49:11.311: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-50ecaf5e-a51c-45e3-8104-f5bbd0b95afa container test-container: <nil>
    STEP: delete the pod 01/18/23 21:49:11.324
    Jan 18 21:49:11.339: INFO: Waiting for pod pod-50ecaf5e-a51c-45e3-8104-f5bbd0b95afa to disappear
    Jan 18 21:49:11.344: INFO: Pod pod-50ecaf5e-a51c-45e3-8104-f5bbd0b95afa no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 21:49:11.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3003" for this suite. 01/18/23 21:49:11.35
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:49:11.359
Jan 18 21:49:11.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename kubelet-test 01/18/23 21:49:11.361
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:49:11.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:49:11.386
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 18 21:49:15.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1841" for this suite. 01/18/23 21:49:15.422
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":91,"skipped":1862,"failed":0}
------------------------------
• [4.073 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:49:11.359
    Jan 18 21:49:11.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename kubelet-test 01/18/23 21:49:11.361
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:49:11.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:49:11.386
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 18 21:49:15.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-1841" for this suite. 01/18/23 21:49:15.422
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:49:15.433
Jan 18 21:49:15.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename podtemplate 01/18/23 21:49:15.437
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:49:15.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:49:15.465
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 01/18/23 21:49:15.469
STEP: Replace a pod template 01/18/23 21:49:15.476
Jan 18 21:49:15.487: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan 18 21:49:15.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1536" for this suite. 01/18/23 21:49:15.498
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":92,"skipped":1868,"failed":0}
------------------------------
• [0.073 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:49:15.433
    Jan 18 21:49:15.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename podtemplate 01/18/23 21:49:15.437
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:49:15.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:49:15.465
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 01/18/23 21:49:15.469
    STEP: Replace a pod template 01/18/23 21:49:15.476
    Jan 18 21:49:15.487: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan 18 21:49:15.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-1536" for this suite. 01/18/23 21:49:15.498
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:49:15.511
Jan 18 21:49:15.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename resourcequota 01/18/23 21:49:15.512
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:49:15.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:49:15.534
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 01/18/23 21:49:32.546
STEP: Creating a ResourceQuota 01/18/23 21:49:37.551
STEP: Ensuring resource quota status is calculated 01/18/23 21:49:37.56
STEP: Creating a ConfigMap 01/18/23 21:49:39.567
STEP: Ensuring resource quota status captures configMap creation 01/18/23 21:49:39.581
STEP: Deleting a ConfigMap 01/18/23 21:49:41.588
STEP: Ensuring resource quota status released usage 01/18/23 21:49:41.597
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 21:49:43.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1661" for this suite. 01/18/23 21:49:43.614
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":93,"skipped":1876,"failed":0}
------------------------------
• [SLOW TEST] [28.110 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:49:15.511
    Jan 18 21:49:15.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename resourcequota 01/18/23 21:49:15.512
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:49:15.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:49:15.534
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 01/18/23 21:49:32.546
    STEP: Creating a ResourceQuota 01/18/23 21:49:37.551
    STEP: Ensuring resource quota status is calculated 01/18/23 21:49:37.56
    STEP: Creating a ConfigMap 01/18/23 21:49:39.567
    STEP: Ensuring resource quota status captures configMap creation 01/18/23 21:49:39.581
    STEP: Deleting a ConfigMap 01/18/23 21:49:41.588
    STEP: Ensuring resource quota status released usage 01/18/23 21:49:41.597
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 21:49:43.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1661" for this suite. 01/18/23 21:49:43.614
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:49:43.625
Jan 18 21:49:43.625: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename disruption 01/18/23 21:49:43.628
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:49:43.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:49:43.657
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 01/18/23 21:49:43.665
STEP: Waiting for the pdb to be processed 01/18/23 21:49:43.673
STEP: updating the pdb 01/18/23 21:49:45.686
STEP: Waiting for the pdb to be processed 01/18/23 21:49:45.698
STEP: patching the pdb 01/18/23 21:49:47.709
STEP: Waiting for the pdb to be processed 01/18/23 21:49:47.722
STEP: Waiting for the pdb to be deleted 01/18/23 21:49:49.737
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 18 21:49:49.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1041" for this suite. 01/18/23 21:49:49.748
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":94,"skipped":1876,"failed":0}
------------------------------
• [SLOW TEST] [6.131 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:49:43.625
    Jan 18 21:49:43.625: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename disruption 01/18/23 21:49:43.628
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:49:43.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:49:43.657
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 01/18/23 21:49:43.665
    STEP: Waiting for the pdb to be processed 01/18/23 21:49:43.673
    STEP: updating the pdb 01/18/23 21:49:45.686
    STEP: Waiting for the pdb to be processed 01/18/23 21:49:45.698
    STEP: patching the pdb 01/18/23 21:49:47.709
    STEP: Waiting for the pdb to be processed 01/18/23 21:49:47.722
    STEP: Waiting for the pdb to be deleted 01/18/23 21:49:49.737
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 18 21:49:49.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-1041" for this suite. 01/18/23 21:49:49.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:49:49.761
Jan 18 21:49:49.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename endpointslice 01/18/23 21:49:49.763
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:49:49.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:49:49.787
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 18 21:49:53.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8892" for this suite. 01/18/23 21:49:53.887
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":95,"skipped":1923,"failed":0}
------------------------------
• [4.133 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:49:49.761
    Jan 18 21:49:49.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename endpointslice 01/18/23 21:49:49.763
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:49:49.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:49:49.787
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 18 21:49:53.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-8892" for this suite. 01/18/23 21:49:53.887
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:49:53.898
Jan 18 21:49:53.898: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename dns 01/18/23 21:49:53.9
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:49:53.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:49:53.927
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 01/18/23 21:49:53.935
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3568.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3568.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3568.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3568.svc.cluster.local;sleep 1; done
 01/18/23 21:49:53.945
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3568.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3568.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3568.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3568.svc.cluster.local;sleep 1; done
 01/18/23 21:49:53.946
STEP: creating a pod to probe DNS 01/18/23 21:49:53.946
STEP: submitting the pod to kubernetes 01/18/23 21:49:53.946
Jan 18 21:49:53.973: INFO: Waiting up to 15m0s for pod "dns-test-d279aaf4-2024-4d86-956b-8643f0556c60" in namespace "dns-3568" to be "running"
Jan 18 21:49:53.981: INFO: Pod "dns-test-d279aaf4-2024-4d86-956b-8643f0556c60": Phase="Pending", Reason="", readiness=false. Elapsed: 8.40623ms
Jan 18 21:49:55.986: INFO: Pod "dns-test-d279aaf4-2024-4d86-956b-8643f0556c60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013817306s
Jan 18 21:49:57.986: INFO: Pod "dns-test-d279aaf4-2024-4d86-956b-8643f0556c60": Phase="Running", Reason="", readiness=true. Elapsed: 4.013601707s
Jan 18 21:49:57.987: INFO: Pod "dns-test-d279aaf4-2024-4d86-956b-8643f0556c60" satisfied condition "running"
STEP: retrieving the pod 01/18/23 21:49:57.987
STEP: looking for the results for each expected name from probers 01/18/23 21:49:57.991
Jan 18 21:49:57.996: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:49:58.001: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:49:58.007: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:49:58.014: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:49:58.019: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:49:58.024: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:49:58.030: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:49:58.035: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:49:58.035: INFO: Lookups using dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3568.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3568.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local jessie_udp@dns-test-service-2.dns-3568.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3568.svc.cluster.local]

Jan 18 21:50:03.042: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:03.048: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:03.053: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:03.059: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:03.064: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:03.070: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:03.074: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:03.079: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:03.079: INFO: Lookups using dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3568.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3568.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local jessie_udp@dns-test-service-2.dns-3568.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3568.svc.cluster.local]

Jan 18 21:50:08.040: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:08.056: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:08.063: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:08.068: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:08.073: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:08.078: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:08.083: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:08.095: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:08.095: INFO: Lookups using dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3568.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3568.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local jessie_udp@dns-test-service-2.dns-3568.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3568.svc.cluster.local]

Jan 18 21:50:13.042: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:13.048: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:13.052: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:13.057: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:13.062: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:13.066: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:13.071: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:13.077: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:13.077: INFO: Lookups using dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3568.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3568.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local jessie_udp@dns-test-service-2.dns-3568.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3568.svc.cluster.local]

Jan 18 21:50:18.042: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:18.047: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:18.052: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:18.057: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:18.062: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:18.067: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:18.072: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:18.077: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:18.077: INFO: Lookups using dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3568.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3568.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local jessie_udp@dns-test-service-2.dns-3568.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3568.svc.cluster.local]

Jan 18 21:50:23.041: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:23.046: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:23.051: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:23.056: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:23.061: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:23.066: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:23.072: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:23.077: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
Jan 18 21:50:23.077: INFO: Lookups using dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3568.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3568.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local jessie_udp@dns-test-service-2.dns-3568.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3568.svc.cluster.local]

Jan 18 21:50:28.075: INFO: DNS probes using dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60 succeeded

STEP: deleting the pod 01/18/23 21:50:28.075
STEP: deleting the test headless service 01/18/23 21:50:28.106
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 18 21:50:28.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3568" for this suite. 01/18/23 21:50:28.165
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":96,"skipped":1925,"failed":0}
------------------------------
• [SLOW TEST] [34.285 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:49:53.898
    Jan 18 21:49:53.898: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename dns 01/18/23 21:49:53.9
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:49:53.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:49:53.927
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 01/18/23 21:49:53.935
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3568.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3568.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3568.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3568.svc.cluster.local;sleep 1; done
     01/18/23 21:49:53.945
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3568.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3568.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3568.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3568.svc.cluster.local;sleep 1; done
     01/18/23 21:49:53.946
    STEP: creating a pod to probe DNS 01/18/23 21:49:53.946
    STEP: submitting the pod to kubernetes 01/18/23 21:49:53.946
    Jan 18 21:49:53.973: INFO: Waiting up to 15m0s for pod "dns-test-d279aaf4-2024-4d86-956b-8643f0556c60" in namespace "dns-3568" to be "running"
    Jan 18 21:49:53.981: INFO: Pod "dns-test-d279aaf4-2024-4d86-956b-8643f0556c60": Phase="Pending", Reason="", readiness=false. Elapsed: 8.40623ms
    Jan 18 21:49:55.986: INFO: Pod "dns-test-d279aaf4-2024-4d86-956b-8643f0556c60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013817306s
    Jan 18 21:49:57.986: INFO: Pod "dns-test-d279aaf4-2024-4d86-956b-8643f0556c60": Phase="Running", Reason="", readiness=true. Elapsed: 4.013601707s
    Jan 18 21:49:57.987: INFO: Pod "dns-test-d279aaf4-2024-4d86-956b-8643f0556c60" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 21:49:57.987
    STEP: looking for the results for each expected name from probers 01/18/23 21:49:57.991
    Jan 18 21:49:57.996: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:49:58.001: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:49:58.007: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:49:58.014: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:49:58.019: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:49:58.024: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:49:58.030: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:49:58.035: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:49:58.035: INFO: Lookups using dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3568.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3568.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local jessie_udp@dns-test-service-2.dns-3568.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3568.svc.cluster.local]

    Jan 18 21:50:03.042: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:03.048: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:03.053: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:03.059: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:03.064: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:03.070: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:03.074: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:03.079: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:03.079: INFO: Lookups using dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3568.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3568.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local jessie_udp@dns-test-service-2.dns-3568.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3568.svc.cluster.local]

    Jan 18 21:50:08.040: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:08.056: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:08.063: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:08.068: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:08.073: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:08.078: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:08.083: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:08.095: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:08.095: INFO: Lookups using dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3568.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3568.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local jessie_udp@dns-test-service-2.dns-3568.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3568.svc.cluster.local]

    Jan 18 21:50:13.042: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:13.048: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:13.052: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:13.057: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:13.062: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:13.066: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:13.071: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:13.077: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:13.077: INFO: Lookups using dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3568.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3568.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local jessie_udp@dns-test-service-2.dns-3568.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3568.svc.cluster.local]

    Jan 18 21:50:18.042: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:18.047: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:18.052: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:18.057: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:18.062: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:18.067: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:18.072: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:18.077: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:18.077: INFO: Lookups using dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3568.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3568.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local jessie_udp@dns-test-service-2.dns-3568.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3568.svc.cluster.local]

    Jan 18 21:50:23.041: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:23.046: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:23.051: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:23.056: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:23.061: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:23.066: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:23.072: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:23.077: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3568.svc.cluster.local from pod dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60: the server could not find the requested resource (get pods dns-test-d279aaf4-2024-4d86-956b-8643f0556c60)
    Jan 18 21:50:23.077: INFO: Lookups using dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3568.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3568.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3568.svc.cluster.local jessie_udp@dns-test-service-2.dns-3568.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3568.svc.cluster.local]

    Jan 18 21:50:28.075: INFO: DNS probes using dns-3568/dns-test-d279aaf4-2024-4d86-956b-8643f0556c60 succeeded

    STEP: deleting the pod 01/18/23 21:50:28.075
    STEP: deleting the test headless service 01/18/23 21:50:28.106
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 18 21:50:28.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3568" for this suite. 01/18/23 21:50:28.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:50:28.195
Jan 18 21:50:28.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename events 01/18/23 21:50:28.198
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:28.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:28.255
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 01/18/23 21:50:28.259
STEP: listing events in all namespaces 01/18/23 21:50:28.267
STEP: listing events in test namespace 01/18/23 21:50:28.274
STEP: listing events with field selection filtering on source 01/18/23 21:50:28.281
STEP: listing events with field selection filtering on reportingController 01/18/23 21:50:28.288
STEP: getting the test event 01/18/23 21:50:28.292
STEP: patching the test event 01/18/23 21:50:28.295
STEP: getting the test event 01/18/23 21:50:28.303
STEP: updating the test event 01/18/23 21:50:28.313
STEP: getting the test event 01/18/23 21:50:28.333
STEP: deleting the test event 01/18/23 21:50:28.337
STEP: listing events in all namespaces 01/18/23 21:50:28.355
STEP: listing events in test namespace 01/18/23 21:50:28.364
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jan 18 21:50:28.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9269" for this suite. 01/18/23 21:50:28.379
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":97,"skipped":1999,"failed":0}
------------------------------
• [0.193 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:50:28.195
    Jan 18 21:50:28.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename events 01/18/23 21:50:28.198
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:28.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:28.255
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 01/18/23 21:50:28.259
    STEP: listing events in all namespaces 01/18/23 21:50:28.267
    STEP: listing events in test namespace 01/18/23 21:50:28.274
    STEP: listing events with field selection filtering on source 01/18/23 21:50:28.281
    STEP: listing events with field selection filtering on reportingController 01/18/23 21:50:28.288
    STEP: getting the test event 01/18/23 21:50:28.292
    STEP: patching the test event 01/18/23 21:50:28.295
    STEP: getting the test event 01/18/23 21:50:28.303
    STEP: updating the test event 01/18/23 21:50:28.313
    STEP: getting the test event 01/18/23 21:50:28.333
    STEP: deleting the test event 01/18/23 21:50:28.337
    STEP: listing events in all namespaces 01/18/23 21:50:28.355
    STEP: listing events in test namespace 01/18/23 21:50:28.364
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jan 18 21:50:28.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-9269" for this suite. 01/18/23 21:50:28.379
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:50:28.391
Jan 18 21:50:28.391: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename configmap 01/18/23 21:50:28.393
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:28.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:28.421
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 01/18/23 21:50:28.426
STEP: fetching the ConfigMap 01/18/23 21:50:28.432
STEP: patching the ConfigMap 01/18/23 21:50:28.437
STEP: listing all ConfigMaps in all namespaces with a label selector 01/18/23 21:50:28.444
STEP: deleting the ConfigMap by collection with a label selector 01/18/23 21:50:28.451
STEP: listing all ConfigMaps in test namespace 01/18/23 21:50:28.461
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 21:50:28.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7168" for this suite. 01/18/23 21:50:28.472
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":98,"skipped":2017,"failed":0}
------------------------------
• [0.090 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:50:28.391
    Jan 18 21:50:28.391: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename configmap 01/18/23 21:50:28.393
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:28.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:28.421
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 01/18/23 21:50:28.426
    STEP: fetching the ConfigMap 01/18/23 21:50:28.432
    STEP: patching the ConfigMap 01/18/23 21:50:28.437
    STEP: listing all ConfigMaps in all namespaces with a label selector 01/18/23 21:50:28.444
    STEP: deleting the ConfigMap by collection with a label selector 01/18/23 21:50:28.451
    STEP: listing all ConfigMaps in test namespace 01/18/23 21:50:28.461
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 21:50:28.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7168" for this suite. 01/18/23 21:50:28.472
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:50:28.487
Jan 18 21:50:28.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 21:50:28.489
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:28.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:28.518
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 01/18/23 21:50:28.522
Jan 18 21:50:28.532: INFO: Waiting up to 5m0s for pod "downwardapi-volume-38e12474-7d73-4bcb-b380-106dd6b7e8d4" in namespace "projected-9751" to be "Succeeded or Failed"
Jan 18 21:50:28.542: INFO: Pod "downwardapi-volume-38e12474-7d73-4bcb-b380-106dd6b7e8d4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.682154ms
Jan 18 21:50:30.548: INFO: Pod "downwardapi-volume-38e12474-7d73-4bcb-b380-106dd6b7e8d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015700188s
Jan 18 21:50:32.548: INFO: Pod "downwardapi-volume-38e12474-7d73-4bcb-b380-106dd6b7e8d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015436097s
STEP: Saw pod success 01/18/23 21:50:32.548
Jan 18 21:50:32.548: INFO: Pod "downwardapi-volume-38e12474-7d73-4bcb-b380-106dd6b7e8d4" satisfied condition "Succeeded or Failed"
Jan 18 21:50:32.554: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-38e12474-7d73-4bcb-b380-106dd6b7e8d4 container client-container: <nil>
STEP: delete the pod 01/18/23 21:50:32.568
Jan 18 21:50:32.590: INFO: Waiting for pod downwardapi-volume-38e12474-7d73-4bcb-b380-106dd6b7e8d4 to disappear
Jan 18 21:50:32.594: INFO: Pod downwardapi-volume-38e12474-7d73-4bcb-b380-106dd6b7e8d4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 21:50:32.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9751" for this suite. 01/18/23 21:50:32.636
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":99,"skipped":2041,"failed":0}
------------------------------
• [4.158 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:50:28.487
    Jan 18 21:50:28.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 21:50:28.489
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:28.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:28.518
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 01/18/23 21:50:28.522
    Jan 18 21:50:28.532: INFO: Waiting up to 5m0s for pod "downwardapi-volume-38e12474-7d73-4bcb-b380-106dd6b7e8d4" in namespace "projected-9751" to be "Succeeded or Failed"
    Jan 18 21:50:28.542: INFO: Pod "downwardapi-volume-38e12474-7d73-4bcb-b380-106dd6b7e8d4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.682154ms
    Jan 18 21:50:30.548: INFO: Pod "downwardapi-volume-38e12474-7d73-4bcb-b380-106dd6b7e8d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015700188s
    Jan 18 21:50:32.548: INFO: Pod "downwardapi-volume-38e12474-7d73-4bcb-b380-106dd6b7e8d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015436097s
    STEP: Saw pod success 01/18/23 21:50:32.548
    Jan 18 21:50:32.548: INFO: Pod "downwardapi-volume-38e12474-7d73-4bcb-b380-106dd6b7e8d4" satisfied condition "Succeeded or Failed"
    Jan 18 21:50:32.554: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-38e12474-7d73-4bcb-b380-106dd6b7e8d4 container client-container: <nil>
    STEP: delete the pod 01/18/23 21:50:32.568
    Jan 18 21:50:32.590: INFO: Waiting for pod downwardapi-volume-38e12474-7d73-4bcb-b380-106dd6b7e8d4 to disappear
    Jan 18 21:50:32.594: INFO: Pod downwardapi-volume-38e12474-7d73-4bcb-b380-106dd6b7e8d4 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 21:50:32.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9751" for this suite. 01/18/23 21:50:32.636
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:50:32.646
Jan 18 21:50:32.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename emptydir 01/18/23 21:50:32.647
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:32.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:32.667
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 01/18/23 21:50:32.672
Jan 18 21:50:32.684: INFO: Waiting up to 5m0s for pod "pod-bf8ff287-20a6-43eb-a8b6-103767c5a3c0" in namespace "emptydir-1097" to be "Succeeded or Failed"
Jan 18 21:50:32.692: INFO: Pod "pod-bf8ff287-20a6-43eb-a8b6-103767c5a3c0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.938607ms
Jan 18 21:50:34.699: INFO: Pod "pod-bf8ff287-20a6-43eb-a8b6-103767c5a3c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014375153s
Jan 18 21:50:36.698: INFO: Pod "pod-bf8ff287-20a6-43eb-a8b6-103767c5a3c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01327646s
STEP: Saw pod success 01/18/23 21:50:36.698
Jan 18 21:50:36.699: INFO: Pod "pod-bf8ff287-20a6-43eb-a8b6-103767c5a3c0" satisfied condition "Succeeded or Failed"
Jan 18 21:50:36.708: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-bf8ff287-20a6-43eb-a8b6-103767c5a3c0 container test-container: <nil>
STEP: delete the pod 01/18/23 21:50:36.72
Jan 18 21:50:36.734: INFO: Waiting for pod pod-bf8ff287-20a6-43eb-a8b6-103767c5a3c0 to disappear
Jan 18 21:50:36.739: INFO: Pod pod-bf8ff287-20a6-43eb-a8b6-103767c5a3c0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 21:50:36.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1097" for this suite. 01/18/23 21:50:36.755
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":100,"skipped":2047,"failed":0}
------------------------------
• [4.120 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:50:32.646
    Jan 18 21:50:32.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename emptydir 01/18/23 21:50:32.647
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:32.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:32.667
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/18/23 21:50:32.672
    Jan 18 21:50:32.684: INFO: Waiting up to 5m0s for pod "pod-bf8ff287-20a6-43eb-a8b6-103767c5a3c0" in namespace "emptydir-1097" to be "Succeeded or Failed"
    Jan 18 21:50:32.692: INFO: Pod "pod-bf8ff287-20a6-43eb-a8b6-103767c5a3c0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.938607ms
    Jan 18 21:50:34.699: INFO: Pod "pod-bf8ff287-20a6-43eb-a8b6-103767c5a3c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014375153s
    Jan 18 21:50:36.698: INFO: Pod "pod-bf8ff287-20a6-43eb-a8b6-103767c5a3c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01327646s
    STEP: Saw pod success 01/18/23 21:50:36.698
    Jan 18 21:50:36.699: INFO: Pod "pod-bf8ff287-20a6-43eb-a8b6-103767c5a3c0" satisfied condition "Succeeded or Failed"
    Jan 18 21:50:36.708: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-bf8ff287-20a6-43eb-a8b6-103767c5a3c0 container test-container: <nil>
    STEP: delete the pod 01/18/23 21:50:36.72
    Jan 18 21:50:36.734: INFO: Waiting for pod pod-bf8ff287-20a6-43eb-a8b6-103767c5a3c0 to disappear
    Jan 18 21:50:36.739: INFO: Pod pod-bf8ff287-20a6-43eb-a8b6-103767c5a3c0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 21:50:36.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1097" for this suite. 01/18/23 21:50:36.755
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:50:36.772
Jan 18 21:50:36.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename svcaccounts 01/18/23 21:50:36.774
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:36.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:36.805
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Jan 18 21:50:36.847: INFO: Waiting up to 5m0s for pod "pod-service-account-5685835e-9fbe-48f7-b443-f640dea48af7" in namespace "svcaccounts-9868" to be "running"
Jan 18 21:50:36.866: INFO: Pod "pod-service-account-5685835e-9fbe-48f7-b443-f640dea48af7": Phase="Pending", Reason="", readiness=false. Elapsed: 19.151488ms
Jan 18 21:50:38.871: INFO: Pod "pod-service-account-5685835e-9fbe-48f7-b443-f640dea48af7": Phase="Running", Reason="", readiness=true. Elapsed: 2.024460432s
Jan 18 21:50:38.871: INFO: Pod "pod-service-account-5685835e-9fbe-48f7-b443-f640dea48af7" satisfied condition "running"
STEP: reading a file in the container 01/18/23 21:50:38.871
Jan 18 21:50:38.872: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9868 pod-service-account-5685835e-9fbe-48f7-b443-f640dea48af7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 01/18/23 21:50:39.094
Jan 18 21:50:39.094: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9868 pod-service-account-5685835e-9fbe-48f7-b443-f640dea48af7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 01/18/23 21:50:39.342
Jan 18 21:50:39.343: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9868 pod-service-account-5685835e-9fbe-48f7-b443-f640dea48af7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan 18 21:50:39.531: INFO: Got root ca configmap in namespace "svcaccounts-9868"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 18 21:50:39.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9868" for this suite. 01/18/23 21:50:39.54
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":101,"skipped":2063,"failed":0}
------------------------------
• [2.775 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:50:36.772
    Jan 18 21:50:36.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 21:50:36.774
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:36.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:36.805
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Jan 18 21:50:36.847: INFO: Waiting up to 5m0s for pod "pod-service-account-5685835e-9fbe-48f7-b443-f640dea48af7" in namespace "svcaccounts-9868" to be "running"
    Jan 18 21:50:36.866: INFO: Pod "pod-service-account-5685835e-9fbe-48f7-b443-f640dea48af7": Phase="Pending", Reason="", readiness=false. Elapsed: 19.151488ms
    Jan 18 21:50:38.871: INFO: Pod "pod-service-account-5685835e-9fbe-48f7-b443-f640dea48af7": Phase="Running", Reason="", readiness=true. Elapsed: 2.024460432s
    Jan 18 21:50:38.871: INFO: Pod "pod-service-account-5685835e-9fbe-48f7-b443-f640dea48af7" satisfied condition "running"
    STEP: reading a file in the container 01/18/23 21:50:38.871
    Jan 18 21:50:38.872: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9868 pod-service-account-5685835e-9fbe-48f7-b443-f640dea48af7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 01/18/23 21:50:39.094
    Jan 18 21:50:39.094: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9868 pod-service-account-5685835e-9fbe-48f7-b443-f640dea48af7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 01/18/23 21:50:39.342
    Jan 18 21:50:39.343: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9868 pod-service-account-5685835e-9fbe-48f7-b443-f640dea48af7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jan 18 21:50:39.531: INFO: Got root ca configmap in namespace "svcaccounts-9868"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 18 21:50:39.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-9868" for this suite. 01/18/23 21:50:39.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:50:39.553
Jan 18 21:50:39.553: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename replicaset 01/18/23 21:50:39.555
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:39.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:39.588
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 01/18/23 21:50:39.593
STEP: Verify that the required pods have come up 01/18/23 21:50:39.602
Jan 18 21:50:39.607: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan 18 21:50:44.620: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 01/18/23 21:50:44.62
Jan 18 21:50:44.625: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 01/18/23 21:50:44.625
STEP: DeleteCollection of the ReplicaSets 01/18/23 21:50:44.63
STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/18/23 21:50:44.64
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 18 21:50:44.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6391" for this suite. 01/18/23 21:50:44.655
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":102,"skipped":2105,"failed":0}
------------------------------
• [SLOW TEST] [5.151 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:50:39.553
    Jan 18 21:50:39.553: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename replicaset 01/18/23 21:50:39.555
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:39.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:39.588
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 01/18/23 21:50:39.593
    STEP: Verify that the required pods have come up 01/18/23 21:50:39.602
    Jan 18 21:50:39.607: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jan 18 21:50:44.620: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 01/18/23 21:50:44.62
    Jan 18 21:50:44.625: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 01/18/23 21:50:44.625
    STEP: DeleteCollection of the ReplicaSets 01/18/23 21:50:44.63
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/18/23 21:50:44.64
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 18 21:50:44.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-6391" for this suite. 01/18/23 21:50:44.655
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:50:44.708
Jan 18 21:50:44.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename sysctl 01/18/23 21:50:44.71
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:44.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:44.763
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 01/18/23 21:50:44.782
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 18 21:50:44.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-9111" for this suite. 01/18/23 21:50:44.808
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":103,"skipped":2132,"failed":0}
------------------------------
• [0.120 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:50:44.708
    Jan 18 21:50:44.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename sysctl 01/18/23 21:50:44.71
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:44.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:44.763
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 01/18/23 21:50:44.782
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 18 21:50:44.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-9111" for this suite. 01/18/23 21:50:44.808
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:50:44.831
Jan 18 21:50:44.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename daemonsets 01/18/23 21:50:44.833
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:44.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:44.865
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 01/18/23 21:50:44.949
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 21:50:44.962
Jan 18 21:50:44.972: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:50:44.983: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:50:44.983: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 21:50:45.997: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:50:46.011: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:50:46.011: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 21:50:46.997: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:50:47.010: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 21:50:47.010: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
Jan 18 21:50:47.989: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:50:47.994: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 21:50:47.994: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 01/18/23 21:50:48.001
Jan 18 21:50:48.006: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 01/18/23 21:50:48.006
Jan 18 21:50:48.022: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 01/18/23 21:50:48.023
Jan 18 21:50:48.026: INFO: Observed &DaemonSet event: ADDED
Jan 18 21:50:48.027: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 21:50:48.030: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 21:50:48.031: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 21:50:48.031: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 21:50:48.031: INFO: Found daemon set daemon-set in namespace daemonsets-9246 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 18 21:50:48.031: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 01/18/23 21:50:48.031
STEP: watching for the daemon set status to be patched 01/18/23 21:50:48.04
Jan 18 21:50:48.044: INFO: Observed &DaemonSet event: ADDED
Jan 18 21:50:48.044: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 21:50:48.045: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 21:50:48.045: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 21:50:48.045: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 21:50:48.046: INFO: Observed daemon set daemon-set in namespace daemonsets-9246 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 18 21:50:48.046: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 21:50:48.046: INFO: Found daemon set daemon-set in namespace daemonsets-9246 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan 18 21:50:48.046: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/18/23 21:50:48.051
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9246, will wait for the garbage collector to delete the pods 01/18/23 21:50:48.051
Jan 18 21:50:48.112: INFO: Deleting DaemonSet.extensions daemon-set took: 5.892296ms
Jan 18 21:50:48.213: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.358872ms
Jan 18 21:50:50.419: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:50:50.419: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 21:50:50.423: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"142951"},"items":null}

Jan 18 21:50:50.427: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"142951"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 18 21:50:50.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9246" for this suite. 01/18/23 21:50:50.452
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":104,"skipped":2138,"failed":0}
------------------------------
• [SLOW TEST] [5.627 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:50:44.831
    Jan 18 21:50:44.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename daemonsets 01/18/23 21:50:44.833
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:44.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:44.865
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 01/18/23 21:50:44.949
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 21:50:44.962
    Jan 18 21:50:44.972: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:50:44.983: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:50:44.983: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 21:50:45.997: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:50:46.011: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:50:46.011: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 21:50:46.997: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:50:47.010: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 21:50:47.010: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
    Jan 18 21:50:47.989: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:50:47.994: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 21:50:47.994: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 01/18/23 21:50:48.001
    Jan 18 21:50:48.006: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 01/18/23 21:50:48.006
    Jan 18 21:50:48.022: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 01/18/23 21:50:48.023
    Jan 18 21:50:48.026: INFO: Observed &DaemonSet event: ADDED
    Jan 18 21:50:48.027: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 21:50:48.030: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 21:50:48.031: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 21:50:48.031: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 21:50:48.031: INFO: Found daemon set daemon-set in namespace daemonsets-9246 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 18 21:50:48.031: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 01/18/23 21:50:48.031
    STEP: watching for the daemon set status to be patched 01/18/23 21:50:48.04
    Jan 18 21:50:48.044: INFO: Observed &DaemonSet event: ADDED
    Jan 18 21:50:48.044: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 21:50:48.045: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 21:50:48.045: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 21:50:48.045: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 21:50:48.046: INFO: Observed daemon set daemon-set in namespace daemonsets-9246 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 18 21:50:48.046: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 21:50:48.046: INFO: Found daemon set daemon-set in namespace daemonsets-9246 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jan 18 21:50:48.046: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 21:50:48.051
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9246, will wait for the garbage collector to delete the pods 01/18/23 21:50:48.051
    Jan 18 21:50:48.112: INFO: Deleting DaemonSet.extensions daemon-set took: 5.892296ms
    Jan 18 21:50:48.213: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.358872ms
    Jan 18 21:50:50.419: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:50:50.419: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 21:50:50.423: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"142951"},"items":null}

    Jan 18 21:50:50.427: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"142951"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 21:50:50.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-9246" for this suite. 01/18/23 21:50:50.452
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:50:50.47
Jan 18 21:50:50.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename webhook 01/18/23 21:50:50.472
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:50.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:50.498
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 21:50:50.525
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:50:51.1
STEP: Deploying the webhook pod 01/18/23 21:50:51.11
STEP: Wait for the deployment to be ready 01/18/23 21:50:51.123
Jan 18 21:50:51.148: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 21:50:53.16
STEP: Verifying the service has paired with the endpoint 01/18/23 21:50:53.176
Jan 18 21:50:54.177: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Jan 18 21:50:54.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/18/23 21:50:54.7
STEP: Creating a custom resource that should be denied by the webhook 01/18/23 21:50:54.719
STEP: Creating a custom resource whose deletion would be denied by the webhook 01/18/23 21:50:56.775
STEP: Updating the custom resource with disallowed data should be denied 01/18/23 21:50:56.788
STEP: Deleting the custom resource should be denied 01/18/23 21:50:56.81
STEP: Remove the offending key and value from the custom resource data 01/18/23 21:50:56.825
STEP: Deleting the updated custom resource should be successful 01/18/23 21:50:56.844
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 21:50:57.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-703" for this suite. 01/18/23 21:50:57.404
STEP: Destroying namespace "webhook-703-markers" for this suite. 01/18/23 21:50:57.418
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":105,"skipped":2170,"failed":0}
------------------------------
• [SLOW TEST] [7.031 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:50:50.47
    Jan 18 21:50:50.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename webhook 01/18/23 21:50:50.472
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:50.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:50.498
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 21:50:50.525
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:50:51.1
    STEP: Deploying the webhook pod 01/18/23 21:50:51.11
    STEP: Wait for the deployment to be ready 01/18/23 21:50:51.123
    Jan 18 21:50:51.148: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 21:50:53.16
    STEP: Verifying the service has paired with the endpoint 01/18/23 21:50:53.176
    Jan 18 21:50:54.177: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Jan 18 21:50:54.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/18/23 21:50:54.7
    STEP: Creating a custom resource that should be denied by the webhook 01/18/23 21:50:54.719
    STEP: Creating a custom resource whose deletion would be denied by the webhook 01/18/23 21:50:56.775
    STEP: Updating the custom resource with disallowed data should be denied 01/18/23 21:50:56.788
    STEP: Deleting the custom resource should be denied 01/18/23 21:50:56.81
    STEP: Remove the offending key and value from the custom resource data 01/18/23 21:50:56.825
    STEP: Deleting the updated custom resource should be successful 01/18/23 21:50:56.844
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 21:50:57.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-703" for this suite. 01/18/23 21:50:57.404
    STEP: Destroying namespace "webhook-703-markers" for this suite. 01/18/23 21:50:57.418
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:50:57.503
Jan 18 21:50:57.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename runtimeclass 01/18/23 21:50:57.505
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:57.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:57.565
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jan 18 21:50:57.591: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3378 to be scheduled
Jan 18 21:50:57.625: INFO: 1 pods are not scheduled: [runtimeclass-3378/test-runtimeclass-runtimeclass-3378-preconfigured-handler-m4jkq(e057c55a-c3e5-4e3b-8961-58bd60a54f6f)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 18 21:50:59.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3378" for this suite. 01/18/23 21:50:59.643
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":106,"skipped":2170,"failed":0}
------------------------------
• [2.146 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:50:57.503
    Jan 18 21:50:57.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename runtimeclass 01/18/23 21:50:57.505
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:57.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:57.565
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jan 18 21:50:57.591: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3378 to be scheduled
    Jan 18 21:50:57.625: INFO: 1 pods are not scheduled: [runtimeclass-3378/test-runtimeclass-runtimeclass-3378-preconfigured-handler-m4jkq(e057c55a-c3e5-4e3b-8961-58bd60a54f6f)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 18 21:50:59.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3378" for this suite. 01/18/23 21:50:59.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:50:59.652
Jan 18 21:50:59.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename services 01/18/23 21:50:59.654
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:59.675
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:59.679
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 01/18/23 21:50:59.689
STEP: watching for the Service to be added 01/18/23 21:50:59.699
Jan 18 21:50:59.702: INFO: Found Service test-service-kgtz4 in namespace services-1109 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan 18 21:50:59.702: INFO: Service test-service-kgtz4 created
STEP: Getting /status 01/18/23 21:50:59.702
Jan 18 21:50:59.708: INFO: Service test-service-kgtz4 has LoadBalancer: {[]}
STEP: patching the ServiceStatus 01/18/23 21:50:59.709
STEP: watching for the Service to be patched 01/18/23 21:50:59.735
Jan 18 21:50:59.739: INFO: observed Service test-service-kgtz4 in namespace services-1109 with annotations: map[] & LoadBalancer: {[]}
Jan 18 21:50:59.739: INFO: Found Service test-service-kgtz4 in namespace services-1109 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan 18 21:50:59.739: INFO: Service test-service-kgtz4 has service status patched
STEP: updating the ServiceStatus 01/18/23 21:50:59.74
Jan 18 21:50:59.953: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 01/18/23 21:50:59.953
Jan 18 21:50:59.956: INFO: Observed Service test-service-kgtz4 in namespace services-1109 with annotations: map[] & Conditions: {[]}
Jan 18 21:50:59.956: INFO: Observed event: &Service{ObjectMeta:{test-service-kgtz4  services-1109  abcef4ae-af62-4710-a5dd-39be1252b114 143083 0 2023-01-18 21:50:59 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-18 21:50:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-18 21:50:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.31.11,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.31.11],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan 18 21:50:59.957: INFO: Found Service test-service-kgtz4 in namespace services-1109 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 18 21:50:59.957: INFO: Service test-service-kgtz4 has service status updated
STEP: patching the service 01/18/23 21:50:59.957
STEP: watching for the Service to be patched 01/18/23 21:50:59.971
Jan 18 21:50:59.976: INFO: observed Service test-service-kgtz4 in namespace services-1109 with labels: map[test-service-static:true]
Jan 18 21:50:59.976: INFO: observed Service test-service-kgtz4 in namespace services-1109 with labels: map[test-service-static:true]
Jan 18 21:50:59.976: INFO: observed Service test-service-kgtz4 in namespace services-1109 with labels: map[test-service-static:true]
Jan 18 21:50:59.976: INFO: Found Service test-service-kgtz4 in namespace services-1109 with labels: map[test-service:patched test-service-static:true]
Jan 18 21:50:59.976: INFO: Service test-service-kgtz4 patched
STEP: deleting the service 01/18/23 21:50:59.976
STEP: watching for the Service to be deleted 01/18/23 21:50:59.992
Jan 18 21:50:59.995: INFO: Observed event: ADDED
Jan 18 21:50:59.995: INFO: Observed event: MODIFIED
Jan 18 21:50:59.995: INFO: Observed event: MODIFIED
Jan 18 21:50:59.995: INFO: Observed event: MODIFIED
Jan 18 21:50:59.995: INFO: Found Service test-service-kgtz4 in namespace services-1109 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan 18 21:50:59.995: INFO: Service test-service-kgtz4 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 21:50:59.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1109" for this suite. 01/18/23 21:51:00.001
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":107,"skipped":2175,"failed":0}
------------------------------
• [0.358 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:50:59.652
    Jan 18 21:50:59.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename services 01/18/23 21:50:59.654
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:59.675
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:59.679
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 01/18/23 21:50:59.689
    STEP: watching for the Service to be added 01/18/23 21:50:59.699
    Jan 18 21:50:59.702: INFO: Found Service test-service-kgtz4 in namespace services-1109 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jan 18 21:50:59.702: INFO: Service test-service-kgtz4 created
    STEP: Getting /status 01/18/23 21:50:59.702
    Jan 18 21:50:59.708: INFO: Service test-service-kgtz4 has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 01/18/23 21:50:59.709
    STEP: watching for the Service to be patched 01/18/23 21:50:59.735
    Jan 18 21:50:59.739: INFO: observed Service test-service-kgtz4 in namespace services-1109 with annotations: map[] & LoadBalancer: {[]}
    Jan 18 21:50:59.739: INFO: Found Service test-service-kgtz4 in namespace services-1109 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jan 18 21:50:59.739: INFO: Service test-service-kgtz4 has service status patched
    STEP: updating the ServiceStatus 01/18/23 21:50:59.74
    Jan 18 21:50:59.953: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 01/18/23 21:50:59.953
    Jan 18 21:50:59.956: INFO: Observed Service test-service-kgtz4 in namespace services-1109 with annotations: map[] & Conditions: {[]}
    Jan 18 21:50:59.956: INFO: Observed event: &Service{ObjectMeta:{test-service-kgtz4  services-1109  abcef4ae-af62-4710-a5dd-39be1252b114 143083 0 2023-01-18 21:50:59 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-18 21:50:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-18 21:50:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.31.11,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.31.11],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jan 18 21:50:59.957: INFO: Found Service test-service-kgtz4 in namespace services-1109 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 18 21:50:59.957: INFO: Service test-service-kgtz4 has service status updated
    STEP: patching the service 01/18/23 21:50:59.957
    STEP: watching for the Service to be patched 01/18/23 21:50:59.971
    Jan 18 21:50:59.976: INFO: observed Service test-service-kgtz4 in namespace services-1109 with labels: map[test-service-static:true]
    Jan 18 21:50:59.976: INFO: observed Service test-service-kgtz4 in namespace services-1109 with labels: map[test-service-static:true]
    Jan 18 21:50:59.976: INFO: observed Service test-service-kgtz4 in namespace services-1109 with labels: map[test-service-static:true]
    Jan 18 21:50:59.976: INFO: Found Service test-service-kgtz4 in namespace services-1109 with labels: map[test-service:patched test-service-static:true]
    Jan 18 21:50:59.976: INFO: Service test-service-kgtz4 patched
    STEP: deleting the service 01/18/23 21:50:59.976
    STEP: watching for the Service to be deleted 01/18/23 21:50:59.992
    Jan 18 21:50:59.995: INFO: Observed event: ADDED
    Jan 18 21:50:59.995: INFO: Observed event: MODIFIED
    Jan 18 21:50:59.995: INFO: Observed event: MODIFIED
    Jan 18 21:50:59.995: INFO: Observed event: MODIFIED
    Jan 18 21:50:59.995: INFO: Found Service test-service-kgtz4 in namespace services-1109 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jan 18 21:50:59.995: INFO: Service test-service-kgtz4 deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 21:50:59.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1109" for this suite. 01/18/23 21:51:00.001
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:51:00.018
Jan 18 21:51:00.018: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename pods 01/18/23 21:51:00.019
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:00.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:00.063
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Jan 18 21:51:00.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: creating the pod 01/18/23 21:51:00.077
STEP: submitting the pod to kubernetes 01/18/23 21:51:00.078
Jan 18 21:51:00.091: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-3ff31db1-d109-4ea2-a036-95ce8f16c200" in namespace "pods-4852" to be "running and ready"
Jan 18 21:51:00.096: INFO: Pod "pod-logs-websocket-3ff31db1-d109-4ea2-a036-95ce8f16c200": Phase="Pending", Reason="", readiness=false. Elapsed: 4.529978ms
Jan 18 21:51:00.096: INFO: The phase of Pod pod-logs-websocket-3ff31db1-d109-4ea2-a036-95ce8f16c200 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:51:02.101: INFO: Pod "pod-logs-websocket-3ff31db1-d109-4ea2-a036-95ce8f16c200": Phase="Running", Reason="", readiness=true. Elapsed: 2.00989381s
Jan 18 21:51:02.101: INFO: The phase of Pod pod-logs-websocket-3ff31db1-d109-4ea2-a036-95ce8f16c200 is Running (Ready = true)
Jan 18 21:51:02.101: INFO: Pod "pod-logs-websocket-3ff31db1-d109-4ea2-a036-95ce8f16c200" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 21:51:02.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4852" for this suite. 01/18/23 21:51:02.127
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":108,"skipped":2208,"failed":0}
------------------------------
• [2.120 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:51:00.018
    Jan 18 21:51:00.018: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename pods 01/18/23 21:51:00.019
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:00.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:00.063
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Jan 18 21:51:00.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: creating the pod 01/18/23 21:51:00.077
    STEP: submitting the pod to kubernetes 01/18/23 21:51:00.078
    Jan 18 21:51:00.091: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-3ff31db1-d109-4ea2-a036-95ce8f16c200" in namespace "pods-4852" to be "running and ready"
    Jan 18 21:51:00.096: INFO: Pod "pod-logs-websocket-3ff31db1-d109-4ea2-a036-95ce8f16c200": Phase="Pending", Reason="", readiness=false. Elapsed: 4.529978ms
    Jan 18 21:51:00.096: INFO: The phase of Pod pod-logs-websocket-3ff31db1-d109-4ea2-a036-95ce8f16c200 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:51:02.101: INFO: Pod "pod-logs-websocket-3ff31db1-d109-4ea2-a036-95ce8f16c200": Phase="Running", Reason="", readiness=true. Elapsed: 2.00989381s
    Jan 18 21:51:02.101: INFO: The phase of Pod pod-logs-websocket-3ff31db1-d109-4ea2-a036-95ce8f16c200 is Running (Ready = true)
    Jan 18 21:51:02.101: INFO: Pod "pod-logs-websocket-3ff31db1-d109-4ea2-a036-95ce8f16c200" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 21:51:02.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4852" for this suite. 01/18/23 21:51:02.127
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:51:02.14
Jan 18 21:51:02.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename ephemeral-containers-test 01/18/23 21:51:02.143
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:02.159
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:02.164
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 01/18/23 21:51:02.17
Jan 18 21:51:02.186: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9556" to be "running and ready"
Jan 18 21:51:02.206: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.578541ms
Jan 18 21:51:02.206: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:51:04.211: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.024934544s
Jan 18 21:51:04.211: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jan 18 21:51:04.211: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 01/18/23 21:51:04.215
Jan 18 21:51:04.227: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9556" to be "container debugger running"
Jan 18 21:51:04.232: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.460568ms
Jan 18 21:51:06.237: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010005869s
Jan 18 21:51:08.237: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009915421s
Jan 18 21:51:08.237: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 01/18/23 21:51:08.237
Jan 18 21:51:08.237: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-9556 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:51:08.238: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 21:51:08.239: INFO: ExecWithOptions: Clientset creation
Jan 18 21:51:08.239: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-9556/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jan 18 21:51:08.333: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 18 21:51:08.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-9556" for this suite. 01/18/23 21:51:08.351
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":109,"skipped":2221,"failed":0}
------------------------------
• [SLOW TEST] [6.220 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:51:02.14
    Jan 18 21:51:02.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename ephemeral-containers-test 01/18/23 21:51:02.143
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:02.159
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:02.164
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 01/18/23 21:51:02.17
    Jan 18 21:51:02.186: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9556" to be "running and ready"
    Jan 18 21:51:02.206: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.578541ms
    Jan 18 21:51:02.206: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:51:04.211: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.024934544s
    Jan 18 21:51:04.211: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jan 18 21:51:04.211: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 01/18/23 21:51:04.215
    Jan 18 21:51:04.227: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9556" to be "container debugger running"
    Jan 18 21:51:04.232: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.460568ms
    Jan 18 21:51:06.237: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010005869s
    Jan 18 21:51:08.237: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009915421s
    Jan 18 21:51:08.237: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 01/18/23 21:51:08.237
    Jan 18 21:51:08.237: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-9556 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:51:08.238: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 21:51:08.239: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:51:08.239: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-9556/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jan 18 21:51:08.333: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 18 21:51:08.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-9556" for this suite. 01/18/23 21:51:08.351
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:51:08.362
Jan 18 21:51:08.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 21:51:08.364
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:08.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:08.401
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 01/18/23 21:51:08.406
Jan 18 21:51:08.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: mark a version not serverd 01/18/23 21:51:18.079
STEP: check the unserved version gets removed 01/18/23 21:51:18.104
STEP: check the other version is not changed 01/18/23 21:51:22.708
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 21:51:30.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-745" for this suite. 01/18/23 21:51:30.376
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":110,"skipped":2226,"failed":0}
------------------------------
• [SLOW TEST] [22.030 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:51:08.362
    Jan 18 21:51:08.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 21:51:08.364
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:08.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:08.401
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 01/18/23 21:51:08.406
    Jan 18 21:51:08.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: mark a version not serverd 01/18/23 21:51:18.079
    STEP: check the unserved version gets removed 01/18/23 21:51:18.104
    STEP: check the other version is not changed 01/18/23 21:51:22.708
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 21:51:30.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-745" for this suite. 01/18/23 21:51:30.376
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:51:30.393
Jan 18 21:51:30.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename watch 01/18/23 21:51:30.404
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:30.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:30.437
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 01/18/23 21:51:30.441
STEP: modifying the configmap once 01/18/23 21:51:30.447
STEP: modifying the configmap a second time 01/18/23 21:51:30.455
STEP: deleting the configmap 01/18/23 21:51:30.467
STEP: creating a watch on configmaps from the resource version returned by the first update 01/18/23 21:51:30.472
STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/18/23 21:51:30.475
Jan 18 21:51:30.475: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2128  c8754ad4-3e4c-436a-a570-cf5b966755f8 143304 0 2023-01-18 21:51:30 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-18 21:51:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 21:51:30.476: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2128  c8754ad4-3e4c-436a-a570-cf5b966755f8 143305 0 2023-01-18 21:51:30 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-18 21:51:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 18 21:51:30.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2128" for this suite. 01/18/23 21:51:30.482
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":111,"skipped":2236,"failed":0}
------------------------------
• [0.095 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:51:30.393
    Jan 18 21:51:30.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename watch 01/18/23 21:51:30.404
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:30.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:30.437
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 01/18/23 21:51:30.441
    STEP: modifying the configmap once 01/18/23 21:51:30.447
    STEP: modifying the configmap a second time 01/18/23 21:51:30.455
    STEP: deleting the configmap 01/18/23 21:51:30.467
    STEP: creating a watch on configmaps from the resource version returned by the first update 01/18/23 21:51:30.472
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/18/23 21:51:30.475
    Jan 18 21:51:30.475: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2128  c8754ad4-3e4c-436a-a570-cf5b966755f8 143304 0 2023-01-18 21:51:30 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-18 21:51:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 21:51:30.476: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2128  c8754ad4-3e4c-436a-a570-cf5b966755f8 143305 0 2023-01-18 21:51:30 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-18 21:51:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 18 21:51:30.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-2128" for this suite. 01/18/23 21:51:30.482
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:51:30.492
Jan 18 21:51:30.492: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename job 01/18/23 21:51:30.495
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:30.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:30.529
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 01/18/23 21:51:30.534
STEP: Ensuring job reaches completions 01/18/23 21:51:30.541
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 18 21:51:44.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1789" for this suite. 01/18/23 21:51:44.557
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":112,"skipped":2241,"failed":0}
------------------------------
• [SLOW TEST] [14.077 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:51:30.492
    Jan 18 21:51:30.492: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename job 01/18/23 21:51:30.495
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:30.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:30.529
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 01/18/23 21:51:30.534
    STEP: Ensuring job reaches completions 01/18/23 21:51:30.541
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 18 21:51:44.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-1789" for this suite. 01/18/23 21:51:44.557
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:51:44.586
Jan 18 21:51:44.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename svcaccounts 01/18/23 21:51:44.588
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:44.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:44.615
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 01/18/23 21:51:44.621
STEP: watching for the ServiceAccount to be added 01/18/23 21:51:44.633
STEP: patching the ServiceAccount 01/18/23 21:51:44.635
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/18/23 21:51:44.642
STEP: deleting the ServiceAccount 01/18/23 21:51:44.65
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 18 21:51:44.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9008" for this suite. 01/18/23 21:51:44.673
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":113,"skipped":2317,"failed":0}
------------------------------
• [0.093 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:51:44.586
    Jan 18 21:51:44.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 21:51:44.588
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:44.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:44.615
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 01/18/23 21:51:44.621
    STEP: watching for the ServiceAccount to be added 01/18/23 21:51:44.633
    STEP: patching the ServiceAccount 01/18/23 21:51:44.635
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/18/23 21:51:44.642
    STEP: deleting the ServiceAccount 01/18/23 21:51:44.65
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 18 21:51:44.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-9008" for this suite. 01/18/23 21:51:44.673
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:51:44.686
Jan 18 21:51:44.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename downward-api 01/18/23 21:51:44.688
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:44.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:44.721
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 01/18/23 21:51:44.726
Jan 18 21:51:44.744: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8ebb4bcb-0fe4-4bb9-93db-e45eeae1bcab" in namespace "downward-api-3770" to be "Succeeded or Failed"
Jan 18 21:51:44.795: INFO: Pod "downwardapi-volume-8ebb4bcb-0fe4-4bb9-93db-e45eeae1bcab": Phase="Pending", Reason="", readiness=false. Elapsed: 50.468748ms
Jan 18 21:51:46.801: INFO: Pod "downwardapi-volume-8ebb4bcb-0fe4-4bb9-93db-e45eeae1bcab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056626419s
Jan 18 21:51:48.812: INFO: Pod "downwardapi-volume-8ebb4bcb-0fe4-4bb9-93db-e45eeae1bcab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067703389s
STEP: Saw pod success 01/18/23 21:51:48.812
Jan 18 21:51:48.812: INFO: Pod "downwardapi-volume-8ebb4bcb-0fe4-4bb9-93db-e45eeae1bcab" satisfied condition "Succeeded or Failed"
Jan 18 21:51:48.816: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-8ebb4bcb-0fe4-4bb9-93db-e45eeae1bcab container client-container: <nil>
STEP: delete the pod 01/18/23 21:51:48.832
Jan 18 21:51:48.847: INFO: Waiting for pod downwardapi-volume-8ebb4bcb-0fe4-4bb9-93db-e45eeae1bcab to disappear
Jan 18 21:51:48.851: INFO: Pod downwardapi-volume-8ebb4bcb-0fe4-4bb9-93db-e45eeae1bcab no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 21:51:48.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3770" for this suite. 01/18/23 21:51:48.862
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":114,"skipped":2335,"failed":0}
------------------------------
• [4.186 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:51:44.686
    Jan 18 21:51:44.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename downward-api 01/18/23 21:51:44.688
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:44.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:44.721
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 01/18/23 21:51:44.726
    Jan 18 21:51:44.744: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8ebb4bcb-0fe4-4bb9-93db-e45eeae1bcab" in namespace "downward-api-3770" to be "Succeeded or Failed"
    Jan 18 21:51:44.795: INFO: Pod "downwardapi-volume-8ebb4bcb-0fe4-4bb9-93db-e45eeae1bcab": Phase="Pending", Reason="", readiness=false. Elapsed: 50.468748ms
    Jan 18 21:51:46.801: INFO: Pod "downwardapi-volume-8ebb4bcb-0fe4-4bb9-93db-e45eeae1bcab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056626419s
    Jan 18 21:51:48.812: INFO: Pod "downwardapi-volume-8ebb4bcb-0fe4-4bb9-93db-e45eeae1bcab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067703389s
    STEP: Saw pod success 01/18/23 21:51:48.812
    Jan 18 21:51:48.812: INFO: Pod "downwardapi-volume-8ebb4bcb-0fe4-4bb9-93db-e45eeae1bcab" satisfied condition "Succeeded or Failed"
    Jan 18 21:51:48.816: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-8ebb4bcb-0fe4-4bb9-93db-e45eeae1bcab container client-container: <nil>
    STEP: delete the pod 01/18/23 21:51:48.832
    Jan 18 21:51:48.847: INFO: Waiting for pod downwardapi-volume-8ebb4bcb-0fe4-4bb9-93db-e45eeae1bcab to disappear
    Jan 18 21:51:48.851: INFO: Pod downwardapi-volume-8ebb4bcb-0fe4-4bb9-93db-e45eeae1bcab no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 21:51:48.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3770" for this suite. 01/18/23 21:51:48.862
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:51:48.891
Jan 18 21:51:48.891: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename subpath 01/18/23 21:51:48.893
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:48.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:48.918
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/18/23 21:51:48.921
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-5ntp 01/18/23 21:51:48.93
STEP: Creating a pod to test atomic-volume-subpath 01/18/23 21:51:48.931
Jan 18 21:51:48.940: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-5ntp" in namespace "subpath-4442" to be "Succeeded or Failed"
Jan 18 21:51:48.946: INFO: Pod "pod-subpath-test-projected-5ntp": Phase="Pending", Reason="", readiness=false. Elapsed: 5.726666ms
Jan 18 21:51:50.952: INFO: Pod "pod-subpath-test-projected-5ntp": Phase="Running", Reason="", readiness=true. Elapsed: 2.012267515s
Jan 18 21:51:52.950: INFO: Pod "pod-subpath-test-projected-5ntp": Phase="Running", Reason="", readiness=true. Elapsed: 4.010630893s
Jan 18 21:51:54.951: INFO: Pod "pod-subpath-test-projected-5ntp": Phase="Running", Reason="", readiness=true. Elapsed: 6.011589946s
Jan 18 21:51:56.952: INFO: Pod "pod-subpath-test-projected-5ntp": Phase="Running", Reason="", readiness=true. Elapsed: 8.011812928s
Jan 18 21:51:58.953: INFO: Pod "pod-subpath-test-projected-5ntp": Phase="Running", Reason="", readiness=true. Elapsed: 10.013245924s
Jan 18 21:52:00.952: INFO: Pod "pod-subpath-test-projected-5ntp": Phase="Running", Reason="", readiness=true. Elapsed: 12.012072849s
Jan 18 21:52:02.951: INFO: Pod "pod-subpath-test-projected-5ntp": Phase="Running", Reason="", readiness=true. Elapsed: 14.011239213s
Jan 18 21:52:04.953: INFO: Pod "pod-subpath-test-projected-5ntp": Phase="Running", Reason="", readiness=true. Elapsed: 16.012993846s
Jan 18 21:52:06.951: INFO: Pod "pod-subpath-test-projected-5ntp": Phase="Running", Reason="", readiness=true. Elapsed: 18.010955243s
Jan 18 21:52:08.952: INFO: Pod "pod-subpath-test-projected-5ntp": Phase="Running", Reason="", readiness=true. Elapsed: 20.012085757s
Jan 18 21:52:10.953: INFO: Pod "pod-subpath-test-projected-5ntp": Phase="Running", Reason="", readiness=false. Elapsed: 22.01363503s
Jan 18 21:52:12.952: INFO: Pod "pod-subpath-test-projected-5ntp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011716932s
STEP: Saw pod success 01/18/23 21:52:12.952
Jan 18 21:52:12.952: INFO: Pod "pod-subpath-test-projected-5ntp" satisfied condition "Succeeded or Failed"
Jan 18 21:52:12.956: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-subpath-test-projected-5ntp container test-container-subpath-projected-5ntp: <nil>
STEP: delete the pod 01/18/23 21:52:12.966
Jan 18 21:52:12.987: INFO: Waiting for pod pod-subpath-test-projected-5ntp to disappear
Jan 18 21:52:12.992: INFO: Pod pod-subpath-test-projected-5ntp no longer exists
STEP: Deleting pod pod-subpath-test-projected-5ntp 01/18/23 21:52:12.993
Jan 18 21:52:12.993: INFO: Deleting pod "pod-subpath-test-projected-5ntp" in namespace "subpath-4442"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 18 21:52:13.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4442" for this suite. 01/18/23 21:52:13.007
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":115,"skipped":2367,"failed":0}
------------------------------
• [SLOW TEST] [24.128 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:51:48.891
    Jan 18 21:51:48.891: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename subpath 01/18/23 21:51:48.893
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:48.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:48.918
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/18/23 21:51:48.921
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-5ntp 01/18/23 21:51:48.93
    STEP: Creating a pod to test atomic-volume-subpath 01/18/23 21:51:48.931
    Jan 18 21:51:48.940: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-5ntp" in namespace "subpath-4442" to be "Succeeded or Failed"
    Jan 18 21:51:48.946: INFO: Pod "pod-subpath-test-projected-5ntp": Phase="Pending", Reason="", readiness=false. Elapsed: 5.726666ms
    Jan 18 21:51:50.952: INFO: Pod "pod-subpath-test-projected-5ntp": Phase="Running", Reason="", readiness=true. Elapsed: 2.012267515s
    Jan 18 21:51:52.950: INFO: Pod "pod-subpath-test-projected-5ntp": Phase="Running", Reason="", readiness=true. Elapsed: 4.010630893s
    Jan 18 21:51:54.951: INFO: Pod "pod-subpath-test-projected-5ntp": Phase="Running", Reason="", readiness=true. Elapsed: 6.011589946s
    Jan 18 21:51:56.952: INFO: Pod "pod-subpath-test-projected-5ntp": Phase="Running", Reason="", readiness=true. Elapsed: 8.011812928s
    Jan 18 21:51:58.953: INFO: Pod "pod-subpath-test-projected-5ntp": Phase="Running", Reason="", readiness=true. Elapsed: 10.013245924s
    Jan 18 21:52:00.952: INFO: Pod "pod-subpath-test-projected-5ntp": Phase="Running", Reason="", readiness=true. Elapsed: 12.012072849s
    Jan 18 21:52:02.951: INFO: Pod "pod-subpath-test-projected-5ntp": Phase="Running", Reason="", readiness=true. Elapsed: 14.011239213s
    Jan 18 21:52:04.953: INFO: Pod "pod-subpath-test-projected-5ntp": Phase="Running", Reason="", readiness=true. Elapsed: 16.012993846s
    Jan 18 21:52:06.951: INFO: Pod "pod-subpath-test-projected-5ntp": Phase="Running", Reason="", readiness=true. Elapsed: 18.010955243s
    Jan 18 21:52:08.952: INFO: Pod "pod-subpath-test-projected-5ntp": Phase="Running", Reason="", readiness=true. Elapsed: 20.012085757s
    Jan 18 21:52:10.953: INFO: Pod "pod-subpath-test-projected-5ntp": Phase="Running", Reason="", readiness=false. Elapsed: 22.01363503s
    Jan 18 21:52:12.952: INFO: Pod "pod-subpath-test-projected-5ntp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011716932s
    STEP: Saw pod success 01/18/23 21:52:12.952
    Jan 18 21:52:12.952: INFO: Pod "pod-subpath-test-projected-5ntp" satisfied condition "Succeeded or Failed"
    Jan 18 21:52:12.956: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-subpath-test-projected-5ntp container test-container-subpath-projected-5ntp: <nil>
    STEP: delete the pod 01/18/23 21:52:12.966
    Jan 18 21:52:12.987: INFO: Waiting for pod pod-subpath-test-projected-5ntp to disappear
    Jan 18 21:52:12.992: INFO: Pod pod-subpath-test-projected-5ntp no longer exists
    STEP: Deleting pod pod-subpath-test-projected-5ntp 01/18/23 21:52:12.993
    Jan 18 21:52:12.993: INFO: Deleting pod "pod-subpath-test-projected-5ntp" in namespace "subpath-4442"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 18 21:52:13.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-4442" for this suite. 01/18/23 21:52:13.007
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:52:13.021
Jan 18 21:52:13.021: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename var-expansion 01/18/23 21:52:13.024
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:13.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:13.048
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 01/18/23 21:52:13.053
Jan 18 21:52:13.062: INFO: Waiting up to 5m0s for pod "var-expansion-2d13ab21-f23b-45cd-93e9-55531a786712" in namespace "var-expansion-5994" to be "Succeeded or Failed"
Jan 18 21:52:13.067: INFO: Pod "var-expansion-2d13ab21-f23b-45cd-93e9-55531a786712": Phase="Pending", Reason="", readiness=false. Elapsed: 4.12852ms
Jan 18 21:52:15.075: INFO: Pod "var-expansion-2d13ab21-f23b-45cd-93e9-55531a786712": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011795592s
Jan 18 21:52:17.075: INFO: Pod "var-expansion-2d13ab21-f23b-45cd-93e9-55531a786712": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011784719s
STEP: Saw pod success 01/18/23 21:52:17.075
Jan 18 21:52:17.075: INFO: Pod "var-expansion-2d13ab21-f23b-45cd-93e9-55531a786712" satisfied condition "Succeeded or Failed"
Jan 18 21:52:17.079: INFO: Trying to get logs from node v1-25-1-18760-w2 pod var-expansion-2d13ab21-f23b-45cd-93e9-55531a786712 container dapi-container: <nil>
STEP: delete the pod 01/18/23 21:52:17.088
Jan 18 21:52:17.106: INFO: Waiting for pod var-expansion-2d13ab21-f23b-45cd-93e9-55531a786712 to disappear
Jan 18 21:52:17.112: INFO: Pod var-expansion-2d13ab21-f23b-45cd-93e9-55531a786712 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 18 21:52:17.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5994" for this suite. 01/18/23 21:52:17.118
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":116,"skipped":2374,"failed":0}
------------------------------
• [4.104 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:52:13.021
    Jan 18 21:52:13.021: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename var-expansion 01/18/23 21:52:13.024
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:13.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:13.048
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 01/18/23 21:52:13.053
    Jan 18 21:52:13.062: INFO: Waiting up to 5m0s for pod "var-expansion-2d13ab21-f23b-45cd-93e9-55531a786712" in namespace "var-expansion-5994" to be "Succeeded or Failed"
    Jan 18 21:52:13.067: INFO: Pod "var-expansion-2d13ab21-f23b-45cd-93e9-55531a786712": Phase="Pending", Reason="", readiness=false. Elapsed: 4.12852ms
    Jan 18 21:52:15.075: INFO: Pod "var-expansion-2d13ab21-f23b-45cd-93e9-55531a786712": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011795592s
    Jan 18 21:52:17.075: INFO: Pod "var-expansion-2d13ab21-f23b-45cd-93e9-55531a786712": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011784719s
    STEP: Saw pod success 01/18/23 21:52:17.075
    Jan 18 21:52:17.075: INFO: Pod "var-expansion-2d13ab21-f23b-45cd-93e9-55531a786712" satisfied condition "Succeeded or Failed"
    Jan 18 21:52:17.079: INFO: Trying to get logs from node v1-25-1-18760-w2 pod var-expansion-2d13ab21-f23b-45cd-93e9-55531a786712 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 21:52:17.088
    Jan 18 21:52:17.106: INFO: Waiting for pod var-expansion-2d13ab21-f23b-45cd-93e9-55531a786712 to disappear
    Jan 18 21:52:17.112: INFO: Pod var-expansion-2d13ab21-f23b-45cd-93e9-55531a786712 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 18 21:52:17.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5994" for this suite. 01/18/23 21:52:17.118
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:52:17.126
Jan 18 21:52:17.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename container-runtime 01/18/23 21:52:17.128
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:17.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:17.155
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 01/18/23 21:52:17.159
STEP: wait for the container to reach Succeeded 01/18/23 21:52:17.169
STEP: get the container status 01/18/23 21:52:21.204
STEP: the container should be terminated 01/18/23 21:52:21.21
STEP: the termination message should be set 01/18/23 21:52:21.21
Jan 18 21:52:21.210: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 01/18/23 21:52:21.211
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 18 21:52:21.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3870" for this suite. 01/18/23 21:52:21.238
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":117,"skipped":2376,"failed":0}
------------------------------
• [4.123 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:52:17.126
    Jan 18 21:52:17.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename container-runtime 01/18/23 21:52:17.128
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:17.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:17.155
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 01/18/23 21:52:17.159
    STEP: wait for the container to reach Succeeded 01/18/23 21:52:17.169
    STEP: get the container status 01/18/23 21:52:21.204
    STEP: the container should be terminated 01/18/23 21:52:21.21
    STEP: the termination message should be set 01/18/23 21:52:21.21
    Jan 18 21:52:21.210: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 01/18/23 21:52:21.211
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 18 21:52:21.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-3870" for this suite. 01/18/23 21:52:21.238
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:52:21.253
Jan 18 21:52:21.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename gc 01/18/23 21:52:21.255
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:21.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:21.289
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 01/18/23 21:52:21.294
STEP: delete the rc 01/18/23 21:52:26.31
STEP: wait for all pods to be garbage collected 01/18/23 21:52:26.325
STEP: Gathering metrics 01/18/23 21:52:31.341
Jan 18 21:52:31.375: INFO: Waiting up to 5m0s for pod "kube-controller-manager-v1-25-1-18760-m" in namespace "kube-system" to be "running and ready"
Jan 18 21:52:31.381: INFO: Pod "kube-controller-manager-v1-25-1-18760-m": Phase="Running", Reason="", readiness=true. Elapsed: 5.211846ms
Jan 18 21:52:31.381: INFO: The phase of Pod kube-controller-manager-v1-25-1-18760-m is Running (Ready = true)
Jan 18 21:52:31.381: INFO: Pod "kube-controller-manager-v1-25-1-18760-m" satisfied condition "running and ready"
Jan 18 21:52:31.493: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 18 21:52:31.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2961" for this suite. 01/18/23 21:52:31.501
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":118,"skipped":2379,"failed":0}
------------------------------
• [SLOW TEST] [10.261 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:52:21.253
    Jan 18 21:52:21.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename gc 01/18/23 21:52:21.255
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:21.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:21.289
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 01/18/23 21:52:21.294
    STEP: delete the rc 01/18/23 21:52:26.31
    STEP: wait for all pods to be garbage collected 01/18/23 21:52:26.325
    STEP: Gathering metrics 01/18/23 21:52:31.341
    Jan 18 21:52:31.375: INFO: Waiting up to 5m0s for pod "kube-controller-manager-v1-25-1-18760-m" in namespace "kube-system" to be "running and ready"
    Jan 18 21:52:31.381: INFO: Pod "kube-controller-manager-v1-25-1-18760-m": Phase="Running", Reason="", readiness=true. Elapsed: 5.211846ms
    Jan 18 21:52:31.381: INFO: The phase of Pod kube-controller-manager-v1-25-1-18760-m is Running (Ready = true)
    Jan 18 21:52:31.381: INFO: Pod "kube-controller-manager-v1-25-1-18760-m" satisfied condition "running and ready"
    Jan 18 21:52:31.493: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 18 21:52:31.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2961" for this suite. 01/18/23 21:52:31.501
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:52:31.518
Jan 18 21:52:31.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename daemonsets 01/18/23 21:52:31.52
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:31.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:31.539
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Jan 18 21:52:31.652: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 21:52:31.663
Jan 18 21:52:31.672: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:52:31.677: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:52:31.677: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 21:52:32.684: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:52:32.700: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:52:32.700: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 21:52:33.685: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:52:33.690: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 21:52:33.690: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 01/18/23 21:52:33.708
STEP: Check that daemon pods images are updated. 01/18/23 21:52:33.725
Jan 18 21:52:33.733: INFO: Wrong image for pod: daemon-set-2wjl5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 21:52:33.733: INFO: Wrong image for pod: daemon-set-t4flp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 21:52:33.742: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:52:34.748: INFO: Wrong image for pod: daemon-set-t4flp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 21:52:34.758: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:52:35.749: INFO: Wrong image for pod: daemon-set-t4flp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 21:52:35.755: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:52:36.752: INFO: Pod daemon-set-4kpxc is not available
Jan 18 21:52:36.752: INFO: Wrong image for pod: daemon-set-t4flp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 21:52:36.761: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:52:37.749: INFO: Pod daemon-set-4kpxc is not available
Jan 18 21:52:37.749: INFO: Wrong image for pod: daemon-set-t4flp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 21:52:37.755: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:52:38.753: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:52:39.748: INFO: Pod daemon-set-94cfl is not available
Jan 18 21:52:39.753: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 01/18/23 21:52:39.754
Jan 18 21:52:39.760: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:52:39.765: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 21:52:39.765: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 21:52:40.773: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:52:40.779: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 21:52:40.779: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 21:52:41.773: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 21:52:41.777: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 21:52:41.778: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/18/23 21:52:41.819
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6041, will wait for the garbage collector to delete the pods 01/18/23 21:52:41.819
Jan 18 21:52:41.887: INFO: Deleting DaemonSet.extensions daemon-set took: 12.37366ms
Jan 18 21:52:41.988: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.355088ms
Jan 18 21:52:44.493: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:52:44.493: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 21:52:44.498: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"144045"},"items":null}

Jan 18 21:52:44.502: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"144045"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 18 21:52:44.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6041" for this suite. 01/18/23 21:52:44.522
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":119,"skipped":2387,"failed":0}
------------------------------
• [SLOW TEST] [13.013 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:52:31.518
    Jan 18 21:52:31.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename daemonsets 01/18/23 21:52:31.52
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:31.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:31.539
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Jan 18 21:52:31.652: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 21:52:31.663
    Jan 18 21:52:31.672: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:52:31.677: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:52:31.677: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 21:52:32.684: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:52:32.700: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:52:32.700: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 21:52:33.685: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:52:33.690: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 21:52:33.690: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 01/18/23 21:52:33.708
    STEP: Check that daemon pods images are updated. 01/18/23 21:52:33.725
    Jan 18 21:52:33.733: INFO: Wrong image for pod: daemon-set-2wjl5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 18 21:52:33.733: INFO: Wrong image for pod: daemon-set-t4flp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 18 21:52:33.742: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:52:34.748: INFO: Wrong image for pod: daemon-set-t4flp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 18 21:52:34.758: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:52:35.749: INFO: Wrong image for pod: daemon-set-t4flp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 18 21:52:35.755: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:52:36.752: INFO: Pod daemon-set-4kpxc is not available
    Jan 18 21:52:36.752: INFO: Wrong image for pod: daemon-set-t4flp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 18 21:52:36.761: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:52:37.749: INFO: Pod daemon-set-4kpxc is not available
    Jan 18 21:52:37.749: INFO: Wrong image for pod: daemon-set-t4flp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 18 21:52:37.755: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:52:38.753: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:52:39.748: INFO: Pod daemon-set-94cfl is not available
    Jan 18 21:52:39.753: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 01/18/23 21:52:39.754
    Jan 18 21:52:39.760: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:52:39.765: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 21:52:39.765: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 21:52:40.773: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:52:40.779: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 21:52:40.779: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 21:52:41.773: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 21:52:41.777: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 21:52:41.778: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 21:52:41.819
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6041, will wait for the garbage collector to delete the pods 01/18/23 21:52:41.819
    Jan 18 21:52:41.887: INFO: Deleting DaemonSet.extensions daemon-set took: 12.37366ms
    Jan 18 21:52:41.988: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.355088ms
    Jan 18 21:52:44.493: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:52:44.493: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 21:52:44.498: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"144045"},"items":null}

    Jan 18 21:52:44.502: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"144045"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 21:52:44.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-6041" for this suite. 01/18/23 21:52:44.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:52:44.539
Jan 18 21:52:44.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename events 01/18/23 21:52:44.541
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:44.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:44.568
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 01/18/23 21:52:44.573
STEP: listing all events in all namespaces 01/18/23 21:52:44.58
STEP: patching the test event 01/18/23 21:52:44.587
STEP: fetching the test event 01/18/23 21:52:44.602
STEP: updating the test event 01/18/23 21:52:44.607
STEP: getting the test event 01/18/23 21:52:44.619
STEP: deleting the test event 01/18/23 21:52:44.624
STEP: listing all events in all namespaces 01/18/23 21:52:44.632
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jan 18 21:52:44.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2388" for this suite. 01/18/23 21:52:44.644
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":120,"skipped":2406,"failed":0}
------------------------------
• [0.112 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:52:44.539
    Jan 18 21:52:44.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename events 01/18/23 21:52:44.541
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:44.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:44.568
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 01/18/23 21:52:44.573
    STEP: listing all events in all namespaces 01/18/23 21:52:44.58
    STEP: patching the test event 01/18/23 21:52:44.587
    STEP: fetching the test event 01/18/23 21:52:44.602
    STEP: updating the test event 01/18/23 21:52:44.607
    STEP: getting the test event 01/18/23 21:52:44.619
    STEP: deleting the test event 01/18/23 21:52:44.624
    STEP: listing all events in all namespaces 01/18/23 21:52:44.632
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jan 18 21:52:44.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-2388" for this suite. 01/18/23 21:52:44.644
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:52:44.656
Jan 18 21:52:44.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 21:52:44.658
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:44.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:44.692
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 01/18/23 21:52:44.698
Jan 18 21:52:44.708: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ac8d91cb-3b35-4b69-9de9-e096357e0c3b" in namespace "projected-3834" to be "Succeeded or Failed"
Jan 18 21:52:44.714: INFO: Pod "downwardapi-volume-ac8d91cb-3b35-4b69-9de9-e096357e0c3b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.910967ms
Jan 18 21:52:46.720: INFO: Pod "downwardapi-volume-ac8d91cb-3b35-4b69-9de9-e096357e0c3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011948477s
Jan 18 21:52:48.721: INFO: Pod "downwardapi-volume-ac8d91cb-3b35-4b69-9de9-e096357e0c3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012205702s
STEP: Saw pod success 01/18/23 21:52:48.721
Jan 18 21:52:48.721: INFO: Pod "downwardapi-volume-ac8d91cb-3b35-4b69-9de9-e096357e0c3b" satisfied condition "Succeeded or Failed"
Jan 18 21:52:48.728: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-ac8d91cb-3b35-4b69-9de9-e096357e0c3b container client-container: <nil>
STEP: delete the pod 01/18/23 21:52:48.736
Jan 18 21:52:48.754: INFO: Waiting for pod downwardapi-volume-ac8d91cb-3b35-4b69-9de9-e096357e0c3b to disappear
Jan 18 21:52:48.764: INFO: Pod downwardapi-volume-ac8d91cb-3b35-4b69-9de9-e096357e0c3b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 21:52:48.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3834" for this suite. 01/18/23 21:52:48.77
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":121,"skipped":2413,"failed":0}
------------------------------
• [4.123 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:52:44.656
    Jan 18 21:52:44.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 21:52:44.658
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:44.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:44.692
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 01/18/23 21:52:44.698
    Jan 18 21:52:44.708: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ac8d91cb-3b35-4b69-9de9-e096357e0c3b" in namespace "projected-3834" to be "Succeeded or Failed"
    Jan 18 21:52:44.714: INFO: Pod "downwardapi-volume-ac8d91cb-3b35-4b69-9de9-e096357e0c3b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.910967ms
    Jan 18 21:52:46.720: INFO: Pod "downwardapi-volume-ac8d91cb-3b35-4b69-9de9-e096357e0c3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011948477s
    Jan 18 21:52:48.721: INFO: Pod "downwardapi-volume-ac8d91cb-3b35-4b69-9de9-e096357e0c3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012205702s
    STEP: Saw pod success 01/18/23 21:52:48.721
    Jan 18 21:52:48.721: INFO: Pod "downwardapi-volume-ac8d91cb-3b35-4b69-9de9-e096357e0c3b" satisfied condition "Succeeded or Failed"
    Jan 18 21:52:48.728: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-ac8d91cb-3b35-4b69-9de9-e096357e0c3b container client-container: <nil>
    STEP: delete the pod 01/18/23 21:52:48.736
    Jan 18 21:52:48.754: INFO: Waiting for pod downwardapi-volume-ac8d91cb-3b35-4b69-9de9-e096357e0c3b to disappear
    Jan 18 21:52:48.764: INFO: Pod downwardapi-volume-ac8d91cb-3b35-4b69-9de9-e096357e0c3b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 21:52:48.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3834" for this suite. 01/18/23 21:52:48.77
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:52:48.793
Jan 18 21:52:48.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename replication-controller 01/18/23 21:52:48.795
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:48.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:48.817
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Jan 18 21:52:48.824: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/18/23 21:52:48.843
STEP: Checking rc "condition-test" has the desired failure condition set 01/18/23 21:52:48.85
STEP: Scaling down rc "condition-test" to satisfy pod quota 01/18/23 21:52:49.869
Jan 18 21:52:49.908: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 01/18/23 21:52:49.908
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 18 21:52:49.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4948" for this suite. 01/18/23 21:52:49.939
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":122,"skipped":2445,"failed":0}
------------------------------
• [1.171 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:52:48.793
    Jan 18 21:52:48.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename replication-controller 01/18/23 21:52:48.795
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:48.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:48.817
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Jan 18 21:52:48.824: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/18/23 21:52:48.843
    STEP: Checking rc "condition-test" has the desired failure condition set 01/18/23 21:52:48.85
    STEP: Scaling down rc "condition-test" to satisfy pod quota 01/18/23 21:52:49.869
    Jan 18 21:52:49.908: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 01/18/23 21:52:49.908
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 18 21:52:49.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-4948" for this suite. 01/18/23 21:52:49.939
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:52:49.981
Jan 18 21:52:49.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 21:52:49.986
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:50.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:50.055
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-bd5b056a-ecc2-4c79-bb6d-1623abf07e98 01/18/23 21:52:50.064
STEP: Creating a pod to test consume configMaps 01/18/23 21:52:50.072
Jan 18 21:52:50.093: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-52d2bf10-629c-4472-95d0-dc4cd311a562" in namespace "projected-8880" to be "Succeeded or Failed"
Jan 18 21:52:50.103: INFO: Pod "pod-projected-configmaps-52d2bf10-629c-4472-95d0-dc4cd311a562": Phase="Pending", Reason="", readiness=false. Elapsed: 9.734444ms
Jan 18 21:52:52.108: INFO: Pod "pod-projected-configmaps-52d2bf10-629c-4472-95d0-dc4cd311a562": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01464099s
Jan 18 21:52:54.109: INFO: Pod "pod-projected-configmaps-52d2bf10-629c-4472-95d0-dc4cd311a562": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015237314s
Jan 18 21:52:56.109: INFO: Pod "pod-projected-configmaps-52d2bf10-629c-4472-95d0-dc4cd311a562": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015140676s
STEP: Saw pod success 01/18/23 21:52:56.109
Jan 18 21:52:56.109: INFO: Pod "pod-projected-configmaps-52d2bf10-629c-4472-95d0-dc4cd311a562" satisfied condition "Succeeded or Failed"
Jan 18 21:52:56.113: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-52d2bf10-629c-4472-95d0-dc4cd311a562 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 21:52:56.119
Jan 18 21:52:56.135: INFO: Waiting for pod pod-projected-configmaps-52d2bf10-629c-4472-95d0-dc4cd311a562 to disappear
Jan 18 21:52:56.139: INFO: Pod pod-projected-configmaps-52d2bf10-629c-4472-95d0-dc4cd311a562 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 18 21:52:56.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8880" for this suite. 01/18/23 21:52:56.144
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":123,"skipped":2512,"failed":0}
------------------------------
• [SLOW TEST] [6.169 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:52:49.981
    Jan 18 21:52:49.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 21:52:49.986
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:50.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:50.055
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-bd5b056a-ecc2-4c79-bb6d-1623abf07e98 01/18/23 21:52:50.064
    STEP: Creating a pod to test consume configMaps 01/18/23 21:52:50.072
    Jan 18 21:52:50.093: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-52d2bf10-629c-4472-95d0-dc4cd311a562" in namespace "projected-8880" to be "Succeeded or Failed"
    Jan 18 21:52:50.103: INFO: Pod "pod-projected-configmaps-52d2bf10-629c-4472-95d0-dc4cd311a562": Phase="Pending", Reason="", readiness=false. Elapsed: 9.734444ms
    Jan 18 21:52:52.108: INFO: Pod "pod-projected-configmaps-52d2bf10-629c-4472-95d0-dc4cd311a562": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01464099s
    Jan 18 21:52:54.109: INFO: Pod "pod-projected-configmaps-52d2bf10-629c-4472-95d0-dc4cd311a562": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015237314s
    Jan 18 21:52:56.109: INFO: Pod "pod-projected-configmaps-52d2bf10-629c-4472-95d0-dc4cd311a562": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015140676s
    STEP: Saw pod success 01/18/23 21:52:56.109
    Jan 18 21:52:56.109: INFO: Pod "pod-projected-configmaps-52d2bf10-629c-4472-95d0-dc4cd311a562" satisfied condition "Succeeded or Failed"
    Jan 18 21:52:56.113: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-52d2bf10-629c-4472-95d0-dc4cd311a562 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 21:52:56.119
    Jan 18 21:52:56.135: INFO: Waiting for pod pod-projected-configmaps-52d2bf10-629c-4472-95d0-dc4cd311a562 to disappear
    Jan 18 21:52:56.139: INFO: Pod pod-projected-configmaps-52d2bf10-629c-4472-95d0-dc4cd311a562 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 18 21:52:56.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8880" for this suite. 01/18/23 21:52:56.144
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:52:56.155
Jan 18 21:52:56.155: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 21:52:56.167
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:56.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:56.194
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-88d183ae-dafd-47f4-a5df-c6f37bb75f04 01/18/23 21:52:56.203
STEP: Creating a pod to test consume secrets 01/18/23 21:52:56.208
Jan 18 21:52:56.227: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-46700dca-ce95-45ac-90a9-bc5c3653e046" in namespace "projected-5121" to be "Succeeded or Failed"
Jan 18 21:52:56.232: INFO: Pod "pod-projected-secrets-46700dca-ce95-45ac-90a9-bc5c3653e046": Phase="Pending", Reason="", readiness=false. Elapsed: 4.844293ms
Jan 18 21:52:58.237: INFO: Pod "pod-projected-secrets-46700dca-ce95-45ac-90a9-bc5c3653e046": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009765998s
Jan 18 21:53:00.248: INFO: Pod "pod-projected-secrets-46700dca-ce95-45ac-90a9-bc5c3653e046": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020892307s
STEP: Saw pod success 01/18/23 21:53:00.248
Jan 18 21:53:00.249: INFO: Pod "pod-projected-secrets-46700dca-ce95-45ac-90a9-bc5c3653e046" satisfied condition "Succeeded or Failed"
Jan 18 21:53:00.254: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-secrets-46700dca-ce95-45ac-90a9-bc5c3653e046 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/18/23 21:53:00.261
Jan 18 21:53:00.274: INFO: Waiting for pod pod-projected-secrets-46700dca-ce95-45ac-90a9-bc5c3653e046 to disappear
Jan 18 21:53:00.279: INFO: Pod pod-projected-secrets-46700dca-ce95-45ac-90a9-bc5c3653e046 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 18 21:53:00.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5121" for this suite. 01/18/23 21:53:00.285
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":124,"skipped":2512,"failed":0}
------------------------------
• [4.137 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:52:56.155
    Jan 18 21:52:56.155: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 21:52:56.167
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:56.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:56.194
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-88d183ae-dafd-47f4-a5df-c6f37bb75f04 01/18/23 21:52:56.203
    STEP: Creating a pod to test consume secrets 01/18/23 21:52:56.208
    Jan 18 21:52:56.227: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-46700dca-ce95-45ac-90a9-bc5c3653e046" in namespace "projected-5121" to be "Succeeded or Failed"
    Jan 18 21:52:56.232: INFO: Pod "pod-projected-secrets-46700dca-ce95-45ac-90a9-bc5c3653e046": Phase="Pending", Reason="", readiness=false. Elapsed: 4.844293ms
    Jan 18 21:52:58.237: INFO: Pod "pod-projected-secrets-46700dca-ce95-45ac-90a9-bc5c3653e046": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009765998s
    Jan 18 21:53:00.248: INFO: Pod "pod-projected-secrets-46700dca-ce95-45ac-90a9-bc5c3653e046": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020892307s
    STEP: Saw pod success 01/18/23 21:53:00.248
    Jan 18 21:53:00.249: INFO: Pod "pod-projected-secrets-46700dca-ce95-45ac-90a9-bc5c3653e046" satisfied condition "Succeeded or Failed"
    Jan 18 21:53:00.254: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-secrets-46700dca-ce95-45ac-90a9-bc5c3653e046 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 21:53:00.261
    Jan 18 21:53:00.274: INFO: Waiting for pod pod-projected-secrets-46700dca-ce95-45ac-90a9-bc5c3653e046 to disappear
    Jan 18 21:53:00.279: INFO: Pod pod-projected-secrets-46700dca-ce95-45ac-90a9-bc5c3653e046 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 18 21:53:00.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5121" for this suite. 01/18/23 21:53:00.285
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:53:00.293
Jan 18 21:53:00.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename replicaset 01/18/23 21:53:00.295
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:53:00.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:53:00.33
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jan 18 21:53:00.339: INFO: Creating ReplicaSet my-hostname-basic-7866a863-79c8-4112-aaef-a41b37addee8
Jan 18 21:53:00.349: INFO: Pod name my-hostname-basic-7866a863-79c8-4112-aaef-a41b37addee8: Found 0 pods out of 1
Jan 18 21:53:05.364: INFO: Pod name my-hostname-basic-7866a863-79c8-4112-aaef-a41b37addee8: Found 1 pods out of 1
Jan 18 21:53:05.364: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-7866a863-79c8-4112-aaef-a41b37addee8" is running
Jan 18 21:53:05.364: INFO: Waiting up to 5m0s for pod "my-hostname-basic-7866a863-79c8-4112-aaef-a41b37addee8-pc5xv" in namespace "replicaset-7953" to be "running"
Jan 18 21:53:05.379: INFO: Pod "my-hostname-basic-7866a863-79c8-4112-aaef-a41b37addee8-pc5xv": Phase="Running", Reason="", readiness=true. Elapsed: 14.564839ms
Jan 18 21:53:05.379: INFO: Pod "my-hostname-basic-7866a863-79c8-4112-aaef-a41b37addee8-pc5xv" satisfied condition "running"
Jan 18 21:53:05.379: INFO: Pod "my-hostname-basic-7866a863-79c8-4112-aaef-a41b37addee8-pc5xv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 21:53:00 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 21:53:02 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 21:53:02 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 21:53:00 +0000 UTC Reason: Message:}])
Jan 18 21:53:05.379: INFO: Trying to dial the pod
Jan 18 21:53:10.398: INFO: Controller my-hostname-basic-7866a863-79c8-4112-aaef-a41b37addee8: Got expected result from replica 1 [my-hostname-basic-7866a863-79c8-4112-aaef-a41b37addee8-pc5xv]: "my-hostname-basic-7866a863-79c8-4112-aaef-a41b37addee8-pc5xv", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 18 21:53:10.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7953" for this suite. 01/18/23 21:53:10.404
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":125,"skipped":2516,"failed":0}
------------------------------
• [SLOW TEST] [10.119 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:53:00.293
    Jan 18 21:53:00.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename replicaset 01/18/23 21:53:00.295
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:53:00.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:53:00.33
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jan 18 21:53:00.339: INFO: Creating ReplicaSet my-hostname-basic-7866a863-79c8-4112-aaef-a41b37addee8
    Jan 18 21:53:00.349: INFO: Pod name my-hostname-basic-7866a863-79c8-4112-aaef-a41b37addee8: Found 0 pods out of 1
    Jan 18 21:53:05.364: INFO: Pod name my-hostname-basic-7866a863-79c8-4112-aaef-a41b37addee8: Found 1 pods out of 1
    Jan 18 21:53:05.364: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-7866a863-79c8-4112-aaef-a41b37addee8" is running
    Jan 18 21:53:05.364: INFO: Waiting up to 5m0s for pod "my-hostname-basic-7866a863-79c8-4112-aaef-a41b37addee8-pc5xv" in namespace "replicaset-7953" to be "running"
    Jan 18 21:53:05.379: INFO: Pod "my-hostname-basic-7866a863-79c8-4112-aaef-a41b37addee8-pc5xv": Phase="Running", Reason="", readiness=true. Elapsed: 14.564839ms
    Jan 18 21:53:05.379: INFO: Pod "my-hostname-basic-7866a863-79c8-4112-aaef-a41b37addee8-pc5xv" satisfied condition "running"
    Jan 18 21:53:05.379: INFO: Pod "my-hostname-basic-7866a863-79c8-4112-aaef-a41b37addee8-pc5xv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 21:53:00 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 21:53:02 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 21:53:02 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 21:53:00 +0000 UTC Reason: Message:}])
    Jan 18 21:53:05.379: INFO: Trying to dial the pod
    Jan 18 21:53:10.398: INFO: Controller my-hostname-basic-7866a863-79c8-4112-aaef-a41b37addee8: Got expected result from replica 1 [my-hostname-basic-7866a863-79c8-4112-aaef-a41b37addee8-pc5xv]: "my-hostname-basic-7866a863-79c8-4112-aaef-a41b37addee8-pc5xv", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 18 21:53:10.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-7953" for this suite. 01/18/23 21:53:10.404
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:53:10.419
Jan 18 21:53:10.420: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename aggregator 01/18/23 21:53:10.421
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:53:10.442
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:53:10.446
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jan 18 21:53:10.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 01/18/23 21:53:10.452
Jan 18 21:53:11.097: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jan 18 21:53:13.167: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:53:15.174: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:53:17.173: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:53:19.173: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:53:21.172: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:53:23.172: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:53:25.172: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:53:27.171: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:53:29.174: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:53:31.173: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:53:33.320: INFO: Waited 139.709854ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 01/18/23 21:53:33.416
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/18/23 21:53:33.421
STEP: List APIServices 01/18/23 21:53:33.43
Jan 18 21:53:33.442: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Jan 18 21:53:33.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5269" for this suite. 01/18/23 21:53:34
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":126,"skipped":2535,"failed":0}
------------------------------
• [SLOW TEST] [23.591 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:53:10.419
    Jan 18 21:53:10.420: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename aggregator 01/18/23 21:53:10.421
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:53:10.442
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:53:10.446
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jan 18 21:53:10.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 01/18/23 21:53:10.452
    Jan 18 21:53:11.097: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Jan 18 21:53:13.167: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:53:15.174: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:53:17.173: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:53:19.173: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:53:21.172: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:53:23.172: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:53:25.172: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:53:27.171: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:53:29.174: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:53:31.173: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 53, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:53:33.320: INFO: Waited 139.709854ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 01/18/23 21:53:33.416
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/18/23 21:53:33.421
    STEP: List APIServices 01/18/23 21:53:33.43
    Jan 18 21:53:33.442: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Jan 18 21:53:33.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-5269" for this suite. 01/18/23 21:53:34
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:53:34.017
Jan 18 21:53:34.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename deployment 01/18/23 21:53:34.019
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:53:34.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:53:34.06
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jan 18 21:53:34.081: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 18 21:53:39.101: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 21:53:39.101
Jan 18 21:53:39.101: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/18/23 21:53:39.116
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 21:53:43.163: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7026  f4d7780d-c180-446a-9f23-42a47e091722 144631 1 2023-01-18 21:53:39 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-18 21:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 21:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ab0ff8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 21:53:39 +0000 UTC,LastTransitionTime:2023-01-18 21:53:39 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-01-18 21:53:41 +0000 UTC,LastTransitionTime:2023-01-18 21:53:39 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 18 21:53:43.168: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-7026  f5110cc8-949b-4d0b-933d-0462ad37d3ac 144621 1 2023-01-18 21:53:39 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment f4d7780d-c180-446a-9f23-42a47e091722 0xc000e83a17 0xc000e83a18}] [] [{kube-controller-manager Update apps/v1 2023-01-18 21:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4d7780d-c180-446a-9f23-42a47e091722\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 21:53:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000e83c98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 21:53:43.174: INFO: Pod "test-cleanup-deployment-69cb9c5497-f2x2r" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-f2x2r test-cleanup-deployment-69cb9c5497- deployment-7026  b1e8630a-5283-407b-81bd-f54a8e4e563d 144620 0 2023-01-18 21:53:39 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[cni.projectcalico.org/containerID:673d27f9ea011b98398200399ab1f1601e127ab99163f6d0a24b9878ef99db95 cni.projectcalico.org/podIP:10.233.68.136/32 cni.projectcalico.org/podIPs:10.233.68.136/32] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 f5110cc8-949b-4d0b-933d-0462ad37d3ac 0xc00345e2b7 0xc00345e2b8}] [] [{calico Update v1 2023-01-18 21:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-18 21:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5110cc8-949b-4d0b-933d-0462ad37d3ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 21:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jrsdx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jrsdx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:53:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:53:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.136,StartTime:2023-01-18 21:53:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:53:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://de267e2c25384155a3fd49210b7d6d767fdcfb2d73bc4bace85c965cb0297440,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.136,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 18 21:53:43.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7026" for this suite. 01/18/23 21:53:43.181
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":127,"skipped":2543,"failed":0}
------------------------------
• [SLOW TEST] [9.175 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:53:34.017
    Jan 18 21:53:34.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename deployment 01/18/23 21:53:34.019
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:53:34.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:53:34.06
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jan 18 21:53:34.081: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jan 18 21:53:39.101: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 21:53:39.101
    Jan 18 21:53:39.101: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/18/23 21:53:39.116
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 21:53:43.163: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7026  f4d7780d-c180-446a-9f23-42a47e091722 144631 1 2023-01-18 21:53:39 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-18 21:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 21:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ab0ff8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 21:53:39 +0000 UTC,LastTransitionTime:2023-01-18 21:53:39 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-01-18 21:53:41 +0000 UTC,LastTransitionTime:2023-01-18 21:53:39 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 18 21:53:43.168: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-7026  f5110cc8-949b-4d0b-933d-0462ad37d3ac 144621 1 2023-01-18 21:53:39 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment f4d7780d-c180-446a-9f23-42a47e091722 0xc000e83a17 0xc000e83a18}] [] [{kube-controller-manager Update apps/v1 2023-01-18 21:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4d7780d-c180-446a-9f23-42a47e091722\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 21:53:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000e83c98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 21:53:43.174: INFO: Pod "test-cleanup-deployment-69cb9c5497-f2x2r" is available:
    &Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-f2x2r test-cleanup-deployment-69cb9c5497- deployment-7026  b1e8630a-5283-407b-81bd-f54a8e4e563d 144620 0 2023-01-18 21:53:39 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[cni.projectcalico.org/containerID:673d27f9ea011b98398200399ab1f1601e127ab99163f6d0a24b9878ef99db95 cni.projectcalico.org/podIP:10.233.68.136/32 cni.projectcalico.org/podIPs:10.233.68.136/32] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 f5110cc8-949b-4d0b-933d-0462ad37d3ac 0xc00345e2b7 0xc00345e2b8}] [] [{calico Update v1 2023-01-18 21:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-18 21:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5110cc8-949b-4d0b-933d-0462ad37d3ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 21:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jrsdx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jrsdx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:53:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:53:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.136,StartTime:2023-01-18 21:53:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:53:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://de267e2c25384155a3fd49210b7d6d767fdcfb2d73bc4bace85c965cb0297440,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.136,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 18 21:53:43.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7026" for this suite. 01/18/23 21:53:43.181
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:53:43.197
Jan 18 21:53:43.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename dns 01/18/23 21:53:43.2
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:53:43.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:53:43.231
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 01/18/23 21:53:43.236
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5981.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5981.svc.cluster.local; sleep 1; done
 01/18/23 21:53:43.254
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5981.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5981.svc.cluster.local; sleep 1; done
 01/18/23 21:53:43.255
STEP: creating a pod to probe DNS 01/18/23 21:53:43.255
STEP: submitting the pod to kubernetes 01/18/23 21:53:43.255
Jan 18 21:53:43.268: INFO: Waiting up to 15m0s for pod "dns-test-8a0cb350-0898-4c99-b63b-1a13521d4db9" in namespace "dns-5981" to be "running"
Jan 18 21:53:43.283: INFO: Pod "dns-test-8a0cb350-0898-4c99-b63b-1a13521d4db9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.888987ms
Jan 18 21:53:45.290: INFO: Pod "dns-test-8a0cb350-0898-4c99-b63b-1a13521d4db9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021590954s
Jan 18 21:53:47.292: INFO: Pod "dns-test-8a0cb350-0898-4c99-b63b-1a13521d4db9": Phase="Running", Reason="", readiness=true. Elapsed: 4.023838722s
Jan 18 21:53:47.292: INFO: Pod "dns-test-8a0cb350-0898-4c99-b63b-1a13521d4db9" satisfied condition "running"
STEP: retrieving the pod 01/18/23 21:53:47.293
STEP: looking for the results for each expected name from probers 01/18/23 21:53:47.298
Jan 18 21:53:47.311: INFO: DNS probes using dns-test-8a0cb350-0898-4c99-b63b-1a13521d4db9 succeeded

STEP: deleting the pod 01/18/23 21:53:47.311
STEP: changing the externalName to bar.example.com 01/18/23 21:53:47.329
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5981.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5981.svc.cluster.local; sleep 1; done
 01/18/23 21:53:47.347
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5981.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5981.svc.cluster.local; sleep 1; done
 01/18/23 21:53:47.347
STEP: creating a second pod to probe DNS 01/18/23 21:53:47.348
STEP: submitting the pod to kubernetes 01/18/23 21:53:47.348
Jan 18 21:53:47.357: INFO: Waiting up to 15m0s for pod "dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b" in namespace "dns-5981" to be "running"
Jan 18 21:53:47.386: INFO: Pod "dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b": Phase="Pending", Reason="", readiness=false. Elapsed: 29.262743ms
Jan 18 21:53:49.399: INFO: Pod "dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041838506s
Jan 18 21:53:51.391: INFO: Pod "dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b": Phase="Running", Reason="", readiness=true. Elapsed: 4.034599224s
Jan 18 21:53:51.392: INFO: Pod "dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b" satisfied condition "running"
STEP: retrieving the pod 01/18/23 21:53:51.392
STEP: looking for the results for each expected name from probers 01/18/23 21:53:51.401
Jan 18 21:53:51.408: INFO: File wheezy_udp@dns-test-service-3.dns-5981.svc.cluster.local from pod  dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 21:53:51.412: INFO: File jessie_udp@dns-test-service-3.dns-5981.svc.cluster.local from pod  dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 21:53:51.413: INFO: Lookups using dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b failed for: [wheezy_udp@dns-test-service-3.dns-5981.svc.cluster.local jessie_udp@dns-test-service-3.dns-5981.svc.cluster.local]

Jan 18 21:53:56.421: INFO: File wheezy_udp@dns-test-service-3.dns-5981.svc.cluster.local from pod  dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 21:53:56.428: INFO: File jessie_udp@dns-test-service-3.dns-5981.svc.cluster.local from pod  dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 21:53:56.428: INFO: Lookups using dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b failed for: [wheezy_udp@dns-test-service-3.dns-5981.svc.cluster.local jessie_udp@dns-test-service-3.dns-5981.svc.cluster.local]

Jan 18 21:54:01.421: INFO: File wheezy_udp@dns-test-service-3.dns-5981.svc.cluster.local from pod  dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 21:54:01.427: INFO: File jessie_udp@dns-test-service-3.dns-5981.svc.cluster.local from pod  dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 21:54:01.427: INFO: Lookups using dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b failed for: [wheezy_udp@dns-test-service-3.dns-5981.svc.cluster.local jessie_udp@dns-test-service-3.dns-5981.svc.cluster.local]

Jan 18 21:54:06.421: INFO: File wheezy_udp@dns-test-service-3.dns-5981.svc.cluster.local from pod  dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 21:54:06.426: INFO: File jessie_udp@dns-test-service-3.dns-5981.svc.cluster.local from pod  dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 21:54:06.426: INFO: Lookups using dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b failed for: [wheezy_udp@dns-test-service-3.dns-5981.svc.cluster.local jessie_udp@dns-test-service-3.dns-5981.svc.cluster.local]

Jan 18 21:54:11.422: INFO: File wheezy_udp@dns-test-service-3.dns-5981.svc.cluster.local from pod  dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 21:54:11.428: INFO: File jessie_udp@dns-test-service-3.dns-5981.svc.cluster.local from pod  dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 21:54:11.428: INFO: Lookups using dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b failed for: [wheezy_udp@dns-test-service-3.dns-5981.svc.cluster.local jessie_udp@dns-test-service-3.dns-5981.svc.cluster.local]

Jan 18 21:54:16.427: INFO: DNS probes using dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b succeeded

STEP: deleting the pod 01/18/23 21:54:16.428
STEP: changing the service to type=ClusterIP 01/18/23 21:54:16.455
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5981.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5981.svc.cluster.local; sleep 1; done
 01/18/23 21:54:16.515
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5981.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5981.svc.cluster.local; sleep 1; done
 01/18/23 21:54:16.515
STEP: creating a third pod to probe DNS 01/18/23 21:54:16.516
STEP: submitting the pod to kubernetes 01/18/23 21:54:16.534
Jan 18 21:54:16.548: INFO: Waiting up to 15m0s for pod "dns-test-fd9880bc-5d85-4d70-8896-205a5bec827e" in namespace "dns-5981" to be "running"
Jan 18 21:54:16.552: INFO: Pod "dns-test-fd9880bc-5d85-4d70-8896-205a5bec827e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.314199ms
Jan 18 21:54:18.561: INFO: Pod "dns-test-fd9880bc-5d85-4d70-8896-205a5bec827e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012553163s
Jan 18 21:54:20.559: INFO: Pod "dns-test-fd9880bc-5d85-4d70-8896-205a5bec827e": Phase="Running", Reason="", readiness=true. Elapsed: 4.010868427s
Jan 18 21:54:20.559: INFO: Pod "dns-test-fd9880bc-5d85-4d70-8896-205a5bec827e" satisfied condition "running"
STEP: retrieving the pod 01/18/23 21:54:20.559
STEP: looking for the results for each expected name from probers 01/18/23 21:54:20.564
Jan 18 21:54:20.576: INFO: DNS probes using dns-test-fd9880bc-5d85-4d70-8896-205a5bec827e succeeded

STEP: deleting the pod 01/18/23 21:54:20.577
STEP: deleting the test externalName service 01/18/23 21:54:20.595
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 18 21:54:20.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5981" for this suite. 01/18/23 21:54:20.622
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":128,"skipped":2552,"failed":0}
------------------------------
• [SLOW TEST] [37.441 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:53:43.197
    Jan 18 21:53:43.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename dns 01/18/23 21:53:43.2
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:53:43.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:53:43.231
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 01/18/23 21:53:43.236
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5981.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5981.svc.cluster.local; sleep 1; done
     01/18/23 21:53:43.254
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5981.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5981.svc.cluster.local; sleep 1; done
     01/18/23 21:53:43.255
    STEP: creating a pod to probe DNS 01/18/23 21:53:43.255
    STEP: submitting the pod to kubernetes 01/18/23 21:53:43.255
    Jan 18 21:53:43.268: INFO: Waiting up to 15m0s for pod "dns-test-8a0cb350-0898-4c99-b63b-1a13521d4db9" in namespace "dns-5981" to be "running"
    Jan 18 21:53:43.283: INFO: Pod "dns-test-8a0cb350-0898-4c99-b63b-1a13521d4db9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.888987ms
    Jan 18 21:53:45.290: INFO: Pod "dns-test-8a0cb350-0898-4c99-b63b-1a13521d4db9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021590954s
    Jan 18 21:53:47.292: INFO: Pod "dns-test-8a0cb350-0898-4c99-b63b-1a13521d4db9": Phase="Running", Reason="", readiness=true. Elapsed: 4.023838722s
    Jan 18 21:53:47.292: INFO: Pod "dns-test-8a0cb350-0898-4c99-b63b-1a13521d4db9" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 21:53:47.293
    STEP: looking for the results for each expected name from probers 01/18/23 21:53:47.298
    Jan 18 21:53:47.311: INFO: DNS probes using dns-test-8a0cb350-0898-4c99-b63b-1a13521d4db9 succeeded

    STEP: deleting the pod 01/18/23 21:53:47.311
    STEP: changing the externalName to bar.example.com 01/18/23 21:53:47.329
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5981.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5981.svc.cluster.local; sleep 1; done
     01/18/23 21:53:47.347
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5981.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5981.svc.cluster.local; sleep 1; done
     01/18/23 21:53:47.347
    STEP: creating a second pod to probe DNS 01/18/23 21:53:47.348
    STEP: submitting the pod to kubernetes 01/18/23 21:53:47.348
    Jan 18 21:53:47.357: INFO: Waiting up to 15m0s for pod "dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b" in namespace "dns-5981" to be "running"
    Jan 18 21:53:47.386: INFO: Pod "dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b": Phase="Pending", Reason="", readiness=false. Elapsed: 29.262743ms
    Jan 18 21:53:49.399: INFO: Pod "dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041838506s
    Jan 18 21:53:51.391: INFO: Pod "dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b": Phase="Running", Reason="", readiness=true. Elapsed: 4.034599224s
    Jan 18 21:53:51.392: INFO: Pod "dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 21:53:51.392
    STEP: looking for the results for each expected name from probers 01/18/23 21:53:51.401
    Jan 18 21:53:51.408: INFO: File wheezy_udp@dns-test-service-3.dns-5981.svc.cluster.local from pod  dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 21:53:51.412: INFO: File jessie_udp@dns-test-service-3.dns-5981.svc.cluster.local from pod  dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 21:53:51.413: INFO: Lookups using dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b failed for: [wheezy_udp@dns-test-service-3.dns-5981.svc.cluster.local jessie_udp@dns-test-service-3.dns-5981.svc.cluster.local]

    Jan 18 21:53:56.421: INFO: File wheezy_udp@dns-test-service-3.dns-5981.svc.cluster.local from pod  dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 21:53:56.428: INFO: File jessie_udp@dns-test-service-3.dns-5981.svc.cluster.local from pod  dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 21:53:56.428: INFO: Lookups using dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b failed for: [wheezy_udp@dns-test-service-3.dns-5981.svc.cluster.local jessie_udp@dns-test-service-3.dns-5981.svc.cluster.local]

    Jan 18 21:54:01.421: INFO: File wheezy_udp@dns-test-service-3.dns-5981.svc.cluster.local from pod  dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 21:54:01.427: INFO: File jessie_udp@dns-test-service-3.dns-5981.svc.cluster.local from pod  dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 21:54:01.427: INFO: Lookups using dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b failed for: [wheezy_udp@dns-test-service-3.dns-5981.svc.cluster.local jessie_udp@dns-test-service-3.dns-5981.svc.cluster.local]

    Jan 18 21:54:06.421: INFO: File wheezy_udp@dns-test-service-3.dns-5981.svc.cluster.local from pod  dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 21:54:06.426: INFO: File jessie_udp@dns-test-service-3.dns-5981.svc.cluster.local from pod  dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 21:54:06.426: INFO: Lookups using dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b failed for: [wheezy_udp@dns-test-service-3.dns-5981.svc.cluster.local jessie_udp@dns-test-service-3.dns-5981.svc.cluster.local]

    Jan 18 21:54:11.422: INFO: File wheezy_udp@dns-test-service-3.dns-5981.svc.cluster.local from pod  dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 21:54:11.428: INFO: File jessie_udp@dns-test-service-3.dns-5981.svc.cluster.local from pod  dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 21:54:11.428: INFO: Lookups using dns-5981/dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b failed for: [wheezy_udp@dns-test-service-3.dns-5981.svc.cluster.local jessie_udp@dns-test-service-3.dns-5981.svc.cluster.local]

    Jan 18 21:54:16.427: INFO: DNS probes using dns-test-c35c542d-890d-438d-9f05-da1b7dcc5e5b succeeded

    STEP: deleting the pod 01/18/23 21:54:16.428
    STEP: changing the service to type=ClusterIP 01/18/23 21:54:16.455
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5981.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5981.svc.cluster.local; sleep 1; done
     01/18/23 21:54:16.515
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5981.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5981.svc.cluster.local; sleep 1; done
     01/18/23 21:54:16.515
    STEP: creating a third pod to probe DNS 01/18/23 21:54:16.516
    STEP: submitting the pod to kubernetes 01/18/23 21:54:16.534
    Jan 18 21:54:16.548: INFO: Waiting up to 15m0s for pod "dns-test-fd9880bc-5d85-4d70-8896-205a5bec827e" in namespace "dns-5981" to be "running"
    Jan 18 21:54:16.552: INFO: Pod "dns-test-fd9880bc-5d85-4d70-8896-205a5bec827e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.314199ms
    Jan 18 21:54:18.561: INFO: Pod "dns-test-fd9880bc-5d85-4d70-8896-205a5bec827e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012553163s
    Jan 18 21:54:20.559: INFO: Pod "dns-test-fd9880bc-5d85-4d70-8896-205a5bec827e": Phase="Running", Reason="", readiness=true. Elapsed: 4.010868427s
    Jan 18 21:54:20.559: INFO: Pod "dns-test-fd9880bc-5d85-4d70-8896-205a5bec827e" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 21:54:20.559
    STEP: looking for the results for each expected name from probers 01/18/23 21:54:20.564
    Jan 18 21:54:20.576: INFO: DNS probes using dns-test-fd9880bc-5d85-4d70-8896-205a5bec827e succeeded

    STEP: deleting the pod 01/18/23 21:54:20.577
    STEP: deleting the test externalName service 01/18/23 21:54:20.595
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 18 21:54:20.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5981" for this suite. 01/18/23 21:54:20.622
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:54:20.641
Jan 18 21:54:20.641: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename kubectl 01/18/23 21:54:20.644
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:54:20.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:54:20.693
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Jan 18 21:54:20.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7699 create -f -'
Jan 18 21:54:22.297: INFO: stderr: ""
Jan 18 21:54:22.297: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan 18 21:54:22.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7699 create -f -'
Jan 18 21:54:22.672: INFO: stderr: ""
Jan 18 21:54:22.672: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/18/23 21:54:22.672
Jan 18 21:54:23.681: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 21:54:23.681: INFO: Found 0 / 1
Jan 18 21:54:24.679: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 21:54:24.679: INFO: Found 1 / 1
Jan 18 21:54:24.679: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 18 21:54:24.684: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 21:54:24.684: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 18 21:54:24.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7699 describe pod agnhost-primary-bncmz'
Jan 18 21:54:24.854: INFO: stderr: ""
Jan 18 21:54:24.854: INFO: stdout: "Name:             agnhost-primary-bncmz\nNamespace:        kubectl-7699\nPriority:         0\nService Account:  default\nNode:             v1-25-1-18760-w2/192.168.101.216\nStart Time:       Wed, 18 Jan 2023 21:54:22 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: bc5ba800ca5c0f7d56d31bd905fba4545e684f833239fa7561cf3e958044440c\n                  cni.projectcalico.org/podIP: 10.233.68.141/32\n                  cni.projectcalico.org/podIPs: 10.233.68.141/32\nStatus:           Running\nIP:               10.233.68.141\nIPs:\n  IP:           10.233.68.141\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://109cf5c08d913efa4bdcb3b37e8357daa15b94cb714bd0cbe91df1d1b75d61ca\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 18 Jan 2023 21:54:23 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-dp8tz (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-dp8tz:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-7699/agnhost-primary-bncmz to v1-25-1-18760-w2\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Jan 18 21:54:24.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7699 describe rc agnhost-primary'
Jan 18 21:54:25.010: INFO: stderr: ""
Jan 18 21:54:25.010: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7699\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-bncmz\n"
Jan 18 21:54:25.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7699 describe service agnhost-primary'
Jan 18 21:54:25.174: INFO: stderr: ""
Jan 18 21:54:25.174: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7699\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.30.195\nIPs:               10.233.30.195\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.68.141:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 18 21:54:25.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7699 describe node v1-25-1-18760-m'
Jan 18 21:54:25.335: INFO: stderr: ""
Jan 18 21:54:25.335: INFO: stdout: "Name:               v1-25-1-18760-m\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m1.medium\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=RegionOne\n                    failure-domain.beta.kubernetes.io/zone=pod5\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=v1-25-1-18760-m\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=m1.medium\n                    taikun.cloud/org-id=1\n                    taikun.cloud/project-id=18760\n                    taikun.cloud/server-id=37767\n                    topology.cinder.csi.openstack.org/zone=pod5\n                    topology.kubernetes.io/region=RegionOne\n                    topology.kubernetes.io/zone=pod5\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 192.168.101.240\n                    csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"31b97858-80a8-441e-947f-0678d1396043\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.101.240/24\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 18 Jan 2023 11:44:54 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  v1-25-1-18760-m\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 18 Jan 2023 21:54:19 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 18 Jan 2023 11:47:05 +0000   Wed, 18 Jan 2023 11:47:05 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 18 Jan 2023 21:54:23 +0000   Wed, 18 Jan 2023 11:44:51 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 18 Jan 2023 21:54:23 +0000   Wed, 18 Jan 2023 11:44:51 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 18 Jan 2023 21:54:23 +0000   Wed, 18 Jan 2023 11:44:51 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 18 Jan 2023 21:54:23 +0000   Wed, 18 Jan 2023 11:47:04 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.101.240\n  Hostname:    v1-25-1-18760-m\nCapacity:\n  cpu:                2\n  ephemeral-storage:  30308240Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             4030132Ki\n  pods:               110\nAllocatable:\n  cpu:                1800m\n  ephemeral-storage:  27932073938\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3403444Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 bfd6d08d7bc4496a9514c7cc576c35f2\n  System UUID:                31b97858-80a8-441e-947f-0678d1396043\n  Boot ID:                    82a98be4-5790-4809-b102-a84938ce2ec6\n  Kernel Version:             5.8.0-43-generic\n  OS Image:                   Ubuntu 20.04.1 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.14\n  Kubelet Version:            v1.25.4\n  Kube-Proxy Version:         v1.25.4\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nProviderID:                   openstack:///31b97858-80a8-441e-947f-0678d1396043\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-575656fd47-bgrr8                   30m (1%)      1 (55%)     64M (1%)         256M (7%)      10h\n  kube-system                 calico-node-25zdl                                          150m (8%)     300m (16%)  64M (1%)         500M (14%)     10h\n  kube-system                 kube-apiserver-v1-25-1-18760-m                             250m (13%)    0 (0%)      0 (0%)           0 (0%)         10h\n  kube-system                 kube-controller-manager-v1-25-1-18760-m                    200m (11%)    0 (0%)      0 (0%)           0 (0%)         10h\n  kube-system                 kube-proxy-fnlsm                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         10h\n  kube-system                 kube-scheduler-v1-25-1-18760-m                             100m (5%)     0 (0%)      0 (0%)           0 (0%)         10h\n  kube-system                 nodelocaldns-pfhmz                                         100m (5%)     0 (0%)      70Mi (2%)        200Mi (6%)     10h\n  kube-system                 openstack-cinder-csi-nodeplugin-8wfrw                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         7h14m\n  kube-system                 openstack-cloud-controller-manager-n7b8m                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         7h14m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-c355d4758f184dbb-6lth4    0 (0%)        0 (0%)      0 (0%)           0 (0%)         24m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                830m (46%)      1300m (72%)\n  memory             201400320 (5%)  965715200 (27%)\n  ephemeral-storage  0 (0%)          0 (0%)\n  hugepages-1Gi      0 (0%)          0 (0%)\n  hugepages-2Mi      0 (0%)          0 (0%)\nEvents:              <none>\n"
Jan 18 21:54:25.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7699 describe namespace kubectl-7699'
Jan 18 21:54:25.464: INFO: stderr: ""
Jan 18 21:54:25.464: INFO: stdout: "Name:         kubectl-7699\nLabels:       e2e-framework=kubectl\n              e2e-run=86437d04-e981-49ed-a4eb-9ac6ca442d05\n              kubernetes.io/metadata.name=kubectl-7699\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 21:54:25.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7699" for this suite. 01/18/23 21:54:25.471
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":129,"skipped":2565,"failed":0}
------------------------------
• [4.838 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:54:20.641
    Jan 18 21:54:20.641: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename kubectl 01/18/23 21:54:20.644
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:54:20.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:54:20.693
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Jan 18 21:54:20.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7699 create -f -'
    Jan 18 21:54:22.297: INFO: stderr: ""
    Jan 18 21:54:22.297: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jan 18 21:54:22.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7699 create -f -'
    Jan 18 21:54:22.672: INFO: stderr: ""
    Jan 18 21:54:22.672: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/18/23 21:54:22.672
    Jan 18 21:54:23.681: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 21:54:23.681: INFO: Found 0 / 1
    Jan 18 21:54:24.679: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 21:54:24.679: INFO: Found 1 / 1
    Jan 18 21:54:24.679: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 18 21:54:24.684: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 21:54:24.684: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 18 21:54:24.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7699 describe pod agnhost-primary-bncmz'
    Jan 18 21:54:24.854: INFO: stderr: ""
    Jan 18 21:54:24.854: INFO: stdout: "Name:             agnhost-primary-bncmz\nNamespace:        kubectl-7699\nPriority:         0\nService Account:  default\nNode:             v1-25-1-18760-w2/192.168.101.216\nStart Time:       Wed, 18 Jan 2023 21:54:22 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: bc5ba800ca5c0f7d56d31bd905fba4545e684f833239fa7561cf3e958044440c\n                  cni.projectcalico.org/podIP: 10.233.68.141/32\n                  cni.projectcalico.org/podIPs: 10.233.68.141/32\nStatus:           Running\nIP:               10.233.68.141\nIPs:\n  IP:           10.233.68.141\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://109cf5c08d913efa4bdcb3b37e8357daa15b94cb714bd0cbe91df1d1b75d61ca\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 18 Jan 2023 21:54:23 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-dp8tz (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-dp8tz:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-7699/agnhost-primary-bncmz to v1-25-1-18760-w2\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Jan 18 21:54:24.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7699 describe rc agnhost-primary'
    Jan 18 21:54:25.010: INFO: stderr: ""
    Jan 18 21:54:25.010: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7699\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-bncmz\n"
    Jan 18 21:54:25.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7699 describe service agnhost-primary'
    Jan 18 21:54:25.174: INFO: stderr: ""
    Jan 18 21:54:25.174: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7699\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.30.195\nIPs:               10.233.30.195\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.68.141:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jan 18 21:54:25.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7699 describe node v1-25-1-18760-m'
    Jan 18 21:54:25.335: INFO: stderr: ""
    Jan 18 21:54:25.335: INFO: stdout: "Name:               v1-25-1-18760-m\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m1.medium\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=RegionOne\n                    failure-domain.beta.kubernetes.io/zone=pod5\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=v1-25-1-18760-m\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=m1.medium\n                    taikun.cloud/org-id=1\n                    taikun.cloud/project-id=18760\n                    taikun.cloud/server-id=37767\n                    topology.cinder.csi.openstack.org/zone=pod5\n                    topology.kubernetes.io/region=RegionOne\n                    topology.kubernetes.io/zone=pod5\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 192.168.101.240\n                    csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"31b97858-80a8-441e-947f-0678d1396043\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.101.240/24\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 18 Jan 2023 11:44:54 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  v1-25-1-18760-m\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 18 Jan 2023 21:54:19 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 18 Jan 2023 11:47:05 +0000   Wed, 18 Jan 2023 11:47:05 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 18 Jan 2023 21:54:23 +0000   Wed, 18 Jan 2023 11:44:51 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 18 Jan 2023 21:54:23 +0000   Wed, 18 Jan 2023 11:44:51 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 18 Jan 2023 21:54:23 +0000   Wed, 18 Jan 2023 11:44:51 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 18 Jan 2023 21:54:23 +0000   Wed, 18 Jan 2023 11:47:04 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.101.240\n  Hostname:    v1-25-1-18760-m\nCapacity:\n  cpu:                2\n  ephemeral-storage:  30308240Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             4030132Ki\n  pods:               110\nAllocatable:\n  cpu:                1800m\n  ephemeral-storage:  27932073938\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3403444Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 bfd6d08d7bc4496a9514c7cc576c35f2\n  System UUID:                31b97858-80a8-441e-947f-0678d1396043\n  Boot ID:                    82a98be4-5790-4809-b102-a84938ce2ec6\n  Kernel Version:             5.8.0-43-generic\n  OS Image:                   Ubuntu 20.04.1 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.14\n  Kubelet Version:            v1.25.4\n  Kube-Proxy Version:         v1.25.4\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nProviderID:                   openstack:///31b97858-80a8-441e-947f-0678d1396043\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-575656fd47-bgrr8                   30m (1%)      1 (55%)     64M (1%)         256M (7%)      10h\n  kube-system                 calico-node-25zdl                                          150m (8%)     300m (16%)  64M (1%)         500M (14%)     10h\n  kube-system                 kube-apiserver-v1-25-1-18760-m                             250m (13%)    0 (0%)      0 (0%)           0 (0%)         10h\n  kube-system                 kube-controller-manager-v1-25-1-18760-m                    200m (11%)    0 (0%)      0 (0%)           0 (0%)         10h\n  kube-system                 kube-proxy-fnlsm                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         10h\n  kube-system                 kube-scheduler-v1-25-1-18760-m                             100m (5%)     0 (0%)      0 (0%)           0 (0%)         10h\n  kube-system                 nodelocaldns-pfhmz                                         100m (5%)     0 (0%)      70Mi (2%)        200Mi (6%)     10h\n  kube-system                 openstack-cinder-csi-nodeplugin-8wfrw                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         7h14m\n  kube-system                 openstack-cloud-controller-manager-n7b8m                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         7h14m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-c355d4758f184dbb-6lth4    0 (0%)        0 (0%)      0 (0%)           0 (0%)         24m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                830m (46%)      1300m (72%)\n  memory             201400320 (5%)  965715200 (27%)\n  ephemeral-storage  0 (0%)          0 (0%)\n  hugepages-1Gi      0 (0%)          0 (0%)\n  hugepages-2Mi      0 (0%)          0 (0%)\nEvents:              <none>\n"
    Jan 18 21:54:25.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7699 describe namespace kubectl-7699'
    Jan 18 21:54:25.464: INFO: stderr: ""
    Jan 18 21:54:25.464: INFO: stdout: "Name:         kubectl-7699\nLabels:       e2e-framework=kubectl\n              e2e-run=86437d04-e981-49ed-a4eb-9ac6ca442d05\n              kubernetes.io/metadata.name=kubectl-7699\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 21:54:25.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7699" for this suite. 01/18/23 21:54:25.471
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:54:25.482
Jan 18 21:54:25.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename svcaccounts 01/18/23 21:54:25.484
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:54:25.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:54:25.51
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  01/18/23 21:54:25.52
Jan 18 21:54:25.529: INFO: Waiting up to 5m0s for pod "test-pod-f0764044-b0ac-4d8d-b766-69456444bcbe" in namespace "svcaccounts-7655" to be "Succeeded or Failed"
Jan 18 21:54:25.534: INFO: Pod "test-pod-f0764044-b0ac-4d8d-b766-69456444bcbe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.228239ms
Jan 18 21:54:27.542: INFO: Pod "test-pod-f0764044-b0ac-4d8d-b766-69456444bcbe": Phase="Running", Reason="", readiness=true. Elapsed: 2.012171026s
Jan 18 21:54:29.540: INFO: Pod "test-pod-f0764044-b0ac-4d8d-b766-69456444bcbe": Phase="Running", Reason="", readiness=false. Elapsed: 4.010692604s
Jan 18 21:54:31.540: INFO: Pod "test-pod-f0764044-b0ac-4d8d-b766-69456444bcbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010110109s
STEP: Saw pod success 01/18/23 21:54:31.54
Jan 18 21:54:31.541: INFO: Pod "test-pod-f0764044-b0ac-4d8d-b766-69456444bcbe" satisfied condition "Succeeded or Failed"
Jan 18 21:54:31.546: INFO: Trying to get logs from node v1-25-1-18760-w2 pod test-pod-f0764044-b0ac-4d8d-b766-69456444bcbe container agnhost-container: <nil>
STEP: delete the pod 01/18/23 21:54:31.566
Jan 18 21:54:31.582: INFO: Waiting for pod test-pod-f0764044-b0ac-4d8d-b766-69456444bcbe to disappear
Jan 18 21:54:31.588: INFO: Pod test-pod-f0764044-b0ac-4d8d-b766-69456444bcbe no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 18 21:54:31.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7655" for this suite. 01/18/23 21:54:31.596
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":130,"skipped":2577,"failed":0}
------------------------------
• [SLOW TEST] [6.124 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:54:25.482
    Jan 18 21:54:25.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 21:54:25.484
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:54:25.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:54:25.51
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  01/18/23 21:54:25.52
    Jan 18 21:54:25.529: INFO: Waiting up to 5m0s for pod "test-pod-f0764044-b0ac-4d8d-b766-69456444bcbe" in namespace "svcaccounts-7655" to be "Succeeded or Failed"
    Jan 18 21:54:25.534: INFO: Pod "test-pod-f0764044-b0ac-4d8d-b766-69456444bcbe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.228239ms
    Jan 18 21:54:27.542: INFO: Pod "test-pod-f0764044-b0ac-4d8d-b766-69456444bcbe": Phase="Running", Reason="", readiness=true. Elapsed: 2.012171026s
    Jan 18 21:54:29.540: INFO: Pod "test-pod-f0764044-b0ac-4d8d-b766-69456444bcbe": Phase="Running", Reason="", readiness=false. Elapsed: 4.010692604s
    Jan 18 21:54:31.540: INFO: Pod "test-pod-f0764044-b0ac-4d8d-b766-69456444bcbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010110109s
    STEP: Saw pod success 01/18/23 21:54:31.54
    Jan 18 21:54:31.541: INFO: Pod "test-pod-f0764044-b0ac-4d8d-b766-69456444bcbe" satisfied condition "Succeeded or Failed"
    Jan 18 21:54:31.546: INFO: Trying to get logs from node v1-25-1-18760-w2 pod test-pod-f0764044-b0ac-4d8d-b766-69456444bcbe container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 21:54:31.566
    Jan 18 21:54:31.582: INFO: Waiting for pod test-pod-f0764044-b0ac-4d8d-b766-69456444bcbe to disappear
    Jan 18 21:54:31.588: INFO: Pod test-pod-f0764044-b0ac-4d8d-b766-69456444bcbe no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 18 21:54:31.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-7655" for this suite. 01/18/23 21:54:31.596
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:54:31.612
Jan 18 21:54:31.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 21:54:31.616
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:54:31.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:54:31.641
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-724a6fa0-1b4a-4115-9bec-1fa8d52bb766 01/18/23 21:54:31.647
STEP: Creating secret with name secret-projected-all-test-volume-ca144ba8-5e95-4609-8bb6-7a576b82168f 01/18/23 21:54:31.653
STEP: Creating a pod to test Check all projections for projected volume plugin 01/18/23 21:54:31.659
Jan 18 21:54:31.667: INFO: Waiting up to 5m0s for pod "projected-volume-6842c402-ba40-443d-8591-4c489f27fec4" in namespace "projected-8775" to be "Succeeded or Failed"
Jan 18 21:54:31.679: INFO: Pod "projected-volume-6842c402-ba40-443d-8591-4c489f27fec4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.38527ms
Jan 18 21:54:33.688: INFO: Pod "projected-volume-6842c402-ba40-443d-8591-4c489f27fec4": Phase="Running", Reason="", readiness=true. Elapsed: 2.020448924s
Jan 18 21:54:35.686: INFO: Pod "projected-volume-6842c402-ba40-443d-8591-4c489f27fec4": Phase="Running", Reason="", readiness=false. Elapsed: 4.019047134s
Jan 18 21:54:37.685: INFO: Pod "projected-volume-6842c402-ba40-443d-8591-4c489f27fec4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017184182s
STEP: Saw pod success 01/18/23 21:54:37.685
Jan 18 21:54:37.685: INFO: Pod "projected-volume-6842c402-ba40-443d-8591-4c489f27fec4" satisfied condition "Succeeded or Failed"
Jan 18 21:54:37.690: INFO: Trying to get logs from node v1-25-1-18760-w2 pod projected-volume-6842c402-ba40-443d-8591-4c489f27fec4 container projected-all-volume-test: <nil>
STEP: delete the pod 01/18/23 21:54:37.703
Jan 18 21:54:37.722: INFO: Waiting for pod projected-volume-6842c402-ba40-443d-8591-4c489f27fec4 to disappear
Jan 18 21:54:37.727: INFO: Pod projected-volume-6842c402-ba40-443d-8591-4c489f27fec4 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Jan 18 21:54:37.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8775" for this suite. 01/18/23 21:54:37.734
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":131,"skipped":2581,"failed":0}
------------------------------
• [SLOW TEST] [6.132 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:54:31.612
    Jan 18 21:54:31.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 21:54:31.616
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:54:31.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:54:31.641
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-724a6fa0-1b4a-4115-9bec-1fa8d52bb766 01/18/23 21:54:31.647
    STEP: Creating secret with name secret-projected-all-test-volume-ca144ba8-5e95-4609-8bb6-7a576b82168f 01/18/23 21:54:31.653
    STEP: Creating a pod to test Check all projections for projected volume plugin 01/18/23 21:54:31.659
    Jan 18 21:54:31.667: INFO: Waiting up to 5m0s for pod "projected-volume-6842c402-ba40-443d-8591-4c489f27fec4" in namespace "projected-8775" to be "Succeeded or Failed"
    Jan 18 21:54:31.679: INFO: Pod "projected-volume-6842c402-ba40-443d-8591-4c489f27fec4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.38527ms
    Jan 18 21:54:33.688: INFO: Pod "projected-volume-6842c402-ba40-443d-8591-4c489f27fec4": Phase="Running", Reason="", readiness=true. Elapsed: 2.020448924s
    Jan 18 21:54:35.686: INFO: Pod "projected-volume-6842c402-ba40-443d-8591-4c489f27fec4": Phase="Running", Reason="", readiness=false. Elapsed: 4.019047134s
    Jan 18 21:54:37.685: INFO: Pod "projected-volume-6842c402-ba40-443d-8591-4c489f27fec4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017184182s
    STEP: Saw pod success 01/18/23 21:54:37.685
    Jan 18 21:54:37.685: INFO: Pod "projected-volume-6842c402-ba40-443d-8591-4c489f27fec4" satisfied condition "Succeeded or Failed"
    Jan 18 21:54:37.690: INFO: Trying to get logs from node v1-25-1-18760-w2 pod projected-volume-6842c402-ba40-443d-8591-4c489f27fec4 container projected-all-volume-test: <nil>
    STEP: delete the pod 01/18/23 21:54:37.703
    Jan 18 21:54:37.722: INFO: Waiting for pod projected-volume-6842c402-ba40-443d-8591-4c489f27fec4 to disappear
    Jan 18 21:54:37.727: INFO: Pod projected-volume-6842c402-ba40-443d-8591-4c489f27fec4 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Jan 18 21:54:37.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8775" for this suite. 01/18/23 21:54:37.734
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:54:37.745
Jan 18 21:54:37.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename job 01/18/23 21:54:37.748
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:54:37.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:54:37.778
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 01/18/23 21:54:37.783
STEP: Ensuring active pods == parallelism 01/18/23 21:54:37.789
STEP: delete a job 01/18/23 21:54:41.795
STEP: deleting Job.batch foo in namespace job-3924, will wait for the garbage collector to delete the pods 01/18/23 21:54:41.795
Jan 18 21:54:41.859: INFO: Deleting Job.batch foo took: 8.284189ms
Jan 18 21:54:41.960: INFO: Terminating Job.batch foo pods took: 101.621602ms
STEP: Ensuring job was deleted 01/18/23 21:55:13.062
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 18 21:55:13.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3924" for this suite. 01/18/23 21:55:13.074
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":132,"skipped":2585,"failed":0}
------------------------------
• [SLOW TEST] [35.339 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:54:37.745
    Jan 18 21:54:37.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename job 01/18/23 21:54:37.748
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:54:37.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:54:37.778
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 01/18/23 21:54:37.783
    STEP: Ensuring active pods == parallelism 01/18/23 21:54:37.789
    STEP: delete a job 01/18/23 21:54:41.795
    STEP: deleting Job.batch foo in namespace job-3924, will wait for the garbage collector to delete the pods 01/18/23 21:54:41.795
    Jan 18 21:54:41.859: INFO: Deleting Job.batch foo took: 8.284189ms
    Jan 18 21:54:41.960: INFO: Terminating Job.batch foo pods took: 101.621602ms
    STEP: Ensuring job was deleted 01/18/23 21:55:13.062
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 18 21:55:13.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-3924" for this suite. 01/18/23 21:55:13.074
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:55:13.089
Jan 18 21:55:13.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename prestop 01/18/23 21:55:13.091
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:55:13.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:55:13.173
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-6170 01/18/23 21:55:13.178
STEP: Waiting for pods to come up. 01/18/23 21:55:13.188
Jan 18 21:55:13.189: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-6170" to be "running"
Jan 18 21:55:13.194: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 4.682641ms
Jan 18 21:55:15.202: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.013075175s
Jan 18 21:55:15.202: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-6170 01/18/23 21:55:15.207
Jan 18 21:55:15.216: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-6170" to be "running"
Jan 18 21:55:15.227: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 11.086129ms
Jan 18 21:55:17.251: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.034306851s
Jan 18 21:55:17.251: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 01/18/23 21:55:17.251
Jan 18 21:55:22.275: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 01/18/23 21:55:22.276
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Jan 18 21:55:22.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-6170" for this suite. 01/18/23 21:55:22.311
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":133,"skipped":2601,"failed":0}
------------------------------
• [SLOW TEST] [9.243 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:55:13.089
    Jan 18 21:55:13.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename prestop 01/18/23 21:55:13.091
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:55:13.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:55:13.173
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-6170 01/18/23 21:55:13.178
    STEP: Waiting for pods to come up. 01/18/23 21:55:13.188
    Jan 18 21:55:13.189: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-6170" to be "running"
    Jan 18 21:55:13.194: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 4.682641ms
    Jan 18 21:55:15.202: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.013075175s
    Jan 18 21:55:15.202: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-6170 01/18/23 21:55:15.207
    Jan 18 21:55:15.216: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-6170" to be "running"
    Jan 18 21:55:15.227: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 11.086129ms
    Jan 18 21:55:17.251: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.034306851s
    Jan 18 21:55:17.251: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 01/18/23 21:55:17.251
    Jan 18 21:55:22.275: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 01/18/23 21:55:22.276
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Jan 18 21:55:22.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-6170" for this suite. 01/18/23 21:55:22.311
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:55:22.342
Jan 18 21:55:22.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename crd-webhook 01/18/23 21:55:22.345
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:55:22.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:55:22.384
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/18/23 21:55:22.392
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/18/23 21:55:23.073
STEP: Deploying the custom resource conversion webhook pod 01/18/23 21:55:23.092
STEP: Wait for the deployment to be ready 01/18/23 21:55:23.111
Jan 18 21:55:23.122: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 21:55:25.136
STEP: Verifying the service has paired with the endpoint 01/18/23 21:55:25.153
Jan 18 21:55:26.154: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jan 18 21:55:26.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Creating a v1 custom resource 01/18/23 21:55:28.794
STEP: Create a v2 custom resource 01/18/23 21:55:28.818
STEP: List CRs in v1 01/18/23 21:55:28.902
STEP: List CRs in v2 01/18/23 21:55:28.91
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 21:55:29.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3749" for this suite. 01/18/23 21:55:29.438
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":134,"skipped":2640,"failed":0}
------------------------------
• [SLOW TEST] [7.292 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:55:22.342
    Jan 18 21:55:22.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename crd-webhook 01/18/23 21:55:22.345
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:55:22.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:55:22.384
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/18/23 21:55:22.392
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/18/23 21:55:23.073
    STEP: Deploying the custom resource conversion webhook pod 01/18/23 21:55:23.092
    STEP: Wait for the deployment to be ready 01/18/23 21:55:23.111
    Jan 18 21:55:23.122: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 21:55:25.136
    STEP: Verifying the service has paired with the endpoint 01/18/23 21:55:25.153
    Jan 18 21:55:26.154: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jan 18 21:55:26.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Creating a v1 custom resource 01/18/23 21:55:28.794
    STEP: Create a v2 custom resource 01/18/23 21:55:28.818
    STEP: List CRs in v1 01/18/23 21:55:28.902
    STEP: List CRs in v2 01/18/23 21:55:28.91
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 21:55:29.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-3749" for this suite. 01/18/23 21:55:29.438
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:55:29.635
Jan 18 21:55:29.635: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename downward-api 01/18/23 21:55:29.639
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:55:29.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:55:29.719
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 01/18/23 21:55:29.725
Jan 18 21:55:29.756: INFO: Waiting up to 5m0s for pod "downward-api-e20ae385-94e5-4b73-8841-7116da693a7d" in namespace "downward-api-7145" to be "Succeeded or Failed"
Jan 18 21:55:29.784: INFO: Pod "downward-api-e20ae385-94e5-4b73-8841-7116da693a7d": Phase="Pending", Reason="", readiness=false. Elapsed: 27.17499ms
Jan 18 21:55:31.790: INFO: Pod "downward-api-e20ae385-94e5-4b73-8841-7116da693a7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033397234s
Jan 18 21:55:33.789: INFO: Pod "downward-api-e20ae385-94e5-4b73-8841-7116da693a7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032602437s
STEP: Saw pod success 01/18/23 21:55:33.789
Jan 18 21:55:33.790: INFO: Pod "downward-api-e20ae385-94e5-4b73-8841-7116da693a7d" satisfied condition "Succeeded or Failed"
Jan 18 21:55:33.794: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downward-api-e20ae385-94e5-4b73-8841-7116da693a7d container dapi-container: <nil>
STEP: delete the pod 01/18/23 21:55:33.804
Jan 18 21:55:33.823: INFO: Waiting for pod downward-api-e20ae385-94e5-4b73-8841-7116da693a7d to disappear
Jan 18 21:55:33.831: INFO: Pod downward-api-e20ae385-94e5-4b73-8841-7116da693a7d no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 18 21:55:33.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7145" for this suite. 01/18/23 21:55:33.846
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":135,"skipped":2640,"failed":0}
------------------------------
• [4.221 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:55:29.635
    Jan 18 21:55:29.635: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename downward-api 01/18/23 21:55:29.639
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:55:29.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:55:29.719
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 01/18/23 21:55:29.725
    Jan 18 21:55:29.756: INFO: Waiting up to 5m0s for pod "downward-api-e20ae385-94e5-4b73-8841-7116da693a7d" in namespace "downward-api-7145" to be "Succeeded or Failed"
    Jan 18 21:55:29.784: INFO: Pod "downward-api-e20ae385-94e5-4b73-8841-7116da693a7d": Phase="Pending", Reason="", readiness=false. Elapsed: 27.17499ms
    Jan 18 21:55:31.790: INFO: Pod "downward-api-e20ae385-94e5-4b73-8841-7116da693a7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033397234s
    Jan 18 21:55:33.789: INFO: Pod "downward-api-e20ae385-94e5-4b73-8841-7116da693a7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032602437s
    STEP: Saw pod success 01/18/23 21:55:33.789
    Jan 18 21:55:33.790: INFO: Pod "downward-api-e20ae385-94e5-4b73-8841-7116da693a7d" satisfied condition "Succeeded or Failed"
    Jan 18 21:55:33.794: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downward-api-e20ae385-94e5-4b73-8841-7116da693a7d container dapi-container: <nil>
    STEP: delete the pod 01/18/23 21:55:33.804
    Jan 18 21:55:33.823: INFO: Waiting for pod downward-api-e20ae385-94e5-4b73-8841-7116da693a7d to disappear
    Jan 18 21:55:33.831: INFO: Pod downward-api-e20ae385-94e5-4b73-8841-7116da693a7d no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 18 21:55:33.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7145" for this suite. 01/18/23 21:55:33.846
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:55:33.862
Jan 18 21:55:33.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename kubectl 01/18/23 21:55:33.863
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:55:33.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:55:33.892
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 01/18/23 21:55:33.897
Jan 18 21:55:33.899: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4650 proxy --unix-socket=/tmp/kubectl-proxy-unix2959399225/test'
STEP: retrieving proxy /api/ output 01/18/23 21:55:33.997
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 21:55:33.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4650" for this suite. 01/18/23 21:55:34.006
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":136,"skipped":2657,"failed":0}
------------------------------
• [0.158 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:55:33.862
    Jan 18 21:55:33.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename kubectl 01/18/23 21:55:33.863
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:55:33.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:55:33.892
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 01/18/23 21:55:33.897
    Jan 18 21:55:33.899: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4650 proxy --unix-socket=/tmp/kubectl-proxy-unix2959399225/test'
    STEP: retrieving proxy /api/ output 01/18/23 21:55:33.997
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 21:55:33.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4650" for this suite. 01/18/23 21:55:34.006
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:55:34.021
Jan 18 21:55:34.021: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename crd-webhook 01/18/23 21:55:34.023
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:55:34.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:55:34.06
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/18/23 21:55:34.065
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/18/23 21:55:34.712
STEP: Deploying the custom resource conversion webhook pod 01/18/23 21:55:34.72
STEP: Wait for the deployment to be ready 01/18/23 21:55:34.741
Jan 18 21:55:34.790: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 21:55:36.802
STEP: Verifying the service has paired with the endpoint 01/18/23 21:55:36.814
Jan 18 21:55:37.815: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jan 18 21:55:37.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Creating a v1 custom resource 01/18/23 21:55:40.476
STEP: v2 custom resource should be converted 01/18/23 21:55:40.482
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 21:55:41.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6186" for this suite. 01/18/23 21:55:41.01
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":137,"skipped":2661,"failed":0}
------------------------------
• [SLOW TEST] [7.063 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:55:34.021
    Jan 18 21:55:34.021: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename crd-webhook 01/18/23 21:55:34.023
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:55:34.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:55:34.06
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/18/23 21:55:34.065
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/18/23 21:55:34.712
    STEP: Deploying the custom resource conversion webhook pod 01/18/23 21:55:34.72
    STEP: Wait for the deployment to be ready 01/18/23 21:55:34.741
    Jan 18 21:55:34.790: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 21:55:36.802
    STEP: Verifying the service has paired with the endpoint 01/18/23 21:55:36.814
    Jan 18 21:55:37.815: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jan 18 21:55:37.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Creating a v1 custom resource 01/18/23 21:55:40.476
    STEP: v2 custom resource should be converted 01/18/23 21:55:40.482
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 21:55:41.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-6186" for this suite. 01/18/23 21:55:41.01
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:55:41.086
Jan 18 21:55:41.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename secrets 01/18/23 21:55:41.087
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:55:41.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:55:41.166
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-15c87e5d-c8c6-4050-b207-a26c75831c35 01/18/23 21:55:41.179
STEP: Creating secret with name s-test-opt-upd-9d89a817-3596-4755-86e0-fb2947bb4c48 01/18/23 21:55:41.186
STEP: Creating the pod 01/18/23 21:55:41.191
Jan 18 21:55:41.201: INFO: Waiting up to 5m0s for pod "pod-secrets-d6f8e924-d3f7-49e8-a50b-8c14f1f702fb" in namespace "secrets-562" to be "running and ready"
Jan 18 21:55:41.239: INFO: Pod "pod-secrets-d6f8e924-d3f7-49e8-a50b-8c14f1f702fb": Phase="Pending", Reason="", readiness=false. Elapsed: 37.294141ms
Jan 18 21:55:41.241: INFO: The phase of Pod pod-secrets-d6f8e924-d3f7-49e8-a50b-8c14f1f702fb is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:55:43.246: INFO: Pod "pod-secrets-d6f8e924-d3f7-49e8-a50b-8c14f1f702fb": Phase="Running", Reason="", readiness=true. Elapsed: 2.045088001s
Jan 18 21:55:43.247: INFO: The phase of Pod pod-secrets-d6f8e924-d3f7-49e8-a50b-8c14f1f702fb is Running (Ready = true)
Jan 18 21:55:43.247: INFO: Pod "pod-secrets-d6f8e924-d3f7-49e8-a50b-8c14f1f702fb" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-15c87e5d-c8c6-4050-b207-a26c75831c35 01/18/23 21:55:43.279
STEP: Updating secret s-test-opt-upd-9d89a817-3596-4755-86e0-fb2947bb4c48 01/18/23 21:55:43.286
STEP: Creating secret with name s-test-opt-create-cf67e157-5a70-44ae-b82f-36695b70608f 01/18/23 21:55:43.292
STEP: waiting to observe update in volume 01/18/23 21:55:43.297
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 18 21:55:45.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-562" for this suite. 01/18/23 21:55:45.343
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":138,"skipped":2663,"failed":0}
------------------------------
• [4.263 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:55:41.086
    Jan 18 21:55:41.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename secrets 01/18/23 21:55:41.087
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:55:41.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:55:41.166
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-15c87e5d-c8c6-4050-b207-a26c75831c35 01/18/23 21:55:41.179
    STEP: Creating secret with name s-test-opt-upd-9d89a817-3596-4755-86e0-fb2947bb4c48 01/18/23 21:55:41.186
    STEP: Creating the pod 01/18/23 21:55:41.191
    Jan 18 21:55:41.201: INFO: Waiting up to 5m0s for pod "pod-secrets-d6f8e924-d3f7-49e8-a50b-8c14f1f702fb" in namespace "secrets-562" to be "running and ready"
    Jan 18 21:55:41.239: INFO: Pod "pod-secrets-d6f8e924-d3f7-49e8-a50b-8c14f1f702fb": Phase="Pending", Reason="", readiness=false. Elapsed: 37.294141ms
    Jan 18 21:55:41.241: INFO: The phase of Pod pod-secrets-d6f8e924-d3f7-49e8-a50b-8c14f1f702fb is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:55:43.246: INFO: Pod "pod-secrets-d6f8e924-d3f7-49e8-a50b-8c14f1f702fb": Phase="Running", Reason="", readiness=true. Elapsed: 2.045088001s
    Jan 18 21:55:43.247: INFO: The phase of Pod pod-secrets-d6f8e924-d3f7-49e8-a50b-8c14f1f702fb is Running (Ready = true)
    Jan 18 21:55:43.247: INFO: Pod "pod-secrets-d6f8e924-d3f7-49e8-a50b-8c14f1f702fb" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-15c87e5d-c8c6-4050-b207-a26c75831c35 01/18/23 21:55:43.279
    STEP: Updating secret s-test-opt-upd-9d89a817-3596-4755-86e0-fb2947bb4c48 01/18/23 21:55:43.286
    STEP: Creating secret with name s-test-opt-create-cf67e157-5a70-44ae-b82f-36695b70608f 01/18/23 21:55:43.292
    STEP: waiting to observe update in volume 01/18/23 21:55:43.297
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 21:55:45.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-562" for this suite. 01/18/23 21:55:45.343
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:55:45.358
Jan 18 21:55:45.358: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename secrets 01/18/23 21:55:45.359
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:55:45.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:55:45.389
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 01/18/23 21:55:45.394
STEP: listing secrets in all namespaces to ensure that there are more than zero 01/18/23 21:55:45.4
STEP: patching the secret 01/18/23 21:55:45.406
STEP: deleting the secret using a LabelSelector 01/18/23 21:55:45.418
STEP: listing secrets in all namespaces, searching for label name and value in patch 01/18/23 21:55:45.424
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 18 21:55:45.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7342" for this suite. 01/18/23 21:55:45.44
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":139,"skipped":2681,"failed":0}
------------------------------
• [0.094 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:55:45.358
    Jan 18 21:55:45.358: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename secrets 01/18/23 21:55:45.359
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:55:45.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:55:45.389
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 01/18/23 21:55:45.394
    STEP: listing secrets in all namespaces to ensure that there are more than zero 01/18/23 21:55:45.4
    STEP: patching the secret 01/18/23 21:55:45.406
    STEP: deleting the secret using a LabelSelector 01/18/23 21:55:45.418
    STEP: listing secrets in all namespaces, searching for label name and value in patch 01/18/23 21:55:45.424
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 21:55:45.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7342" for this suite. 01/18/23 21:55:45.44
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:55:45.454
Jan 18 21:55:45.454: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename configmap 01/18/23 21:55:45.456
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:55:45.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:55:45.481
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-c962f5d4-b133-43c3-a484-3f112f1b8db8 01/18/23 21:55:45.485
STEP: Creating a pod to test consume configMaps 01/18/23 21:55:45.491
Jan 18 21:55:45.499: INFO: Waiting up to 5m0s for pod "pod-configmaps-d6d477de-0776-465f-af94-c937cd94ddd8" in namespace "configmap-341" to be "Succeeded or Failed"
Jan 18 21:55:45.504: INFO: Pod "pod-configmaps-d6d477de-0776-465f-af94-c937cd94ddd8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.821236ms
Jan 18 21:55:47.510: INFO: Pod "pod-configmaps-d6d477de-0776-465f-af94-c937cd94ddd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010617298s
Jan 18 21:55:49.515: INFO: Pod "pod-configmaps-d6d477de-0776-465f-af94-c937cd94ddd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015481862s
STEP: Saw pod success 01/18/23 21:55:49.515
Jan 18 21:55:49.515: INFO: Pod "pod-configmaps-d6d477de-0776-465f-af94-c937cd94ddd8" satisfied condition "Succeeded or Failed"
Jan 18 21:55:49.519: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-d6d477de-0776-465f-af94-c937cd94ddd8 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 21:55:49.527
Jan 18 21:55:49.547: INFO: Waiting for pod pod-configmaps-d6d477de-0776-465f-af94-c937cd94ddd8 to disappear
Jan 18 21:55:49.561: INFO: Pod pod-configmaps-d6d477de-0776-465f-af94-c937cd94ddd8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 21:55:49.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-341" for this suite. 01/18/23 21:55:49.568
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":140,"skipped":2694,"failed":0}
------------------------------
• [4.141 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:55:45.454
    Jan 18 21:55:45.454: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename configmap 01/18/23 21:55:45.456
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:55:45.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:55:45.481
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-c962f5d4-b133-43c3-a484-3f112f1b8db8 01/18/23 21:55:45.485
    STEP: Creating a pod to test consume configMaps 01/18/23 21:55:45.491
    Jan 18 21:55:45.499: INFO: Waiting up to 5m0s for pod "pod-configmaps-d6d477de-0776-465f-af94-c937cd94ddd8" in namespace "configmap-341" to be "Succeeded or Failed"
    Jan 18 21:55:45.504: INFO: Pod "pod-configmaps-d6d477de-0776-465f-af94-c937cd94ddd8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.821236ms
    Jan 18 21:55:47.510: INFO: Pod "pod-configmaps-d6d477de-0776-465f-af94-c937cd94ddd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010617298s
    Jan 18 21:55:49.515: INFO: Pod "pod-configmaps-d6d477de-0776-465f-af94-c937cd94ddd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015481862s
    STEP: Saw pod success 01/18/23 21:55:49.515
    Jan 18 21:55:49.515: INFO: Pod "pod-configmaps-d6d477de-0776-465f-af94-c937cd94ddd8" satisfied condition "Succeeded or Failed"
    Jan 18 21:55:49.519: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-d6d477de-0776-465f-af94-c937cd94ddd8 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 21:55:49.527
    Jan 18 21:55:49.547: INFO: Waiting for pod pod-configmaps-d6d477de-0776-465f-af94-c937cd94ddd8 to disappear
    Jan 18 21:55:49.561: INFO: Pod pod-configmaps-d6d477de-0776-465f-af94-c937cd94ddd8 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 21:55:49.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-341" for this suite. 01/18/23 21:55:49.568
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:55:49.596
Jan 18 21:55:49.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 21:55:49.615
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:55:49.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:55:49.692
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jan 18 21:55:49.699: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 21:55:56.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4920" for this suite. 01/18/23 21:55:56.192
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":141,"skipped":2697,"failed":0}
------------------------------
• [SLOW TEST] [6.604 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:55:49.596
    Jan 18 21:55:49.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 21:55:49.615
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:55:49.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:55:49.692
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jan 18 21:55:49.699: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 21:55:56.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-4920" for this suite. 01/18/23 21:55:56.192
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:55:56.208
Jan 18 21:55:56.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename security-context-test 01/18/23 21:55:56.211
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:55:56.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:55:56.252
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Jan 18 21:55:56.281: INFO: Waiting up to 5m0s for pod "busybox-user-65534-2b0be27f-120e-413c-a0aa-20f15927b117" in namespace "security-context-test-3187" to be "Succeeded or Failed"
Jan 18 21:55:56.315: INFO: Pod "busybox-user-65534-2b0be27f-120e-413c-a0aa-20f15927b117": Phase="Pending", Reason="", readiness=false. Elapsed: 33.13034ms
Jan 18 21:55:58.322: INFO: Pod "busybox-user-65534-2b0be27f-120e-413c-a0aa-20f15927b117": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040602639s
Jan 18 21:56:00.321: INFO: Pod "busybox-user-65534-2b0be27f-120e-413c-a0aa-20f15927b117": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039313996s
Jan 18 21:56:00.321: INFO: Pod "busybox-user-65534-2b0be27f-120e-413c-a0aa-20f15927b117" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 18 21:56:00.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3187" for this suite. 01/18/23 21:56:00.332
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":142,"skipped":2704,"failed":0}
------------------------------
• [4.133 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:55:56.208
    Jan 18 21:55:56.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename security-context-test 01/18/23 21:55:56.211
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:55:56.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:55:56.252
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Jan 18 21:55:56.281: INFO: Waiting up to 5m0s for pod "busybox-user-65534-2b0be27f-120e-413c-a0aa-20f15927b117" in namespace "security-context-test-3187" to be "Succeeded or Failed"
    Jan 18 21:55:56.315: INFO: Pod "busybox-user-65534-2b0be27f-120e-413c-a0aa-20f15927b117": Phase="Pending", Reason="", readiness=false. Elapsed: 33.13034ms
    Jan 18 21:55:58.322: INFO: Pod "busybox-user-65534-2b0be27f-120e-413c-a0aa-20f15927b117": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040602639s
    Jan 18 21:56:00.321: INFO: Pod "busybox-user-65534-2b0be27f-120e-413c-a0aa-20f15927b117": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039313996s
    Jan 18 21:56:00.321: INFO: Pod "busybox-user-65534-2b0be27f-120e-413c-a0aa-20f15927b117" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 18 21:56:00.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-3187" for this suite. 01/18/23 21:56:00.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:56:00.349
Jan 18 21:56:00.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename hostport 01/18/23 21:56:00.35
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:56:00.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:56:00.383
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/18/23 21:56:00.393
Jan 18 21:56:00.405: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-7294" to be "running and ready"
Jan 18 21:56:00.411: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.596631ms
Jan 18 21:56:00.412: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:56:02.417: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011574468s
Jan 18 21:56:02.417: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 18 21:56:02.417: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.101.216 on the node which pod1 resides and expect scheduled 01/18/23 21:56:02.418
Jan 18 21:56:02.428: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-7294" to be "running and ready"
Jan 18 21:56:02.435: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.547892ms
Jan 18 21:56:02.436: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:56:04.443: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.015026254s
Jan 18 21:56:04.443: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 18 21:56:04.443: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.101.216 but use UDP protocol on the node which pod2 resides 01/18/23 21:56:04.443
Jan 18 21:56:04.450: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-7294" to be "running and ready"
Jan 18 21:56:04.465: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.384042ms
Jan 18 21:56:04.465: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:56:06.479: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.029162281s
Jan 18 21:56:06.480: INFO: The phase of Pod pod3 is Running (Ready = true)
Jan 18 21:56:06.480: INFO: Pod "pod3" satisfied condition "running and ready"
Jan 18 21:56:06.487: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-7294" to be "running and ready"
Jan 18 21:56:06.497: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 10.141443ms
Jan 18 21:56:06.497: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:56:08.502: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.015264044s
Jan 18 21:56:08.502: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jan 18 21:56:08.502: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/18/23 21:56:08.514
Jan 18 21:56:08.514: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.101.216 http://127.0.0.1:54323/hostname] Namespace:hostport-7294 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:56:08.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 21:56:08.516: INFO: ExecWithOptions: Clientset creation
Jan 18 21:56:08.516: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-7294/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.101.216+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.101.216, port: 54323 01/18/23 21:56:08.657
Jan 18 21:56:08.658: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.101.216:54323/hostname] Namespace:hostport-7294 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:56:08.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 21:56:08.659: INFO: ExecWithOptions: Clientset creation
Jan 18 21:56:08.659: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-7294/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.101.216%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.101.216, port: 54323 UDP 01/18/23 21:56:08.766
Jan 18 21:56:08.766: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.101.216 54323] Namespace:hostport-7294 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:56:08.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 21:56:08.767: INFO: ExecWithOptions: Clientset creation
Jan 18 21:56:08.767: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-7294/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.101.216+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Jan 18 21:56:13.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-7294" for this suite. 01/18/23 21:56:13.857
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":143,"skipped":2727,"failed":0}
------------------------------
• [SLOW TEST] [13.518 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:56:00.349
    Jan 18 21:56:00.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename hostport 01/18/23 21:56:00.35
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:56:00.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:56:00.383
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/18/23 21:56:00.393
    Jan 18 21:56:00.405: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-7294" to be "running and ready"
    Jan 18 21:56:00.411: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.596631ms
    Jan 18 21:56:00.412: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:56:02.417: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011574468s
    Jan 18 21:56:02.417: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 18 21:56:02.417: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.101.216 on the node which pod1 resides and expect scheduled 01/18/23 21:56:02.418
    Jan 18 21:56:02.428: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-7294" to be "running and ready"
    Jan 18 21:56:02.435: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.547892ms
    Jan 18 21:56:02.436: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:56:04.443: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.015026254s
    Jan 18 21:56:04.443: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 18 21:56:04.443: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.101.216 but use UDP protocol on the node which pod2 resides 01/18/23 21:56:04.443
    Jan 18 21:56:04.450: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-7294" to be "running and ready"
    Jan 18 21:56:04.465: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.384042ms
    Jan 18 21:56:04.465: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:56:06.479: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.029162281s
    Jan 18 21:56:06.480: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jan 18 21:56:06.480: INFO: Pod "pod3" satisfied condition "running and ready"
    Jan 18 21:56:06.487: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-7294" to be "running and ready"
    Jan 18 21:56:06.497: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 10.141443ms
    Jan 18 21:56:06.497: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:56:08.502: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.015264044s
    Jan 18 21:56:08.502: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jan 18 21:56:08.502: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/18/23 21:56:08.514
    Jan 18 21:56:08.514: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.101.216 http://127.0.0.1:54323/hostname] Namespace:hostport-7294 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:56:08.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 21:56:08.516: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:56:08.516: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-7294/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.101.216+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.101.216, port: 54323 01/18/23 21:56:08.657
    Jan 18 21:56:08.658: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.101.216:54323/hostname] Namespace:hostport-7294 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:56:08.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 21:56:08.659: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:56:08.659: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-7294/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.101.216%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.101.216, port: 54323 UDP 01/18/23 21:56:08.766
    Jan 18 21:56:08.766: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.101.216 54323] Namespace:hostport-7294 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:56:08.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 21:56:08.767: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:56:08.767: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-7294/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.101.216+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Jan 18 21:56:13.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-7294" for this suite. 01/18/23 21:56:13.857
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:56:13.868
Jan 18 21:56:13.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename resourcequota 01/18/23 21:56:13.871
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:56:13.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:56:13.901
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 01/18/23 21:56:13.907
STEP: Creating a ResourceQuota 01/18/23 21:56:18.914
STEP: Ensuring resource quota status is calculated 01/18/23 21:56:18.927
STEP: Creating a Service 01/18/23 21:56:20.933
STEP: Creating a NodePort Service 01/18/23 21:56:20.954
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/18/23 21:56:21.001
STEP: Ensuring resource quota status captures service creation 01/18/23 21:56:21.031
STEP: Deleting Services 01/18/23 21:56:23.053
STEP: Ensuring resource quota status released usage 01/18/23 21:56:23.235
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 21:56:25.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8978" for this suite. 01/18/23 21:56:25.247
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":144,"skipped":2731,"failed":0}
------------------------------
• [SLOW TEST] [11.385 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:56:13.868
    Jan 18 21:56:13.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename resourcequota 01/18/23 21:56:13.871
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:56:13.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:56:13.901
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 01/18/23 21:56:13.907
    STEP: Creating a ResourceQuota 01/18/23 21:56:18.914
    STEP: Ensuring resource quota status is calculated 01/18/23 21:56:18.927
    STEP: Creating a Service 01/18/23 21:56:20.933
    STEP: Creating a NodePort Service 01/18/23 21:56:20.954
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/18/23 21:56:21.001
    STEP: Ensuring resource quota status captures service creation 01/18/23 21:56:21.031
    STEP: Deleting Services 01/18/23 21:56:23.053
    STEP: Ensuring resource quota status released usage 01/18/23 21:56:23.235
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 21:56:25.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8978" for this suite. 01/18/23 21:56:25.247
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:56:25.255
Jan 18 21:56:25.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename emptydir-wrapper 01/18/23 21:56:25.257
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:56:25.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:56:25.281
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 01/18/23 21:56:25.285
STEP: Creating RC which spawns configmap-volume pods 01/18/23 21:56:25.598
Jan 18 21:56:25.614: INFO: Pod name wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde: Found 0 pods out of 5
Jan 18 21:56:30.639: INFO: Pod name wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/18/23 21:56:30.639
Jan 18 21:56:30.639: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-crdw6" in namespace "emptydir-wrapper-3026" to be "running"
Jan 18 21:56:30.645: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-crdw6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.84767ms
Jan 18 21:56:32.651: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-crdw6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012382295s
Jan 18 21:56:34.712: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-crdw6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073691326s
Jan 18 21:56:36.651: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-crdw6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012416084s
Jan 18 21:56:38.654: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-crdw6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015368094s
Jan 18 21:56:40.653: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-crdw6": Phase="Running", Reason="", readiness=true. Elapsed: 10.013783906s
Jan 18 21:56:40.653: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-crdw6" satisfied condition "running"
Jan 18 21:56:40.653: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-fqrj7" in namespace "emptydir-wrapper-3026" to be "running"
Jan 18 21:56:40.658: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-fqrj7": Phase="Running", Reason="", readiness=true. Elapsed: 5.664664ms
Jan 18 21:56:40.659: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-fqrj7" satisfied condition "running"
Jan 18 21:56:40.659: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-g6rq9" in namespace "emptydir-wrapper-3026" to be "running"
Jan 18 21:56:40.664: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-g6rq9": Phase="Running", Reason="", readiness=true. Elapsed: 5.37502ms
Jan 18 21:56:40.665: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-g6rq9" satisfied condition "running"
Jan 18 21:56:40.665: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-h9bsr" in namespace "emptydir-wrapper-3026" to be "running"
Jan 18 21:56:40.670: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-h9bsr": Phase="Running", Reason="", readiness=true. Elapsed: 5.061176ms
Jan 18 21:56:40.670: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-h9bsr" satisfied condition "running"
Jan 18 21:56:40.670: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-vkhbn" in namespace "emptydir-wrapper-3026" to be "running"
Jan 18 21:56:40.676: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-vkhbn": Phase="Running", Reason="", readiness=true. Elapsed: 5.372554ms
Jan 18 21:56:40.676: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-vkhbn" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde in namespace emptydir-wrapper-3026, will wait for the garbage collector to delete the pods 01/18/23 21:56:40.676
Jan 18 21:56:40.749: INFO: Deleting ReplicationController wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde took: 11.829293ms
Jan 18 21:56:40.849: INFO: Terminating ReplicationController wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde pods took: 100.542299ms
STEP: Creating RC which spawns configmap-volume pods 01/18/23 21:56:44.564
Jan 18 21:56:44.597: INFO: Pod name wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462: Found 0 pods out of 5
Jan 18 21:56:49.615: INFO: Pod name wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/18/23 21:56:49.615
Jan 18 21:56:49.616: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-8ltw5" in namespace "emptydir-wrapper-3026" to be "running"
Jan 18 21:56:49.621: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-8ltw5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.169927ms
Jan 18 21:56:51.629: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-8ltw5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012765439s
Jan 18 21:56:53.629: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-8ltw5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012509011s
Jan 18 21:56:55.630: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-8ltw5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013948215s
Jan 18 21:56:57.660: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-8ltw5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.044219915s
Jan 18 21:56:59.627: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-8ltw5": Phase="Running", Reason="", readiness=true. Elapsed: 10.011374412s
Jan 18 21:56:59.628: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-8ltw5" satisfied condition "running"
Jan 18 21:56:59.628: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-nx9qc" in namespace "emptydir-wrapper-3026" to be "running"
Jan 18 21:56:59.634: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-nx9qc": Phase="Running", Reason="", readiness=true. Elapsed: 6.461953ms
Jan 18 21:56:59.635: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-nx9qc" satisfied condition "running"
Jan 18 21:56:59.635: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-t6z6r" in namespace "emptydir-wrapper-3026" to be "running"
Jan 18 21:56:59.639: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-t6z6r": Phase="Running", Reason="", readiness=true. Elapsed: 4.558173ms
Jan 18 21:56:59.640: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-t6z6r" satisfied condition "running"
Jan 18 21:56:59.640: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-v99vg" in namespace "emptydir-wrapper-3026" to be "running"
Jan 18 21:56:59.644: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-v99vg": Phase="Running", Reason="", readiness=true. Elapsed: 4.550796ms
Jan 18 21:56:59.644: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-v99vg" satisfied condition "running"
Jan 18 21:56:59.645: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-x6xpr" in namespace "emptydir-wrapper-3026" to be "running"
Jan 18 21:56:59.650: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-x6xpr": Phase="Running", Reason="", readiness=true. Elapsed: 4.897113ms
Jan 18 21:56:59.650: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-x6xpr" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462 in namespace emptydir-wrapper-3026, will wait for the garbage collector to delete the pods 01/18/23 21:56:59.65
Jan 18 21:56:59.714: INFO: Deleting ReplicationController wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462 took: 9.028494ms
Jan 18 21:56:59.916: INFO: Terminating ReplicationController wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462 pods took: 202.244188ms
STEP: Creating RC which spawns configmap-volume pods 01/18/23 21:57:03.928
Jan 18 21:57:03.947: INFO: Pod name wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888: Found 0 pods out of 5
Jan 18 21:57:08.959: INFO: Pod name wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/18/23 21:57:08.959
Jan 18 21:57:08.960: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-dxx4j" in namespace "emptydir-wrapper-3026" to be "running"
Jan 18 21:57:08.964: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-dxx4j": Phase="Pending", Reason="", readiness=false. Elapsed: 4.818403ms
Jan 18 21:57:10.975: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-dxx4j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015365685s
Jan 18 21:57:12.971: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-dxx4j": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011086927s
Jan 18 21:57:14.971: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-dxx4j": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011251579s
Jan 18 21:57:17.087: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-dxx4j": Phase="Pending", Reason="", readiness=false. Elapsed: 8.127565102s
Jan 18 21:57:18.971: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-dxx4j": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011493487s
Jan 18 21:57:20.972: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-dxx4j": Phase="Running", Reason="", readiness=true. Elapsed: 12.012136753s
Jan 18 21:57:20.972: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-dxx4j" satisfied condition "running"
Jan 18 21:57:20.972: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-nbnf9" in namespace "emptydir-wrapper-3026" to be "running"
Jan 18 21:57:20.977: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-nbnf9": Phase="Running", Reason="", readiness=true. Elapsed: 5.635371ms
Jan 18 21:57:20.978: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-nbnf9" satisfied condition "running"
Jan 18 21:57:20.978: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-qm7bq" in namespace "emptydir-wrapper-3026" to be "running"
Jan 18 21:57:20.983: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-qm7bq": Phase="Running", Reason="", readiness=true. Elapsed: 5.543509ms
Jan 18 21:57:20.983: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-qm7bq" satisfied condition "running"
Jan 18 21:57:20.983: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-s2hxq" in namespace "emptydir-wrapper-3026" to be "running"
Jan 18 21:57:20.988: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-s2hxq": Phase="Running", Reason="", readiness=true. Elapsed: 5.150829ms
Jan 18 21:57:20.989: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-s2hxq" satisfied condition "running"
Jan 18 21:57:20.989: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-zs924" in namespace "emptydir-wrapper-3026" to be "running"
Jan 18 21:57:20.993: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-zs924": Phase="Running", Reason="", readiness=true. Elapsed: 4.782551ms
Jan 18 21:57:20.994: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-zs924" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888 in namespace emptydir-wrapper-3026, will wait for the garbage collector to delete the pods 01/18/23 21:57:20.994
Jan 18 21:57:21.058: INFO: Deleting ReplicationController wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888 took: 8.214688ms
Jan 18 21:57:21.159: INFO: Terminating ReplicationController wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888 pods took: 101.328736ms
STEP: Cleaning up the configMaps 01/18/23 21:57:24.761
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jan 18 21:57:25.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3026" for this suite. 01/18/23 21:57:25.606
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":145,"skipped":2738,"failed":0}
------------------------------
• [SLOW TEST] [60.360 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:56:25.255
    Jan 18 21:56:25.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename emptydir-wrapper 01/18/23 21:56:25.257
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:56:25.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:56:25.281
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 01/18/23 21:56:25.285
    STEP: Creating RC which spawns configmap-volume pods 01/18/23 21:56:25.598
    Jan 18 21:56:25.614: INFO: Pod name wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde: Found 0 pods out of 5
    Jan 18 21:56:30.639: INFO: Pod name wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/18/23 21:56:30.639
    Jan 18 21:56:30.639: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-crdw6" in namespace "emptydir-wrapper-3026" to be "running"
    Jan 18 21:56:30.645: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-crdw6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.84767ms
    Jan 18 21:56:32.651: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-crdw6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012382295s
    Jan 18 21:56:34.712: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-crdw6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073691326s
    Jan 18 21:56:36.651: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-crdw6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012416084s
    Jan 18 21:56:38.654: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-crdw6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015368094s
    Jan 18 21:56:40.653: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-crdw6": Phase="Running", Reason="", readiness=true. Elapsed: 10.013783906s
    Jan 18 21:56:40.653: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-crdw6" satisfied condition "running"
    Jan 18 21:56:40.653: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-fqrj7" in namespace "emptydir-wrapper-3026" to be "running"
    Jan 18 21:56:40.658: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-fqrj7": Phase="Running", Reason="", readiness=true. Elapsed: 5.664664ms
    Jan 18 21:56:40.659: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-fqrj7" satisfied condition "running"
    Jan 18 21:56:40.659: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-g6rq9" in namespace "emptydir-wrapper-3026" to be "running"
    Jan 18 21:56:40.664: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-g6rq9": Phase="Running", Reason="", readiness=true. Elapsed: 5.37502ms
    Jan 18 21:56:40.665: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-g6rq9" satisfied condition "running"
    Jan 18 21:56:40.665: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-h9bsr" in namespace "emptydir-wrapper-3026" to be "running"
    Jan 18 21:56:40.670: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-h9bsr": Phase="Running", Reason="", readiness=true. Elapsed: 5.061176ms
    Jan 18 21:56:40.670: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-h9bsr" satisfied condition "running"
    Jan 18 21:56:40.670: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-vkhbn" in namespace "emptydir-wrapper-3026" to be "running"
    Jan 18 21:56:40.676: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-vkhbn": Phase="Running", Reason="", readiness=true. Elapsed: 5.372554ms
    Jan 18 21:56:40.676: INFO: Pod "wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde-vkhbn" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde in namespace emptydir-wrapper-3026, will wait for the garbage collector to delete the pods 01/18/23 21:56:40.676
    Jan 18 21:56:40.749: INFO: Deleting ReplicationController wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde took: 11.829293ms
    Jan 18 21:56:40.849: INFO: Terminating ReplicationController wrapped-volume-race-cf0dbcf2-c2dc-4964-97b8-aca59f9efcde pods took: 100.542299ms
    STEP: Creating RC which spawns configmap-volume pods 01/18/23 21:56:44.564
    Jan 18 21:56:44.597: INFO: Pod name wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462: Found 0 pods out of 5
    Jan 18 21:56:49.615: INFO: Pod name wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/18/23 21:56:49.615
    Jan 18 21:56:49.616: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-8ltw5" in namespace "emptydir-wrapper-3026" to be "running"
    Jan 18 21:56:49.621: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-8ltw5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.169927ms
    Jan 18 21:56:51.629: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-8ltw5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012765439s
    Jan 18 21:56:53.629: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-8ltw5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012509011s
    Jan 18 21:56:55.630: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-8ltw5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013948215s
    Jan 18 21:56:57.660: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-8ltw5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.044219915s
    Jan 18 21:56:59.627: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-8ltw5": Phase="Running", Reason="", readiness=true. Elapsed: 10.011374412s
    Jan 18 21:56:59.628: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-8ltw5" satisfied condition "running"
    Jan 18 21:56:59.628: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-nx9qc" in namespace "emptydir-wrapper-3026" to be "running"
    Jan 18 21:56:59.634: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-nx9qc": Phase="Running", Reason="", readiness=true. Elapsed: 6.461953ms
    Jan 18 21:56:59.635: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-nx9qc" satisfied condition "running"
    Jan 18 21:56:59.635: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-t6z6r" in namespace "emptydir-wrapper-3026" to be "running"
    Jan 18 21:56:59.639: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-t6z6r": Phase="Running", Reason="", readiness=true. Elapsed: 4.558173ms
    Jan 18 21:56:59.640: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-t6z6r" satisfied condition "running"
    Jan 18 21:56:59.640: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-v99vg" in namespace "emptydir-wrapper-3026" to be "running"
    Jan 18 21:56:59.644: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-v99vg": Phase="Running", Reason="", readiness=true. Elapsed: 4.550796ms
    Jan 18 21:56:59.644: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-v99vg" satisfied condition "running"
    Jan 18 21:56:59.645: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-x6xpr" in namespace "emptydir-wrapper-3026" to be "running"
    Jan 18 21:56:59.650: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-x6xpr": Phase="Running", Reason="", readiness=true. Elapsed: 4.897113ms
    Jan 18 21:56:59.650: INFO: Pod "wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462-x6xpr" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462 in namespace emptydir-wrapper-3026, will wait for the garbage collector to delete the pods 01/18/23 21:56:59.65
    Jan 18 21:56:59.714: INFO: Deleting ReplicationController wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462 took: 9.028494ms
    Jan 18 21:56:59.916: INFO: Terminating ReplicationController wrapped-volume-race-6520b9fa-7baa-4d17-b952-c3fd599f3462 pods took: 202.244188ms
    STEP: Creating RC which spawns configmap-volume pods 01/18/23 21:57:03.928
    Jan 18 21:57:03.947: INFO: Pod name wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888: Found 0 pods out of 5
    Jan 18 21:57:08.959: INFO: Pod name wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/18/23 21:57:08.959
    Jan 18 21:57:08.960: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-dxx4j" in namespace "emptydir-wrapper-3026" to be "running"
    Jan 18 21:57:08.964: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-dxx4j": Phase="Pending", Reason="", readiness=false. Elapsed: 4.818403ms
    Jan 18 21:57:10.975: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-dxx4j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015365685s
    Jan 18 21:57:12.971: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-dxx4j": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011086927s
    Jan 18 21:57:14.971: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-dxx4j": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011251579s
    Jan 18 21:57:17.087: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-dxx4j": Phase="Pending", Reason="", readiness=false. Elapsed: 8.127565102s
    Jan 18 21:57:18.971: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-dxx4j": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011493487s
    Jan 18 21:57:20.972: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-dxx4j": Phase="Running", Reason="", readiness=true. Elapsed: 12.012136753s
    Jan 18 21:57:20.972: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-dxx4j" satisfied condition "running"
    Jan 18 21:57:20.972: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-nbnf9" in namespace "emptydir-wrapper-3026" to be "running"
    Jan 18 21:57:20.977: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-nbnf9": Phase="Running", Reason="", readiness=true. Elapsed: 5.635371ms
    Jan 18 21:57:20.978: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-nbnf9" satisfied condition "running"
    Jan 18 21:57:20.978: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-qm7bq" in namespace "emptydir-wrapper-3026" to be "running"
    Jan 18 21:57:20.983: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-qm7bq": Phase="Running", Reason="", readiness=true. Elapsed: 5.543509ms
    Jan 18 21:57:20.983: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-qm7bq" satisfied condition "running"
    Jan 18 21:57:20.983: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-s2hxq" in namespace "emptydir-wrapper-3026" to be "running"
    Jan 18 21:57:20.988: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-s2hxq": Phase="Running", Reason="", readiness=true. Elapsed: 5.150829ms
    Jan 18 21:57:20.989: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-s2hxq" satisfied condition "running"
    Jan 18 21:57:20.989: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-zs924" in namespace "emptydir-wrapper-3026" to be "running"
    Jan 18 21:57:20.993: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-zs924": Phase="Running", Reason="", readiness=true. Elapsed: 4.782551ms
    Jan 18 21:57:20.994: INFO: Pod "wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888-zs924" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888 in namespace emptydir-wrapper-3026, will wait for the garbage collector to delete the pods 01/18/23 21:57:20.994
    Jan 18 21:57:21.058: INFO: Deleting ReplicationController wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888 took: 8.214688ms
    Jan 18 21:57:21.159: INFO: Terminating ReplicationController wrapped-volume-race-1f04d716-2efd-44aa-91fb-859ccbee5888 pods took: 101.328736ms
    STEP: Cleaning up the configMaps 01/18/23 21:57:24.761
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jan 18 21:57:25.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-3026" for this suite. 01/18/23 21:57:25.606
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:57:25.62
Jan 18 21:57:25.621: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename emptydir 01/18/23 21:57:25.623
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:57:25.64
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:57:25.647
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 01/18/23 21:57:25.653
Jan 18 21:57:25.663: INFO: Waiting up to 5m0s for pod "pod-97a2ff84-3e64-491a-952c-37ce57d96e6b" in namespace "emptydir-3954" to be "Succeeded or Failed"
Jan 18 21:57:25.668: INFO: Pod "pod-97a2ff84-3e64-491a-952c-37ce57d96e6b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.231018ms
Jan 18 21:57:27.674: INFO: Pod "pod-97a2ff84-3e64-491a-952c-37ce57d96e6b": Phase="Running", Reason="", readiness=true. Elapsed: 2.010809037s
Jan 18 21:57:29.674: INFO: Pod "pod-97a2ff84-3e64-491a-952c-37ce57d96e6b": Phase="Running", Reason="", readiness=false. Elapsed: 4.011209031s
Jan 18 21:57:31.687: INFO: Pod "pod-97a2ff84-3e64-491a-952c-37ce57d96e6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024407674s
STEP: Saw pod success 01/18/23 21:57:31.687
Jan 18 21:57:31.688: INFO: Pod "pod-97a2ff84-3e64-491a-952c-37ce57d96e6b" satisfied condition "Succeeded or Failed"
Jan 18 21:57:31.696: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-97a2ff84-3e64-491a-952c-37ce57d96e6b container test-container: <nil>
STEP: delete the pod 01/18/23 21:57:31.723
Jan 18 21:57:31.736: INFO: Waiting for pod pod-97a2ff84-3e64-491a-952c-37ce57d96e6b to disappear
Jan 18 21:57:31.744: INFO: Pod pod-97a2ff84-3e64-491a-952c-37ce57d96e6b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 21:57:31.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3954" for this suite. 01/18/23 21:57:31.753
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":146,"skipped":2741,"failed":0}
------------------------------
• [SLOW TEST] [6.145 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:57:25.62
    Jan 18 21:57:25.621: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename emptydir 01/18/23 21:57:25.623
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:57:25.64
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:57:25.647
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/18/23 21:57:25.653
    Jan 18 21:57:25.663: INFO: Waiting up to 5m0s for pod "pod-97a2ff84-3e64-491a-952c-37ce57d96e6b" in namespace "emptydir-3954" to be "Succeeded or Failed"
    Jan 18 21:57:25.668: INFO: Pod "pod-97a2ff84-3e64-491a-952c-37ce57d96e6b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.231018ms
    Jan 18 21:57:27.674: INFO: Pod "pod-97a2ff84-3e64-491a-952c-37ce57d96e6b": Phase="Running", Reason="", readiness=true. Elapsed: 2.010809037s
    Jan 18 21:57:29.674: INFO: Pod "pod-97a2ff84-3e64-491a-952c-37ce57d96e6b": Phase="Running", Reason="", readiness=false. Elapsed: 4.011209031s
    Jan 18 21:57:31.687: INFO: Pod "pod-97a2ff84-3e64-491a-952c-37ce57d96e6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024407674s
    STEP: Saw pod success 01/18/23 21:57:31.687
    Jan 18 21:57:31.688: INFO: Pod "pod-97a2ff84-3e64-491a-952c-37ce57d96e6b" satisfied condition "Succeeded or Failed"
    Jan 18 21:57:31.696: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-97a2ff84-3e64-491a-952c-37ce57d96e6b container test-container: <nil>
    STEP: delete the pod 01/18/23 21:57:31.723
    Jan 18 21:57:31.736: INFO: Waiting for pod pod-97a2ff84-3e64-491a-952c-37ce57d96e6b to disappear
    Jan 18 21:57:31.744: INFO: Pod pod-97a2ff84-3e64-491a-952c-37ce57d96e6b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 21:57:31.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3954" for this suite. 01/18/23 21:57:31.753
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:57:31.77
Jan 18 21:57:31.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename secrets 01/18/23 21:57:31.777
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:57:31.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:57:31.817
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-3a57bf9b-d440-4793-9c5c-55d7028ee56c 01/18/23 21:57:31.822
STEP: Creating a pod to test consume secrets 01/18/23 21:57:31.833
Jan 18 21:57:31.847: INFO: Waiting up to 5m0s for pod "pod-secrets-908cd3ce-1720-49ad-811c-fcfa01131244" in namespace "secrets-4955" to be "Succeeded or Failed"
Jan 18 21:57:31.861: INFO: Pod "pod-secrets-908cd3ce-1720-49ad-811c-fcfa01131244": Phase="Pending", Reason="", readiness=false. Elapsed: 13.975168ms
Jan 18 21:57:33.871: INFO: Pod "pod-secrets-908cd3ce-1720-49ad-811c-fcfa01131244": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023982062s
Jan 18 21:57:35.865: INFO: Pod "pod-secrets-908cd3ce-1720-49ad-811c-fcfa01131244": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018676329s
Jan 18 21:57:37.867: INFO: Pod "pod-secrets-908cd3ce-1720-49ad-811c-fcfa01131244": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019960741s
STEP: Saw pod success 01/18/23 21:57:37.867
Jan 18 21:57:37.867: INFO: Pod "pod-secrets-908cd3ce-1720-49ad-811c-fcfa01131244" satisfied condition "Succeeded or Failed"
Jan 18 21:57:37.871: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-908cd3ce-1720-49ad-811c-fcfa01131244 container secret-env-test: <nil>
STEP: delete the pod 01/18/23 21:57:37.879
Jan 18 21:57:37.893: INFO: Waiting for pod pod-secrets-908cd3ce-1720-49ad-811c-fcfa01131244 to disappear
Jan 18 21:57:37.897: INFO: Pod pod-secrets-908cd3ce-1720-49ad-811c-fcfa01131244 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 18 21:57:37.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4955" for this suite. 01/18/23 21:57:37.902
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":147,"skipped":2743,"failed":0}
------------------------------
• [SLOW TEST] [6.139 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:57:31.77
    Jan 18 21:57:31.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename secrets 01/18/23 21:57:31.777
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:57:31.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:57:31.817
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-3a57bf9b-d440-4793-9c5c-55d7028ee56c 01/18/23 21:57:31.822
    STEP: Creating a pod to test consume secrets 01/18/23 21:57:31.833
    Jan 18 21:57:31.847: INFO: Waiting up to 5m0s for pod "pod-secrets-908cd3ce-1720-49ad-811c-fcfa01131244" in namespace "secrets-4955" to be "Succeeded or Failed"
    Jan 18 21:57:31.861: INFO: Pod "pod-secrets-908cd3ce-1720-49ad-811c-fcfa01131244": Phase="Pending", Reason="", readiness=false. Elapsed: 13.975168ms
    Jan 18 21:57:33.871: INFO: Pod "pod-secrets-908cd3ce-1720-49ad-811c-fcfa01131244": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023982062s
    Jan 18 21:57:35.865: INFO: Pod "pod-secrets-908cd3ce-1720-49ad-811c-fcfa01131244": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018676329s
    Jan 18 21:57:37.867: INFO: Pod "pod-secrets-908cd3ce-1720-49ad-811c-fcfa01131244": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019960741s
    STEP: Saw pod success 01/18/23 21:57:37.867
    Jan 18 21:57:37.867: INFO: Pod "pod-secrets-908cd3ce-1720-49ad-811c-fcfa01131244" satisfied condition "Succeeded or Failed"
    Jan 18 21:57:37.871: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-908cd3ce-1720-49ad-811c-fcfa01131244 container secret-env-test: <nil>
    STEP: delete the pod 01/18/23 21:57:37.879
    Jan 18 21:57:37.893: INFO: Waiting for pod pod-secrets-908cd3ce-1720-49ad-811c-fcfa01131244 to disappear
    Jan 18 21:57:37.897: INFO: Pod pod-secrets-908cd3ce-1720-49ad-811c-fcfa01131244 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 21:57:37.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4955" for this suite. 01/18/23 21:57:37.902
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:57:37.918
Jan 18 21:57:37.918: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename downward-api 01/18/23 21:57:37.92
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:57:37.943
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:57:37.954
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 01/18/23 21:57:37.959
Jan 18 21:57:37.972: INFO: Waiting up to 5m0s for pod "downwardapi-volume-09d37d23-d711-43e4-bd35-4abd64fe7f0f" in namespace "downward-api-9837" to be "Succeeded or Failed"
Jan 18 21:57:37.979: INFO: Pod "downwardapi-volume-09d37d23-d711-43e4-bd35-4abd64fe7f0f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.922235ms
Jan 18 21:57:39.989: INFO: Pod "downwardapi-volume-09d37d23-d711-43e4-bd35-4abd64fe7f0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016884537s
Jan 18 21:57:41.987: INFO: Pod "downwardapi-volume-09d37d23-d711-43e4-bd35-4abd64fe7f0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014821755s
STEP: Saw pod success 01/18/23 21:57:41.987
Jan 18 21:57:41.987: INFO: Pod "downwardapi-volume-09d37d23-d711-43e4-bd35-4abd64fe7f0f" satisfied condition "Succeeded or Failed"
Jan 18 21:57:41.993: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-09d37d23-d711-43e4-bd35-4abd64fe7f0f container client-container: <nil>
STEP: delete the pod 01/18/23 21:57:42.003
Jan 18 21:57:42.021: INFO: Waiting for pod downwardapi-volume-09d37d23-d711-43e4-bd35-4abd64fe7f0f to disappear
Jan 18 21:57:42.025: INFO: Pod downwardapi-volume-09d37d23-d711-43e4-bd35-4abd64fe7f0f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 21:57:42.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9837" for this suite. 01/18/23 21:57:42.037
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":148,"skipped":2757,"failed":0}
------------------------------
• [4.126 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:57:37.918
    Jan 18 21:57:37.918: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename downward-api 01/18/23 21:57:37.92
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:57:37.943
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:57:37.954
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 01/18/23 21:57:37.959
    Jan 18 21:57:37.972: INFO: Waiting up to 5m0s for pod "downwardapi-volume-09d37d23-d711-43e4-bd35-4abd64fe7f0f" in namespace "downward-api-9837" to be "Succeeded or Failed"
    Jan 18 21:57:37.979: INFO: Pod "downwardapi-volume-09d37d23-d711-43e4-bd35-4abd64fe7f0f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.922235ms
    Jan 18 21:57:39.989: INFO: Pod "downwardapi-volume-09d37d23-d711-43e4-bd35-4abd64fe7f0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016884537s
    Jan 18 21:57:41.987: INFO: Pod "downwardapi-volume-09d37d23-d711-43e4-bd35-4abd64fe7f0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014821755s
    STEP: Saw pod success 01/18/23 21:57:41.987
    Jan 18 21:57:41.987: INFO: Pod "downwardapi-volume-09d37d23-d711-43e4-bd35-4abd64fe7f0f" satisfied condition "Succeeded or Failed"
    Jan 18 21:57:41.993: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-09d37d23-d711-43e4-bd35-4abd64fe7f0f container client-container: <nil>
    STEP: delete the pod 01/18/23 21:57:42.003
    Jan 18 21:57:42.021: INFO: Waiting for pod downwardapi-volume-09d37d23-d711-43e4-bd35-4abd64fe7f0f to disappear
    Jan 18 21:57:42.025: INFO: Pod downwardapi-volume-09d37d23-d711-43e4-bd35-4abd64fe7f0f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 21:57:42.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9837" for this suite. 01/18/23 21:57:42.037
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:57:42.045
Jan 18 21:57:42.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename proxy 01/18/23 21:57:42.046
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:57:42.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:57:42.085
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 01/18/23 21:57:42.103
STEP: creating replication controller proxy-service-qg8xq in namespace proxy-8249 01/18/23 21:57:42.103
I0118 21:57:42.120280      19 runners.go:193] Created replication controller with name: proxy-service-qg8xq, namespace: proxy-8249, replica count: 1
I0118 21:57:43.171924      19 runners.go:193] proxy-service-qg8xq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 21:57:44.172291      19 runners.go:193] proxy-service-qg8xq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0118 21:57:45.173040      19 runners.go:193] proxy-service-qg8xq Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 21:57:45.178: INFO: setup took 3.087789042s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/18/23 21:57:45.178
Jan 18 21:57:45.205: INFO: (0) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 24.744763ms)
Jan 18 21:57:45.205: INFO: (0) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 25.58007ms)
Jan 18 21:57:45.205: INFO: (0) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 24.738453ms)
Jan 18 21:57:45.205: INFO: (0) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 24.224524ms)
Jan 18 21:57:45.205: INFO: (0) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 26.181574ms)
Jan 18 21:57:45.205: INFO: (0) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 25.327287ms)
Jan 18 21:57:45.205: INFO: (0) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 24.249706ms)
Jan 18 21:57:45.205: INFO: (0) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 24.725266ms)
Jan 18 21:57:45.205: INFO: (0) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 26.151548ms)
Jan 18 21:57:45.205: INFO: (0) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 25.997629ms)
Jan 18 21:57:45.211: INFO: (0) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 30.014745ms)
Jan 18 21:57:45.212: INFO: (0) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 31.463609ms)
Jan 18 21:57:45.214: INFO: (0) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 32.508131ms)
Jan 18 21:57:45.214: INFO: (0) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 33.945221ms)
Jan 18 21:57:45.214: INFO: (0) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 34.377315ms)
Jan 18 21:57:45.217: INFO: (0) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 36.110557ms)
Jan 18 21:57:45.229: INFO: (1) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 10.970363ms)
Jan 18 21:57:45.229: INFO: (1) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 10.9164ms)
Jan 18 21:57:45.229: INFO: (1) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 11.269279ms)
Jan 18 21:57:45.237: INFO: (1) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 18.036054ms)
Jan 18 21:57:45.238: INFO: (1) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 18.991124ms)
Jan 18 21:57:45.238: INFO: (1) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 19.865153ms)
Jan 18 21:57:45.238: INFO: (1) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 19.829733ms)
Jan 18 21:57:45.238: INFO: (1) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 19.966185ms)
Jan 18 21:57:45.239: INFO: (1) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 20.361822ms)
Jan 18 21:57:45.239: INFO: (1) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 20.087181ms)
Jan 18 21:57:45.239: INFO: (1) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 20.189512ms)
Jan 18 21:57:45.239: INFO: (1) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 19.962596ms)
Jan 18 21:57:45.239: INFO: (1) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 20.176009ms)
Jan 18 21:57:45.239: INFO: (1) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 20.306111ms)
Jan 18 21:57:45.239: INFO: (1) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 20.840503ms)
Jan 18 21:57:45.239: INFO: (1) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 20.819728ms)
Jan 18 21:57:45.248: INFO: (2) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 7.327534ms)
Jan 18 21:57:45.249: INFO: (2) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 8.493237ms)
Jan 18 21:57:45.249: INFO: (2) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 9.331329ms)
Jan 18 21:57:45.250: INFO: (2) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 8.987473ms)
Jan 18 21:57:45.254: INFO: (2) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 12.758483ms)
Jan 18 21:57:45.254: INFO: (2) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 14.289923ms)
Jan 18 21:57:45.255: INFO: (2) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 15.475324ms)
Jan 18 21:57:45.256: INFO: (2) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 15.334822ms)
Jan 18 21:57:45.257: INFO: (2) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 15.557355ms)
Jan 18 21:57:45.257: INFO: (2) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 15.757127ms)
Jan 18 21:57:45.257: INFO: (2) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 16.460422ms)
Jan 18 21:57:45.258: INFO: (2) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 16.498061ms)
Jan 18 21:57:45.258: INFO: (2) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 16.675912ms)
Jan 18 21:57:45.260: INFO: (2) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 19.050313ms)
Jan 18 21:57:45.262: INFO: (2) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 21.172431ms)
Jan 18 21:57:45.262: INFO: (2) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 21.125575ms)
Jan 18 21:57:45.274: INFO: (3) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 11.302051ms)
Jan 18 21:57:45.274: INFO: (3) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 11.698913ms)
Jan 18 21:57:45.276: INFO: (3) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 13.173525ms)
Jan 18 21:57:45.276: INFO: (3) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 13.550743ms)
Jan 18 21:57:45.277: INFO: (3) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 13.763361ms)
Jan 18 21:57:45.277: INFO: (3) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 13.765235ms)
Jan 18 21:57:45.279: INFO: (3) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 15.603382ms)
Jan 18 21:57:45.280: INFO: (3) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 16.617611ms)
Jan 18 21:57:45.280: INFO: (3) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 16.563008ms)
Jan 18 21:57:45.280: INFO: (3) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 16.665383ms)
Jan 18 21:57:45.280: INFO: (3) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 16.628423ms)
Jan 18 21:57:45.280: INFO: (3) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 16.829833ms)
Jan 18 21:57:45.280: INFO: (3) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 17.213213ms)
Jan 18 21:57:45.280: INFO: (3) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 17.555934ms)
Jan 18 21:57:45.280: INFO: (3) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 16.879705ms)
Jan 18 21:57:45.282: INFO: (3) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 18.323002ms)
Jan 18 21:57:45.288: INFO: (4) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 5.644592ms)
Jan 18 21:57:45.288: INFO: (4) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 6.069297ms)
Jan 18 21:57:45.289: INFO: (4) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 7.020791ms)
Jan 18 21:57:45.293: INFO: (4) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 10.481223ms)
Jan 18 21:57:45.293: INFO: (4) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 10.423557ms)
Jan 18 21:57:45.301: INFO: (4) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 18.14262ms)
Jan 18 21:57:45.302: INFO: (4) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 18.834643ms)
Jan 18 21:57:45.304: INFO: (4) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 21.361766ms)
Jan 18 21:57:45.305: INFO: (4) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 21.553287ms)
Jan 18 21:57:45.305: INFO: (4) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 22.390228ms)
Jan 18 21:57:45.305: INFO: (4) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 22.278458ms)
Jan 18 21:57:45.305: INFO: (4) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 22.171832ms)
Jan 18 21:57:45.307: INFO: (4) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 23.624255ms)
Jan 18 21:57:45.307: INFO: (4) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 24.116229ms)
Jan 18 21:57:45.308: INFO: (4) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 24.682887ms)
Jan 18 21:57:45.308: INFO: (4) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 25.25549ms)
Jan 18 21:57:45.325: INFO: (5) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 16.612914ms)
Jan 18 21:57:45.327: INFO: (5) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 19.061478ms)
Jan 18 21:57:45.328: INFO: (5) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 19.179066ms)
Jan 18 21:57:45.328: INFO: (5) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 19.413682ms)
Jan 18 21:57:45.328: INFO: (5) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 19.30109ms)
Jan 18 21:57:45.329: INFO: (5) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 19.825755ms)
Jan 18 21:57:45.329: INFO: (5) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 20.174222ms)
Jan 18 21:57:45.329: INFO: (5) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 20.891232ms)
Jan 18 21:57:45.329: INFO: (5) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 20.714764ms)
Jan 18 21:57:45.329: INFO: (5) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 20.861867ms)
Jan 18 21:57:45.330: INFO: (5) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 20.982178ms)
Jan 18 21:57:45.330: INFO: (5) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 21.185362ms)
Jan 18 21:57:45.330: INFO: (5) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 21.471552ms)
Jan 18 21:57:45.330: INFO: (5) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 22.044361ms)
Jan 18 21:57:45.330: INFO: (5) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 21.531849ms)
Jan 18 21:57:45.330: INFO: (5) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 21.639247ms)
Jan 18 21:57:45.338: INFO: (6) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 6.715652ms)
Jan 18 21:57:45.338: INFO: (6) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 7.083612ms)
Jan 18 21:57:45.338: INFO: (6) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 7.874782ms)
Jan 18 21:57:45.345: INFO: (6) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 13.372213ms)
Jan 18 21:57:45.346: INFO: (6) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 14.203284ms)
Jan 18 21:57:45.346: INFO: (6) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 14.380579ms)
Jan 18 21:57:45.346: INFO: (6) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 14.564008ms)
Jan 18 21:57:45.346: INFO: (6) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 15.394156ms)
Jan 18 21:57:45.347: INFO: (6) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 16.766544ms)
Jan 18 21:57:45.347: INFO: (6) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 15.670942ms)
Jan 18 21:57:45.347: INFO: (6) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 15.53924ms)
Jan 18 21:57:45.347: INFO: (6) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 16.055249ms)
Jan 18 21:57:45.348: INFO: (6) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 15.692341ms)
Jan 18 21:57:45.348: INFO: (6) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 16.100883ms)
Jan 18 21:57:45.350: INFO: (6) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 18.572726ms)
Jan 18 21:57:45.351: INFO: (6) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 18.839924ms)
Jan 18 21:57:45.357: INFO: (7) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 6.499937ms)
Jan 18 21:57:45.363: INFO: (7) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 11.185077ms)
Jan 18 21:57:45.363: INFO: (7) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 11.079948ms)
Jan 18 21:57:45.363: INFO: (7) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 11.463848ms)
Jan 18 21:57:45.363: INFO: (7) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 11.684796ms)
Jan 18 21:57:45.364: INFO: (7) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 11.5397ms)
Jan 18 21:57:45.364: INFO: (7) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 11.969179ms)
Jan 18 21:57:45.364: INFO: (7) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 12.65401ms)
Jan 18 21:57:45.365: INFO: (7) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 13.397019ms)
Jan 18 21:57:45.368: INFO: (7) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 16.108365ms)
Jan 18 21:57:45.368: INFO: (7) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 16.184796ms)
Jan 18 21:57:45.368: INFO: (7) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 15.944956ms)
Jan 18 21:57:45.368: INFO: (7) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 16.122772ms)
Jan 18 21:57:45.369: INFO: (7) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 16.619714ms)
Jan 18 21:57:45.369: INFO: (7) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 17.525227ms)
Jan 18 21:57:45.369: INFO: (7) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 16.858176ms)
Jan 18 21:57:45.379: INFO: (8) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 10.12565ms)
Jan 18 21:57:45.379: INFO: (8) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 10.425469ms)
Jan 18 21:57:45.385: INFO: (8) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 15.10887ms)
Jan 18 21:57:45.385: INFO: (8) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 15.267638ms)
Jan 18 21:57:45.385: INFO: (8) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 15.539517ms)
Jan 18 21:57:45.385: INFO: (8) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 15.492693ms)
Jan 18 21:57:45.387: INFO: (8) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 17.32241ms)
Jan 18 21:57:45.387: INFO: (8) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 17.71933ms)
Jan 18 21:57:45.387: INFO: (8) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 17.443518ms)
Jan 18 21:57:45.387: INFO: (8) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 18.159688ms)
Jan 18 21:57:45.387: INFO: (8) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 18.22678ms)
Jan 18 21:57:45.387: INFO: (8) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 18.115471ms)
Jan 18 21:57:45.388: INFO: (8) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 18.259823ms)
Jan 18 21:57:45.388: INFO: (8) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 18.178949ms)
Jan 18 21:57:45.388: INFO: (8) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 18.088604ms)
Jan 18 21:57:45.388: INFO: (8) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 18.251851ms)
Jan 18 21:57:45.396: INFO: (9) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 6.512561ms)
Jan 18 21:57:45.397: INFO: (9) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 7.618466ms)
Jan 18 21:57:45.401: INFO: (9) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 11.629902ms)
Jan 18 21:57:45.402: INFO: (9) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 12.811433ms)
Jan 18 21:57:45.402: INFO: (9) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 12.213158ms)
Jan 18 21:57:45.402: INFO: (9) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 12.646023ms)
Jan 18 21:57:45.402: INFO: (9) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 12.732143ms)
Jan 18 21:57:45.402: INFO: (9) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 12.432786ms)
Jan 18 21:57:45.410: INFO: (9) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 20.182783ms)
Jan 18 21:57:45.410: INFO: (9) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 19.779213ms)
Jan 18 21:57:45.411: INFO: (9) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 20.184389ms)
Jan 18 21:57:45.412: INFO: (9) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 21.989389ms)
Jan 18 21:57:45.412: INFO: (9) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 21.85374ms)
Jan 18 21:57:45.412: INFO: (9) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 22.231752ms)
Jan 18 21:57:45.413: INFO: (9) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 22.598356ms)
Jan 18 21:57:45.413: INFO: (9) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 22.464184ms)
Jan 18 21:57:45.437: INFO: (10) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 23.248867ms)
Jan 18 21:57:45.437: INFO: (10) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 23.987022ms)
Jan 18 21:57:45.438: INFO: (10) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 23.282576ms)
Jan 18 21:57:45.438: INFO: (10) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 23.899252ms)
Jan 18 21:57:45.438: INFO: (10) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 25.391895ms)
Jan 18 21:57:45.439: INFO: (10) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 24.508164ms)
Jan 18 21:57:45.440: INFO: (10) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 26.103168ms)
Jan 18 21:57:45.445: INFO: (10) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 32.264921ms)
Jan 18 21:57:45.448: INFO: (10) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 33.215835ms)
Jan 18 21:57:45.451: INFO: (10) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 37.454759ms)
Jan 18 21:57:45.452: INFO: (10) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 38.045311ms)
Jan 18 21:57:45.452: INFO: (10) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 37.729126ms)
Jan 18 21:57:45.452: INFO: (10) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 38.630273ms)
Jan 18 21:57:45.452: INFO: (10) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 38.686477ms)
Jan 18 21:57:45.453: INFO: (10) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 38.447364ms)
Jan 18 21:57:45.453: INFO: (10) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 40.019205ms)
Jan 18 21:57:45.473: INFO: (11) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 20.214398ms)
Jan 18 21:57:45.473: INFO: (11) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 20.020326ms)
Jan 18 21:57:45.475: INFO: (11) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 21.472206ms)
Jan 18 21:57:45.476: INFO: (11) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 23.051236ms)
Jan 18 21:57:45.477: INFO: (11) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 23.030516ms)
Jan 18 21:57:45.477: INFO: (11) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 22.976199ms)
Jan 18 21:57:45.478: INFO: (11) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 24.097222ms)
Jan 18 21:57:45.478: INFO: (11) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 25.093784ms)
Jan 18 21:57:45.479: INFO: (11) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 25.199572ms)
Jan 18 21:57:45.479: INFO: (11) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 24.72423ms)
Jan 18 21:57:45.480: INFO: (11) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 27.020235ms)
Jan 18 21:57:45.480: INFO: (11) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 26.730475ms)
Jan 18 21:57:45.480: INFO: (11) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 26.530084ms)
Jan 18 21:57:45.481: INFO: (11) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 26.917329ms)
Jan 18 21:57:45.481: INFO: (11) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 27.748547ms)
Jan 18 21:57:45.481: INFO: (11) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 27.369064ms)
Jan 18 21:57:45.498: INFO: (12) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 16.845157ms)
Jan 18 21:57:45.498: INFO: (12) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 16.952563ms)
Jan 18 21:57:45.499: INFO: (12) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 17.073486ms)
Jan 18 21:57:45.499: INFO: (12) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 17.505832ms)
Jan 18 21:57:45.499: INFO: (12) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 18.033177ms)
Jan 18 21:57:45.499: INFO: (12) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 16.8999ms)
Jan 18 21:57:45.499: INFO: (12) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 16.854546ms)
Jan 18 21:57:45.507: INFO: (12) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 24.197397ms)
Jan 18 21:57:45.507: INFO: (12) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 24.123542ms)
Jan 18 21:57:45.510: INFO: (12) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 27.148919ms)
Jan 18 21:57:45.510: INFO: (12) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 27.265704ms)
Jan 18 21:57:45.511: INFO: (12) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 28.1031ms)
Jan 18 21:57:45.511: INFO: (12) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 28.735658ms)
Jan 18 21:57:45.511: INFO: (12) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 29.170672ms)
Jan 18 21:57:45.511: INFO: (12) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 29.130754ms)
Jan 18 21:57:45.511: INFO: (12) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 30.190313ms)
Jan 18 21:57:45.524: INFO: (13) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 11.654113ms)
Jan 18 21:57:45.594: INFO: (13) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 81.712954ms)
Jan 18 21:57:45.595: INFO: (13) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 83.466684ms)
Jan 18 21:57:45.595: INFO: (13) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 82.814995ms)
Jan 18 21:57:45.596: INFO: (13) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 82.36258ms)
Jan 18 21:57:45.596: INFO: (13) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 82.831603ms)
Jan 18 21:57:45.596: INFO: (13) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 82.955857ms)
Jan 18 21:57:45.596: INFO: (13) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 83.287556ms)
Jan 18 21:57:45.597: INFO: (13) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 84.365984ms)
Jan 18 21:57:45.597: INFO: (13) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 83.929682ms)
Jan 18 21:57:45.598: INFO: (13) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 85.351074ms)
Jan 18 21:57:45.608: INFO: (13) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 95.872384ms)
Jan 18 21:57:45.609: INFO: (13) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 96.792607ms)
Jan 18 21:57:45.609: INFO: (13) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 96.385292ms)
Jan 18 21:57:45.609: INFO: (13) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 96.025287ms)
Jan 18 21:57:45.609: INFO: (13) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 96.615741ms)
Jan 18 21:57:45.644: INFO: (14) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 34.336969ms)
Jan 18 21:57:45.645: INFO: (14) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 35.243654ms)
Jan 18 21:57:45.645: INFO: (14) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 35.370056ms)
Jan 18 21:57:45.645: INFO: (14) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 35.588952ms)
Jan 18 21:57:45.645: INFO: (14) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 35.473887ms)
Jan 18 21:57:45.645: INFO: (14) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 35.885822ms)
Jan 18 21:57:45.649: INFO: (14) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 39.157978ms)
Jan 18 21:57:45.650: INFO: (14) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 39.884333ms)
Jan 18 21:57:45.650: INFO: (14) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 39.846053ms)
Jan 18 21:57:45.650: INFO: (14) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 39.853732ms)
Jan 18 21:57:45.650: INFO: (14) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 39.800619ms)
Jan 18 21:57:45.653: INFO: (14) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 42.588716ms)
Jan 18 21:57:45.653: INFO: (14) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 42.987816ms)
Jan 18 21:57:45.653: INFO: (14) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 43.233014ms)
Jan 18 21:57:45.654: INFO: (14) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 43.725916ms)
Jan 18 21:57:45.654: INFO: (14) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 43.62397ms)
Jan 18 21:57:45.663: INFO: (15) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 9.158117ms)
Jan 18 21:57:45.663: INFO: (15) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 9.552398ms)
Jan 18 21:57:45.664: INFO: (15) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 9.072521ms)
Jan 18 21:57:45.665: INFO: (15) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 10.683969ms)
Jan 18 21:57:45.665: INFO: (15) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 10.984112ms)
Jan 18 21:57:45.666: INFO: (15) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 11.424672ms)
Jan 18 21:57:45.666: INFO: (15) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 12.0219ms)
Jan 18 21:57:45.667: INFO: (15) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 12.313324ms)
Jan 18 21:57:45.668: INFO: (15) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 12.952095ms)
Jan 18 21:57:45.669: INFO: (15) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 14.412692ms)
Jan 18 21:57:45.669: INFO: (15) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 15.185229ms)
Jan 18 21:57:45.691: INFO: (15) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 36.767671ms)
Jan 18 21:57:45.691: INFO: (15) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 36.972696ms)
Jan 18 21:57:45.691: INFO: (15) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 36.901333ms)
Jan 18 21:57:45.693: INFO: (15) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 37.838314ms)
Jan 18 21:57:45.693: INFO: (15) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 38.785686ms)
Jan 18 21:57:45.707: INFO: (16) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 13.436982ms)
Jan 18 21:57:45.712: INFO: (16) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 18.302007ms)
Jan 18 21:57:45.712: INFO: (16) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 18.029826ms)
Jan 18 21:57:45.713: INFO: (16) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 18.638785ms)
Jan 18 21:57:45.714: INFO: (16) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 20.044146ms)
Jan 18 21:57:45.714: INFO: (16) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 20.836729ms)
Jan 18 21:57:45.716: INFO: (16) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 22.401357ms)
Jan 18 21:57:45.716: INFO: (16) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 22.411943ms)
Jan 18 21:57:45.717: INFO: (16) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 22.801998ms)
Jan 18 21:57:45.717: INFO: (16) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 22.936512ms)
Jan 18 21:57:45.717: INFO: (16) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 23.066311ms)
Jan 18 21:57:45.717: INFO: (16) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 23.877021ms)
Jan 18 21:57:45.717: INFO: (16) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 23.218951ms)
Jan 18 21:57:45.717: INFO: (16) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 23.275985ms)
Jan 18 21:57:45.718: INFO: (16) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 23.714415ms)
Jan 18 21:57:45.718: INFO: (16) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 23.294771ms)
Jan 18 21:57:45.759: INFO: (17) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 40.712657ms)
Jan 18 21:57:45.760: INFO: (17) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 42.142596ms)
Jan 18 21:57:45.768: INFO: (17) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 49.482218ms)
Jan 18 21:57:45.770: INFO: (17) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 51.256728ms)
Jan 18 21:57:45.770: INFO: (17) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 51.395232ms)
Jan 18 21:57:45.770: INFO: (17) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 51.42525ms)
Jan 18 21:57:45.770: INFO: (17) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 51.923636ms)
Jan 18 21:57:45.777: INFO: (17) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 58.749671ms)
Jan 18 21:57:45.778: INFO: (17) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 59.624218ms)
Jan 18 21:57:45.778: INFO: (17) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 59.483923ms)
Jan 18 21:57:45.778: INFO: (17) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 59.630529ms)
Jan 18 21:57:45.778: INFO: (17) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 59.681908ms)
Jan 18 21:57:45.783: INFO: (17) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 64.085686ms)
Jan 18 21:57:45.783: INFO: (17) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 64.130151ms)
Jan 18 21:57:45.783: INFO: (17) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 64.6479ms)
Jan 18 21:57:45.783: INFO: (17) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 63.827325ms)
Jan 18 21:57:45.803: INFO: (18) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 19.531194ms)
Jan 18 21:57:45.804: INFO: (18) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 19.799591ms)
Jan 18 21:57:45.804: INFO: (18) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 19.521217ms)
Jan 18 21:57:45.806: INFO: (18) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 21.133818ms)
Jan 18 21:57:45.806: INFO: (18) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 21.276785ms)
Jan 18 21:57:45.806: INFO: (18) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 22.186833ms)
Jan 18 21:57:45.806: INFO: (18) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 22.052924ms)
Jan 18 21:57:45.806: INFO: (18) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 22.22724ms)
Jan 18 21:57:45.811: INFO: (18) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 27.677919ms)
Jan 18 21:57:45.814: INFO: (18) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 29.42636ms)
Jan 18 21:57:45.814: INFO: (18) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 29.160062ms)
Jan 18 21:57:45.814: INFO: (18) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 29.651549ms)
Jan 18 21:57:45.814: INFO: (18) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 30.069681ms)
Jan 18 21:57:45.814: INFO: (18) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 30.479871ms)
Jan 18 21:57:45.814: INFO: (18) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 30.483951ms)
Jan 18 21:57:45.815: INFO: (18) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 30.484657ms)
Jan 18 21:57:45.830: INFO: (19) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 14.901987ms)
Jan 18 21:57:45.830: INFO: (19) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 14.588341ms)
Jan 18 21:57:45.830: INFO: (19) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 15.365151ms)
Jan 18 21:57:45.834: INFO: (19) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 18.485721ms)
Jan 18 21:57:45.841: INFO: (19) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 24.528262ms)
Jan 18 21:57:45.852: INFO: (19) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 35.759302ms)
Jan 18 21:57:45.853: INFO: (19) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 37.335226ms)
Jan 18 21:57:45.853: INFO: (19) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 38.069094ms)
Jan 18 21:57:45.853: INFO: (19) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 36.741774ms)
Jan 18 21:57:45.854: INFO: (19) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 37.676028ms)
Jan 18 21:57:45.855: INFO: (19) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 39.561464ms)
Jan 18 21:57:45.856: INFO: (19) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 41.092623ms)
Jan 18 21:57:45.857: INFO: (19) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 40.614656ms)
Jan 18 21:57:45.857: INFO: (19) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 41.296945ms)
Jan 18 21:57:45.857: INFO: (19) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 40.869106ms)
Jan 18 21:57:45.858: INFO: (19) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 42.173594ms)
STEP: deleting ReplicationController proxy-service-qg8xq in namespace proxy-8249, will wait for the garbage collector to delete the pods 01/18/23 21:57:45.858
Jan 18 21:57:45.924: INFO: Deleting ReplicationController proxy-service-qg8xq took: 7.162574ms
Jan 18 21:57:46.026: INFO: Terminating ReplicationController proxy-service-qg8xq pods took: 101.448112ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan 18 21:57:49.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8249" for this suite. 01/18/23 21:57:49.032
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":149,"skipped":2759,"failed":0}
------------------------------
• [SLOW TEST] [6.995 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:57:42.045
    Jan 18 21:57:42.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename proxy 01/18/23 21:57:42.046
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:57:42.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:57:42.085
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 01/18/23 21:57:42.103
    STEP: creating replication controller proxy-service-qg8xq in namespace proxy-8249 01/18/23 21:57:42.103
    I0118 21:57:42.120280      19 runners.go:193] Created replication controller with name: proxy-service-qg8xq, namespace: proxy-8249, replica count: 1
    I0118 21:57:43.171924      19 runners.go:193] proxy-service-qg8xq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0118 21:57:44.172291      19 runners.go:193] proxy-service-qg8xq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0118 21:57:45.173040      19 runners.go:193] proxy-service-qg8xq Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 21:57:45.178: INFO: setup took 3.087789042s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/18/23 21:57:45.178
    Jan 18 21:57:45.205: INFO: (0) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 24.744763ms)
    Jan 18 21:57:45.205: INFO: (0) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 25.58007ms)
    Jan 18 21:57:45.205: INFO: (0) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 24.738453ms)
    Jan 18 21:57:45.205: INFO: (0) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 24.224524ms)
    Jan 18 21:57:45.205: INFO: (0) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 26.181574ms)
    Jan 18 21:57:45.205: INFO: (0) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 25.327287ms)
    Jan 18 21:57:45.205: INFO: (0) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 24.249706ms)
    Jan 18 21:57:45.205: INFO: (0) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 24.725266ms)
    Jan 18 21:57:45.205: INFO: (0) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 26.151548ms)
    Jan 18 21:57:45.205: INFO: (0) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 25.997629ms)
    Jan 18 21:57:45.211: INFO: (0) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 30.014745ms)
    Jan 18 21:57:45.212: INFO: (0) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 31.463609ms)
    Jan 18 21:57:45.214: INFO: (0) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 32.508131ms)
    Jan 18 21:57:45.214: INFO: (0) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 33.945221ms)
    Jan 18 21:57:45.214: INFO: (0) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 34.377315ms)
    Jan 18 21:57:45.217: INFO: (0) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 36.110557ms)
    Jan 18 21:57:45.229: INFO: (1) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 10.970363ms)
    Jan 18 21:57:45.229: INFO: (1) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 10.9164ms)
    Jan 18 21:57:45.229: INFO: (1) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 11.269279ms)
    Jan 18 21:57:45.237: INFO: (1) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 18.036054ms)
    Jan 18 21:57:45.238: INFO: (1) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 18.991124ms)
    Jan 18 21:57:45.238: INFO: (1) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 19.865153ms)
    Jan 18 21:57:45.238: INFO: (1) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 19.829733ms)
    Jan 18 21:57:45.238: INFO: (1) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 19.966185ms)
    Jan 18 21:57:45.239: INFO: (1) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 20.361822ms)
    Jan 18 21:57:45.239: INFO: (1) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 20.087181ms)
    Jan 18 21:57:45.239: INFO: (1) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 20.189512ms)
    Jan 18 21:57:45.239: INFO: (1) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 19.962596ms)
    Jan 18 21:57:45.239: INFO: (1) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 20.176009ms)
    Jan 18 21:57:45.239: INFO: (1) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 20.306111ms)
    Jan 18 21:57:45.239: INFO: (1) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 20.840503ms)
    Jan 18 21:57:45.239: INFO: (1) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 20.819728ms)
    Jan 18 21:57:45.248: INFO: (2) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 7.327534ms)
    Jan 18 21:57:45.249: INFO: (2) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 8.493237ms)
    Jan 18 21:57:45.249: INFO: (2) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 9.331329ms)
    Jan 18 21:57:45.250: INFO: (2) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 8.987473ms)
    Jan 18 21:57:45.254: INFO: (2) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 12.758483ms)
    Jan 18 21:57:45.254: INFO: (2) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 14.289923ms)
    Jan 18 21:57:45.255: INFO: (2) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 15.475324ms)
    Jan 18 21:57:45.256: INFO: (2) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 15.334822ms)
    Jan 18 21:57:45.257: INFO: (2) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 15.557355ms)
    Jan 18 21:57:45.257: INFO: (2) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 15.757127ms)
    Jan 18 21:57:45.257: INFO: (2) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 16.460422ms)
    Jan 18 21:57:45.258: INFO: (2) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 16.498061ms)
    Jan 18 21:57:45.258: INFO: (2) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 16.675912ms)
    Jan 18 21:57:45.260: INFO: (2) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 19.050313ms)
    Jan 18 21:57:45.262: INFO: (2) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 21.172431ms)
    Jan 18 21:57:45.262: INFO: (2) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 21.125575ms)
    Jan 18 21:57:45.274: INFO: (3) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 11.302051ms)
    Jan 18 21:57:45.274: INFO: (3) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 11.698913ms)
    Jan 18 21:57:45.276: INFO: (3) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 13.173525ms)
    Jan 18 21:57:45.276: INFO: (3) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 13.550743ms)
    Jan 18 21:57:45.277: INFO: (3) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 13.763361ms)
    Jan 18 21:57:45.277: INFO: (3) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 13.765235ms)
    Jan 18 21:57:45.279: INFO: (3) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 15.603382ms)
    Jan 18 21:57:45.280: INFO: (3) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 16.617611ms)
    Jan 18 21:57:45.280: INFO: (3) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 16.563008ms)
    Jan 18 21:57:45.280: INFO: (3) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 16.665383ms)
    Jan 18 21:57:45.280: INFO: (3) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 16.628423ms)
    Jan 18 21:57:45.280: INFO: (3) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 16.829833ms)
    Jan 18 21:57:45.280: INFO: (3) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 17.213213ms)
    Jan 18 21:57:45.280: INFO: (3) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 17.555934ms)
    Jan 18 21:57:45.280: INFO: (3) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 16.879705ms)
    Jan 18 21:57:45.282: INFO: (3) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 18.323002ms)
    Jan 18 21:57:45.288: INFO: (4) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 5.644592ms)
    Jan 18 21:57:45.288: INFO: (4) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 6.069297ms)
    Jan 18 21:57:45.289: INFO: (4) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 7.020791ms)
    Jan 18 21:57:45.293: INFO: (4) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 10.481223ms)
    Jan 18 21:57:45.293: INFO: (4) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 10.423557ms)
    Jan 18 21:57:45.301: INFO: (4) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 18.14262ms)
    Jan 18 21:57:45.302: INFO: (4) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 18.834643ms)
    Jan 18 21:57:45.304: INFO: (4) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 21.361766ms)
    Jan 18 21:57:45.305: INFO: (4) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 21.553287ms)
    Jan 18 21:57:45.305: INFO: (4) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 22.390228ms)
    Jan 18 21:57:45.305: INFO: (4) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 22.278458ms)
    Jan 18 21:57:45.305: INFO: (4) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 22.171832ms)
    Jan 18 21:57:45.307: INFO: (4) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 23.624255ms)
    Jan 18 21:57:45.307: INFO: (4) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 24.116229ms)
    Jan 18 21:57:45.308: INFO: (4) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 24.682887ms)
    Jan 18 21:57:45.308: INFO: (4) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 25.25549ms)
    Jan 18 21:57:45.325: INFO: (5) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 16.612914ms)
    Jan 18 21:57:45.327: INFO: (5) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 19.061478ms)
    Jan 18 21:57:45.328: INFO: (5) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 19.179066ms)
    Jan 18 21:57:45.328: INFO: (5) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 19.413682ms)
    Jan 18 21:57:45.328: INFO: (5) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 19.30109ms)
    Jan 18 21:57:45.329: INFO: (5) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 19.825755ms)
    Jan 18 21:57:45.329: INFO: (5) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 20.174222ms)
    Jan 18 21:57:45.329: INFO: (5) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 20.891232ms)
    Jan 18 21:57:45.329: INFO: (5) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 20.714764ms)
    Jan 18 21:57:45.329: INFO: (5) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 20.861867ms)
    Jan 18 21:57:45.330: INFO: (5) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 20.982178ms)
    Jan 18 21:57:45.330: INFO: (5) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 21.185362ms)
    Jan 18 21:57:45.330: INFO: (5) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 21.471552ms)
    Jan 18 21:57:45.330: INFO: (5) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 22.044361ms)
    Jan 18 21:57:45.330: INFO: (5) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 21.531849ms)
    Jan 18 21:57:45.330: INFO: (5) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 21.639247ms)
    Jan 18 21:57:45.338: INFO: (6) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 6.715652ms)
    Jan 18 21:57:45.338: INFO: (6) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 7.083612ms)
    Jan 18 21:57:45.338: INFO: (6) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 7.874782ms)
    Jan 18 21:57:45.345: INFO: (6) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 13.372213ms)
    Jan 18 21:57:45.346: INFO: (6) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 14.203284ms)
    Jan 18 21:57:45.346: INFO: (6) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 14.380579ms)
    Jan 18 21:57:45.346: INFO: (6) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 14.564008ms)
    Jan 18 21:57:45.346: INFO: (6) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 15.394156ms)
    Jan 18 21:57:45.347: INFO: (6) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 16.766544ms)
    Jan 18 21:57:45.347: INFO: (6) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 15.670942ms)
    Jan 18 21:57:45.347: INFO: (6) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 15.53924ms)
    Jan 18 21:57:45.347: INFO: (6) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 16.055249ms)
    Jan 18 21:57:45.348: INFO: (6) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 15.692341ms)
    Jan 18 21:57:45.348: INFO: (6) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 16.100883ms)
    Jan 18 21:57:45.350: INFO: (6) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 18.572726ms)
    Jan 18 21:57:45.351: INFO: (6) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 18.839924ms)
    Jan 18 21:57:45.357: INFO: (7) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 6.499937ms)
    Jan 18 21:57:45.363: INFO: (7) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 11.185077ms)
    Jan 18 21:57:45.363: INFO: (7) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 11.079948ms)
    Jan 18 21:57:45.363: INFO: (7) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 11.463848ms)
    Jan 18 21:57:45.363: INFO: (7) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 11.684796ms)
    Jan 18 21:57:45.364: INFO: (7) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 11.5397ms)
    Jan 18 21:57:45.364: INFO: (7) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 11.969179ms)
    Jan 18 21:57:45.364: INFO: (7) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 12.65401ms)
    Jan 18 21:57:45.365: INFO: (7) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 13.397019ms)
    Jan 18 21:57:45.368: INFO: (7) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 16.108365ms)
    Jan 18 21:57:45.368: INFO: (7) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 16.184796ms)
    Jan 18 21:57:45.368: INFO: (7) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 15.944956ms)
    Jan 18 21:57:45.368: INFO: (7) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 16.122772ms)
    Jan 18 21:57:45.369: INFO: (7) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 16.619714ms)
    Jan 18 21:57:45.369: INFO: (7) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 17.525227ms)
    Jan 18 21:57:45.369: INFO: (7) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 16.858176ms)
    Jan 18 21:57:45.379: INFO: (8) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 10.12565ms)
    Jan 18 21:57:45.379: INFO: (8) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 10.425469ms)
    Jan 18 21:57:45.385: INFO: (8) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 15.10887ms)
    Jan 18 21:57:45.385: INFO: (8) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 15.267638ms)
    Jan 18 21:57:45.385: INFO: (8) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 15.539517ms)
    Jan 18 21:57:45.385: INFO: (8) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 15.492693ms)
    Jan 18 21:57:45.387: INFO: (8) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 17.32241ms)
    Jan 18 21:57:45.387: INFO: (8) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 17.71933ms)
    Jan 18 21:57:45.387: INFO: (8) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 17.443518ms)
    Jan 18 21:57:45.387: INFO: (8) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 18.159688ms)
    Jan 18 21:57:45.387: INFO: (8) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 18.22678ms)
    Jan 18 21:57:45.387: INFO: (8) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 18.115471ms)
    Jan 18 21:57:45.388: INFO: (8) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 18.259823ms)
    Jan 18 21:57:45.388: INFO: (8) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 18.178949ms)
    Jan 18 21:57:45.388: INFO: (8) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 18.088604ms)
    Jan 18 21:57:45.388: INFO: (8) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 18.251851ms)
    Jan 18 21:57:45.396: INFO: (9) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 6.512561ms)
    Jan 18 21:57:45.397: INFO: (9) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 7.618466ms)
    Jan 18 21:57:45.401: INFO: (9) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 11.629902ms)
    Jan 18 21:57:45.402: INFO: (9) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 12.811433ms)
    Jan 18 21:57:45.402: INFO: (9) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 12.213158ms)
    Jan 18 21:57:45.402: INFO: (9) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 12.646023ms)
    Jan 18 21:57:45.402: INFO: (9) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 12.732143ms)
    Jan 18 21:57:45.402: INFO: (9) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 12.432786ms)
    Jan 18 21:57:45.410: INFO: (9) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 20.182783ms)
    Jan 18 21:57:45.410: INFO: (9) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 19.779213ms)
    Jan 18 21:57:45.411: INFO: (9) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 20.184389ms)
    Jan 18 21:57:45.412: INFO: (9) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 21.989389ms)
    Jan 18 21:57:45.412: INFO: (9) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 21.85374ms)
    Jan 18 21:57:45.412: INFO: (9) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 22.231752ms)
    Jan 18 21:57:45.413: INFO: (9) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 22.598356ms)
    Jan 18 21:57:45.413: INFO: (9) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 22.464184ms)
    Jan 18 21:57:45.437: INFO: (10) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 23.248867ms)
    Jan 18 21:57:45.437: INFO: (10) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 23.987022ms)
    Jan 18 21:57:45.438: INFO: (10) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 23.282576ms)
    Jan 18 21:57:45.438: INFO: (10) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 23.899252ms)
    Jan 18 21:57:45.438: INFO: (10) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 25.391895ms)
    Jan 18 21:57:45.439: INFO: (10) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 24.508164ms)
    Jan 18 21:57:45.440: INFO: (10) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 26.103168ms)
    Jan 18 21:57:45.445: INFO: (10) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 32.264921ms)
    Jan 18 21:57:45.448: INFO: (10) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 33.215835ms)
    Jan 18 21:57:45.451: INFO: (10) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 37.454759ms)
    Jan 18 21:57:45.452: INFO: (10) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 38.045311ms)
    Jan 18 21:57:45.452: INFO: (10) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 37.729126ms)
    Jan 18 21:57:45.452: INFO: (10) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 38.630273ms)
    Jan 18 21:57:45.452: INFO: (10) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 38.686477ms)
    Jan 18 21:57:45.453: INFO: (10) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 38.447364ms)
    Jan 18 21:57:45.453: INFO: (10) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 40.019205ms)
    Jan 18 21:57:45.473: INFO: (11) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 20.214398ms)
    Jan 18 21:57:45.473: INFO: (11) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 20.020326ms)
    Jan 18 21:57:45.475: INFO: (11) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 21.472206ms)
    Jan 18 21:57:45.476: INFO: (11) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 23.051236ms)
    Jan 18 21:57:45.477: INFO: (11) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 23.030516ms)
    Jan 18 21:57:45.477: INFO: (11) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 22.976199ms)
    Jan 18 21:57:45.478: INFO: (11) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 24.097222ms)
    Jan 18 21:57:45.478: INFO: (11) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 25.093784ms)
    Jan 18 21:57:45.479: INFO: (11) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 25.199572ms)
    Jan 18 21:57:45.479: INFO: (11) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 24.72423ms)
    Jan 18 21:57:45.480: INFO: (11) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 27.020235ms)
    Jan 18 21:57:45.480: INFO: (11) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 26.730475ms)
    Jan 18 21:57:45.480: INFO: (11) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 26.530084ms)
    Jan 18 21:57:45.481: INFO: (11) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 26.917329ms)
    Jan 18 21:57:45.481: INFO: (11) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 27.748547ms)
    Jan 18 21:57:45.481: INFO: (11) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 27.369064ms)
    Jan 18 21:57:45.498: INFO: (12) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 16.845157ms)
    Jan 18 21:57:45.498: INFO: (12) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 16.952563ms)
    Jan 18 21:57:45.499: INFO: (12) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 17.073486ms)
    Jan 18 21:57:45.499: INFO: (12) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 17.505832ms)
    Jan 18 21:57:45.499: INFO: (12) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 18.033177ms)
    Jan 18 21:57:45.499: INFO: (12) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 16.8999ms)
    Jan 18 21:57:45.499: INFO: (12) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 16.854546ms)
    Jan 18 21:57:45.507: INFO: (12) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 24.197397ms)
    Jan 18 21:57:45.507: INFO: (12) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 24.123542ms)
    Jan 18 21:57:45.510: INFO: (12) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 27.148919ms)
    Jan 18 21:57:45.510: INFO: (12) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 27.265704ms)
    Jan 18 21:57:45.511: INFO: (12) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 28.1031ms)
    Jan 18 21:57:45.511: INFO: (12) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 28.735658ms)
    Jan 18 21:57:45.511: INFO: (12) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 29.170672ms)
    Jan 18 21:57:45.511: INFO: (12) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 29.130754ms)
    Jan 18 21:57:45.511: INFO: (12) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 30.190313ms)
    Jan 18 21:57:45.524: INFO: (13) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 11.654113ms)
    Jan 18 21:57:45.594: INFO: (13) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 81.712954ms)
    Jan 18 21:57:45.595: INFO: (13) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 83.466684ms)
    Jan 18 21:57:45.595: INFO: (13) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 82.814995ms)
    Jan 18 21:57:45.596: INFO: (13) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 82.36258ms)
    Jan 18 21:57:45.596: INFO: (13) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 82.831603ms)
    Jan 18 21:57:45.596: INFO: (13) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 82.955857ms)
    Jan 18 21:57:45.596: INFO: (13) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 83.287556ms)
    Jan 18 21:57:45.597: INFO: (13) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 84.365984ms)
    Jan 18 21:57:45.597: INFO: (13) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 83.929682ms)
    Jan 18 21:57:45.598: INFO: (13) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 85.351074ms)
    Jan 18 21:57:45.608: INFO: (13) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 95.872384ms)
    Jan 18 21:57:45.609: INFO: (13) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 96.792607ms)
    Jan 18 21:57:45.609: INFO: (13) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 96.385292ms)
    Jan 18 21:57:45.609: INFO: (13) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 96.025287ms)
    Jan 18 21:57:45.609: INFO: (13) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 96.615741ms)
    Jan 18 21:57:45.644: INFO: (14) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 34.336969ms)
    Jan 18 21:57:45.645: INFO: (14) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 35.243654ms)
    Jan 18 21:57:45.645: INFO: (14) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 35.370056ms)
    Jan 18 21:57:45.645: INFO: (14) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 35.588952ms)
    Jan 18 21:57:45.645: INFO: (14) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 35.473887ms)
    Jan 18 21:57:45.645: INFO: (14) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 35.885822ms)
    Jan 18 21:57:45.649: INFO: (14) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 39.157978ms)
    Jan 18 21:57:45.650: INFO: (14) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 39.884333ms)
    Jan 18 21:57:45.650: INFO: (14) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 39.846053ms)
    Jan 18 21:57:45.650: INFO: (14) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 39.853732ms)
    Jan 18 21:57:45.650: INFO: (14) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 39.800619ms)
    Jan 18 21:57:45.653: INFO: (14) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 42.588716ms)
    Jan 18 21:57:45.653: INFO: (14) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 42.987816ms)
    Jan 18 21:57:45.653: INFO: (14) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 43.233014ms)
    Jan 18 21:57:45.654: INFO: (14) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 43.725916ms)
    Jan 18 21:57:45.654: INFO: (14) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 43.62397ms)
    Jan 18 21:57:45.663: INFO: (15) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 9.158117ms)
    Jan 18 21:57:45.663: INFO: (15) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 9.552398ms)
    Jan 18 21:57:45.664: INFO: (15) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 9.072521ms)
    Jan 18 21:57:45.665: INFO: (15) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 10.683969ms)
    Jan 18 21:57:45.665: INFO: (15) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 10.984112ms)
    Jan 18 21:57:45.666: INFO: (15) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 11.424672ms)
    Jan 18 21:57:45.666: INFO: (15) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 12.0219ms)
    Jan 18 21:57:45.667: INFO: (15) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 12.313324ms)
    Jan 18 21:57:45.668: INFO: (15) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 12.952095ms)
    Jan 18 21:57:45.669: INFO: (15) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 14.412692ms)
    Jan 18 21:57:45.669: INFO: (15) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 15.185229ms)
    Jan 18 21:57:45.691: INFO: (15) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 36.767671ms)
    Jan 18 21:57:45.691: INFO: (15) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 36.972696ms)
    Jan 18 21:57:45.691: INFO: (15) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 36.901333ms)
    Jan 18 21:57:45.693: INFO: (15) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 37.838314ms)
    Jan 18 21:57:45.693: INFO: (15) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 38.785686ms)
    Jan 18 21:57:45.707: INFO: (16) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 13.436982ms)
    Jan 18 21:57:45.712: INFO: (16) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 18.302007ms)
    Jan 18 21:57:45.712: INFO: (16) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 18.029826ms)
    Jan 18 21:57:45.713: INFO: (16) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 18.638785ms)
    Jan 18 21:57:45.714: INFO: (16) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 20.044146ms)
    Jan 18 21:57:45.714: INFO: (16) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 20.836729ms)
    Jan 18 21:57:45.716: INFO: (16) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 22.401357ms)
    Jan 18 21:57:45.716: INFO: (16) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 22.411943ms)
    Jan 18 21:57:45.717: INFO: (16) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 22.801998ms)
    Jan 18 21:57:45.717: INFO: (16) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 22.936512ms)
    Jan 18 21:57:45.717: INFO: (16) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 23.066311ms)
    Jan 18 21:57:45.717: INFO: (16) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 23.877021ms)
    Jan 18 21:57:45.717: INFO: (16) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 23.218951ms)
    Jan 18 21:57:45.717: INFO: (16) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 23.275985ms)
    Jan 18 21:57:45.718: INFO: (16) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 23.714415ms)
    Jan 18 21:57:45.718: INFO: (16) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 23.294771ms)
    Jan 18 21:57:45.759: INFO: (17) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 40.712657ms)
    Jan 18 21:57:45.760: INFO: (17) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 42.142596ms)
    Jan 18 21:57:45.768: INFO: (17) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 49.482218ms)
    Jan 18 21:57:45.770: INFO: (17) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 51.256728ms)
    Jan 18 21:57:45.770: INFO: (17) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 51.395232ms)
    Jan 18 21:57:45.770: INFO: (17) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 51.42525ms)
    Jan 18 21:57:45.770: INFO: (17) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 51.923636ms)
    Jan 18 21:57:45.777: INFO: (17) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 58.749671ms)
    Jan 18 21:57:45.778: INFO: (17) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 59.624218ms)
    Jan 18 21:57:45.778: INFO: (17) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 59.483923ms)
    Jan 18 21:57:45.778: INFO: (17) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 59.630529ms)
    Jan 18 21:57:45.778: INFO: (17) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 59.681908ms)
    Jan 18 21:57:45.783: INFO: (17) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 64.085686ms)
    Jan 18 21:57:45.783: INFO: (17) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 64.130151ms)
    Jan 18 21:57:45.783: INFO: (17) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 64.6479ms)
    Jan 18 21:57:45.783: INFO: (17) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 63.827325ms)
    Jan 18 21:57:45.803: INFO: (18) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 19.531194ms)
    Jan 18 21:57:45.804: INFO: (18) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 19.799591ms)
    Jan 18 21:57:45.804: INFO: (18) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 19.521217ms)
    Jan 18 21:57:45.806: INFO: (18) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 21.133818ms)
    Jan 18 21:57:45.806: INFO: (18) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 21.276785ms)
    Jan 18 21:57:45.806: INFO: (18) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 22.186833ms)
    Jan 18 21:57:45.806: INFO: (18) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 22.052924ms)
    Jan 18 21:57:45.806: INFO: (18) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 22.22724ms)
    Jan 18 21:57:45.811: INFO: (18) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 27.677919ms)
    Jan 18 21:57:45.814: INFO: (18) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 29.42636ms)
    Jan 18 21:57:45.814: INFO: (18) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 29.160062ms)
    Jan 18 21:57:45.814: INFO: (18) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 29.651549ms)
    Jan 18 21:57:45.814: INFO: (18) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 30.069681ms)
    Jan 18 21:57:45.814: INFO: (18) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 30.479871ms)
    Jan 18 21:57:45.814: INFO: (18) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 30.483951ms)
    Jan 18 21:57:45.815: INFO: (18) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 30.484657ms)
    Jan 18 21:57:45.830: INFO: (19) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:462/proxy/: tls qux (200; 14.901987ms)
    Jan 18 21:57:45.830: INFO: (19) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">test<... (200; 14.588341ms)
    Jan 18 21:57:45.830: INFO: (19) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 15.365151ms)
    Jan 18 21:57:45.834: INFO: (19) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 18.485721ms)
    Jan 18 21:57:45.841: INFO: (19) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88:162/proxy/: bar (200; 24.528262ms)
    Jan 18 21:57:45.852: INFO: (19) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname2/proxy/: bar (200; 35.759302ms)
    Jan 18 21:57:45.853: INFO: (19) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname1/proxy/: foo (200; 37.335226ms)
    Jan 18 21:57:45.853: INFO: (19) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname1/proxy/: tls baz (200; 38.069094ms)
    Jan 18 21:57:45.853: INFO: (19) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:160/proxy/: foo (200; 36.741774ms)
    Jan 18 21:57:45.854: INFO: (19) /api/v1/namespaces/proxy-8249/services/http:proxy-service-qg8xq:portname2/proxy/: bar (200; 37.676028ms)
    Jan 18 21:57:45.855: INFO: (19) /api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/proxy-service-qg8xq-2qj88/proxy/rewriteme">test</a> (200; 39.561464ms)
    Jan 18 21:57:45.856: INFO: (19) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:460/proxy/: tls baz (200; 41.092623ms)
    Jan 18 21:57:45.857: INFO: (19) /api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/https:proxy-service-qg8xq-2qj88:443/proxy/tlsrewritem... (200; 40.614656ms)
    Jan 18 21:57:45.857: INFO: (19) /api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/: <a href="/api/v1/namespaces/proxy-8249/pods/http:proxy-service-qg8xq-2qj88:1080/proxy/rewriteme">... (200; 41.296945ms)
    Jan 18 21:57:45.857: INFO: (19) /api/v1/namespaces/proxy-8249/services/proxy-service-qg8xq:portname1/proxy/: foo (200; 40.869106ms)
    Jan 18 21:57:45.858: INFO: (19) /api/v1/namespaces/proxy-8249/services/https:proxy-service-qg8xq:tlsportname2/proxy/: tls qux (200; 42.173594ms)
    STEP: deleting ReplicationController proxy-service-qg8xq in namespace proxy-8249, will wait for the garbage collector to delete the pods 01/18/23 21:57:45.858
    Jan 18 21:57:45.924: INFO: Deleting ReplicationController proxy-service-qg8xq took: 7.162574ms
    Jan 18 21:57:46.026: INFO: Terminating ReplicationController proxy-service-qg8xq pods took: 101.448112ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan 18 21:57:49.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-8249" for this suite. 01/18/23 21:57:49.032
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:57:49.047
Jan 18 21:57:49.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename security-context 01/18/23 21:57:49.049
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:57:49.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:57:49.077
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/18/23 21:57:49.082
Jan 18 21:57:49.096: INFO: Waiting up to 5m0s for pod "security-context-19949133-4418-404d-b8ef-a42052290d45" in namespace "security-context-797" to be "Succeeded or Failed"
Jan 18 21:57:49.112: INFO: Pod "security-context-19949133-4418-404d-b8ef-a42052290d45": Phase="Pending", Reason="", readiness=false. Elapsed: 15.26711ms
Jan 18 21:57:51.118: INFO: Pod "security-context-19949133-4418-404d-b8ef-a42052290d45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022091831s
Jan 18 21:57:53.118: INFO: Pod "security-context-19949133-4418-404d-b8ef-a42052290d45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021189103s
STEP: Saw pod success 01/18/23 21:57:53.118
Jan 18 21:57:53.118: INFO: Pod "security-context-19949133-4418-404d-b8ef-a42052290d45" satisfied condition "Succeeded or Failed"
Jan 18 21:57:53.122: INFO: Trying to get logs from node v1-25-1-18760-w2 pod security-context-19949133-4418-404d-b8ef-a42052290d45 container test-container: <nil>
STEP: delete the pod 01/18/23 21:57:53.131
Jan 18 21:57:53.147: INFO: Waiting for pod security-context-19949133-4418-404d-b8ef-a42052290d45 to disappear
Jan 18 21:57:53.161: INFO: Pod security-context-19949133-4418-404d-b8ef-a42052290d45 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 18 21:57:53.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-797" for this suite. 01/18/23 21:57:53.167
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":150,"skipped":2772,"failed":0}
------------------------------
• [4.126 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:57:49.047
    Jan 18 21:57:49.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename security-context 01/18/23 21:57:49.049
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:57:49.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:57:49.077
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/18/23 21:57:49.082
    Jan 18 21:57:49.096: INFO: Waiting up to 5m0s for pod "security-context-19949133-4418-404d-b8ef-a42052290d45" in namespace "security-context-797" to be "Succeeded or Failed"
    Jan 18 21:57:49.112: INFO: Pod "security-context-19949133-4418-404d-b8ef-a42052290d45": Phase="Pending", Reason="", readiness=false. Elapsed: 15.26711ms
    Jan 18 21:57:51.118: INFO: Pod "security-context-19949133-4418-404d-b8ef-a42052290d45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022091831s
    Jan 18 21:57:53.118: INFO: Pod "security-context-19949133-4418-404d-b8ef-a42052290d45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021189103s
    STEP: Saw pod success 01/18/23 21:57:53.118
    Jan 18 21:57:53.118: INFO: Pod "security-context-19949133-4418-404d-b8ef-a42052290d45" satisfied condition "Succeeded or Failed"
    Jan 18 21:57:53.122: INFO: Trying to get logs from node v1-25-1-18760-w2 pod security-context-19949133-4418-404d-b8ef-a42052290d45 container test-container: <nil>
    STEP: delete the pod 01/18/23 21:57:53.131
    Jan 18 21:57:53.147: INFO: Waiting for pod security-context-19949133-4418-404d-b8ef-a42052290d45 to disappear
    Jan 18 21:57:53.161: INFO: Pod security-context-19949133-4418-404d-b8ef-a42052290d45 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 18 21:57:53.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-797" for this suite. 01/18/23 21:57:53.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:57:53.18
Jan 18 21:57:53.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 21:57:53.182
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:57:53.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:57:53.205
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jan 18 21:57:53.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 21:57:53.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1035" for this suite. 01/18/23 21:57:53.798
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":151,"skipped":2792,"failed":0}
------------------------------
• [0.630 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:57:53.18
    Jan 18 21:57:53.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 21:57:53.182
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:57:53.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:57:53.205
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jan 18 21:57:53.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 21:57:53.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-1035" for this suite. 01/18/23 21:57:53.798
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:57:53.819
Jan 18 21:57:53.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename resourcequota 01/18/23 21:57:53.821
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:57:53.857
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:57:53.866
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 01/18/23 21:57:53.87
STEP: Creating a ResourceQuota 01/18/23 21:57:58.877
STEP: Ensuring resource quota status is calculated 01/18/23 21:57:58.886
STEP: Creating a Pod that fits quota 01/18/23 21:58:00.892
STEP: Ensuring ResourceQuota status captures the pod usage 01/18/23 21:58:00.911
STEP: Not allowing a pod to be created that exceeds remaining quota 01/18/23 21:58:02.95
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/18/23 21:58:02.954
STEP: Ensuring a pod cannot update its resource requirements 01/18/23 21:58:02.959
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/18/23 21:58:02.965
STEP: Deleting the pod 01/18/23 21:58:04.971
STEP: Ensuring resource quota status released the pod usage 01/18/23 21:58:05.055
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 21:58:07.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6734" for this suite. 01/18/23 21:58:07.069
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":152,"skipped":2830,"failed":0}
------------------------------
• [SLOW TEST] [13.258 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:57:53.819
    Jan 18 21:57:53.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename resourcequota 01/18/23 21:57:53.821
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:57:53.857
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:57:53.866
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 01/18/23 21:57:53.87
    STEP: Creating a ResourceQuota 01/18/23 21:57:58.877
    STEP: Ensuring resource quota status is calculated 01/18/23 21:57:58.886
    STEP: Creating a Pod that fits quota 01/18/23 21:58:00.892
    STEP: Ensuring ResourceQuota status captures the pod usage 01/18/23 21:58:00.911
    STEP: Not allowing a pod to be created that exceeds remaining quota 01/18/23 21:58:02.95
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/18/23 21:58:02.954
    STEP: Ensuring a pod cannot update its resource requirements 01/18/23 21:58:02.959
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/18/23 21:58:02.965
    STEP: Deleting the pod 01/18/23 21:58:04.971
    STEP: Ensuring resource quota status released the pod usage 01/18/23 21:58:05.055
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 21:58:07.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6734" for this suite. 01/18/23 21:58:07.069
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:58:07.093
Jan 18 21:58:07.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename statefulset 01/18/23 21:58:07.095
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:58:07.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:58:07.122
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7162 01/18/23 21:58:07.126
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Jan 18 21:58:07.152: INFO: Found 0 stateful pods, waiting for 1
Jan 18 21:58:17.159: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 01/18/23 21:58:17.176
W0118 21:58:17.190378      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 18 21:58:17.202: INFO: Found 1 stateful pods, waiting for 2
Jan 18 21:58:27.212: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 21:58:27.213: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 01/18/23 21:58:27.224
STEP: Delete all of the StatefulSets 01/18/23 21:58:27.229
STEP: Verify that StatefulSets have been deleted 01/18/23 21:58:27.238
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 21:58:27.245: INFO: Deleting all statefulset in ns statefulset-7162
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 18 21:58:27.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7162" for this suite. 01/18/23 21:58:27.284
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":153,"skipped":2892,"failed":0}
------------------------------
• [SLOW TEST] [20.222 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:58:07.093
    Jan 18 21:58:07.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename statefulset 01/18/23 21:58:07.095
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:58:07.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:58:07.122
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7162 01/18/23 21:58:07.126
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Jan 18 21:58:07.152: INFO: Found 0 stateful pods, waiting for 1
    Jan 18 21:58:17.159: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 01/18/23 21:58:17.176
    W0118 21:58:17.190378      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 18 21:58:17.202: INFO: Found 1 stateful pods, waiting for 2
    Jan 18 21:58:27.212: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 21:58:27.213: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 01/18/23 21:58:27.224
    STEP: Delete all of the StatefulSets 01/18/23 21:58:27.229
    STEP: Verify that StatefulSets have been deleted 01/18/23 21:58:27.238
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 18 21:58:27.245: INFO: Deleting all statefulset in ns statefulset-7162
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 18 21:58:27.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7162" for this suite. 01/18/23 21:58:27.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:58:27.32
Jan 18 21:58:27.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 21:58:27.331
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:58:27.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:58:27.357
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-198bea95-b10b-49c1-976e-079d9f7fde37 01/18/23 21:58:27.361
STEP: Creating a pod to test consume secrets 01/18/23 21:58:27.368
Jan 18 21:58:27.378: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e021c366-91f0-4bdd-a890-00d992349227" in namespace "projected-1684" to be "Succeeded or Failed"
Jan 18 21:58:27.385: INFO: Pod "pod-projected-secrets-e021c366-91f0-4bdd-a890-00d992349227": Phase="Pending", Reason="", readiness=false. Elapsed: 6.092146ms
Jan 18 21:58:29.389: INFO: Pod "pod-projected-secrets-e021c366-91f0-4bdd-a890-00d992349227": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010331088s
Jan 18 21:58:31.392: INFO: Pod "pod-projected-secrets-e021c366-91f0-4bdd-a890-00d992349227": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012800001s
STEP: Saw pod success 01/18/23 21:58:31.392
Jan 18 21:58:31.392: INFO: Pod "pod-projected-secrets-e021c366-91f0-4bdd-a890-00d992349227" satisfied condition "Succeeded or Failed"
Jan 18 21:58:31.398: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-secrets-e021c366-91f0-4bdd-a890-00d992349227 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/18/23 21:58:31.409
Jan 18 21:58:31.422: INFO: Waiting for pod pod-projected-secrets-e021c366-91f0-4bdd-a890-00d992349227 to disappear
Jan 18 21:58:31.426: INFO: Pod pod-projected-secrets-e021c366-91f0-4bdd-a890-00d992349227 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 18 21:58:31.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1684" for this suite. 01/18/23 21:58:31.431
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":154,"skipped":2908,"failed":0}
------------------------------
• [4.118 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:58:27.32
    Jan 18 21:58:27.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 21:58:27.331
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:58:27.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:58:27.357
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-198bea95-b10b-49c1-976e-079d9f7fde37 01/18/23 21:58:27.361
    STEP: Creating a pod to test consume secrets 01/18/23 21:58:27.368
    Jan 18 21:58:27.378: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e021c366-91f0-4bdd-a890-00d992349227" in namespace "projected-1684" to be "Succeeded or Failed"
    Jan 18 21:58:27.385: INFO: Pod "pod-projected-secrets-e021c366-91f0-4bdd-a890-00d992349227": Phase="Pending", Reason="", readiness=false. Elapsed: 6.092146ms
    Jan 18 21:58:29.389: INFO: Pod "pod-projected-secrets-e021c366-91f0-4bdd-a890-00d992349227": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010331088s
    Jan 18 21:58:31.392: INFO: Pod "pod-projected-secrets-e021c366-91f0-4bdd-a890-00d992349227": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012800001s
    STEP: Saw pod success 01/18/23 21:58:31.392
    Jan 18 21:58:31.392: INFO: Pod "pod-projected-secrets-e021c366-91f0-4bdd-a890-00d992349227" satisfied condition "Succeeded or Failed"
    Jan 18 21:58:31.398: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-secrets-e021c366-91f0-4bdd-a890-00d992349227 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 21:58:31.409
    Jan 18 21:58:31.422: INFO: Waiting for pod pod-projected-secrets-e021c366-91f0-4bdd-a890-00d992349227 to disappear
    Jan 18 21:58:31.426: INFO: Pod pod-projected-secrets-e021c366-91f0-4bdd-a890-00d992349227 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 18 21:58:31.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1684" for this suite. 01/18/23 21:58:31.431
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:58:31.44
Jan 18 21:58:31.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename sched-preemption 01/18/23 21:58:31.442
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:58:31.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:58:31.483
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 18 21:58:31.533: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 21:59:31.589: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 01/18/23 21:59:31.594
Jan 18 21:59:31.626: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 18 21:59:31.638: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 18 21:59:31.683: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 18 21:59:31.693: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/18/23 21:59:31.693
Jan 18 21:59:31.693: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-674" to be "running"
Jan 18 21:59:31.703: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 9.322602ms
Jan 18 21:59:33.710: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016071412s
Jan 18 21:59:35.710: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016375454s
Jan 18 21:59:37.707: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013687705s
Jan 18 21:59:39.709: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015750238s
Jan 18 21:59:41.711: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.017414535s
Jan 18 21:59:43.710: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.01678709s
Jan 18 21:59:43.710: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 18 21:59:43.711: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-674" to be "running"
Jan 18 21:59:43.716: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.067476ms
Jan 18 21:59:43.716: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 18 21:59:43.717: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-674" to be "running"
Jan 18 21:59:43.721: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.305107ms
Jan 18 21:59:43.721: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 18 21:59:43.721: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-674" to be "running"
Jan 18 21:59:43.725: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.159524ms
Jan 18 21:59:43.725: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 01/18/23 21:59:43.725
Jan 18 21:59:43.736: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jan 18 21:59:43.749: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.330587ms
Jan 18 21:59:45.755: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0190956s
Jan 18 21:59:47.755: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01925976s
Jan 18 21:59:49.756: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.020712258s
Jan 18 21:59:49.757: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 18 21:59:49.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-674" for this suite. 01/18/23 21:59:49.798
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":155,"skipped":2919,"failed":0}
------------------------------
• [SLOW TEST] [78.413 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:58:31.44
    Jan 18 21:58:31.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename sched-preemption 01/18/23 21:58:31.442
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:58:31.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:58:31.483
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 18 21:58:31.533: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 21:59:31.589: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 01/18/23 21:59:31.594
    Jan 18 21:59:31.626: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 18 21:59:31.638: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 18 21:59:31.683: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 18 21:59:31.693: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/18/23 21:59:31.693
    Jan 18 21:59:31.693: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-674" to be "running"
    Jan 18 21:59:31.703: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 9.322602ms
    Jan 18 21:59:33.710: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016071412s
    Jan 18 21:59:35.710: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016375454s
    Jan 18 21:59:37.707: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013687705s
    Jan 18 21:59:39.709: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015750238s
    Jan 18 21:59:41.711: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.017414535s
    Jan 18 21:59:43.710: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.01678709s
    Jan 18 21:59:43.710: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 18 21:59:43.711: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-674" to be "running"
    Jan 18 21:59:43.716: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.067476ms
    Jan 18 21:59:43.716: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 18 21:59:43.717: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-674" to be "running"
    Jan 18 21:59:43.721: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.305107ms
    Jan 18 21:59:43.721: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 18 21:59:43.721: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-674" to be "running"
    Jan 18 21:59:43.725: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.159524ms
    Jan 18 21:59:43.725: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 01/18/23 21:59:43.725
    Jan 18 21:59:43.736: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jan 18 21:59:43.749: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.330587ms
    Jan 18 21:59:45.755: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0190956s
    Jan 18 21:59:47.755: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01925976s
    Jan 18 21:59:49.756: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.020712258s
    Jan 18 21:59:49.757: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 21:59:49.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-674" for this suite. 01/18/23 21:59:49.798
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:59:49.858
Jan 18 21:59:49.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 21:59:49.861
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:59:49.887
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:59:49.893
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-bd4cf9de-fc37-4095-a160-61f65da817f5 01/18/23 21:59:49.897
STEP: Creating a pod to test consume secrets 01/18/23 21:59:49.905
Jan 18 21:59:49.924: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f45323cf-eae4-4ce9-9d19-e365c97d4e75" in namespace "projected-9897" to be "Succeeded or Failed"
Jan 18 21:59:49.938: INFO: Pod "pod-projected-secrets-f45323cf-eae4-4ce9-9d19-e365c97d4e75": Phase="Pending", Reason="", readiness=false. Elapsed: 14.431211ms
Jan 18 21:59:51.944: INFO: Pod "pod-projected-secrets-f45323cf-eae4-4ce9-9d19-e365c97d4e75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020321283s
Jan 18 21:59:53.945: INFO: Pod "pod-projected-secrets-f45323cf-eae4-4ce9-9d19-e365c97d4e75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020946211s
STEP: Saw pod success 01/18/23 21:59:53.945
Jan 18 21:59:53.946: INFO: Pod "pod-projected-secrets-f45323cf-eae4-4ce9-9d19-e365c97d4e75" satisfied condition "Succeeded or Failed"
Jan 18 21:59:53.952: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-secrets-f45323cf-eae4-4ce9-9d19-e365c97d4e75 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/18/23 21:59:53.961
Jan 18 21:59:53.981: INFO: Waiting for pod pod-projected-secrets-f45323cf-eae4-4ce9-9d19-e365c97d4e75 to disappear
Jan 18 21:59:53.986: INFO: Pod pod-projected-secrets-f45323cf-eae4-4ce9-9d19-e365c97d4e75 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 18 21:59:53.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9897" for this suite. 01/18/23 21:59:53.993
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":156,"skipped":2923,"failed":0}
------------------------------
• [4.144 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:59:49.858
    Jan 18 21:59:49.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 21:59:49.861
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:59:49.887
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:59:49.893
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-bd4cf9de-fc37-4095-a160-61f65da817f5 01/18/23 21:59:49.897
    STEP: Creating a pod to test consume secrets 01/18/23 21:59:49.905
    Jan 18 21:59:49.924: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f45323cf-eae4-4ce9-9d19-e365c97d4e75" in namespace "projected-9897" to be "Succeeded or Failed"
    Jan 18 21:59:49.938: INFO: Pod "pod-projected-secrets-f45323cf-eae4-4ce9-9d19-e365c97d4e75": Phase="Pending", Reason="", readiness=false. Elapsed: 14.431211ms
    Jan 18 21:59:51.944: INFO: Pod "pod-projected-secrets-f45323cf-eae4-4ce9-9d19-e365c97d4e75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020321283s
    Jan 18 21:59:53.945: INFO: Pod "pod-projected-secrets-f45323cf-eae4-4ce9-9d19-e365c97d4e75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020946211s
    STEP: Saw pod success 01/18/23 21:59:53.945
    Jan 18 21:59:53.946: INFO: Pod "pod-projected-secrets-f45323cf-eae4-4ce9-9d19-e365c97d4e75" satisfied condition "Succeeded or Failed"
    Jan 18 21:59:53.952: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-secrets-f45323cf-eae4-4ce9-9d19-e365c97d4e75 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 21:59:53.961
    Jan 18 21:59:53.981: INFO: Waiting for pod pod-projected-secrets-f45323cf-eae4-4ce9-9d19-e365c97d4e75 to disappear
    Jan 18 21:59:53.986: INFO: Pod pod-projected-secrets-f45323cf-eae4-4ce9-9d19-e365c97d4e75 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 18 21:59:53.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9897" for this suite. 01/18/23 21:59:53.993
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 21:59:54.005
Jan 18 21:59:54.006: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename downward-api 01/18/23 21:59:54.008
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:59:54.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:59:54.064
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 01/18/23 21:59:54.07
Jan 18 21:59:54.081: INFO: Waiting up to 5m0s for pod "labelsupdatebe5be980-d089-4891-92eb-a16f38fd4d76" in namespace "downward-api-8724" to be "running and ready"
Jan 18 21:59:54.085: INFO: Pod "labelsupdatebe5be980-d089-4891-92eb-a16f38fd4d76": Phase="Pending", Reason="", readiness=false. Elapsed: 3.930131ms
Jan 18 21:59:54.085: INFO: The phase of Pod labelsupdatebe5be980-d089-4891-92eb-a16f38fd4d76 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:59:56.097: INFO: Pod "labelsupdatebe5be980-d089-4891-92eb-a16f38fd4d76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015324963s
Jan 18 21:59:56.097: INFO: The phase of Pod labelsupdatebe5be980-d089-4891-92eb-a16f38fd4d76 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:59:58.092: INFO: Pod "labelsupdatebe5be980-d089-4891-92eb-a16f38fd4d76": Phase="Running", Reason="", readiness=true. Elapsed: 4.010829856s
Jan 18 21:59:58.092: INFO: The phase of Pod labelsupdatebe5be980-d089-4891-92eb-a16f38fd4d76 is Running (Ready = true)
Jan 18 21:59:58.092: INFO: Pod "labelsupdatebe5be980-d089-4891-92eb-a16f38fd4d76" satisfied condition "running and ready"
Jan 18 21:59:58.626: INFO: Successfully updated pod "labelsupdatebe5be980-d089-4891-92eb-a16f38fd4d76"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 22:00:00.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8724" for this suite. 01/18/23 22:00:00.649
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":157,"skipped":2942,"failed":0}
------------------------------
• [SLOW TEST] [6.654 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 21:59:54.005
    Jan 18 21:59:54.006: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename downward-api 01/18/23 21:59:54.008
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:59:54.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:59:54.064
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 01/18/23 21:59:54.07
    Jan 18 21:59:54.081: INFO: Waiting up to 5m0s for pod "labelsupdatebe5be980-d089-4891-92eb-a16f38fd4d76" in namespace "downward-api-8724" to be "running and ready"
    Jan 18 21:59:54.085: INFO: Pod "labelsupdatebe5be980-d089-4891-92eb-a16f38fd4d76": Phase="Pending", Reason="", readiness=false. Elapsed: 3.930131ms
    Jan 18 21:59:54.085: INFO: The phase of Pod labelsupdatebe5be980-d089-4891-92eb-a16f38fd4d76 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:59:56.097: INFO: Pod "labelsupdatebe5be980-d089-4891-92eb-a16f38fd4d76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015324963s
    Jan 18 21:59:56.097: INFO: The phase of Pod labelsupdatebe5be980-d089-4891-92eb-a16f38fd4d76 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:59:58.092: INFO: Pod "labelsupdatebe5be980-d089-4891-92eb-a16f38fd4d76": Phase="Running", Reason="", readiness=true. Elapsed: 4.010829856s
    Jan 18 21:59:58.092: INFO: The phase of Pod labelsupdatebe5be980-d089-4891-92eb-a16f38fd4d76 is Running (Ready = true)
    Jan 18 21:59:58.092: INFO: Pod "labelsupdatebe5be980-d089-4891-92eb-a16f38fd4d76" satisfied condition "running and ready"
    Jan 18 21:59:58.626: INFO: Successfully updated pod "labelsupdatebe5be980-d089-4891-92eb-a16f38fd4d76"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 22:00:00.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8724" for this suite. 01/18/23 22:00:00.649
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:00:00.666
Jan 18 22:00:00.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 22:00:00.668
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:00:00.706
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:00:00.713
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 01/18/23 22:00:00.719
Jan 18 22:00:00.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: rename a version 01/18/23 22:00:10.441
STEP: check the new version name is served 01/18/23 22:00:10.461
STEP: check the old version name is removed 01/18/23 22:00:14.768
STEP: check the other version is not changed 01/18/23 22:00:16.942
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 22:00:24.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2395" for this suite. 01/18/23 22:00:24.663
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":158,"skipped":2950,"failed":0}
------------------------------
• [SLOW TEST] [24.005 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:00:00.666
    Jan 18 22:00:00.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 22:00:00.668
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:00:00.706
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:00:00.713
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 01/18/23 22:00:00.719
    Jan 18 22:00:00.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: rename a version 01/18/23 22:00:10.441
    STEP: check the new version name is served 01/18/23 22:00:10.461
    STEP: check the old version name is removed 01/18/23 22:00:14.768
    STEP: check the other version is not changed 01/18/23 22:00:16.942
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 22:00:24.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2395" for this suite. 01/18/23 22:00:24.663
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:00:24.682
Jan 18 22:00:24.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename deployment 01/18/23 22:00:24.684
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:00:24.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:00:24.721
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jan 18 22:00:24.753: INFO: Creating deployment "webserver-deployment"
Jan 18 22:00:24.764: INFO: Waiting for observed generation 1
Jan 18 22:00:26.797: INFO: Waiting for all required pods to come up
Jan 18 22:00:26.815: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 01/18/23 22:00:26.815
Jan 18 22:00:26.815: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-xl267" in namespace "deployment-2141" to be "running"
Jan 18 22:00:26.816: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-47p64" in namespace "deployment-2141" to be "running"
Jan 18 22:00:26.816: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-54fsp" in namespace "deployment-2141" to be "running"
Jan 18 22:00:26.816: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-bv5wl" in namespace "deployment-2141" to be "running"
Jan 18 22:00:26.816: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-f5cgz" in namespace "deployment-2141" to be "running"
Jan 18 22:00:26.816: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-mp2cf" in namespace "deployment-2141" to be "running"
Jan 18 22:00:26.817: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-prsgm" in namespace "deployment-2141" to be "running"
Jan 18 22:00:26.817: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-qlrf2" in namespace "deployment-2141" to be "running"
Jan 18 22:00:26.817: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-srvfg" in namespace "deployment-2141" to be "running"
Jan 18 22:00:26.817: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-xcwjm" in namespace "deployment-2141" to be "running"
Jan 18 22:00:26.839: INFO: Pod "webserver-deployment-845c8977d9-mp2cf": Phase="Pending", Reason="", readiness=false. Elapsed: 22.159036ms
Jan 18 22:00:26.839: INFO: Pod "webserver-deployment-845c8977d9-47p64": Phase="Pending", Reason="", readiness=false. Elapsed: 23.024137ms
Jan 18 22:00:26.839: INFO: Pod "webserver-deployment-845c8977d9-xl267": Phase="Pending", Reason="", readiness=false. Elapsed: 23.766395ms
Jan 18 22:00:26.839: INFO: Pod "webserver-deployment-845c8977d9-qlrf2": Phase="Pending", Reason="", readiness=false. Elapsed: 22.586259ms
Jan 18 22:00:26.839: INFO: Pod "webserver-deployment-845c8977d9-54fsp": Phase="Pending", Reason="", readiness=false. Elapsed: 23.447116ms
Jan 18 22:00:26.839: INFO: Pod "webserver-deployment-845c8977d9-prsgm": Phase="Pending", Reason="", readiness=false. Elapsed: 22.825797ms
Jan 18 22:00:26.840: INFO: Pod "webserver-deployment-845c8977d9-f5cgz": Phase="Pending", Reason="", readiness=false. Elapsed: 23.304173ms
Jan 18 22:00:26.840: INFO: Pod "webserver-deployment-845c8977d9-srvfg": Phase="Pending", Reason="", readiness=false. Elapsed: 22.855327ms
Jan 18 22:00:26.840: INFO: Pod "webserver-deployment-845c8977d9-xcwjm": Phase="Pending", Reason="", readiness=false. Elapsed: 22.89018ms
Jan 18 22:00:26.839: INFO: Pod "webserver-deployment-845c8977d9-bv5wl": Phase="Pending", Reason="", readiness=false. Elapsed: 22.592499ms
Jan 18 22:00:28.851: INFO: Pod "webserver-deployment-845c8977d9-prsgm": Phase="Running", Reason="", readiness=true. Elapsed: 2.034581168s
Jan 18 22:00:28.851: INFO: Pod "webserver-deployment-845c8977d9-prsgm" satisfied condition "running"
Jan 18 22:00:28.854: INFO: Pod "webserver-deployment-845c8977d9-mp2cf": Phase="Running", Reason="", readiness=true. Elapsed: 2.037090968s
Jan 18 22:00:28.854: INFO: Pod "webserver-deployment-845c8977d9-mp2cf" satisfied condition "running"
Jan 18 22:00:28.854: INFO: Pod "webserver-deployment-845c8977d9-47p64": Phase="Running", Reason="", readiness=true. Elapsed: 2.0379658s
Jan 18 22:00:28.854: INFO: Pod "webserver-deployment-845c8977d9-47p64" satisfied condition "running"
Jan 18 22:00:28.861: INFO: Pod "webserver-deployment-845c8977d9-bv5wl": Phase="Running", Reason="", readiness=true. Elapsed: 2.04511501s
Jan 18 22:00:28.861: INFO: Pod "webserver-deployment-845c8977d9-bv5wl" satisfied condition "running"
Jan 18 22:00:28.862: INFO: Pod "webserver-deployment-845c8977d9-xcwjm": Phase="Running", Reason="", readiness=true. Elapsed: 2.044585253s
Jan 18 22:00:28.862: INFO: Pod "webserver-deployment-845c8977d9-xcwjm" satisfied condition "running"
Jan 18 22:00:28.862: INFO: Pod "webserver-deployment-845c8977d9-srvfg": Phase="Running", Reason="", readiness=true. Elapsed: 2.044898448s
Jan 18 22:00:28.862: INFO: Pod "webserver-deployment-845c8977d9-srvfg" satisfied condition "running"
Jan 18 22:00:28.862: INFO: Pod "webserver-deployment-845c8977d9-qlrf2": Phase="Running", Reason="", readiness=true. Elapsed: 2.045168013s
Jan 18 22:00:28.862: INFO: Pod "webserver-deployment-845c8977d9-qlrf2" satisfied condition "running"
Jan 18 22:00:28.862: INFO: Pod "webserver-deployment-845c8977d9-54fsp": Phase="Running", Reason="", readiness=true. Elapsed: 2.046070843s
Jan 18 22:00:28.862: INFO: Pod "webserver-deployment-845c8977d9-54fsp" satisfied condition "running"
Jan 18 22:00:28.862: INFO: Pod "webserver-deployment-845c8977d9-f5cgz": Phase="Running", Reason="", readiness=true. Elapsed: 2.045882125s
Jan 18 22:00:28.862: INFO: Pod "webserver-deployment-845c8977d9-f5cgz" satisfied condition "running"
Jan 18 22:00:28.862: INFO: Pod "webserver-deployment-845c8977d9-xl267": Phase="Running", Reason="", readiness=true. Elapsed: 2.04693817s
Jan 18 22:00:28.862: INFO: Pod "webserver-deployment-845c8977d9-xl267" satisfied condition "running"
Jan 18 22:00:28.863: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 18 22:00:28.879: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 18 22:00:28.896: INFO: Updating deployment webserver-deployment
Jan 18 22:00:28.896: INFO: Waiting for observed generation 2
Jan 18 22:00:30.916: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 18 22:00:30.920: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 18 22:00:30.923: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 18 22:00:30.946: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 18 22:00:30.946: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 18 22:00:30.950: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 18 22:00:30.960: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 18 22:00:30.960: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 18 22:00:30.980: INFO: Updating deployment webserver-deployment
Jan 18 22:00:30.980: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 18 22:00:31.188: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 18 22:00:33.392: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 22:00:33.427: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2141  1c7eb0c9-fc05-417e-b7f2-a5555bbb4124 148791 3 2023-01-18 22:00:24 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 22:00:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044175b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-18 22:00:31 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-01-18 22:00:31 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 18 22:00:33.436: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-2141  f5c6fca7-22ae-4bd9-abce-33929f0b43d7 148777 3 2023-01-18 22:00:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 1c7eb0c9-fc05-417e-b7f2-a5555bbb4124 0xc004417b57 0xc004417b58}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c7eb0c9-fc05-417e-b7f2-a5555bbb4124\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004417c28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 22:00:33.436: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 18 22:00:33.436: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-2141  e0acab07-df5b-4821-b8e4-ffda450c40b5 148789 3 2023-01-18 22:00:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 1c7eb0c9-fc05-417e-b7f2-a5555bbb4124 0xc004417cc7 0xc004417cc8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:00:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c7eb0c9-fc05-417e-b7f2-a5555bbb4124\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004417da8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 18 22:00:33.449: INFO: Pod "webserver-deployment-69b7448995-49nsm" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-49nsm webserver-deployment-69b7448995- deployment-2141  443b69fa-a746-4754-b34a-458e118dd06b 148741 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f5c6fca7-22ae-4bd9-abce-33929f0b43d7 0xc00443a207 0xc00443a208}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c6fca7-22ae-4bd9-abce-33929f0b43d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s4zpc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s4zpc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.449: INFO: Pod "webserver-deployment-69b7448995-4dgw7" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-4dgw7 webserver-deployment-69b7448995- deployment-2141  f430ad70-5cd9-4352-ab23-d3610f96777d 148673 0 2023-01-18 22:00:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:8e89b853d76f36292507bbe33ea9d9afa3fba9d786f5afba6d08d4a75bed4b92 cni.projectcalico.org/podIP:10.233.78.234/32 cni.projectcalico.org/podIPs:10.233.78.234/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f5c6fca7-22ae-4bd9-abce-33929f0b43d7 0xc00443a480 0xc00443a481}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c6fca7-22ae-4bd9-abce-33929f0b43d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:00:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 22:00:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-txzkz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-txzkz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:,StartTime:2023-01-18 22:00:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.450: INFO: Pod "webserver-deployment-69b7448995-59xtf" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-59xtf webserver-deployment-69b7448995- deployment-2141  73e5786c-b3f1-4e6d-aee8-596c44d8877b 148775 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f5c6fca7-22ae-4bd9-abce-33929f0b43d7 0xc00443a767 0xc00443a768}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c6fca7-22ae-4bd9-abce-33929f0b43d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fkbnx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fkbnx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.450: INFO: Pod "webserver-deployment-69b7448995-6pz2h" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-6pz2h webserver-deployment-69b7448995- deployment-2141  8ffa0771-33ec-4028-8310-fd11e5f2f44e 148685 0 2023-01-18 22:00:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:aaf7d6cb0f367a591263e4077f346f575317cf8b1984acb9fa17207f8d80f8d4 cni.projectcalico.org/podIP:10.233.78.235/32 cni.projectcalico.org/podIPs:10.233.78.235/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f5c6fca7-22ae-4bd9-abce-33929f0b43d7 0xc00443a9a0 0xc00443a9a1}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c6fca7-22ae-4bd9-abce-33929f0b43d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:00:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 22:00:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tllw9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tllw9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:,StartTime:2023-01-18 22:00:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.450: INFO: Pod "webserver-deployment-69b7448995-6wdrc" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-6wdrc webserver-deployment-69b7448995- deployment-2141  d4120a42-6c1e-4af7-8425-16e3e934b298 148776 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f5c6fca7-22ae-4bd9-abce-33929f0b43d7 0xc00443abe7 0xc00443abe8}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c6fca7-22ae-4bd9-abce-33929f0b43d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cdd9f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cdd9f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:,StartTime:2023-01-18 22:00:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.451: INFO: Pod "webserver-deployment-69b7448995-8zt24" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-8zt24 webserver-deployment-69b7448995- deployment-2141  0407f9c0-f0b9-45a6-928f-1dfc9bab2048 148767 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f5c6fca7-22ae-4bd9-abce-33929f0b43d7 0xc00443ae57 0xc00443ae58}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c6fca7-22ae-4bd9-abce-33929f0b43d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vc62b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vc62b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.451: INFO: Pod "webserver-deployment-69b7448995-9dw5z" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-9dw5z webserver-deployment-69b7448995- deployment-2141  17e76b1b-0f66-4089-bd0f-bc73c3cd154d 148755 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f5c6fca7-22ae-4bd9-abce-33929f0b43d7 0xc00443aff0 0xc00443aff1}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c6fca7-22ae-4bd9-abce-33929f0b43d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fgnnk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fgnnk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.451: INFO: Pod "webserver-deployment-69b7448995-cmsjm" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-cmsjm webserver-deployment-69b7448995- deployment-2141  33ba1e48-a880-44b8-9b2e-6d499341e265 148675 0 2023-01-18 22:00:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:f4a71d26c8b5dc38d84b233f74142f71a152efffe72ff82b06c5db0bed9dd426 cni.projectcalico.org/podIP:10.233.68.179/32 cni.projectcalico.org/podIPs:10.233.68.179/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f5c6fca7-22ae-4bd9-abce-33929f0b43d7 0xc00443b2a0 0xc00443b2a1}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c6fca7-22ae-4bd9-abce-33929f0b43d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:00:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 22:00:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kblpc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kblpc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:,StartTime:2023-01-18 22:00:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.451: INFO: Pod "webserver-deployment-69b7448995-htmw5" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-htmw5 webserver-deployment-69b7448995- deployment-2141  d5e30a07-5029-44bc-87b7-7d367fcdf000 148764 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f5c6fca7-22ae-4bd9-abce-33929f0b43d7 0xc00443b547 0xc00443b548}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c6fca7-22ae-4bd9-abce-33929f0b43d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pl7lf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pl7lf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.451: INFO: Pod "webserver-deployment-69b7448995-kqppk" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-kqppk webserver-deployment-69b7448995- deployment-2141  5b87c324-8f9e-49f6-9486-4cf0ab614bf1 148766 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f5c6fca7-22ae-4bd9-abce-33929f0b43d7 0xc00443b720 0xc00443b721}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c6fca7-22ae-4bd9-abce-33929f0b43d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l9867,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l9867,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.452: INFO: Pod "webserver-deployment-69b7448995-lv9gv" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-lv9gv webserver-deployment-69b7448995- deployment-2141  8658b974-8ee4-4728-93d0-ffcb3b09bc40 148686 0 2023-01-18 22:00:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:79a054e07fd664dfbd87446a1b50550f83ab288019292345eecedd678f66508e cni.projectcalico.org/podIP:10.233.68.185/32 cni.projectcalico.org/podIPs:10.233.68.185/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f5c6fca7-22ae-4bd9-abce-33929f0b43d7 0xc00443b930 0xc00443b931}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c6fca7-22ae-4bd9-abce-33929f0b43d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:00:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 22:00:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g6ljp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g6ljp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:,StartTime:2023-01-18 22:00:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.452: INFO: Pod "webserver-deployment-69b7448995-r2szl" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-r2szl webserver-deployment-69b7448995- deployment-2141  be4361d9-3593-4655-8620-beabd63b553e 148836 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:4450fa28354c10dfb4d0d3bf8bd5b954cd4a6f5181209f1b2672acb7b2bd1fdf cni.projectcalico.org/podIP:10.233.68.187/32 cni.projectcalico.org/podIPs:10.233.68.187/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f5c6fca7-22ae-4bd9-abce-33929f0b43d7 0xc00443bbf7 0xc00443bbf8}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c6fca7-22ae-4bd9-abce-33929f0b43d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 22:00:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6l4xq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6l4xq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:,StartTime:2023-01-18 22:00:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.452: INFO: Pod "webserver-deployment-69b7448995-xwkpk" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-xwkpk webserver-deployment-69b7448995- deployment-2141  77875a61-ab4a-4fb2-a852-4a2a7f925059 148695 0 2023-01-18 22:00:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:246d837a906188ad56478f7bc3fb4f6bc9d7f7d486054742e20e8317641c775d cni.projectcalico.org/podIP:10.233.68.182/32 cni.projectcalico.org/podIPs:10.233.68.182/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f5c6fca7-22ae-4bd9-abce-33929f0b43d7 0xc00443be37 0xc00443be38}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c6fca7-22ae-4bd9-abce-33929f0b43d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:00:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 22:00:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-92rcz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-92rcz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:,StartTime:2023-01-18 22:00:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.452: INFO: Pod "webserver-deployment-845c8977d9-47p64" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-47p64 webserver-deployment-845c8977d9- deployment-2141  1fcae03e-9627-44e9-b5d4-14ecbb2f906d 148583 0 2023-01-18 22:00:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:133f8f7c25580f2060437ca8f87954ad22cbdb41de51b3c93021164c72d64a4e cni.projectcalico.org/podIP:10.233.68.180/32 cni.projectcalico.org/podIPs:10.233.68.180/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a6067 0xc0044a6068}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:00:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 22:00:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.180\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6njgb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6njgb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.180,StartTime:2023-01-18 22:00:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:00:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://e8e1390a5fd354829dc53070dfbf993a17c2a83d74ec41302bd676f1d8ed657e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.180,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.454: INFO: Pod "webserver-deployment-845c8977d9-9txbn" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-9txbn webserver-deployment-845c8977d9- deployment-2141  36a18906-d6ba-4003-82c5-cfff83df3f7a 148765 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a62f7 0xc0044a62f8}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lwqws,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lwqws,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.454: INFO: Pod "webserver-deployment-845c8977d9-bv5wl" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-bv5wl webserver-deployment-845c8977d9- deployment-2141  d4efd5b3-08ea-4826-90e1-b96f6b504d1c 148579 0 2023-01-18 22:00:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:7294d02c5eec56a3035d55ff9022cb8910c218992dde21cc65d1fae9ad388bba cni.projectcalico.org/podIP:10.233.68.174/32 cni.projectcalico.org/podIPs:10.233.68.174/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a64a0 0xc0044a64a1}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:00:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 22:00:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cmbsc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cmbsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.174,StartTime:2023-01-18 22:00:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:00:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://10c2f7fca72c44a12d6562223a828527d41bcc9b1a3b560a59e12d6acb5762cc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.174,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.454: INFO: Pod "webserver-deployment-845c8977d9-dfm28" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-dfm28 webserver-deployment-845c8977d9- deployment-2141  961d58e1-bf91-4ed9-a830-b7bacd052ede 148763 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a6727 0xc0044a6728}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cs66v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cs66v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.455: INFO: Pod "webserver-deployment-845c8977d9-f5cgz" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-f5cgz webserver-deployment-845c8977d9- deployment-2141  079ce550-81e7-41ab-84aa-da7fe73d3a98 148561 0 2023-01-18 22:00:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8eb7fb668f69006aba8d736fef30254ea80da8d59a3ccec1dcbf5b4acc89151d cni.projectcalico.org/podIP:10.233.78.229/32 cni.projectcalico.org/podIPs:10.233.78.229/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a6920 0xc0044a6921}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:00:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 22:00:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.229\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g7wp4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g7wp4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:10.233.78.229,StartTime:2023-01-18 22:00:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:00:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://83110d94bb8bfd5f8989d6cac1d1567494f5bae4e76c200b731fd2bf30d47a39,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.229,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.455: INFO: Pod "webserver-deployment-845c8977d9-lh8pb" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-lh8pb webserver-deployment-845c8977d9- deployment-2141  71b9d3e2-aa43-4370-93bd-b3849c272981 148768 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a6ba7 0xc0044a6ba8}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n56bd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n56bd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.455: INFO: Pod "webserver-deployment-845c8977d9-prsgm" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-prsgm webserver-deployment-845c8977d9- deployment-2141  e8d96499-421f-4aa2-a6fd-51df5ded2738 148605 0 2023-01-18 22:00:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:593962979809383fe664688d59c92693227af22d167a97e20a164f445f9d0f6c cni.projectcalico.org/podIP:10.233.78.233/32 cni.projectcalico.org/podIPs:10.233.78.233/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a6de0 0xc0044a6de1}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:00:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 22:00:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.233\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zj59m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zj59m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:10.233.78.233,StartTime:2023-01-18 22:00:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:00:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://e8fa3dfc5c86691b0209cc5040ca1d2f2e814c4d29de70440958072af548c28b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.233,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.456: INFO: Pod "webserver-deployment-845c8977d9-qlrf2" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-qlrf2 webserver-deployment-845c8977d9- deployment-2141  aeafa170-2caf-4cae-adda-6a439533d097 148609 0 2023-01-18 22:00:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:6233ea2de493c2d39286c7f0f94a88be36e82d6cba5d0b21f5422a9bac3e39c1 cni.projectcalico.org/podIP:10.233.68.181/32 cni.projectcalico.org/podIPs:10.233.68.181/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a70e7 0xc0044a70e8}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:00:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 22:00:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s5tcw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s5tcw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.181,StartTime:2023-01-18 22:00:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:00:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://6c0e6a96151ddf8478d0a76fbe2e5906caca4249a6357cd3d48a5949c1c905e2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.456: INFO: Pod "webserver-deployment-845c8977d9-qpg4h" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-qpg4h webserver-deployment-845c8977d9- deployment-2141  6bea42d4-0e9d-494f-8845-48cf98e82c0f 148756 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a7377 0xc0044a7378}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2lzx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2lzx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.456: INFO: Pod "webserver-deployment-845c8977d9-qz5nr" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-qz5nr webserver-deployment-845c8977d9- deployment-2141  27406dc2-63fe-4c8c-98e3-1bca30b44bb0 148839 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:d8d18c57c573b4975276becdd468b16183dd1b4ac80fad3f3b7148703e7667e7 cni.projectcalico.org/podIP:10.233.78.238/32 cni.projectcalico.org/podIPs:10.233.78.238/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a7560 0xc0044a7561}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 22:00:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-58v5r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-58v5r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:,StartTime:2023-01-18 22:00:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.456: INFO: Pod "webserver-deployment-845c8977d9-s8p6w" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-s8p6w webserver-deployment-845c8977d9- deployment-2141  9e176ef8-cb9e-46fb-b587-bf14efd62c65 148849 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:a598ef09033d1317531e707d6bc29a81ae486f68c5a2dde6965338b4e4d19d70 cni.projectcalico.org/podIP:10.233.68.183/32 cni.projectcalico.org/podIPs:10.233.68.183/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a7817 0xc0044a7818}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:00:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7zfvb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7zfvb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.457: INFO: Pod "webserver-deployment-845c8977d9-srvfg" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-srvfg webserver-deployment-845c8977d9- deployment-2141  bd924786-c723-4a63-9a36-ccb03ddd1ed3 148611 0 2023-01-18 22:00:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:d9e295386b66affc5de1132419c1e0b04a97ab72ceeb0dcfb7f261a91ade45cb cni.projectcalico.org/podIP:10.233.68.178/32 cni.projectcalico.org/podIPs:10.233.68.178/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a79f0 0xc0044a79f1}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:00:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 22:00:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nd4pw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nd4pw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.178,StartTime:2023-01-18 22:00:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:00:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://85301d81fd42f5b4c74f59f451f98a054b5e9ef4ad7f1f68022ac3d915434136,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.178,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.457: INFO: Pod "webserver-deployment-845c8977d9-t29vv" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-t29vv webserver-deployment-845c8977d9- deployment-2141  caa3963a-1fde-49c5-8b76-3587e8c1d593 148825 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:6a95e0c4dcf689a4024444de62161095f9ab72591af72ed355e8ff097ce679dc cni.projectcalico.org/podIP:10.233.68.186/32 cni.projectcalico.org/podIPs:10.233.68.186/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a7ce7 0xc0044a7ce8}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 22:00:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hmnfp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hmnfp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:,StartTime:2023-01-18 22:00:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.457: INFO: Pod "webserver-deployment-845c8977d9-vkf2t" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-vkf2t webserver-deployment-845c8977d9- deployment-2141  b0549805-b7ec-4bf7-ac23-cd9620e32c3b 148829 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e6c9c13f4baed4d445fc7d77392eddd4d39f939dfc2b1a0dcb20c97f5e61f617 cni.projectcalico.org/podIP:10.233.78.237/32 cni.projectcalico.org/podIPs:10.233.78.237/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a7f87 0xc0044a7f88}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 22:00:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-96sgc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-96sgc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:,StartTime:2023-01-18 22:00:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.457: INFO: Pod "webserver-deployment-845c8977d9-vp5s8" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-vp5s8 webserver-deployment-845c8977d9- deployment-2141  7be1d197-72e1-4896-b714-6c6112094879 148850 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044ce1c7 0xc0044ce1c8}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:00:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dclcw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dclcw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:,StartTime:2023-01-18 22:00:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.458: INFO: Pod "webserver-deployment-845c8977d9-vsr75" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-vsr75 webserver-deployment-845c8977d9- deployment-2141  206f783a-da74-4522-a3e4-6dc2dd0c7893 148738 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044ce3f7 0xc0044ce3f8}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-74nhf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-74nhf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.458: INFO: Pod "webserver-deployment-845c8977d9-vzs8f" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-vzs8f webserver-deployment-845c8977d9- deployment-2141  8bea1958-8c86-43b3-98e1-d65a29edfa1c 148817 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:12762eabb85658fdbb280513dd424375530c6aaa5924fa7e17a51c2690ade0de cni.projectcalico.org/podIP:10.233.68.184/32 cni.projectcalico.org/podIPs:10.233.68.184/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044ce610 0xc0044ce611}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 22:00:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f8qvm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f8qvm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:,StartTime:2023-01-18 22:00:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.459: INFO: Pod "webserver-deployment-845c8977d9-whtrt" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-whtrt webserver-deployment-845c8977d9- deployment-2141  eece1da7-bfa7-45da-b541-28e65bbd1c80 148816 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:dd4df0a00251ea30dd967d17483eb1deb6531b6f89202d1205046cd3355616e9 cni.projectcalico.org/podIP:10.233.78.236/32 cni.projectcalico.org/podIPs:10.233.78.236/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044ce897 0xc0044ce898}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 22:00:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-48m2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-48m2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:,StartTime:2023-01-18 22:00:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.459: INFO: Pod "webserver-deployment-845c8977d9-xcwjm" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-xcwjm webserver-deployment-845c8977d9- deployment-2141  2772732d-6c33-4eb3-a599-08823195aa7f 148588 0 2023-01-18 22:00:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:10fc1d516f17a7054669814f71e64845982a3a9944106b654808d70764025e55 cni.projectcalico.org/podIP:10.233.68.177/32 cni.projectcalico.org/podIPs:10.233.68.177/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044ceab7 0xc0044ceab8}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:00:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 22:00:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sg8k4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sg8k4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.177,StartTime:2023-01-18 22:00:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:00:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5814a9bc3dddb34c0f54f0b506d4c5f023d5b2d3392ec71503c029ccab012f78,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:00:33.460: INFO: Pod "webserver-deployment-845c8977d9-xl267" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-xl267 webserver-deployment-845c8977d9- deployment-2141  834a4ea1-5708-4488-b3b4-c9fa013eee93 148571 0 2023-01-18 22:00:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:d53b8fcbf5e1923cda92d89a67b0ae1a6c4eae24915d9157c0da01592d56e70b cni.projectcalico.org/podIP:10.233.78.230/32 cni.projectcalico.org/podIPs:10.233.78.230/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044cece7 0xc0044cece8}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:00:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 22:00:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fq4zl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fq4zl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:10.233.78.230,StartTime:2023-01-18 22:00:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:00:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0304a2b5cf26f4b1c177fcff1642c8c6d6f94ddd8994fe3b45ef529fd87cac4e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 18 22:00:33.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2141" for this suite. 01/18/23 22:00:33.474
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":159,"skipped":2999,"failed":0}
------------------------------
• [SLOW TEST] [8.809 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:00:24.682
    Jan 18 22:00:24.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename deployment 01/18/23 22:00:24.684
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:00:24.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:00:24.721
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jan 18 22:00:24.753: INFO: Creating deployment "webserver-deployment"
    Jan 18 22:00:24.764: INFO: Waiting for observed generation 1
    Jan 18 22:00:26.797: INFO: Waiting for all required pods to come up
    Jan 18 22:00:26.815: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 01/18/23 22:00:26.815
    Jan 18 22:00:26.815: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-xl267" in namespace "deployment-2141" to be "running"
    Jan 18 22:00:26.816: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-47p64" in namespace "deployment-2141" to be "running"
    Jan 18 22:00:26.816: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-54fsp" in namespace "deployment-2141" to be "running"
    Jan 18 22:00:26.816: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-bv5wl" in namespace "deployment-2141" to be "running"
    Jan 18 22:00:26.816: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-f5cgz" in namespace "deployment-2141" to be "running"
    Jan 18 22:00:26.816: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-mp2cf" in namespace "deployment-2141" to be "running"
    Jan 18 22:00:26.817: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-prsgm" in namespace "deployment-2141" to be "running"
    Jan 18 22:00:26.817: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-qlrf2" in namespace "deployment-2141" to be "running"
    Jan 18 22:00:26.817: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-srvfg" in namespace "deployment-2141" to be "running"
    Jan 18 22:00:26.817: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-xcwjm" in namespace "deployment-2141" to be "running"
    Jan 18 22:00:26.839: INFO: Pod "webserver-deployment-845c8977d9-mp2cf": Phase="Pending", Reason="", readiness=false. Elapsed: 22.159036ms
    Jan 18 22:00:26.839: INFO: Pod "webserver-deployment-845c8977d9-47p64": Phase="Pending", Reason="", readiness=false. Elapsed: 23.024137ms
    Jan 18 22:00:26.839: INFO: Pod "webserver-deployment-845c8977d9-xl267": Phase="Pending", Reason="", readiness=false. Elapsed: 23.766395ms
    Jan 18 22:00:26.839: INFO: Pod "webserver-deployment-845c8977d9-qlrf2": Phase="Pending", Reason="", readiness=false. Elapsed: 22.586259ms
    Jan 18 22:00:26.839: INFO: Pod "webserver-deployment-845c8977d9-54fsp": Phase="Pending", Reason="", readiness=false. Elapsed: 23.447116ms
    Jan 18 22:00:26.839: INFO: Pod "webserver-deployment-845c8977d9-prsgm": Phase="Pending", Reason="", readiness=false. Elapsed: 22.825797ms
    Jan 18 22:00:26.840: INFO: Pod "webserver-deployment-845c8977d9-f5cgz": Phase="Pending", Reason="", readiness=false. Elapsed: 23.304173ms
    Jan 18 22:00:26.840: INFO: Pod "webserver-deployment-845c8977d9-srvfg": Phase="Pending", Reason="", readiness=false. Elapsed: 22.855327ms
    Jan 18 22:00:26.840: INFO: Pod "webserver-deployment-845c8977d9-xcwjm": Phase="Pending", Reason="", readiness=false. Elapsed: 22.89018ms
    Jan 18 22:00:26.839: INFO: Pod "webserver-deployment-845c8977d9-bv5wl": Phase="Pending", Reason="", readiness=false. Elapsed: 22.592499ms
    Jan 18 22:00:28.851: INFO: Pod "webserver-deployment-845c8977d9-prsgm": Phase="Running", Reason="", readiness=true. Elapsed: 2.034581168s
    Jan 18 22:00:28.851: INFO: Pod "webserver-deployment-845c8977d9-prsgm" satisfied condition "running"
    Jan 18 22:00:28.854: INFO: Pod "webserver-deployment-845c8977d9-mp2cf": Phase="Running", Reason="", readiness=true. Elapsed: 2.037090968s
    Jan 18 22:00:28.854: INFO: Pod "webserver-deployment-845c8977d9-mp2cf" satisfied condition "running"
    Jan 18 22:00:28.854: INFO: Pod "webserver-deployment-845c8977d9-47p64": Phase="Running", Reason="", readiness=true. Elapsed: 2.0379658s
    Jan 18 22:00:28.854: INFO: Pod "webserver-deployment-845c8977d9-47p64" satisfied condition "running"
    Jan 18 22:00:28.861: INFO: Pod "webserver-deployment-845c8977d9-bv5wl": Phase="Running", Reason="", readiness=true. Elapsed: 2.04511501s
    Jan 18 22:00:28.861: INFO: Pod "webserver-deployment-845c8977d9-bv5wl" satisfied condition "running"
    Jan 18 22:00:28.862: INFO: Pod "webserver-deployment-845c8977d9-xcwjm": Phase="Running", Reason="", readiness=true. Elapsed: 2.044585253s
    Jan 18 22:00:28.862: INFO: Pod "webserver-deployment-845c8977d9-xcwjm" satisfied condition "running"
    Jan 18 22:00:28.862: INFO: Pod "webserver-deployment-845c8977d9-srvfg": Phase="Running", Reason="", readiness=true. Elapsed: 2.044898448s
    Jan 18 22:00:28.862: INFO: Pod "webserver-deployment-845c8977d9-srvfg" satisfied condition "running"
    Jan 18 22:00:28.862: INFO: Pod "webserver-deployment-845c8977d9-qlrf2": Phase="Running", Reason="", readiness=true. Elapsed: 2.045168013s
    Jan 18 22:00:28.862: INFO: Pod "webserver-deployment-845c8977d9-qlrf2" satisfied condition "running"
    Jan 18 22:00:28.862: INFO: Pod "webserver-deployment-845c8977d9-54fsp": Phase="Running", Reason="", readiness=true. Elapsed: 2.046070843s
    Jan 18 22:00:28.862: INFO: Pod "webserver-deployment-845c8977d9-54fsp" satisfied condition "running"
    Jan 18 22:00:28.862: INFO: Pod "webserver-deployment-845c8977d9-f5cgz": Phase="Running", Reason="", readiness=true. Elapsed: 2.045882125s
    Jan 18 22:00:28.862: INFO: Pod "webserver-deployment-845c8977d9-f5cgz" satisfied condition "running"
    Jan 18 22:00:28.862: INFO: Pod "webserver-deployment-845c8977d9-xl267": Phase="Running", Reason="", readiness=true. Elapsed: 2.04693817s
    Jan 18 22:00:28.862: INFO: Pod "webserver-deployment-845c8977d9-xl267" satisfied condition "running"
    Jan 18 22:00:28.863: INFO: Waiting for deployment "webserver-deployment" to complete
    Jan 18 22:00:28.879: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jan 18 22:00:28.896: INFO: Updating deployment webserver-deployment
    Jan 18 22:00:28.896: INFO: Waiting for observed generation 2
    Jan 18 22:00:30.916: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jan 18 22:00:30.920: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jan 18 22:00:30.923: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 18 22:00:30.946: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jan 18 22:00:30.946: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jan 18 22:00:30.950: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 18 22:00:30.960: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jan 18 22:00:30.960: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jan 18 22:00:30.980: INFO: Updating deployment webserver-deployment
    Jan 18 22:00:30.980: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jan 18 22:00:31.188: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jan 18 22:00:33.392: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 22:00:33.427: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-2141  1c7eb0c9-fc05-417e-b7f2-a5555bbb4124 148791 3 2023-01-18 22:00:24 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 22:00:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044175b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-18 22:00:31 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-01-18 22:00:31 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jan 18 22:00:33.436: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-2141  f5c6fca7-22ae-4bd9-abce-33929f0b43d7 148777 3 2023-01-18 22:00:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 1c7eb0c9-fc05-417e-b7f2-a5555bbb4124 0xc004417b57 0xc004417b58}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c7eb0c9-fc05-417e-b7f2-a5555bbb4124\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004417c28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 22:00:33.436: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jan 18 22:00:33.436: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-2141  e0acab07-df5b-4821-b8e4-ffda450c40b5 148789 3 2023-01-18 22:00:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 1c7eb0c9-fc05-417e-b7f2-a5555bbb4124 0xc004417cc7 0xc004417cc8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:00:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c7eb0c9-fc05-417e-b7f2-a5555bbb4124\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004417da8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 22:00:33.449: INFO: Pod "webserver-deployment-69b7448995-49nsm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-49nsm webserver-deployment-69b7448995- deployment-2141  443b69fa-a746-4754-b34a-458e118dd06b 148741 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f5c6fca7-22ae-4bd9-abce-33929f0b43d7 0xc00443a207 0xc00443a208}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c6fca7-22ae-4bd9-abce-33929f0b43d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s4zpc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s4zpc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.449: INFO: Pod "webserver-deployment-69b7448995-4dgw7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-4dgw7 webserver-deployment-69b7448995- deployment-2141  f430ad70-5cd9-4352-ab23-d3610f96777d 148673 0 2023-01-18 22:00:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:8e89b853d76f36292507bbe33ea9d9afa3fba9d786f5afba6d08d4a75bed4b92 cni.projectcalico.org/podIP:10.233.78.234/32 cni.projectcalico.org/podIPs:10.233.78.234/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f5c6fca7-22ae-4bd9-abce-33929f0b43d7 0xc00443a480 0xc00443a481}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c6fca7-22ae-4bd9-abce-33929f0b43d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:00:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 22:00:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-txzkz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-txzkz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:,StartTime:2023-01-18 22:00:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.450: INFO: Pod "webserver-deployment-69b7448995-59xtf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-59xtf webserver-deployment-69b7448995- deployment-2141  73e5786c-b3f1-4e6d-aee8-596c44d8877b 148775 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f5c6fca7-22ae-4bd9-abce-33929f0b43d7 0xc00443a767 0xc00443a768}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c6fca7-22ae-4bd9-abce-33929f0b43d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fkbnx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fkbnx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.450: INFO: Pod "webserver-deployment-69b7448995-6pz2h" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-6pz2h webserver-deployment-69b7448995- deployment-2141  8ffa0771-33ec-4028-8310-fd11e5f2f44e 148685 0 2023-01-18 22:00:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:aaf7d6cb0f367a591263e4077f346f575317cf8b1984acb9fa17207f8d80f8d4 cni.projectcalico.org/podIP:10.233.78.235/32 cni.projectcalico.org/podIPs:10.233.78.235/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f5c6fca7-22ae-4bd9-abce-33929f0b43d7 0xc00443a9a0 0xc00443a9a1}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c6fca7-22ae-4bd9-abce-33929f0b43d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:00:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 22:00:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tllw9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tllw9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:,StartTime:2023-01-18 22:00:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.450: INFO: Pod "webserver-deployment-69b7448995-6wdrc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-6wdrc webserver-deployment-69b7448995- deployment-2141  d4120a42-6c1e-4af7-8425-16e3e934b298 148776 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f5c6fca7-22ae-4bd9-abce-33929f0b43d7 0xc00443abe7 0xc00443abe8}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c6fca7-22ae-4bd9-abce-33929f0b43d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cdd9f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cdd9f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:,StartTime:2023-01-18 22:00:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.451: INFO: Pod "webserver-deployment-69b7448995-8zt24" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-8zt24 webserver-deployment-69b7448995- deployment-2141  0407f9c0-f0b9-45a6-928f-1dfc9bab2048 148767 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f5c6fca7-22ae-4bd9-abce-33929f0b43d7 0xc00443ae57 0xc00443ae58}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c6fca7-22ae-4bd9-abce-33929f0b43d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vc62b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vc62b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.451: INFO: Pod "webserver-deployment-69b7448995-9dw5z" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-9dw5z webserver-deployment-69b7448995- deployment-2141  17e76b1b-0f66-4089-bd0f-bc73c3cd154d 148755 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f5c6fca7-22ae-4bd9-abce-33929f0b43d7 0xc00443aff0 0xc00443aff1}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c6fca7-22ae-4bd9-abce-33929f0b43d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fgnnk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fgnnk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.451: INFO: Pod "webserver-deployment-69b7448995-cmsjm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-cmsjm webserver-deployment-69b7448995- deployment-2141  33ba1e48-a880-44b8-9b2e-6d499341e265 148675 0 2023-01-18 22:00:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:f4a71d26c8b5dc38d84b233f74142f71a152efffe72ff82b06c5db0bed9dd426 cni.projectcalico.org/podIP:10.233.68.179/32 cni.projectcalico.org/podIPs:10.233.68.179/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f5c6fca7-22ae-4bd9-abce-33929f0b43d7 0xc00443b2a0 0xc00443b2a1}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c6fca7-22ae-4bd9-abce-33929f0b43d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:00:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 22:00:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kblpc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kblpc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:,StartTime:2023-01-18 22:00:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.451: INFO: Pod "webserver-deployment-69b7448995-htmw5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-htmw5 webserver-deployment-69b7448995- deployment-2141  d5e30a07-5029-44bc-87b7-7d367fcdf000 148764 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f5c6fca7-22ae-4bd9-abce-33929f0b43d7 0xc00443b547 0xc00443b548}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c6fca7-22ae-4bd9-abce-33929f0b43d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pl7lf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pl7lf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.451: INFO: Pod "webserver-deployment-69b7448995-kqppk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-kqppk webserver-deployment-69b7448995- deployment-2141  5b87c324-8f9e-49f6-9486-4cf0ab614bf1 148766 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f5c6fca7-22ae-4bd9-abce-33929f0b43d7 0xc00443b720 0xc00443b721}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c6fca7-22ae-4bd9-abce-33929f0b43d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l9867,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l9867,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.452: INFO: Pod "webserver-deployment-69b7448995-lv9gv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-lv9gv webserver-deployment-69b7448995- deployment-2141  8658b974-8ee4-4728-93d0-ffcb3b09bc40 148686 0 2023-01-18 22:00:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:79a054e07fd664dfbd87446a1b50550f83ab288019292345eecedd678f66508e cni.projectcalico.org/podIP:10.233.68.185/32 cni.projectcalico.org/podIPs:10.233.68.185/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f5c6fca7-22ae-4bd9-abce-33929f0b43d7 0xc00443b930 0xc00443b931}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c6fca7-22ae-4bd9-abce-33929f0b43d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:00:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 22:00:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g6ljp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g6ljp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:,StartTime:2023-01-18 22:00:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.452: INFO: Pod "webserver-deployment-69b7448995-r2szl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-r2szl webserver-deployment-69b7448995- deployment-2141  be4361d9-3593-4655-8620-beabd63b553e 148836 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:4450fa28354c10dfb4d0d3bf8bd5b954cd4a6f5181209f1b2672acb7b2bd1fdf cni.projectcalico.org/podIP:10.233.68.187/32 cni.projectcalico.org/podIPs:10.233.68.187/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f5c6fca7-22ae-4bd9-abce-33929f0b43d7 0xc00443bbf7 0xc00443bbf8}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c6fca7-22ae-4bd9-abce-33929f0b43d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 22:00:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6l4xq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6l4xq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:,StartTime:2023-01-18 22:00:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.452: INFO: Pod "webserver-deployment-69b7448995-xwkpk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-xwkpk webserver-deployment-69b7448995- deployment-2141  77875a61-ab4a-4fb2-a852-4a2a7f925059 148695 0 2023-01-18 22:00:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:246d837a906188ad56478f7bc3fb4f6bc9d7f7d486054742e20e8317641c775d cni.projectcalico.org/podIP:10.233.68.182/32 cni.projectcalico.org/podIPs:10.233.68.182/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f5c6fca7-22ae-4bd9-abce-33929f0b43d7 0xc00443be37 0xc00443be38}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c6fca7-22ae-4bd9-abce-33929f0b43d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:00:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 22:00:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-92rcz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-92rcz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:,StartTime:2023-01-18 22:00:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.452: INFO: Pod "webserver-deployment-845c8977d9-47p64" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-47p64 webserver-deployment-845c8977d9- deployment-2141  1fcae03e-9627-44e9-b5d4-14ecbb2f906d 148583 0 2023-01-18 22:00:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:133f8f7c25580f2060437ca8f87954ad22cbdb41de51b3c93021164c72d64a4e cni.projectcalico.org/podIP:10.233.68.180/32 cni.projectcalico.org/podIPs:10.233.68.180/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a6067 0xc0044a6068}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:00:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 22:00:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.180\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6njgb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6njgb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.180,StartTime:2023-01-18 22:00:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:00:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://e8e1390a5fd354829dc53070dfbf993a17c2a83d74ec41302bd676f1d8ed657e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.180,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.454: INFO: Pod "webserver-deployment-845c8977d9-9txbn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-9txbn webserver-deployment-845c8977d9- deployment-2141  36a18906-d6ba-4003-82c5-cfff83df3f7a 148765 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a62f7 0xc0044a62f8}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lwqws,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lwqws,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.454: INFO: Pod "webserver-deployment-845c8977d9-bv5wl" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-bv5wl webserver-deployment-845c8977d9- deployment-2141  d4efd5b3-08ea-4826-90e1-b96f6b504d1c 148579 0 2023-01-18 22:00:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:7294d02c5eec56a3035d55ff9022cb8910c218992dde21cc65d1fae9ad388bba cni.projectcalico.org/podIP:10.233.68.174/32 cni.projectcalico.org/podIPs:10.233.68.174/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a64a0 0xc0044a64a1}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:00:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 22:00:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cmbsc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cmbsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.174,StartTime:2023-01-18 22:00:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:00:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://10c2f7fca72c44a12d6562223a828527d41bcc9b1a3b560a59e12d6acb5762cc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.174,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.454: INFO: Pod "webserver-deployment-845c8977d9-dfm28" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-dfm28 webserver-deployment-845c8977d9- deployment-2141  961d58e1-bf91-4ed9-a830-b7bacd052ede 148763 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a6727 0xc0044a6728}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cs66v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cs66v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.455: INFO: Pod "webserver-deployment-845c8977d9-f5cgz" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-f5cgz webserver-deployment-845c8977d9- deployment-2141  079ce550-81e7-41ab-84aa-da7fe73d3a98 148561 0 2023-01-18 22:00:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8eb7fb668f69006aba8d736fef30254ea80da8d59a3ccec1dcbf5b4acc89151d cni.projectcalico.org/podIP:10.233.78.229/32 cni.projectcalico.org/podIPs:10.233.78.229/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a6920 0xc0044a6921}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:00:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 22:00:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.229\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g7wp4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g7wp4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:10.233.78.229,StartTime:2023-01-18 22:00:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:00:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://83110d94bb8bfd5f8989d6cac1d1567494f5bae4e76c200b731fd2bf30d47a39,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.229,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.455: INFO: Pod "webserver-deployment-845c8977d9-lh8pb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-lh8pb webserver-deployment-845c8977d9- deployment-2141  71b9d3e2-aa43-4370-93bd-b3849c272981 148768 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a6ba7 0xc0044a6ba8}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n56bd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n56bd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.455: INFO: Pod "webserver-deployment-845c8977d9-prsgm" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-prsgm webserver-deployment-845c8977d9- deployment-2141  e8d96499-421f-4aa2-a6fd-51df5ded2738 148605 0 2023-01-18 22:00:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:593962979809383fe664688d59c92693227af22d167a97e20a164f445f9d0f6c cni.projectcalico.org/podIP:10.233.78.233/32 cni.projectcalico.org/podIPs:10.233.78.233/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a6de0 0xc0044a6de1}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:00:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 22:00:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.233\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zj59m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zj59m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:10.233.78.233,StartTime:2023-01-18 22:00:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:00:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://e8fa3dfc5c86691b0209cc5040ca1d2f2e814c4d29de70440958072af548c28b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.233,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.456: INFO: Pod "webserver-deployment-845c8977d9-qlrf2" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-qlrf2 webserver-deployment-845c8977d9- deployment-2141  aeafa170-2caf-4cae-adda-6a439533d097 148609 0 2023-01-18 22:00:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:6233ea2de493c2d39286c7f0f94a88be36e82d6cba5d0b21f5422a9bac3e39c1 cni.projectcalico.org/podIP:10.233.68.181/32 cni.projectcalico.org/podIPs:10.233.68.181/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a70e7 0xc0044a70e8}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:00:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 22:00:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s5tcw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s5tcw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.181,StartTime:2023-01-18 22:00:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:00:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://6c0e6a96151ddf8478d0a76fbe2e5906caca4249a6357cd3d48a5949c1c905e2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.456: INFO: Pod "webserver-deployment-845c8977d9-qpg4h" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-qpg4h webserver-deployment-845c8977d9- deployment-2141  6bea42d4-0e9d-494f-8845-48cf98e82c0f 148756 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a7377 0xc0044a7378}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2lzx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2lzx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.456: INFO: Pod "webserver-deployment-845c8977d9-qz5nr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-qz5nr webserver-deployment-845c8977d9- deployment-2141  27406dc2-63fe-4c8c-98e3-1bca30b44bb0 148839 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:d8d18c57c573b4975276becdd468b16183dd1b4ac80fad3f3b7148703e7667e7 cni.projectcalico.org/podIP:10.233.78.238/32 cni.projectcalico.org/podIPs:10.233.78.238/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a7560 0xc0044a7561}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 22:00:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-58v5r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-58v5r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:,StartTime:2023-01-18 22:00:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.456: INFO: Pod "webserver-deployment-845c8977d9-s8p6w" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-s8p6w webserver-deployment-845c8977d9- deployment-2141  9e176ef8-cb9e-46fb-b587-bf14efd62c65 148849 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:a598ef09033d1317531e707d6bc29a81ae486f68c5a2dde6965338b4e4d19d70 cni.projectcalico.org/podIP:10.233.68.183/32 cni.projectcalico.org/podIPs:10.233.68.183/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a7817 0xc0044a7818}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:00:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7zfvb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7zfvb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.457: INFO: Pod "webserver-deployment-845c8977d9-srvfg" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-srvfg webserver-deployment-845c8977d9- deployment-2141  bd924786-c723-4a63-9a36-ccb03ddd1ed3 148611 0 2023-01-18 22:00:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:d9e295386b66affc5de1132419c1e0b04a97ab72ceeb0dcfb7f261a91ade45cb cni.projectcalico.org/podIP:10.233.68.178/32 cni.projectcalico.org/podIPs:10.233.68.178/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a79f0 0xc0044a79f1}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:00:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 22:00:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nd4pw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nd4pw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.178,StartTime:2023-01-18 22:00:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:00:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://85301d81fd42f5b4c74f59f451f98a054b5e9ef4ad7f1f68022ac3d915434136,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.178,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.457: INFO: Pod "webserver-deployment-845c8977d9-t29vv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-t29vv webserver-deployment-845c8977d9- deployment-2141  caa3963a-1fde-49c5-8b76-3587e8c1d593 148825 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:6a95e0c4dcf689a4024444de62161095f9ab72591af72ed355e8ff097ce679dc cni.projectcalico.org/podIP:10.233.68.186/32 cni.projectcalico.org/podIPs:10.233.68.186/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a7ce7 0xc0044a7ce8}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 22:00:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hmnfp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hmnfp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:,StartTime:2023-01-18 22:00:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.457: INFO: Pod "webserver-deployment-845c8977d9-vkf2t" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-vkf2t webserver-deployment-845c8977d9- deployment-2141  b0549805-b7ec-4bf7-ac23-cd9620e32c3b 148829 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e6c9c13f4baed4d445fc7d77392eddd4d39f939dfc2b1a0dcb20c97f5e61f617 cni.projectcalico.org/podIP:10.233.78.237/32 cni.projectcalico.org/podIPs:10.233.78.237/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044a7f87 0xc0044a7f88}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 22:00:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-96sgc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-96sgc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:,StartTime:2023-01-18 22:00:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.457: INFO: Pod "webserver-deployment-845c8977d9-vp5s8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-vp5s8 webserver-deployment-845c8977d9- deployment-2141  7be1d197-72e1-4896-b714-6c6112094879 148850 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044ce1c7 0xc0044ce1c8}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:00:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dclcw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dclcw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:,StartTime:2023-01-18 22:00:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.458: INFO: Pod "webserver-deployment-845c8977d9-vsr75" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-vsr75 webserver-deployment-845c8977d9- deployment-2141  206f783a-da74-4522-a3e4-6dc2dd0c7893 148738 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044ce3f7 0xc0044ce3f8}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-74nhf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-74nhf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.458: INFO: Pod "webserver-deployment-845c8977d9-vzs8f" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-vzs8f webserver-deployment-845c8977d9- deployment-2141  8bea1958-8c86-43b3-98e1-d65a29edfa1c 148817 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:12762eabb85658fdbb280513dd424375530c6aaa5924fa7e17a51c2690ade0de cni.projectcalico.org/podIP:10.233.68.184/32 cni.projectcalico.org/podIPs:10.233.68.184/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044ce610 0xc0044ce611}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 22:00:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f8qvm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f8qvm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:,StartTime:2023-01-18 22:00:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.459: INFO: Pod "webserver-deployment-845c8977d9-whtrt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-whtrt webserver-deployment-845c8977d9- deployment-2141  eece1da7-bfa7-45da-b541-28e65bbd1c80 148816 0 2023-01-18 22:00:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:dd4df0a00251ea30dd967d17483eb1deb6531b6f89202d1205046cd3355616e9 cni.projectcalico.org/podIP:10.233.78.236/32 cni.projectcalico.org/podIPs:10.233.78.236/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044ce897 0xc0044ce898}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:00:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 22:00:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-48m2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-48m2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:,StartTime:2023-01-18 22:00:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.459: INFO: Pod "webserver-deployment-845c8977d9-xcwjm" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-xcwjm webserver-deployment-845c8977d9- deployment-2141  2772732d-6c33-4eb3-a599-08823195aa7f 148588 0 2023-01-18 22:00:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:10fc1d516f17a7054669814f71e64845982a3a9944106b654808d70764025e55 cni.projectcalico.org/podIP:10.233.68.177/32 cni.projectcalico.org/podIPs:10.233.68.177/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044ceab7 0xc0044ceab8}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:00:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 22:00:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sg8k4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sg8k4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.177,StartTime:2023-01-18 22:00:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:00:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5814a9bc3dddb34c0f54f0b506d4c5f023d5b2d3392ec71503c029ccab012f78,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:00:33.460: INFO: Pod "webserver-deployment-845c8977d9-xl267" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-xl267 webserver-deployment-845c8977d9- deployment-2141  834a4ea1-5708-4488-b3b4-c9fa013eee93 148571 0 2023-01-18 22:00:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:d53b8fcbf5e1923cda92d89a67b0ae1a6c4eae24915d9157c0da01592d56e70b cni.projectcalico.org/podIP:10.233.78.230/32 cni.projectcalico.org/podIPs:10.233.78.230/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 e0acab07-df5b-4821-b8e4-ffda450c40b5 0xc0044cece7 0xc0044cece8}] [] [{kube-controller-manager Update v1 2023-01-18 22:00:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0acab07-df5b-4821-b8e4-ffda450c40b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:00:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 22:00:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fq4zl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fq4zl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:00:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:10.233.78.230,StartTime:2023-01-18 22:00:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:00:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0304a2b5cf26f4b1c177fcff1642c8c6d6f94ddd8994fe3b45ef529fd87cac4e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 18 22:00:33.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2141" for this suite. 01/18/23 22:00:33.474
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:00:33.508
Jan 18 22:00:33.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename namespaces 01/18/23 22:00:33.51
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:00:33.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:00:33.604
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 01/18/23 22:00:33.619
Jan 18 22:00:33.646: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 01/18/23 22:00:33.646
Jan 18 22:00:33.656: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 01/18/23 22:00:33.656
Jan 18 22:00:33.669: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 18 22:00:33.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9529" for this suite. 01/18/23 22:00:33.685
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":160,"skipped":3010,"failed":0}
------------------------------
• [0.186 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:00:33.508
    Jan 18 22:00:33.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename namespaces 01/18/23 22:00:33.51
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:00:33.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:00:33.604
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 01/18/23 22:00:33.619
    Jan 18 22:00:33.646: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 01/18/23 22:00:33.646
    Jan 18 22:00:33.656: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 01/18/23 22:00:33.656
    Jan 18 22:00:33.669: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 22:00:33.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-9529" for this suite. 01/18/23 22:00:33.685
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:00:33.694
Jan 18 22:00:33.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename pods 01/18/23 22:00:33.706
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:00:33.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:00:33.766
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 01/18/23 22:00:33.771
Jan 18 22:00:33.794: INFO: created test-pod-1
Jan 18 22:00:33.808: INFO: created test-pod-2
Jan 18 22:00:33.896: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 01/18/23 22:00:33.896
Jan 18 22:00:33.896: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-2708' to be running and ready
Jan 18 22:00:33.945: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 22:00:33.945: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 22:00:33.945: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 22:00:33.945: INFO: 0 / 3 pods in namespace 'pods-2708' are running and ready (0 seconds elapsed)
Jan 18 22:00:33.945: INFO: expected 0 pod replicas in namespace 'pods-2708', 0 are Running and Ready.
Jan 18 22:00:33.945: INFO: POD         NODE              PHASE    GRACE  CONDITIONS
Jan 18 22:00:33.945: INFO: test-pod-1  v1-25-1-18760-w2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC  }]
Jan 18 22:00:33.945: INFO: test-pod-2  v1-25-1-18760-w2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC  }]
Jan 18 22:00:33.945: INFO: test-pod-3  v1-25-1-18760-w2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC  }]
Jan 18 22:00:33.945: INFO: 
Jan 18 22:00:35.964: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 22:00:35.964: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 22:00:35.964: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 22:00:35.964: INFO: 0 / 3 pods in namespace 'pods-2708' are running and ready (2 seconds elapsed)
Jan 18 22:00:35.964: INFO: expected 0 pod replicas in namespace 'pods-2708', 0 are Running and Ready.
Jan 18 22:00:35.964: INFO: POD         NODE              PHASE    GRACE  CONDITIONS
Jan 18 22:00:35.964: INFO: test-pod-1  v1-25-1-18760-w2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC  }]
Jan 18 22:00:35.964: INFO: test-pod-2  v1-25-1-18760-w2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC  }]
Jan 18 22:00:35.964: INFO: test-pod-3  v1-25-1-18760-w2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC  }]
Jan 18 22:00:35.964: INFO: 
Jan 18 22:00:37.957: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 22:00:37.957: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 22:00:37.957: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 22:00:37.957: INFO: 0 / 3 pods in namespace 'pods-2708' are running and ready (4 seconds elapsed)
Jan 18 22:00:37.957: INFO: expected 0 pod replicas in namespace 'pods-2708', 0 are Running and Ready.
Jan 18 22:00:37.957: INFO: POD         NODE              PHASE    GRACE  CONDITIONS
Jan 18 22:00:37.957: INFO: test-pod-1  v1-25-1-18760-w2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC  }]
Jan 18 22:00:37.957: INFO: test-pod-2  v1-25-1-18760-w2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC  }]
Jan 18 22:00:37.957: INFO: test-pod-3  v1-25-1-18760-w2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC  }]
Jan 18 22:00:37.957: INFO: 
Jan 18 22:00:39.983: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 22:00:39.983: INFO: 2 / 3 pods in namespace 'pods-2708' are running and ready (6 seconds elapsed)
Jan 18 22:00:39.983: INFO: expected 0 pod replicas in namespace 'pods-2708', 0 are Running and Ready.
Jan 18 22:00:39.983: INFO: POD         NODE              PHASE    GRACE  CONDITIONS
Jan 18 22:00:39.983: INFO: test-pod-2  v1-25-1-18760-w2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC  }]
Jan 18 22:00:39.983: INFO: 
Jan 18 22:00:41.987: INFO: 3 / 3 pods in namespace 'pods-2708' are running and ready (8 seconds elapsed)
Jan 18 22:00:41.999: INFO: expected 0 pod replicas in namespace 'pods-2708', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 01/18/23 22:00:42.19
Jan 18 22:00:42.450: INFO: Pod quantity 3 is different from expected quantity 0
Jan 18 22:00:43.486: INFO: Pod quantity 3 is different from expected quantity 0
Jan 18 22:00:44.456: INFO: Pod quantity 3 is different from expected quantity 0
Jan 18 22:00:45.530: INFO: Pod quantity 3 is different from expected quantity 0
Jan 18 22:00:46.456: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 22:00:47.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2708" for this suite. 01/18/23 22:00:47.465
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":161,"skipped":3010,"failed":0}
------------------------------
• [SLOW TEST] [13.782 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:00:33.694
    Jan 18 22:00:33.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename pods 01/18/23 22:00:33.706
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:00:33.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:00:33.766
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 01/18/23 22:00:33.771
    Jan 18 22:00:33.794: INFO: created test-pod-1
    Jan 18 22:00:33.808: INFO: created test-pod-2
    Jan 18 22:00:33.896: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 01/18/23 22:00:33.896
    Jan 18 22:00:33.896: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-2708' to be running and ready
    Jan 18 22:00:33.945: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 18 22:00:33.945: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 18 22:00:33.945: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 18 22:00:33.945: INFO: 0 / 3 pods in namespace 'pods-2708' are running and ready (0 seconds elapsed)
    Jan 18 22:00:33.945: INFO: expected 0 pod replicas in namespace 'pods-2708', 0 are Running and Ready.
    Jan 18 22:00:33.945: INFO: POD         NODE              PHASE    GRACE  CONDITIONS
    Jan 18 22:00:33.945: INFO: test-pod-1  v1-25-1-18760-w2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC  }]
    Jan 18 22:00:33.945: INFO: test-pod-2  v1-25-1-18760-w2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC  }]
    Jan 18 22:00:33.945: INFO: test-pod-3  v1-25-1-18760-w2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC  }]
    Jan 18 22:00:33.945: INFO: 
    Jan 18 22:00:35.964: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 18 22:00:35.964: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 18 22:00:35.964: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 18 22:00:35.964: INFO: 0 / 3 pods in namespace 'pods-2708' are running and ready (2 seconds elapsed)
    Jan 18 22:00:35.964: INFO: expected 0 pod replicas in namespace 'pods-2708', 0 are Running and Ready.
    Jan 18 22:00:35.964: INFO: POD         NODE              PHASE    GRACE  CONDITIONS
    Jan 18 22:00:35.964: INFO: test-pod-1  v1-25-1-18760-w2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC  }]
    Jan 18 22:00:35.964: INFO: test-pod-2  v1-25-1-18760-w2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC  }]
    Jan 18 22:00:35.964: INFO: test-pod-3  v1-25-1-18760-w2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC  }]
    Jan 18 22:00:35.964: INFO: 
    Jan 18 22:00:37.957: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 18 22:00:37.957: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 18 22:00:37.957: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 18 22:00:37.957: INFO: 0 / 3 pods in namespace 'pods-2708' are running and ready (4 seconds elapsed)
    Jan 18 22:00:37.957: INFO: expected 0 pod replicas in namespace 'pods-2708', 0 are Running and Ready.
    Jan 18 22:00:37.957: INFO: POD         NODE              PHASE    GRACE  CONDITIONS
    Jan 18 22:00:37.957: INFO: test-pod-1  v1-25-1-18760-w2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC  }]
    Jan 18 22:00:37.957: INFO: test-pod-2  v1-25-1-18760-w2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC  }]
    Jan 18 22:00:37.957: INFO: test-pod-3  v1-25-1-18760-w2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC  }]
    Jan 18 22:00:37.957: INFO: 
    Jan 18 22:00:39.983: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 18 22:00:39.983: INFO: 2 / 3 pods in namespace 'pods-2708' are running and ready (6 seconds elapsed)
    Jan 18 22:00:39.983: INFO: expected 0 pod replicas in namespace 'pods-2708', 0 are Running and Ready.
    Jan 18 22:00:39.983: INFO: POD         NODE              PHASE    GRACE  CONDITIONS
    Jan 18 22:00:39.983: INFO: test-pod-2  v1-25-1-18760-w2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:00:33 +0000 UTC  }]
    Jan 18 22:00:39.983: INFO: 
    Jan 18 22:00:41.987: INFO: 3 / 3 pods in namespace 'pods-2708' are running and ready (8 seconds elapsed)
    Jan 18 22:00:41.999: INFO: expected 0 pod replicas in namespace 'pods-2708', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 01/18/23 22:00:42.19
    Jan 18 22:00:42.450: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 18 22:00:43.486: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 18 22:00:44.456: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 18 22:00:45.530: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 18 22:00:46.456: INFO: Pod quantity 2 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 22:00:47.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2708" for this suite. 01/18/23 22:00:47.465
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:00:47.483
Jan 18 22:00:47.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename statefulset 01/18/23 22:00:47.486
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:00:47.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:00:47.545
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6607 01/18/23 22:00:47.553
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 01/18/23 22:00:47.562
Jan 18 22:00:47.644: INFO: Found 0 stateful pods, waiting for 3
Jan 18 22:00:57.651: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 22:00:57.651: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 22:00:57.651: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 22:00:57.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-6607 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 22:00:57.948: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 22:00:57.949: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 22:00:57.949: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/18/23 22:01:07.986
Jan 18 22:01:08.012: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/18/23 22:01:08.012
STEP: Updating Pods in reverse ordinal order 01/18/23 22:01:18.036
Jan 18 22:01:18.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-6607 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 22:01:18.263: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 22:01:18.263: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 22:01:18.263: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 22:01:28.295: INFO: Waiting for StatefulSet statefulset-6607/ss2 to complete update
STEP: Rolling back to a previous revision 01/18/23 22:01:38.306
Jan 18 22:01:38.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-6607 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 22:01:38.556: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 22:01:38.556: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 22:01:38.556: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 22:01:48.601: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 01/18/23 22:01:58.657
Jan 18 22:01:58.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-6607 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 22:01:58.859: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 22:01:58.859: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 22:01:58.859: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 22:02:08.891: INFO: Deleting all statefulset in ns statefulset-6607
Jan 18 22:02:08.895: INFO: Scaling statefulset ss2 to 0
Jan 18 22:02:18.919: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 22:02:18.923: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 18 22:02:18.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6607" for this suite. 01/18/23 22:02:18.953
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":162,"skipped":3022,"failed":0}
------------------------------
• [SLOW TEST] [91.476 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:00:47.483
    Jan 18 22:00:47.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename statefulset 01/18/23 22:00:47.486
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:00:47.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:00:47.545
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-6607 01/18/23 22:00:47.553
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 01/18/23 22:00:47.562
    Jan 18 22:00:47.644: INFO: Found 0 stateful pods, waiting for 3
    Jan 18 22:00:57.651: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 22:00:57.651: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 22:00:57.651: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 22:00:57.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-6607 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 22:00:57.948: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 22:00:57.949: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 22:00:57.949: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/18/23 22:01:07.986
    Jan 18 22:01:08.012: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/18/23 22:01:08.012
    STEP: Updating Pods in reverse ordinal order 01/18/23 22:01:18.036
    Jan 18 22:01:18.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-6607 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 22:01:18.263: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 22:01:18.263: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 22:01:18.263: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 22:01:28.295: INFO: Waiting for StatefulSet statefulset-6607/ss2 to complete update
    STEP: Rolling back to a previous revision 01/18/23 22:01:38.306
    Jan 18 22:01:38.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-6607 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 22:01:38.556: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 22:01:38.556: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 22:01:38.556: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 22:01:48.601: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 01/18/23 22:01:58.657
    Jan 18 22:01:58.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-6607 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 22:01:58.859: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 22:01:58.859: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 22:01:58.859: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 18 22:02:08.891: INFO: Deleting all statefulset in ns statefulset-6607
    Jan 18 22:02:08.895: INFO: Scaling statefulset ss2 to 0
    Jan 18 22:02:18.919: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 22:02:18.923: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 18 22:02:18.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-6607" for this suite. 01/18/23 22:02:18.953
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:02:18.962
Jan 18 22:02:18.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename dns 01/18/23 22:02:18.966
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:02:18.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:02:19.008
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 01/18/23 22:02:19.036
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1785.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1785.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1785.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1785.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1785.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 232.48.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.48.232_udp@PTR;check="$$(dig +tcp +noall +answer +search 232.48.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.48.232_tcp@PTR;sleep 1; done
 01/18/23 22:02:19.07
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1785.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1785.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1785.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1785.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1785.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1785.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 232.48.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.48.232_udp@PTR;check="$$(dig +tcp +noall +answer +search 232.48.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.48.232_tcp@PTR;sleep 1; done
 01/18/23 22:02:19.071
STEP: creating a pod to probe DNS 01/18/23 22:02:19.071
STEP: submitting the pod to kubernetes 01/18/23 22:02:19.072
Jan 18 22:02:19.130: INFO: Waiting up to 15m0s for pod "dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45" in namespace "dns-1785" to be "running"
Jan 18 22:02:19.139: INFO: Pod "dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45": Phase="Pending", Reason="", readiness=false. Elapsed: 9.463159ms
Jan 18 22:02:21.152: INFO: Pod "dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45": Phase="Running", Reason="", readiness=true. Elapsed: 2.022634976s
Jan 18 22:02:21.153: INFO: Pod "dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45" satisfied condition "running"
STEP: retrieving the pod 01/18/23 22:02:21.153
STEP: looking for the results for each expected name from probers 01/18/23 22:02:21.157
Jan 18 22:02:21.165: INFO: Unable to read wheezy_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:21.170: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:21.176: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:21.186: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:21.215: INFO: Unable to read jessie_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:21.220: INFO: Unable to read jessie_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:21.224: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:21.230: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:21.251: INFO: Lookups using dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45 failed for: [wheezy_udp@dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_udp@dns-test-service.dns-1785.svc.cluster.local jessie_tcp@dns-test-service.dns-1785.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local]

Jan 18 22:02:26.262: INFO: Unable to read wheezy_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:26.279: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:26.284: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:26.300: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:26.342: INFO: Unable to read jessie_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:26.350: INFO: Unable to read jessie_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:26.367: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:26.375: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:26.394: INFO: Lookups using dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45 failed for: [wheezy_udp@dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_udp@dns-test-service.dns-1785.svc.cluster.local jessie_tcp@dns-test-service.dns-1785.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local]

Jan 18 22:02:31.261: INFO: Unable to read wheezy_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:31.266: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:31.272: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:31.278: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:31.309: INFO: Unable to read jessie_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:31.315: INFO: Unable to read jessie_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:31.319: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:31.324: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:31.345: INFO: Lookups using dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45 failed for: [wheezy_udp@dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_udp@dns-test-service.dns-1785.svc.cluster.local jessie_tcp@dns-test-service.dns-1785.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local]

Jan 18 22:02:36.256: INFO: Unable to read wheezy_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:36.265: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:36.273: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:36.279: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:36.329: INFO: Unable to read jessie_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:36.336: INFO: Unable to read jessie_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:36.341: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:36.356: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:36.388: INFO: Lookups using dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45 failed for: [wheezy_udp@dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_udp@dns-test-service.dns-1785.svc.cluster.local jessie_tcp@dns-test-service.dns-1785.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local]

Jan 18 22:02:41.261: INFO: Unable to read wheezy_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:41.269: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:41.275: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:41.280: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:41.314: INFO: Unable to read jessie_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:41.319: INFO: Unable to read jessie_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:41.324: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:41.330: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:41.350: INFO: Lookups using dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45 failed for: [wheezy_udp@dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_udp@dns-test-service.dns-1785.svc.cluster.local jessie_tcp@dns-test-service.dns-1785.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local]

Jan 18 22:02:46.287: INFO: Unable to read wheezy_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:46.300: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:46.306: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:46.339: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:46.395: INFO: Unable to read jessie_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:46.400: INFO: Unable to read jessie_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:46.405: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:46.410: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:46.465: INFO: Lookups using dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45 failed for: [wheezy_udp@dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_udp@dns-test-service.dns-1785.svc.cluster.local jessie_tcp@dns-test-service.dns-1785.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local]

Jan 18 22:02:51.260: INFO: Unable to read wheezy_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:51.266: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:51.272: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:51.283: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:51.308: INFO: Unable to read jessie_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:51.313: INFO: Unable to read jessie_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:51.319: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:51.323: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
Jan 18 22:02:51.340: INFO: Lookups using dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45 failed for: [wheezy_udp@dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_udp@dns-test-service.dns-1785.svc.cluster.local jessie_tcp@dns-test-service.dns-1785.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local]

Jan 18 22:02:56.524: INFO: DNS probes using dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45 succeeded

STEP: deleting the pod 01/18/23 22:02:56.525
STEP: deleting the test service 01/18/23 22:02:56.568
STEP: deleting the test headless service 01/18/23 22:02:56.629
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 18 22:02:56.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1785" for this suite. 01/18/23 22:02:56.662
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":163,"skipped":3028,"failed":0}
------------------------------
• [SLOW TEST] [37.709 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:02:18.962
    Jan 18 22:02:18.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename dns 01/18/23 22:02:18.966
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:02:18.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:02:19.008
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 01/18/23 22:02:19.036
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1785.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1785.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1785.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1785.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1785.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 232.48.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.48.232_udp@PTR;check="$$(dig +tcp +noall +answer +search 232.48.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.48.232_tcp@PTR;sleep 1; done
     01/18/23 22:02:19.07
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1785.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1785.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1785.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1785.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1785.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1785.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 232.48.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.48.232_udp@PTR;check="$$(dig +tcp +noall +answer +search 232.48.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.48.232_tcp@PTR;sleep 1; done
     01/18/23 22:02:19.071
    STEP: creating a pod to probe DNS 01/18/23 22:02:19.071
    STEP: submitting the pod to kubernetes 01/18/23 22:02:19.072
    Jan 18 22:02:19.130: INFO: Waiting up to 15m0s for pod "dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45" in namespace "dns-1785" to be "running"
    Jan 18 22:02:19.139: INFO: Pod "dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45": Phase="Pending", Reason="", readiness=false. Elapsed: 9.463159ms
    Jan 18 22:02:21.152: INFO: Pod "dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45": Phase="Running", Reason="", readiness=true. Elapsed: 2.022634976s
    Jan 18 22:02:21.153: INFO: Pod "dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 22:02:21.153
    STEP: looking for the results for each expected name from probers 01/18/23 22:02:21.157
    Jan 18 22:02:21.165: INFO: Unable to read wheezy_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:21.170: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:21.176: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:21.186: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:21.215: INFO: Unable to read jessie_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:21.220: INFO: Unable to read jessie_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:21.224: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:21.230: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:21.251: INFO: Lookups using dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45 failed for: [wheezy_udp@dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_udp@dns-test-service.dns-1785.svc.cluster.local jessie_tcp@dns-test-service.dns-1785.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local]

    Jan 18 22:02:26.262: INFO: Unable to read wheezy_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:26.279: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:26.284: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:26.300: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:26.342: INFO: Unable to read jessie_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:26.350: INFO: Unable to read jessie_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:26.367: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:26.375: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:26.394: INFO: Lookups using dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45 failed for: [wheezy_udp@dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_udp@dns-test-service.dns-1785.svc.cluster.local jessie_tcp@dns-test-service.dns-1785.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local]

    Jan 18 22:02:31.261: INFO: Unable to read wheezy_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:31.266: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:31.272: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:31.278: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:31.309: INFO: Unable to read jessie_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:31.315: INFO: Unable to read jessie_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:31.319: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:31.324: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:31.345: INFO: Lookups using dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45 failed for: [wheezy_udp@dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_udp@dns-test-service.dns-1785.svc.cluster.local jessie_tcp@dns-test-service.dns-1785.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local]

    Jan 18 22:02:36.256: INFO: Unable to read wheezy_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:36.265: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:36.273: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:36.279: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:36.329: INFO: Unable to read jessie_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:36.336: INFO: Unable to read jessie_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:36.341: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:36.356: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:36.388: INFO: Lookups using dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45 failed for: [wheezy_udp@dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_udp@dns-test-service.dns-1785.svc.cluster.local jessie_tcp@dns-test-service.dns-1785.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local]

    Jan 18 22:02:41.261: INFO: Unable to read wheezy_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:41.269: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:41.275: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:41.280: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:41.314: INFO: Unable to read jessie_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:41.319: INFO: Unable to read jessie_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:41.324: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:41.330: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:41.350: INFO: Lookups using dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45 failed for: [wheezy_udp@dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_udp@dns-test-service.dns-1785.svc.cluster.local jessie_tcp@dns-test-service.dns-1785.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local]

    Jan 18 22:02:46.287: INFO: Unable to read wheezy_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:46.300: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:46.306: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:46.339: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:46.395: INFO: Unable to read jessie_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:46.400: INFO: Unable to read jessie_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:46.405: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:46.410: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:46.465: INFO: Lookups using dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45 failed for: [wheezy_udp@dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_udp@dns-test-service.dns-1785.svc.cluster.local jessie_tcp@dns-test-service.dns-1785.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local]

    Jan 18 22:02:51.260: INFO: Unable to read wheezy_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:51.266: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:51.272: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:51.283: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:51.308: INFO: Unable to read jessie_udp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:51.313: INFO: Unable to read jessie_tcp@dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:51.319: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:51.323: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local from pod dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45: the server could not find the requested resource (get pods dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45)
    Jan 18 22:02:51.340: INFO: Lookups using dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45 failed for: [wheezy_udp@dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@dns-test-service.dns-1785.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_udp@dns-test-service.dns-1785.svc.cluster.local jessie_tcp@dns-test-service.dns-1785.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1785.svc.cluster.local]

    Jan 18 22:02:56.524: INFO: DNS probes using dns-1785/dns-test-6551bf7e-f9f9-426b-861e-4ff3ec408c45 succeeded

    STEP: deleting the pod 01/18/23 22:02:56.525
    STEP: deleting the test service 01/18/23 22:02:56.568
    STEP: deleting the test headless service 01/18/23 22:02:56.629
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 18 22:02:56.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1785" for this suite. 01/18/23 22:02:56.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:02:56.679
Jan 18 22:02:56.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 22:02:56.681
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:02:56.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:02:56.758
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Jan 18 22:02:56.777: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/18/23 22:03:01.938
Jan 18 22:03:01.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 --namespace=crd-publish-openapi-571 create -f -'
Jan 18 22:03:03.201: INFO: stderr: ""
Jan 18 22:03:03.201: INFO: stdout: "e2e-test-crd-publish-openapi-8979-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 18 22:03:03.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 --namespace=crd-publish-openapi-571 delete e2e-test-crd-publish-openapi-8979-crds test-foo'
Jan 18 22:03:03.312: INFO: stderr: ""
Jan 18 22:03:03.312: INFO: stdout: "e2e-test-crd-publish-openapi-8979-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 18 22:03:03.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 --namespace=crd-publish-openapi-571 apply -f -'
Jan 18 22:03:03.654: INFO: stderr: ""
Jan 18 22:03:03.654: INFO: stdout: "e2e-test-crd-publish-openapi-8979-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 18 22:03:03.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 --namespace=crd-publish-openapi-571 delete e2e-test-crd-publish-openapi-8979-crds test-foo'
Jan 18 22:03:03.765: INFO: stderr: ""
Jan 18 22:03:03.765: INFO: stdout: "e2e-test-crd-publish-openapi-8979-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/18/23 22:03:03.765
Jan 18 22:03:03.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 --namespace=crd-publish-openapi-571 create -f -'
Jan 18 22:03:04.100: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/18/23 22:03:04.1
Jan 18 22:03:04.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 --namespace=crd-publish-openapi-571 create -f -'
Jan 18 22:03:04.434: INFO: rc: 1
Jan 18 22:03:04.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 --namespace=crd-publish-openapi-571 apply -f -'
Jan 18 22:03:04.782: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/18/23 22:03:04.782
Jan 18 22:03:04.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 --namespace=crd-publish-openapi-571 create -f -'
Jan 18 22:03:05.175: INFO: rc: 1
Jan 18 22:03:05.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 --namespace=crd-publish-openapi-571 apply -f -'
Jan 18 22:03:05.531: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 01/18/23 22:03:05.532
Jan 18 22:03:05.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 explain e2e-test-crd-publish-openapi-8979-crds'
Jan 18 22:03:05.888: INFO: stderr: ""
Jan 18 22:03:05.889: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8979-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 01/18/23 22:03:05.889
Jan 18 22:03:05.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 explain e2e-test-crd-publish-openapi-8979-crds.metadata'
Jan 18 22:03:06.221: INFO: stderr: ""
Jan 18 22:03:06.221: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8979-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 18 22:03:06.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 explain e2e-test-crd-publish-openapi-8979-crds.spec'
Jan 18 22:03:06.600: INFO: stderr: ""
Jan 18 22:03:06.600: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8979-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 18 22:03:06.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 explain e2e-test-crd-publish-openapi-8979-crds.spec.bars'
Jan 18 22:03:06.929: INFO: stderr: ""
Jan 18 22:03:06.929: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8979-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/18/23 22:03:06.929
Jan 18 22:03:06.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 explain e2e-test-crd-publish-openapi-8979-crds.spec.bars2'
Jan 18 22:03:07.221: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 22:03:12.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-571" for this suite. 01/18/23 22:03:12.366
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":164,"skipped":3035,"failed":0}
------------------------------
• [SLOW TEST] [15.694 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:02:56.679
    Jan 18 22:02:56.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 22:02:56.681
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:02:56.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:02:56.758
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Jan 18 22:02:56.777: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/18/23 22:03:01.938
    Jan 18 22:03:01.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 --namespace=crd-publish-openapi-571 create -f -'
    Jan 18 22:03:03.201: INFO: stderr: ""
    Jan 18 22:03:03.201: INFO: stdout: "e2e-test-crd-publish-openapi-8979-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 18 22:03:03.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 --namespace=crd-publish-openapi-571 delete e2e-test-crd-publish-openapi-8979-crds test-foo'
    Jan 18 22:03:03.312: INFO: stderr: ""
    Jan 18 22:03:03.312: INFO: stdout: "e2e-test-crd-publish-openapi-8979-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jan 18 22:03:03.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 --namespace=crd-publish-openapi-571 apply -f -'
    Jan 18 22:03:03.654: INFO: stderr: ""
    Jan 18 22:03:03.654: INFO: stdout: "e2e-test-crd-publish-openapi-8979-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 18 22:03:03.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 --namespace=crd-publish-openapi-571 delete e2e-test-crd-publish-openapi-8979-crds test-foo'
    Jan 18 22:03:03.765: INFO: stderr: ""
    Jan 18 22:03:03.765: INFO: stdout: "e2e-test-crd-publish-openapi-8979-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/18/23 22:03:03.765
    Jan 18 22:03:03.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 --namespace=crd-publish-openapi-571 create -f -'
    Jan 18 22:03:04.100: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/18/23 22:03:04.1
    Jan 18 22:03:04.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 --namespace=crd-publish-openapi-571 create -f -'
    Jan 18 22:03:04.434: INFO: rc: 1
    Jan 18 22:03:04.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 --namespace=crd-publish-openapi-571 apply -f -'
    Jan 18 22:03:04.782: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/18/23 22:03:04.782
    Jan 18 22:03:04.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 --namespace=crd-publish-openapi-571 create -f -'
    Jan 18 22:03:05.175: INFO: rc: 1
    Jan 18 22:03:05.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 --namespace=crd-publish-openapi-571 apply -f -'
    Jan 18 22:03:05.531: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 01/18/23 22:03:05.532
    Jan 18 22:03:05.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 explain e2e-test-crd-publish-openapi-8979-crds'
    Jan 18 22:03:05.888: INFO: stderr: ""
    Jan 18 22:03:05.889: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8979-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 01/18/23 22:03:05.889
    Jan 18 22:03:05.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 explain e2e-test-crd-publish-openapi-8979-crds.metadata'
    Jan 18 22:03:06.221: INFO: stderr: ""
    Jan 18 22:03:06.221: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8979-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jan 18 22:03:06.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 explain e2e-test-crd-publish-openapi-8979-crds.spec'
    Jan 18 22:03:06.600: INFO: stderr: ""
    Jan 18 22:03:06.600: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8979-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jan 18 22:03:06.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 explain e2e-test-crd-publish-openapi-8979-crds.spec.bars'
    Jan 18 22:03:06.929: INFO: stderr: ""
    Jan 18 22:03:06.929: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8979-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/18/23 22:03:06.929
    Jan 18 22:03:06.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-571 explain e2e-test-crd-publish-openapi-8979-crds.spec.bars2'
    Jan 18 22:03:07.221: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 22:03:12.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-571" for this suite. 01/18/23 22:03:12.366
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:03:12.381
Jan 18 22:03:12.381: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename sched-pred 01/18/23 22:03:12.382
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:03:12.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:03:12.434
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 18 22:03:12.438: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 18 22:03:12.449: INFO: Waiting for terminating namespaces to be deleted...
Jan 18 22:03:12.453: INFO: 
Logging pods the apiserver thinks is on node v1-25-1-18760-w before test
Jan 18 22:03:12.488: INFO: helm-controller-578bd944df-dzr2c from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:03:12.489: INFO: 	Container manager ready: true, restart count 0
Jan 18 22:03:12.489: INFO: notification-controller-6459696d4f-gdmj8 from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:03:12.489: INFO: 	Container manager ready: true, restart count 0
Jan 18 22:03:12.489: INFO: source-controller-84f6bcbfb8-ljjjm from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:03:12.489: INFO: 	Container manager ready: true, restart count 0
Jan 18 22:03:12.489: INFO: calico-node-59tts from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
Jan 18 22:03:12.489: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 22:03:12.489: INFO: coredns-coredns-5f9b955d9-pjqxb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:03:12.489: INFO: 	Container coredns ready: true, restart count 0
Jan 18 22:03:12.489: INFO: coredns-coredns-autoscaler-7d98f7496d-7zbwb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:03:12.489: INFO: 	Container autoscaler ready: true, restart count 0
Jan 18 22:03:12.489: INFO: kube-proxy-rzflq from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
Jan 18 22:03:12.489: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 22:03:12.490: INFO: metrics-server-58498b9c56-q88cp from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:03:12.490: INFO: 	Container metrics-server ready: true, restart count 0
Jan 18 22:03:12.490: INFO: nginx-proxy-v1-25-1-18760-w from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
Jan 18 22:03:12.490: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 18 22:03:12.490: INFO: nodelocaldns-skdxm from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
Jan 18 22:03:12.490: INFO: 	Container node-cache ready: true, restart count 0
Jan 18 22:03:12.490: INFO: openstack-cinder-csi-controllerplugin-55d57dd799-6496v from kube-system started at 2023-01-18 14:39:48 +0000 UTC (6 container statuses recorded)
Jan 18 22:03:12.490: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 18 22:03:12.490: INFO: 	Container csi-attacher ready: true, restart count 0
Jan 18 22:03:12.490: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan 18 22:03:12.490: INFO: 	Container csi-resizer ready: true, restart count 0
Jan 18 22:03:12.490: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan 18 22:03:12.490: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 18 22:03:12.490: INFO: openstack-cinder-csi-nodeplugin-w7sxz from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
Jan 18 22:03:12.490: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 18 22:03:12.491: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 18 22:03:12.491: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 18 22:03:12.491: INFO: sonobuoy-systemd-logs-daemon-set-c355d4758f184dbb-d68zx from sonobuoy started at 2023-01-18 21:29:30 +0000 UTC (2 container statuses recorded)
Jan 18 22:03:12.491: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:03:12.491: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 22:03:12.491: INFO: 
Logging pods the apiserver thinks is on node v1-25-1-18760-w2 before test
Jan 18 22:03:12.505: INFO: calico-node-qx4f9 from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
Jan 18 22:03:12.505: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 22:03:12.505: INFO: coredns-coredns-5f9b955d9-qggng from kube-system started at 2023-01-18 14:40:34 +0000 UTC (1 container statuses recorded)
Jan 18 22:03:12.505: INFO: 	Container coredns ready: true, restart count 0
Jan 18 22:03:12.505: INFO: kube-proxy-8r2pt from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
Jan 18 22:03:12.505: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 22:03:12.506: INFO: nginx-proxy-v1-25-1-18760-w2 from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
Jan 18 22:03:12.506: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 18 22:03:12.506: INFO: nodelocaldns-dzwbj from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
Jan 18 22:03:12.506: INFO: 	Container node-cache ready: true, restart count 0
Jan 18 22:03:12.506: INFO: openstack-cinder-csi-nodeplugin-qm575 from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
Jan 18 22:03:12.506: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 18 22:03:12.506: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 18 22:03:12.506: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 18 22:03:12.506: INFO: sonobuoy from sonobuoy started at 2023-01-18 21:29:28 +0000 UTC (1 container statuses recorded)
Jan 18 22:03:12.506: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 18 22:03:12.506: INFO: sonobuoy-e2e-job-2f3390b6578e4b8c from sonobuoy started at 2023-01-18 21:29:29 +0000 UTC (2 container statuses recorded)
Jan 18 22:03:12.506: INFO: 	Container e2e ready: true, restart count 0
Jan 18 22:03:12.506: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:03:12.506: INFO: sonobuoy-systemd-logs-daemon-set-c355d4758f184dbb-svdn4 from sonobuoy started at 2023-01-18 21:29:29 +0000 UTC (2 container statuses recorded)
Jan 18 22:03:12.506: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:03:12.506: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node v1-25-1-18760-w 01/18/23 22:03:12.553
STEP: verifying the node has the label node v1-25-1-18760-w2 01/18/23 22:03:12.603
Jan 18 22:03:12.650: INFO: Pod helm-controller-578bd944df-dzr2c requesting resource cpu=100m on Node v1-25-1-18760-w
Jan 18 22:03:12.650: INFO: Pod notification-controller-6459696d4f-gdmj8 requesting resource cpu=100m on Node v1-25-1-18760-w
Jan 18 22:03:12.650: INFO: Pod source-controller-84f6bcbfb8-ljjjm requesting resource cpu=50m on Node v1-25-1-18760-w
Jan 18 22:03:12.651: INFO: Pod calico-node-59tts requesting resource cpu=150m on Node v1-25-1-18760-w
Jan 18 22:03:12.651: INFO: Pod calico-node-qx4f9 requesting resource cpu=150m on Node v1-25-1-18760-w2
Jan 18 22:03:12.651: INFO: Pod coredns-coredns-5f9b955d9-pjqxb requesting resource cpu=100m on Node v1-25-1-18760-w
Jan 18 22:03:12.651: INFO: Pod coredns-coredns-5f9b955d9-qggng requesting resource cpu=100m on Node v1-25-1-18760-w2
Jan 18 22:03:12.651: INFO: Pod coredns-coredns-autoscaler-7d98f7496d-7zbwb requesting resource cpu=20m on Node v1-25-1-18760-w
Jan 18 22:03:12.651: INFO: Pod kube-proxy-8r2pt requesting resource cpu=0m on Node v1-25-1-18760-w2
Jan 18 22:03:12.651: INFO: Pod kube-proxy-rzflq requesting resource cpu=0m on Node v1-25-1-18760-w
Jan 18 22:03:12.652: INFO: Pod metrics-server-58498b9c56-q88cp requesting resource cpu=0m on Node v1-25-1-18760-w
Jan 18 22:03:12.652: INFO: Pod nginx-proxy-v1-25-1-18760-w requesting resource cpu=25m on Node v1-25-1-18760-w
Jan 18 22:03:12.652: INFO: Pod nginx-proxy-v1-25-1-18760-w2 requesting resource cpu=25m on Node v1-25-1-18760-w2
Jan 18 22:03:12.652: INFO: Pod nodelocaldns-dzwbj requesting resource cpu=100m on Node v1-25-1-18760-w2
Jan 18 22:03:12.652: INFO: Pod nodelocaldns-skdxm requesting resource cpu=100m on Node v1-25-1-18760-w
Jan 18 22:03:12.652: INFO: Pod openstack-cinder-csi-controllerplugin-55d57dd799-6496v requesting resource cpu=0m on Node v1-25-1-18760-w
Jan 18 22:03:12.652: INFO: Pod openstack-cinder-csi-nodeplugin-qm575 requesting resource cpu=0m on Node v1-25-1-18760-w2
Jan 18 22:03:12.652: INFO: Pod openstack-cinder-csi-nodeplugin-w7sxz requesting resource cpu=0m on Node v1-25-1-18760-w
Jan 18 22:03:12.652: INFO: Pod sonobuoy requesting resource cpu=0m on Node v1-25-1-18760-w2
Jan 18 22:03:12.653: INFO: Pod sonobuoy-e2e-job-2f3390b6578e4b8c requesting resource cpu=0m on Node v1-25-1-18760-w2
Jan 18 22:03:12.653: INFO: Pod sonobuoy-systemd-logs-daemon-set-c355d4758f184dbb-d68zx requesting resource cpu=0m on Node v1-25-1-18760-w
Jan 18 22:03:12.653: INFO: Pod sonobuoy-systemd-logs-daemon-set-c355d4758f184dbb-svdn4 requesting resource cpu=0m on Node v1-25-1-18760-w2
STEP: Starting Pods to consume most of the cluster CPU. 01/18/23 22:03:12.653
Jan 18 22:03:12.653: INFO: Creating a pod which consumes cpu=878m on Node v1-25-1-18760-w
Jan 18 22:03:12.671: INFO: Creating a pod which consumes cpu=1067m on Node v1-25-1-18760-w2
Jan 18 22:03:12.687: INFO: Waiting up to 5m0s for pod "filler-pod-9bc2550e-a6ae-4b0f-b3aa-130a85c4f2df" in namespace "sched-pred-7711" to be "running"
Jan 18 22:03:12.701: INFO: Pod "filler-pod-9bc2550e-a6ae-4b0f-b3aa-130a85c4f2df": Phase="Pending", Reason="", readiness=false. Elapsed: 13.684993ms
Jan 18 22:03:14.708: INFO: Pod "filler-pod-9bc2550e-a6ae-4b0f-b3aa-130a85c4f2df": Phase="Running", Reason="", readiness=true. Elapsed: 2.020162657s
Jan 18 22:03:14.708: INFO: Pod "filler-pod-9bc2550e-a6ae-4b0f-b3aa-130a85c4f2df" satisfied condition "running"
Jan 18 22:03:14.708: INFO: Waiting up to 5m0s for pod "filler-pod-d7605ca2-6f18-4434-88f1-0193e361d9cc" in namespace "sched-pred-7711" to be "running"
Jan 18 22:03:14.714: INFO: Pod "filler-pod-d7605ca2-6f18-4434-88f1-0193e361d9cc": Phase="Running", Reason="", readiness=true. Elapsed: 6.551653ms
Jan 18 22:03:14.715: INFO: Pod "filler-pod-d7605ca2-6f18-4434-88f1-0193e361d9cc" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 01/18/23 22:03:14.715
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9bc2550e-a6ae-4b0f-b3aa-130a85c4f2df.173b8656870793df], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7711/filler-pod-9bc2550e-a6ae-4b0f-b3aa-130a85c4f2df to v1-25-1-18760-w] 01/18/23 22:03:14.722
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9bc2550e-a6ae-4b0f-b3aa-130a85c4f2df.173b8656bcf84801], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/18/23 22:03:14.723
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9bc2550e-a6ae-4b0f-b3aa-130a85c4f2df.173b8656c099157f], Reason = [Created], Message = [Created container filler-pod-9bc2550e-a6ae-4b0f-b3aa-130a85c4f2df] 01/18/23 22:03:14.723
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9bc2550e-a6ae-4b0f-b3aa-130a85c4f2df.173b8656caa242c2], Reason = [Started], Message = [Started container filler-pod-9bc2550e-a6ae-4b0f-b3aa-130a85c4f2df] 01/18/23 22:03:14.723
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d7605ca2-6f18-4434-88f1-0193e361d9cc.173b865687eaad5b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7711/filler-pod-d7605ca2-6f18-4434-88f1-0193e361d9cc to v1-25-1-18760-w2] 01/18/23 22:03:14.723
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d7605ca2-6f18-4434-88f1-0193e361d9cc.173b8656be80a2b8], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/18/23 22:03:14.723
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d7605ca2-6f18-4434-88f1-0193e361d9cc.173b8656c23125e7], Reason = [Created], Message = [Created container filler-pod-d7605ca2-6f18-4434-88f1-0193e361d9cc] 01/18/23 22:03:14.723
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d7605ca2-6f18-4434-88f1-0193e361d9cc.173b8656cc6422ab], Reason = [Started], Message = [Started container filler-pod-d7605ca2-6f18-4434-88f1-0193e361d9cc] 01/18/23 22:03:14.723
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.173b865700dfc31d], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.] 01/18/23 22:03:14.74
STEP: removing the label node off the node v1-25-1-18760-w2 01/18/23 22:03:15.746
STEP: verifying the node doesn't have the label node 01/18/23 22:03:15.773
STEP: removing the label node off the node v1-25-1-18760-w 01/18/23 22:03:15.78
STEP: verifying the node doesn't have the label node 01/18/23 22:03:15.806
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 18 22:03:15.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7711" for this suite. 01/18/23 22:03:15.831
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":165,"skipped":3077,"failed":0}
------------------------------
• [3.467 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:03:12.381
    Jan 18 22:03:12.381: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename sched-pred 01/18/23 22:03:12.382
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:03:12.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:03:12.434
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 18 22:03:12.438: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 18 22:03:12.449: INFO: Waiting for terminating namespaces to be deleted...
    Jan 18 22:03:12.453: INFO: 
    Logging pods the apiserver thinks is on node v1-25-1-18760-w before test
    Jan 18 22:03:12.488: INFO: helm-controller-578bd944df-dzr2c from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 22:03:12.489: INFO: 	Container manager ready: true, restart count 0
    Jan 18 22:03:12.489: INFO: notification-controller-6459696d4f-gdmj8 from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 22:03:12.489: INFO: 	Container manager ready: true, restart count 0
    Jan 18 22:03:12.489: INFO: source-controller-84f6bcbfb8-ljjjm from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 22:03:12.489: INFO: 	Container manager ready: true, restart count 0
    Jan 18 22:03:12.489: INFO: calico-node-59tts from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
    Jan 18 22:03:12.489: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 22:03:12.489: INFO: coredns-coredns-5f9b955d9-pjqxb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 22:03:12.489: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 22:03:12.489: INFO: coredns-coredns-autoscaler-7d98f7496d-7zbwb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 22:03:12.489: INFO: 	Container autoscaler ready: true, restart count 0
    Jan 18 22:03:12.489: INFO: kube-proxy-rzflq from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
    Jan 18 22:03:12.489: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 22:03:12.490: INFO: metrics-server-58498b9c56-q88cp from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 22:03:12.490: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 18 22:03:12.490: INFO: nginx-proxy-v1-25-1-18760-w from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
    Jan 18 22:03:12.490: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 18 22:03:12.490: INFO: nodelocaldns-skdxm from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
    Jan 18 22:03:12.490: INFO: 	Container node-cache ready: true, restart count 0
    Jan 18 22:03:12.490: INFO: openstack-cinder-csi-controllerplugin-55d57dd799-6496v from kube-system started at 2023-01-18 14:39:48 +0000 UTC (6 container statuses recorded)
    Jan 18 22:03:12.490: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 18 22:03:12.490: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan 18 22:03:12.490: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan 18 22:03:12.490: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan 18 22:03:12.490: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan 18 22:03:12.490: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 18 22:03:12.490: INFO: openstack-cinder-csi-nodeplugin-w7sxz from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
    Jan 18 22:03:12.490: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 18 22:03:12.491: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 18 22:03:12.491: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 18 22:03:12.491: INFO: sonobuoy-systemd-logs-daemon-set-c355d4758f184dbb-d68zx from sonobuoy started at 2023-01-18 21:29:30 +0000 UTC (2 container statuses recorded)
    Jan 18 22:03:12.491: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 22:03:12.491: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 18 22:03:12.491: INFO: 
    Logging pods the apiserver thinks is on node v1-25-1-18760-w2 before test
    Jan 18 22:03:12.505: INFO: calico-node-qx4f9 from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
    Jan 18 22:03:12.505: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 22:03:12.505: INFO: coredns-coredns-5f9b955d9-qggng from kube-system started at 2023-01-18 14:40:34 +0000 UTC (1 container statuses recorded)
    Jan 18 22:03:12.505: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 22:03:12.505: INFO: kube-proxy-8r2pt from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
    Jan 18 22:03:12.505: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 22:03:12.506: INFO: nginx-proxy-v1-25-1-18760-w2 from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
    Jan 18 22:03:12.506: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 18 22:03:12.506: INFO: nodelocaldns-dzwbj from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
    Jan 18 22:03:12.506: INFO: 	Container node-cache ready: true, restart count 0
    Jan 18 22:03:12.506: INFO: openstack-cinder-csi-nodeplugin-qm575 from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
    Jan 18 22:03:12.506: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 18 22:03:12.506: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 18 22:03:12.506: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 18 22:03:12.506: INFO: sonobuoy from sonobuoy started at 2023-01-18 21:29:28 +0000 UTC (1 container statuses recorded)
    Jan 18 22:03:12.506: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 18 22:03:12.506: INFO: sonobuoy-e2e-job-2f3390b6578e4b8c from sonobuoy started at 2023-01-18 21:29:29 +0000 UTC (2 container statuses recorded)
    Jan 18 22:03:12.506: INFO: 	Container e2e ready: true, restart count 0
    Jan 18 22:03:12.506: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 22:03:12.506: INFO: sonobuoy-systemd-logs-daemon-set-c355d4758f184dbb-svdn4 from sonobuoy started at 2023-01-18 21:29:29 +0000 UTC (2 container statuses recorded)
    Jan 18 22:03:12.506: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 22:03:12.506: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node v1-25-1-18760-w 01/18/23 22:03:12.553
    STEP: verifying the node has the label node v1-25-1-18760-w2 01/18/23 22:03:12.603
    Jan 18 22:03:12.650: INFO: Pod helm-controller-578bd944df-dzr2c requesting resource cpu=100m on Node v1-25-1-18760-w
    Jan 18 22:03:12.650: INFO: Pod notification-controller-6459696d4f-gdmj8 requesting resource cpu=100m on Node v1-25-1-18760-w
    Jan 18 22:03:12.650: INFO: Pod source-controller-84f6bcbfb8-ljjjm requesting resource cpu=50m on Node v1-25-1-18760-w
    Jan 18 22:03:12.651: INFO: Pod calico-node-59tts requesting resource cpu=150m on Node v1-25-1-18760-w
    Jan 18 22:03:12.651: INFO: Pod calico-node-qx4f9 requesting resource cpu=150m on Node v1-25-1-18760-w2
    Jan 18 22:03:12.651: INFO: Pod coredns-coredns-5f9b955d9-pjqxb requesting resource cpu=100m on Node v1-25-1-18760-w
    Jan 18 22:03:12.651: INFO: Pod coredns-coredns-5f9b955d9-qggng requesting resource cpu=100m on Node v1-25-1-18760-w2
    Jan 18 22:03:12.651: INFO: Pod coredns-coredns-autoscaler-7d98f7496d-7zbwb requesting resource cpu=20m on Node v1-25-1-18760-w
    Jan 18 22:03:12.651: INFO: Pod kube-proxy-8r2pt requesting resource cpu=0m on Node v1-25-1-18760-w2
    Jan 18 22:03:12.651: INFO: Pod kube-proxy-rzflq requesting resource cpu=0m on Node v1-25-1-18760-w
    Jan 18 22:03:12.652: INFO: Pod metrics-server-58498b9c56-q88cp requesting resource cpu=0m on Node v1-25-1-18760-w
    Jan 18 22:03:12.652: INFO: Pod nginx-proxy-v1-25-1-18760-w requesting resource cpu=25m on Node v1-25-1-18760-w
    Jan 18 22:03:12.652: INFO: Pod nginx-proxy-v1-25-1-18760-w2 requesting resource cpu=25m on Node v1-25-1-18760-w2
    Jan 18 22:03:12.652: INFO: Pod nodelocaldns-dzwbj requesting resource cpu=100m on Node v1-25-1-18760-w2
    Jan 18 22:03:12.652: INFO: Pod nodelocaldns-skdxm requesting resource cpu=100m on Node v1-25-1-18760-w
    Jan 18 22:03:12.652: INFO: Pod openstack-cinder-csi-controllerplugin-55d57dd799-6496v requesting resource cpu=0m on Node v1-25-1-18760-w
    Jan 18 22:03:12.652: INFO: Pod openstack-cinder-csi-nodeplugin-qm575 requesting resource cpu=0m on Node v1-25-1-18760-w2
    Jan 18 22:03:12.652: INFO: Pod openstack-cinder-csi-nodeplugin-w7sxz requesting resource cpu=0m on Node v1-25-1-18760-w
    Jan 18 22:03:12.652: INFO: Pod sonobuoy requesting resource cpu=0m on Node v1-25-1-18760-w2
    Jan 18 22:03:12.653: INFO: Pod sonobuoy-e2e-job-2f3390b6578e4b8c requesting resource cpu=0m on Node v1-25-1-18760-w2
    Jan 18 22:03:12.653: INFO: Pod sonobuoy-systemd-logs-daemon-set-c355d4758f184dbb-d68zx requesting resource cpu=0m on Node v1-25-1-18760-w
    Jan 18 22:03:12.653: INFO: Pod sonobuoy-systemd-logs-daemon-set-c355d4758f184dbb-svdn4 requesting resource cpu=0m on Node v1-25-1-18760-w2
    STEP: Starting Pods to consume most of the cluster CPU. 01/18/23 22:03:12.653
    Jan 18 22:03:12.653: INFO: Creating a pod which consumes cpu=878m on Node v1-25-1-18760-w
    Jan 18 22:03:12.671: INFO: Creating a pod which consumes cpu=1067m on Node v1-25-1-18760-w2
    Jan 18 22:03:12.687: INFO: Waiting up to 5m0s for pod "filler-pod-9bc2550e-a6ae-4b0f-b3aa-130a85c4f2df" in namespace "sched-pred-7711" to be "running"
    Jan 18 22:03:12.701: INFO: Pod "filler-pod-9bc2550e-a6ae-4b0f-b3aa-130a85c4f2df": Phase="Pending", Reason="", readiness=false. Elapsed: 13.684993ms
    Jan 18 22:03:14.708: INFO: Pod "filler-pod-9bc2550e-a6ae-4b0f-b3aa-130a85c4f2df": Phase="Running", Reason="", readiness=true. Elapsed: 2.020162657s
    Jan 18 22:03:14.708: INFO: Pod "filler-pod-9bc2550e-a6ae-4b0f-b3aa-130a85c4f2df" satisfied condition "running"
    Jan 18 22:03:14.708: INFO: Waiting up to 5m0s for pod "filler-pod-d7605ca2-6f18-4434-88f1-0193e361d9cc" in namespace "sched-pred-7711" to be "running"
    Jan 18 22:03:14.714: INFO: Pod "filler-pod-d7605ca2-6f18-4434-88f1-0193e361d9cc": Phase="Running", Reason="", readiness=true. Elapsed: 6.551653ms
    Jan 18 22:03:14.715: INFO: Pod "filler-pod-d7605ca2-6f18-4434-88f1-0193e361d9cc" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 01/18/23 22:03:14.715
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-9bc2550e-a6ae-4b0f-b3aa-130a85c4f2df.173b8656870793df], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7711/filler-pod-9bc2550e-a6ae-4b0f-b3aa-130a85c4f2df to v1-25-1-18760-w] 01/18/23 22:03:14.722
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-9bc2550e-a6ae-4b0f-b3aa-130a85c4f2df.173b8656bcf84801], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/18/23 22:03:14.723
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-9bc2550e-a6ae-4b0f-b3aa-130a85c4f2df.173b8656c099157f], Reason = [Created], Message = [Created container filler-pod-9bc2550e-a6ae-4b0f-b3aa-130a85c4f2df] 01/18/23 22:03:14.723
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-9bc2550e-a6ae-4b0f-b3aa-130a85c4f2df.173b8656caa242c2], Reason = [Started], Message = [Started container filler-pod-9bc2550e-a6ae-4b0f-b3aa-130a85c4f2df] 01/18/23 22:03:14.723
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-d7605ca2-6f18-4434-88f1-0193e361d9cc.173b865687eaad5b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7711/filler-pod-d7605ca2-6f18-4434-88f1-0193e361d9cc to v1-25-1-18760-w2] 01/18/23 22:03:14.723
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-d7605ca2-6f18-4434-88f1-0193e361d9cc.173b8656be80a2b8], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/18/23 22:03:14.723
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-d7605ca2-6f18-4434-88f1-0193e361d9cc.173b8656c23125e7], Reason = [Created], Message = [Created container filler-pod-d7605ca2-6f18-4434-88f1-0193e361d9cc] 01/18/23 22:03:14.723
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-d7605ca2-6f18-4434-88f1-0193e361d9cc.173b8656cc6422ab], Reason = [Started], Message = [Started container filler-pod-d7605ca2-6f18-4434-88f1-0193e361d9cc] 01/18/23 22:03:14.723
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.173b865700dfc31d], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.] 01/18/23 22:03:14.74
    STEP: removing the label node off the node v1-25-1-18760-w2 01/18/23 22:03:15.746
    STEP: verifying the node doesn't have the label node 01/18/23 22:03:15.773
    STEP: removing the label node off the node v1-25-1-18760-w 01/18/23 22:03:15.78
    STEP: verifying the node doesn't have the label node 01/18/23 22:03:15.806
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 22:03:15.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-7711" for this suite. 01/18/23 22:03:15.831
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:03:15.857
Jan 18 22:03:15.857: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename containers 01/18/23 22:03:15.864
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:03:15.882
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:03:15.888
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Jan 18 22:03:15.923: INFO: Waiting up to 5m0s for pod "client-containers-4cbc88cf-8983-41a1-b780-c0ab1455175e" in namespace "containers-8091" to be "running"
Jan 18 22:03:16.081: INFO: Pod "client-containers-4cbc88cf-8983-41a1-b780-c0ab1455175e": Phase="Pending", Reason="", readiness=false. Elapsed: 155.398437ms
Jan 18 22:03:18.087: INFO: Pod "client-containers-4cbc88cf-8983-41a1-b780-c0ab1455175e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.161343876s
Jan 18 22:03:20.087: INFO: Pod "client-containers-4cbc88cf-8983-41a1-b780-c0ab1455175e": Phase="Running", Reason="", readiness=true. Elapsed: 4.16174318s
Jan 18 22:03:20.087: INFO: Pod "client-containers-4cbc88cf-8983-41a1-b780-c0ab1455175e" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 18 22:03:20.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8091" for this suite. 01/18/23 22:03:20.117
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":166,"skipped":3098,"failed":0}
------------------------------
• [4.269 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:03:15.857
    Jan 18 22:03:15.857: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename containers 01/18/23 22:03:15.864
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:03:15.882
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:03:15.888
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Jan 18 22:03:15.923: INFO: Waiting up to 5m0s for pod "client-containers-4cbc88cf-8983-41a1-b780-c0ab1455175e" in namespace "containers-8091" to be "running"
    Jan 18 22:03:16.081: INFO: Pod "client-containers-4cbc88cf-8983-41a1-b780-c0ab1455175e": Phase="Pending", Reason="", readiness=false. Elapsed: 155.398437ms
    Jan 18 22:03:18.087: INFO: Pod "client-containers-4cbc88cf-8983-41a1-b780-c0ab1455175e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.161343876s
    Jan 18 22:03:20.087: INFO: Pod "client-containers-4cbc88cf-8983-41a1-b780-c0ab1455175e": Phase="Running", Reason="", readiness=true. Elapsed: 4.16174318s
    Jan 18 22:03:20.087: INFO: Pod "client-containers-4cbc88cf-8983-41a1-b780-c0ab1455175e" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 18 22:03:20.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-8091" for this suite. 01/18/23 22:03:20.117
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:03:20.13
Jan 18 22:03:20.130: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename conformance-tests 01/18/23 22:03:20.143
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:03:20.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:03:20.206
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 01/18/23 22:03:20.211
Jan 18 22:03:20.212: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Jan 18 22:03:20.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-8454" for this suite. 01/18/23 22:03:20.227
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":167,"skipped":3101,"failed":0}
------------------------------
• [0.106 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:03:20.13
    Jan 18 22:03:20.130: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename conformance-tests 01/18/23 22:03:20.143
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:03:20.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:03:20.206
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 01/18/23 22:03:20.211
    Jan 18 22:03:20.212: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Jan 18 22:03:20.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-8454" for this suite. 01/18/23 22:03:20.227
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:03:20.244
Jan 18 22:03:20.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename emptydir 01/18/23 22:03:20.245
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:03:20.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:03:20.272
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 01/18/23 22:03:20.282
Jan 18 22:03:20.302: INFO: Waiting up to 5m0s for pod "pod-b467e285-39f0-4f64-83a3-e88bcaff25ba" in namespace "emptydir-5349" to be "Succeeded or Failed"
Jan 18 22:03:20.307: INFO: Pod "pod-b467e285-39f0-4f64-83a3-e88bcaff25ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.828511ms
Jan 18 22:03:22.326: INFO: Pod "pod-b467e285-39f0-4f64-83a3-e88bcaff25ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023949237s
Jan 18 22:03:24.320: INFO: Pod "pod-b467e285-39f0-4f64-83a3-e88bcaff25ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017738806s
STEP: Saw pod success 01/18/23 22:03:24.32
Jan 18 22:03:24.321: INFO: Pod "pod-b467e285-39f0-4f64-83a3-e88bcaff25ba" satisfied condition "Succeeded or Failed"
Jan 18 22:03:24.325: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-b467e285-39f0-4f64-83a3-e88bcaff25ba container test-container: <nil>
STEP: delete the pod 01/18/23 22:03:24.333
Jan 18 22:03:24.349: INFO: Waiting for pod pod-b467e285-39f0-4f64-83a3-e88bcaff25ba to disappear
Jan 18 22:03:24.352: INFO: Pod pod-b467e285-39f0-4f64-83a3-e88bcaff25ba no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 22:03:24.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5349" for this suite. 01/18/23 22:03:24.357
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":168,"skipped":3116,"failed":0}
------------------------------
• [4.121 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:03:20.244
    Jan 18 22:03:20.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename emptydir 01/18/23 22:03:20.245
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:03:20.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:03:20.272
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/18/23 22:03:20.282
    Jan 18 22:03:20.302: INFO: Waiting up to 5m0s for pod "pod-b467e285-39f0-4f64-83a3-e88bcaff25ba" in namespace "emptydir-5349" to be "Succeeded or Failed"
    Jan 18 22:03:20.307: INFO: Pod "pod-b467e285-39f0-4f64-83a3-e88bcaff25ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.828511ms
    Jan 18 22:03:22.326: INFO: Pod "pod-b467e285-39f0-4f64-83a3-e88bcaff25ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023949237s
    Jan 18 22:03:24.320: INFO: Pod "pod-b467e285-39f0-4f64-83a3-e88bcaff25ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017738806s
    STEP: Saw pod success 01/18/23 22:03:24.32
    Jan 18 22:03:24.321: INFO: Pod "pod-b467e285-39f0-4f64-83a3-e88bcaff25ba" satisfied condition "Succeeded or Failed"
    Jan 18 22:03:24.325: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-b467e285-39f0-4f64-83a3-e88bcaff25ba container test-container: <nil>
    STEP: delete the pod 01/18/23 22:03:24.333
    Jan 18 22:03:24.349: INFO: Waiting for pod pod-b467e285-39f0-4f64-83a3-e88bcaff25ba to disappear
    Jan 18 22:03:24.352: INFO: Pod pod-b467e285-39f0-4f64-83a3-e88bcaff25ba no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 22:03:24.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5349" for this suite. 01/18/23 22:03:24.357
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:03:24.372
Jan 18 22:03:24.372: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename init-container 01/18/23 22:03:24.374
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:03:24.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:03:24.403
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 01/18/23 22:03:24.41
Jan 18 22:03:24.411: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 18 22:03:29.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6520" for this suite. 01/18/23 22:03:29.264
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":169,"skipped":3122,"failed":0}
------------------------------
• [4.903 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:03:24.372
    Jan 18 22:03:24.372: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename init-container 01/18/23 22:03:24.374
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:03:24.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:03:24.403
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 01/18/23 22:03:24.41
    Jan 18 22:03:24.411: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 18 22:03:29.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-6520" for this suite. 01/18/23 22:03:29.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:03:29.28
Jan 18 22:03:29.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename daemonsets 01/18/23 22:03:29.282
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:03:29.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:03:29.324
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 01/18/23 22:03:29.356
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 22:03:29.363
Jan 18 22:03:29.371: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:03:29.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:03:29.378: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 22:03:30.398: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:03:30.410: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:03:30.410: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 22:03:31.384: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:03:31.388: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 22:03:31.388: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 01/18/23 22:03:31.392
Jan 18 22:03:31.421: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:03:31.428: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 22:03:31.428: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
Jan 18 22:03:32.436: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:03:32.441: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 22:03:32.441: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
Jan 18 22:03:33.436: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:03:33.442: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 22:03:33.442: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
Jan 18 22:03:34.436: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:03:34.443: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 22:03:34.443: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
Jan 18 22:03:35.444: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:03:35.448: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 22:03:35.448: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
Jan 18 22:03:36.435: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:03:36.441: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 22:03:36.442: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/18/23 22:03:36.446
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8837, will wait for the garbage collector to delete the pods 01/18/23 22:03:36.447
Jan 18 22:03:36.511: INFO: Deleting DaemonSet.extensions daemon-set took: 8.615653ms
Jan 18 22:03:36.612: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.022539ms
Jan 18 22:03:39.417: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:03:39.417: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 22:03:39.421: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"150779"},"items":null}

Jan 18 22:03:39.426: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"150779"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 18 22:03:39.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8837" for this suite. 01/18/23 22:03:39.449
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":170,"skipped":3152,"failed":0}
------------------------------
• [SLOW TEST] [10.176 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:03:29.28
    Jan 18 22:03:29.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename daemonsets 01/18/23 22:03:29.282
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:03:29.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:03:29.324
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 01/18/23 22:03:29.356
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 22:03:29.363
    Jan 18 22:03:29.371: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 22:03:29.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:03:29.378: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 22:03:30.398: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 22:03:30.410: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:03:30.410: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 22:03:31.384: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 22:03:31.388: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 22:03:31.388: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 01/18/23 22:03:31.392
    Jan 18 22:03:31.421: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 22:03:31.428: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 22:03:31.428: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
    Jan 18 22:03:32.436: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 22:03:32.441: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 22:03:32.441: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
    Jan 18 22:03:33.436: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 22:03:33.442: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 22:03:33.442: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
    Jan 18 22:03:34.436: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 22:03:34.443: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 22:03:34.443: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
    Jan 18 22:03:35.444: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 22:03:35.448: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 22:03:35.448: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
    Jan 18 22:03:36.435: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 22:03:36.441: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 22:03:36.442: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 22:03:36.446
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8837, will wait for the garbage collector to delete the pods 01/18/23 22:03:36.447
    Jan 18 22:03:36.511: INFO: Deleting DaemonSet.extensions daemon-set took: 8.615653ms
    Jan 18 22:03:36.612: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.022539ms
    Jan 18 22:03:39.417: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:03:39.417: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 22:03:39.421: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"150779"},"items":null}

    Jan 18 22:03:39.426: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"150779"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 22:03:39.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-8837" for this suite. 01/18/23 22:03:39.449
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:03:39.457
Jan 18 22:03:39.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename security-context-test 01/18/23 22:03:39.46
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:03:39.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:03:39.489
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Jan 18 22:03:39.507: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-520eb51d-465f-482f-b8f2-afcd6544697b" in namespace "security-context-test-9556" to be "Succeeded or Failed"
Jan 18 22:03:39.516: INFO: Pod "busybox-privileged-false-520eb51d-465f-482f-b8f2-afcd6544697b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.639395ms
Jan 18 22:03:41.522: INFO: Pod "busybox-privileged-false-520eb51d-465f-482f-b8f2-afcd6544697b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015668048s
Jan 18 22:03:43.529: INFO: Pod "busybox-privileged-false-520eb51d-465f-482f-b8f2-afcd6544697b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022220866s
Jan 18 22:03:43.529: INFO: Pod "busybox-privileged-false-520eb51d-465f-482f-b8f2-afcd6544697b" satisfied condition "Succeeded or Failed"
Jan 18 22:03:43.538: INFO: Got logs for pod "busybox-privileged-false-520eb51d-465f-482f-b8f2-afcd6544697b": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 18 22:03:43.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9556" for this suite. 01/18/23 22:03:43.546
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":171,"skipped":3153,"failed":0}
------------------------------
• [4.099 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:03:39.457
    Jan 18 22:03:39.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename security-context-test 01/18/23 22:03:39.46
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:03:39.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:03:39.489
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Jan 18 22:03:39.507: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-520eb51d-465f-482f-b8f2-afcd6544697b" in namespace "security-context-test-9556" to be "Succeeded or Failed"
    Jan 18 22:03:39.516: INFO: Pod "busybox-privileged-false-520eb51d-465f-482f-b8f2-afcd6544697b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.639395ms
    Jan 18 22:03:41.522: INFO: Pod "busybox-privileged-false-520eb51d-465f-482f-b8f2-afcd6544697b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015668048s
    Jan 18 22:03:43.529: INFO: Pod "busybox-privileged-false-520eb51d-465f-482f-b8f2-afcd6544697b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022220866s
    Jan 18 22:03:43.529: INFO: Pod "busybox-privileged-false-520eb51d-465f-482f-b8f2-afcd6544697b" satisfied condition "Succeeded or Failed"
    Jan 18 22:03:43.538: INFO: Got logs for pod "busybox-privileged-false-520eb51d-465f-482f-b8f2-afcd6544697b": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 18 22:03:43.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-9556" for this suite. 01/18/23 22:03:43.546
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:03:43.558
Jan 18 22:03:43.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename endpointslice 01/18/23 22:03:43.56
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:03:43.583
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:03:43.588
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 01/18/23 22:03:43.626
STEP: getting /apis/discovery.k8s.io 01/18/23 22:03:43.631
STEP: getting /apis/discovery.k8s.iov1 01/18/23 22:03:43.634
STEP: creating 01/18/23 22:03:43.637
STEP: getting 01/18/23 22:03:43.673
STEP: listing 01/18/23 22:03:43.679
STEP: watching 01/18/23 22:03:43.683
Jan 18 22:03:43.684: INFO: starting watch
STEP: cluster-wide listing 01/18/23 22:03:43.686
STEP: cluster-wide watching 01/18/23 22:03:43.691
Jan 18 22:03:43.692: INFO: starting watch
STEP: patching 01/18/23 22:03:43.694
STEP: updating 01/18/23 22:03:43.718
Jan 18 22:03:43.732: INFO: waiting for watch events with expected annotations
Jan 18 22:03:43.732: INFO: saw patched and updated annotations
STEP: deleting 01/18/23 22:03:43.732
STEP: deleting a collection 01/18/23 22:03:43.762
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 18 22:03:43.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9333" for this suite. 01/18/23 22:03:43.786
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":172,"skipped":3159,"failed":0}
------------------------------
• [0.234 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:03:43.558
    Jan 18 22:03:43.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename endpointslice 01/18/23 22:03:43.56
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:03:43.583
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:03:43.588
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 01/18/23 22:03:43.626
    STEP: getting /apis/discovery.k8s.io 01/18/23 22:03:43.631
    STEP: getting /apis/discovery.k8s.iov1 01/18/23 22:03:43.634
    STEP: creating 01/18/23 22:03:43.637
    STEP: getting 01/18/23 22:03:43.673
    STEP: listing 01/18/23 22:03:43.679
    STEP: watching 01/18/23 22:03:43.683
    Jan 18 22:03:43.684: INFO: starting watch
    STEP: cluster-wide listing 01/18/23 22:03:43.686
    STEP: cluster-wide watching 01/18/23 22:03:43.691
    Jan 18 22:03:43.692: INFO: starting watch
    STEP: patching 01/18/23 22:03:43.694
    STEP: updating 01/18/23 22:03:43.718
    Jan 18 22:03:43.732: INFO: waiting for watch events with expected annotations
    Jan 18 22:03:43.732: INFO: saw patched and updated annotations
    STEP: deleting 01/18/23 22:03:43.732
    STEP: deleting a collection 01/18/23 22:03:43.762
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 18 22:03:43.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-9333" for this suite. 01/18/23 22:03:43.786
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:03:43.798
Jan 18 22:03:43.798: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename controllerrevisions 01/18/23 22:03:43.801
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:03:43.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:03:43.857
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-wfq7m-daemon-set" 01/18/23 22:03:43.902
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 22:03:43.916
Jan 18 22:03:43.932: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:03:43.947: INFO: Number of nodes with available pods controlled by daemonset e2e-wfq7m-daemon-set: 0
Jan 18 22:03:43.947: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 22:03:44.962: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:03:44.970: INFO: Number of nodes with available pods controlled by daemonset e2e-wfq7m-daemon-set: 0
Jan 18 22:03:44.971: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 22:03:45.955: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 22:03:45.963: INFO: Number of nodes with available pods controlled by daemonset e2e-wfq7m-daemon-set: 2
Jan 18 22:03:45.963: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-wfq7m-daemon-set
STEP: Confirm DaemonSet "e2e-wfq7m-daemon-set" successfully created with "daemonset-name=e2e-wfq7m-daemon-set" label 01/18/23 22:03:45.967
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-wfq7m-daemon-set" 01/18/23 22:03:45.979
Jan 18 22:03:45.987: INFO: Located ControllerRevision: "e2e-wfq7m-daemon-set-5c9ff649ff"
STEP: Patching ControllerRevision "e2e-wfq7m-daemon-set-5c9ff649ff" 01/18/23 22:03:45.994
Jan 18 22:03:46.003: INFO: e2e-wfq7m-daemon-set-5c9ff649ff has been patched
STEP: Create a new ControllerRevision 01/18/23 22:03:46.003
Jan 18 22:03:46.010: INFO: Created ControllerRevision: e2e-wfq7m-daemon-set-5768c4896c
STEP: Confirm that there are two ControllerRevisions 01/18/23 22:03:46.01
Jan 18 22:03:46.010: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 18 22:03:46.024: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-wfq7m-daemon-set-5c9ff649ff" 01/18/23 22:03:46.024
STEP: Confirm that there is only one ControllerRevision 01/18/23 22:03:46.049
Jan 18 22:03:46.049: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 18 22:03:46.058: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-wfq7m-daemon-set-5768c4896c" 01/18/23 22:03:46.063
Jan 18 22:03:46.081: INFO: e2e-wfq7m-daemon-set-5768c4896c has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 01/18/23 22:03:46.082
W0118 22:03:46.092183      19 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 01/18/23 22:03:46.092
Jan 18 22:03:46.092: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 18 22:03:47.097: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 18 22:03:47.103: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-wfq7m-daemon-set-5768c4896c=updated" 01/18/23 22:03:47.103
STEP: Confirm that there is only one ControllerRevision 01/18/23 22:03:47.113
Jan 18 22:03:47.113: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 18 22:03:47.117: INFO: Found 1 ControllerRevisions
Jan 18 22:03:47.121: INFO: ControllerRevision "e2e-wfq7m-daemon-set-7bd9dc779d" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-wfq7m-daemon-set" 01/18/23 22:03:47.126
STEP: deleting DaemonSet.extensions e2e-wfq7m-daemon-set in namespace controllerrevisions-4897, will wait for the garbage collector to delete the pods 01/18/23 22:03:47.126
Jan 18 22:03:47.196: INFO: Deleting DaemonSet.extensions e2e-wfq7m-daemon-set took: 9.272996ms
Jan 18 22:03:47.296: INFO: Terminating DaemonSet.extensions e2e-wfq7m-daemon-set pods took: 100.639952ms
Jan 18 22:03:48.509: INFO: Number of nodes with available pods controlled by daemonset e2e-wfq7m-daemon-set: 0
Jan 18 22:03:48.509: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-wfq7m-daemon-set
Jan 18 22:03:48.516: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"150930"},"items":null}

Jan 18 22:03:48.521: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"150930"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Jan 18 22:03:48.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-4897" for this suite. 01/18/23 22:03:48.55
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":173,"skipped":3159,"failed":0}
------------------------------
• [4.768 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:03:43.798
    Jan 18 22:03:43.798: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename controllerrevisions 01/18/23 22:03:43.801
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:03:43.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:03:43.857
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-wfq7m-daemon-set" 01/18/23 22:03:43.902
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 22:03:43.916
    Jan 18 22:03:43.932: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 22:03:43.947: INFO: Number of nodes with available pods controlled by daemonset e2e-wfq7m-daemon-set: 0
    Jan 18 22:03:43.947: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 22:03:44.962: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 22:03:44.970: INFO: Number of nodes with available pods controlled by daemonset e2e-wfq7m-daemon-set: 0
    Jan 18 22:03:44.971: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 22:03:45.955: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 22:03:45.963: INFO: Number of nodes with available pods controlled by daemonset e2e-wfq7m-daemon-set: 2
    Jan 18 22:03:45.963: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-wfq7m-daemon-set
    STEP: Confirm DaemonSet "e2e-wfq7m-daemon-set" successfully created with "daemonset-name=e2e-wfq7m-daemon-set" label 01/18/23 22:03:45.967
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-wfq7m-daemon-set" 01/18/23 22:03:45.979
    Jan 18 22:03:45.987: INFO: Located ControllerRevision: "e2e-wfq7m-daemon-set-5c9ff649ff"
    STEP: Patching ControllerRevision "e2e-wfq7m-daemon-set-5c9ff649ff" 01/18/23 22:03:45.994
    Jan 18 22:03:46.003: INFO: e2e-wfq7m-daemon-set-5c9ff649ff has been patched
    STEP: Create a new ControllerRevision 01/18/23 22:03:46.003
    Jan 18 22:03:46.010: INFO: Created ControllerRevision: e2e-wfq7m-daemon-set-5768c4896c
    STEP: Confirm that there are two ControllerRevisions 01/18/23 22:03:46.01
    Jan 18 22:03:46.010: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 18 22:03:46.024: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-wfq7m-daemon-set-5c9ff649ff" 01/18/23 22:03:46.024
    STEP: Confirm that there is only one ControllerRevision 01/18/23 22:03:46.049
    Jan 18 22:03:46.049: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 18 22:03:46.058: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-wfq7m-daemon-set-5768c4896c" 01/18/23 22:03:46.063
    Jan 18 22:03:46.081: INFO: e2e-wfq7m-daemon-set-5768c4896c has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 01/18/23 22:03:46.082
    W0118 22:03:46.092183      19 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 01/18/23 22:03:46.092
    Jan 18 22:03:46.092: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 18 22:03:47.097: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 18 22:03:47.103: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-wfq7m-daemon-set-5768c4896c=updated" 01/18/23 22:03:47.103
    STEP: Confirm that there is only one ControllerRevision 01/18/23 22:03:47.113
    Jan 18 22:03:47.113: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 18 22:03:47.117: INFO: Found 1 ControllerRevisions
    Jan 18 22:03:47.121: INFO: ControllerRevision "e2e-wfq7m-daemon-set-7bd9dc779d" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-wfq7m-daemon-set" 01/18/23 22:03:47.126
    STEP: deleting DaemonSet.extensions e2e-wfq7m-daemon-set in namespace controllerrevisions-4897, will wait for the garbage collector to delete the pods 01/18/23 22:03:47.126
    Jan 18 22:03:47.196: INFO: Deleting DaemonSet.extensions e2e-wfq7m-daemon-set took: 9.272996ms
    Jan 18 22:03:47.296: INFO: Terminating DaemonSet.extensions e2e-wfq7m-daemon-set pods took: 100.639952ms
    Jan 18 22:03:48.509: INFO: Number of nodes with available pods controlled by daemonset e2e-wfq7m-daemon-set: 0
    Jan 18 22:03:48.509: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-wfq7m-daemon-set
    Jan 18 22:03:48.516: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"150930"},"items":null}

    Jan 18 22:03:48.521: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"150930"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 22:03:48.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-4897" for this suite. 01/18/23 22:03:48.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:03:48.574
Jan 18 22:03:48.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename var-expansion 01/18/23 22:03:48.576
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:03:48.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:03:48.694
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 01/18/23 22:03:48.703
Jan 18 22:03:48.721: INFO: Waiting up to 5m0s for pod "var-expansion-aa77c262-ac3f-4b20-8e14-cf0dc2e3e635" in namespace "var-expansion-142" to be "Succeeded or Failed"
Jan 18 22:03:48.732: INFO: Pod "var-expansion-aa77c262-ac3f-4b20-8e14-cf0dc2e3e635": Phase="Pending", Reason="", readiness=false. Elapsed: 10.038896ms
Jan 18 22:03:50.742: INFO: Pod "var-expansion-aa77c262-ac3f-4b20-8e14-cf0dc2e3e635": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019690446s
Jan 18 22:03:52.738: INFO: Pod "var-expansion-aa77c262-ac3f-4b20-8e14-cf0dc2e3e635": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016148344s
STEP: Saw pod success 01/18/23 22:03:52.738
Jan 18 22:03:52.739: INFO: Pod "var-expansion-aa77c262-ac3f-4b20-8e14-cf0dc2e3e635" satisfied condition "Succeeded or Failed"
Jan 18 22:03:52.743: INFO: Trying to get logs from node v1-25-1-18760-w2 pod var-expansion-aa77c262-ac3f-4b20-8e14-cf0dc2e3e635 container dapi-container: <nil>
STEP: delete the pod 01/18/23 22:03:52.753
Jan 18 22:03:52.772: INFO: Waiting for pod var-expansion-aa77c262-ac3f-4b20-8e14-cf0dc2e3e635 to disappear
Jan 18 22:03:52.781: INFO: Pod var-expansion-aa77c262-ac3f-4b20-8e14-cf0dc2e3e635 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 18 22:03:52.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-142" for this suite. 01/18/23 22:03:52.789
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":174,"skipped":3232,"failed":0}
------------------------------
• [4.226 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:03:48.574
    Jan 18 22:03:48.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename var-expansion 01/18/23 22:03:48.576
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:03:48.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:03:48.694
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 01/18/23 22:03:48.703
    Jan 18 22:03:48.721: INFO: Waiting up to 5m0s for pod "var-expansion-aa77c262-ac3f-4b20-8e14-cf0dc2e3e635" in namespace "var-expansion-142" to be "Succeeded or Failed"
    Jan 18 22:03:48.732: INFO: Pod "var-expansion-aa77c262-ac3f-4b20-8e14-cf0dc2e3e635": Phase="Pending", Reason="", readiness=false. Elapsed: 10.038896ms
    Jan 18 22:03:50.742: INFO: Pod "var-expansion-aa77c262-ac3f-4b20-8e14-cf0dc2e3e635": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019690446s
    Jan 18 22:03:52.738: INFO: Pod "var-expansion-aa77c262-ac3f-4b20-8e14-cf0dc2e3e635": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016148344s
    STEP: Saw pod success 01/18/23 22:03:52.738
    Jan 18 22:03:52.739: INFO: Pod "var-expansion-aa77c262-ac3f-4b20-8e14-cf0dc2e3e635" satisfied condition "Succeeded or Failed"
    Jan 18 22:03:52.743: INFO: Trying to get logs from node v1-25-1-18760-w2 pod var-expansion-aa77c262-ac3f-4b20-8e14-cf0dc2e3e635 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 22:03:52.753
    Jan 18 22:03:52.772: INFO: Waiting for pod var-expansion-aa77c262-ac3f-4b20-8e14-cf0dc2e3e635 to disappear
    Jan 18 22:03:52.781: INFO: Pod var-expansion-aa77c262-ac3f-4b20-8e14-cf0dc2e3e635 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 18 22:03:52.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-142" for this suite. 01/18/23 22:03:52.789
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:03:52.809
Jan 18 22:03:52.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename containers 01/18/23 22:03:52.811
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:03:52.88
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:03:52.891
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 01/18/23 22:03:52.898
Jan 18 22:03:52.911: INFO: Waiting up to 5m0s for pod "client-containers-425557bd-bf1c-4336-b0ab-5a8699eec9e8" in namespace "containers-9045" to be "Succeeded or Failed"
Jan 18 22:03:52.919: INFO: Pod "client-containers-425557bd-bf1c-4336-b0ab-5a8699eec9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.597ms
Jan 18 22:03:54.925: INFO: Pod "client-containers-425557bd-bf1c-4336-b0ab-5a8699eec9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012684695s
Jan 18 22:03:56.925: INFO: Pod "client-containers-425557bd-bf1c-4336-b0ab-5a8699eec9e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012432914s
STEP: Saw pod success 01/18/23 22:03:56.925
Jan 18 22:03:56.925: INFO: Pod "client-containers-425557bd-bf1c-4336-b0ab-5a8699eec9e8" satisfied condition "Succeeded or Failed"
Jan 18 22:03:56.930: INFO: Trying to get logs from node v1-25-1-18760-w2 pod client-containers-425557bd-bf1c-4336-b0ab-5a8699eec9e8 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 22:03:56.939
Jan 18 22:03:56.960: INFO: Waiting for pod client-containers-425557bd-bf1c-4336-b0ab-5a8699eec9e8 to disappear
Jan 18 22:03:56.965: INFO: Pod client-containers-425557bd-bf1c-4336-b0ab-5a8699eec9e8 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 18 22:03:56.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9045" for this suite. 01/18/23 22:03:56.971
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":175,"skipped":3248,"failed":0}
------------------------------
• [4.182 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:03:52.809
    Jan 18 22:03:52.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename containers 01/18/23 22:03:52.811
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:03:52.88
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:03:52.891
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 01/18/23 22:03:52.898
    Jan 18 22:03:52.911: INFO: Waiting up to 5m0s for pod "client-containers-425557bd-bf1c-4336-b0ab-5a8699eec9e8" in namespace "containers-9045" to be "Succeeded or Failed"
    Jan 18 22:03:52.919: INFO: Pod "client-containers-425557bd-bf1c-4336-b0ab-5a8699eec9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.597ms
    Jan 18 22:03:54.925: INFO: Pod "client-containers-425557bd-bf1c-4336-b0ab-5a8699eec9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012684695s
    Jan 18 22:03:56.925: INFO: Pod "client-containers-425557bd-bf1c-4336-b0ab-5a8699eec9e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012432914s
    STEP: Saw pod success 01/18/23 22:03:56.925
    Jan 18 22:03:56.925: INFO: Pod "client-containers-425557bd-bf1c-4336-b0ab-5a8699eec9e8" satisfied condition "Succeeded or Failed"
    Jan 18 22:03:56.930: INFO: Trying to get logs from node v1-25-1-18760-w2 pod client-containers-425557bd-bf1c-4336-b0ab-5a8699eec9e8 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 22:03:56.939
    Jan 18 22:03:56.960: INFO: Waiting for pod client-containers-425557bd-bf1c-4336-b0ab-5a8699eec9e8 to disappear
    Jan 18 22:03:56.965: INFO: Pod client-containers-425557bd-bf1c-4336-b0ab-5a8699eec9e8 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 18 22:03:56.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-9045" for this suite. 01/18/23 22:03:56.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:03:56.993
Jan 18 22:03:56.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename pod-network-test 01/18/23 22:03:56.995
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:03:57.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:03:57.022
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-5504 01/18/23 22:03:57.029
STEP: creating a selector 01/18/23 22:03:57.029
STEP: Creating the service pods in kubernetes 01/18/23 22:03:57.029
Jan 18 22:03:57.029: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 18 22:03:57.059: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5504" to be "running and ready"
Jan 18 22:03:57.069: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.501532ms
Jan 18 22:03:57.069: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:03:59.240: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.180029518s
Jan 18 22:03:59.240: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:04:01.076: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01596806s
Jan 18 22:04:01.076: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:04:03.079: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01979649s
Jan 18 22:04:03.079: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:04:05.087: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.027268711s
Jan 18 22:04:05.087: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:04:07.074: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.014695504s
Jan 18 22:04:07.074: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:04:09.075: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015680875s
Jan 18 22:04:09.075: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:04:11.079: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.019579271s
Jan 18 22:04:11.079: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:04:13.075: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.015676619s
Jan 18 22:04:13.075: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:04:15.075: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.015854922s
Jan 18 22:04:15.076: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:04:17.074: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014173276s
Jan 18 22:04:17.074: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:04:19.074: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014538384s
Jan 18 22:04:19.074: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 18 22:04:19.074: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 18 22:04:19.086: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5504" to be "running and ready"
Jan 18 22:04:19.091: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.775714ms
Jan 18 22:04:19.091: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 18 22:04:19.091: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/18/23 22:04:19.095
Jan 18 22:04:19.118: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5504" to be "running"
Jan 18 22:04:19.127: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.700456ms
Jan 18 22:04:21.135: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017058887s
Jan 18 22:04:21.135: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 18 22:04:21.143: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5504" to be "running"
Jan 18 22:04:21.149: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.77726ms
Jan 18 22:04:21.149: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 18 22:04:21.154: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 18 22:04:21.154: INFO: Going to poll 10.233.78.252 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 18 22:04:21.158: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.78.252:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5504 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:04:21.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 22:04:21.160: INFO: ExecWithOptions: Clientset creation
Jan 18 22:04:21.160: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5504/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.78.252%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 22:04:21.303: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 18 22:04:21.303: INFO: Going to poll 10.233.68.229 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 18 22:04:21.308: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.68.229:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5504 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:04:21.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 22:04:21.312: INFO: ExecWithOptions: Clientset creation
Jan 18 22:04:21.312: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5504/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.68.229%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 22:04:21.420: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 18 22:04:21.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5504" for this suite. 01/18/23 22:04:21.426
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":176,"skipped":3253,"failed":0}
------------------------------
• [SLOW TEST] [24.442 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:03:56.993
    Jan 18 22:03:56.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename pod-network-test 01/18/23 22:03:56.995
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:03:57.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:03:57.022
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-5504 01/18/23 22:03:57.029
    STEP: creating a selector 01/18/23 22:03:57.029
    STEP: Creating the service pods in kubernetes 01/18/23 22:03:57.029
    Jan 18 22:03:57.029: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 18 22:03:57.059: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5504" to be "running and ready"
    Jan 18 22:03:57.069: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.501532ms
    Jan 18 22:03:57.069: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:03:59.240: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.180029518s
    Jan 18 22:03:59.240: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:04:01.076: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01596806s
    Jan 18 22:04:01.076: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:04:03.079: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01979649s
    Jan 18 22:04:03.079: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:04:05.087: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.027268711s
    Jan 18 22:04:05.087: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:04:07.074: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.014695504s
    Jan 18 22:04:07.074: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:04:09.075: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015680875s
    Jan 18 22:04:09.075: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:04:11.079: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.019579271s
    Jan 18 22:04:11.079: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:04:13.075: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.015676619s
    Jan 18 22:04:13.075: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:04:15.075: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.015854922s
    Jan 18 22:04:15.076: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:04:17.074: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014173276s
    Jan 18 22:04:17.074: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:04:19.074: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014538384s
    Jan 18 22:04:19.074: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 18 22:04:19.074: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 18 22:04:19.086: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5504" to be "running and ready"
    Jan 18 22:04:19.091: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.775714ms
    Jan 18 22:04:19.091: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 18 22:04:19.091: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/18/23 22:04:19.095
    Jan 18 22:04:19.118: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5504" to be "running"
    Jan 18 22:04:19.127: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.700456ms
    Jan 18 22:04:21.135: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017058887s
    Jan 18 22:04:21.135: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 18 22:04:21.143: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5504" to be "running"
    Jan 18 22:04:21.149: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.77726ms
    Jan 18 22:04:21.149: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 18 22:04:21.154: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 18 22:04:21.154: INFO: Going to poll 10.233.78.252 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Jan 18 22:04:21.158: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.78.252:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5504 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:04:21.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 22:04:21.160: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:04:21.160: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5504/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.78.252%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 18 22:04:21.303: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 18 22:04:21.303: INFO: Going to poll 10.233.68.229 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Jan 18 22:04:21.308: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.68.229:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5504 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:04:21.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 22:04:21.312: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:04:21.312: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5504/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.68.229%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 18 22:04:21.420: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 18 22:04:21.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-5504" for this suite. 01/18/23 22:04:21.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:04:21.437
Jan 18 22:04:21.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename watch 01/18/23 22:04:21.44
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:04:21.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:04:21.467
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 01/18/23 22:04:21.471
STEP: creating a new configmap 01/18/23 22:04:21.473
STEP: modifying the configmap once 01/18/23 22:04:21.486
STEP: closing the watch once it receives two notifications 01/18/23 22:04:21.496
Jan 18 22:04:21.496: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9459  c4151c27-2e3f-4809-a988-b82118276854 151197 0 2023-01-18 22:04:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 22:04:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 22:04:21.497: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9459  c4151c27-2e3f-4809-a988-b82118276854 151198 0 2023-01-18 22:04:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 22:04:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 01/18/23 22:04:21.498
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/18/23 22:04:21.51
STEP: deleting the configmap 01/18/23 22:04:21.513
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/18/23 22:04:21.524
Jan 18 22:04:21.525: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9459  c4151c27-2e3f-4809-a988-b82118276854 151199 0 2023-01-18 22:04:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 22:04:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 22:04:21.525: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9459  c4151c27-2e3f-4809-a988-b82118276854 151200 0 2023-01-18 22:04:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 22:04:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 18 22:04:21.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9459" for this suite. 01/18/23 22:04:21.531
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":177,"skipped":3263,"failed":0}
------------------------------
• [0.101 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:04:21.437
    Jan 18 22:04:21.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename watch 01/18/23 22:04:21.44
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:04:21.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:04:21.467
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 01/18/23 22:04:21.471
    STEP: creating a new configmap 01/18/23 22:04:21.473
    STEP: modifying the configmap once 01/18/23 22:04:21.486
    STEP: closing the watch once it receives two notifications 01/18/23 22:04:21.496
    Jan 18 22:04:21.496: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9459  c4151c27-2e3f-4809-a988-b82118276854 151197 0 2023-01-18 22:04:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 22:04:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 22:04:21.497: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9459  c4151c27-2e3f-4809-a988-b82118276854 151198 0 2023-01-18 22:04:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 22:04:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 01/18/23 22:04:21.498
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/18/23 22:04:21.51
    STEP: deleting the configmap 01/18/23 22:04:21.513
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/18/23 22:04:21.524
    Jan 18 22:04:21.525: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9459  c4151c27-2e3f-4809-a988-b82118276854 151199 0 2023-01-18 22:04:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 22:04:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 22:04:21.525: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9459  c4151c27-2e3f-4809-a988-b82118276854 151200 0 2023-01-18 22:04:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 22:04:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 18 22:04:21.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-9459" for this suite. 01/18/23 22:04:21.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:04:21.543
Jan 18 22:04:21.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename svcaccounts 01/18/23 22:04:21.544
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:04:21.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:04:21.571
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Jan 18 22:04:21.579: INFO: Got root ca configmap in namespace "svcaccounts-3536"
Jan 18 22:04:21.593: INFO: Deleted root ca configmap in namespace "svcaccounts-3536"
STEP: waiting for a new root ca configmap created 01/18/23 22:04:22.094
Jan 18 22:04:22.098: INFO: Recreated root ca configmap in namespace "svcaccounts-3536"
Jan 18 22:04:22.105: INFO: Updated root ca configmap in namespace "svcaccounts-3536"
STEP: waiting for the root ca configmap reconciled 01/18/23 22:04:22.606
Jan 18 22:04:22.611: INFO: Reconciled root ca configmap in namespace "svcaccounts-3536"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 18 22:04:22.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3536" for this suite. 01/18/23 22:04:22.62
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":178,"skipped":3299,"failed":0}
------------------------------
• [1.084 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:04:21.543
    Jan 18 22:04:21.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 22:04:21.544
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:04:21.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:04:21.571
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Jan 18 22:04:21.579: INFO: Got root ca configmap in namespace "svcaccounts-3536"
    Jan 18 22:04:21.593: INFO: Deleted root ca configmap in namespace "svcaccounts-3536"
    STEP: waiting for a new root ca configmap created 01/18/23 22:04:22.094
    Jan 18 22:04:22.098: INFO: Recreated root ca configmap in namespace "svcaccounts-3536"
    Jan 18 22:04:22.105: INFO: Updated root ca configmap in namespace "svcaccounts-3536"
    STEP: waiting for the root ca configmap reconciled 01/18/23 22:04:22.606
    Jan 18 22:04:22.611: INFO: Reconciled root ca configmap in namespace "svcaccounts-3536"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 18 22:04:22.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-3536" for this suite. 01/18/23 22:04:22.62
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:04:22.631
Jan 18 22:04:22.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename webhook 01/18/23 22:04:22.632
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:04:22.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:04:22.661
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 22:04:22.683
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:04:23.491
STEP: Deploying the webhook pod 01/18/23 22:04:23.501
STEP: Wait for the deployment to be ready 01/18/23 22:04:23.52
Jan 18 22:04:23.529: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 18 22:04:25.561: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 4, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 4, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 4, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 4, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/18/23 22:04:27.57
STEP: Verifying the service has paired with the endpoint 01/18/23 22:04:27.585
Jan 18 22:04:28.586: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Jan 18 22:04:28.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-266-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 22:04:29.104
STEP: Creating a custom resource that should be mutated by the webhook 01/18/23 22:04:29.13
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 22:04:31.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3522" for this suite. 01/18/23 22:04:31.762
STEP: Destroying namespace "webhook-3522-markers" for this suite. 01/18/23 22:04:31.772
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":179,"skipped":3329,"failed":0}
------------------------------
• [SLOW TEST] [9.255 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:04:22.631
    Jan 18 22:04:22.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename webhook 01/18/23 22:04:22.632
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:04:22.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:04:22.661
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 22:04:22.683
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:04:23.491
    STEP: Deploying the webhook pod 01/18/23 22:04:23.501
    STEP: Wait for the deployment to be ready 01/18/23 22:04:23.52
    Jan 18 22:04:23.529: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 18 22:04:25.561: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 4, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 4, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 4, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 4, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/18/23 22:04:27.57
    STEP: Verifying the service has paired with the endpoint 01/18/23 22:04:27.585
    Jan 18 22:04:28.586: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Jan 18 22:04:28.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-266-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 22:04:29.104
    STEP: Creating a custom resource that should be mutated by the webhook 01/18/23 22:04:29.13
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 22:04:31.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3522" for this suite. 01/18/23 22:04:31.762
    STEP: Destroying namespace "webhook-3522-markers" for this suite. 01/18/23 22:04:31.772
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:04:31.913
Jan 18 22:04:31.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename services 01/18/23 22:04:31.916
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:04:31.943
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:04:31.955
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-484 01/18/23 22:04:31.963
STEP: creating service affinity-clusterip in namespace services-484 01/18/23 22:04:31.964
STEP: creating replication controller affinity-clusterip in namespace services-484 01/18/23 22:04:31.992
I0118 22:04:32.014281      19 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-484, replica count: 3
I0118 22:04:35.069416      19 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 22:04:35.080: INFO: Creating new exec pod
Jan 18 22:04:35.088: INFO: Waiting up to 5m0s for pod "execpod-affinitycjdkf" in namespace "services-484" to be "running"
Jan 18 22:04:35.097: INFO: Pod "execpod-affinitycjdkf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.990205ms
Jan 18 22:04:37.109: INFO: Pod "execpod-affinitycjdkf": Phase="Running", Reason="", readiness=true. Elapsed: 2.020288235s
Jan 18 22:04:37.110: INFO: Pod "execpod-affinitycjdkf" satisfied condition "running"
Jan 18 22:04:38.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-484 exec execpod-affinitycjdkf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jan 18 22:04:38.335: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Jan 18 22:04:38.335: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 22:04:38.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-484 exec execpod-affinitycjdkf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.59.181 80'
Jan 18 22:04:38.547: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.59.181 80\nConnection to 10.233.59.181 80 port [tcp/http] succeeded!\n"
Jan 18 22:04:38.548: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 22:04:38.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-484 exec execpod-affinitycjdkf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.59.181:80/ ; done'
Jan 18 22:04:38.986: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n"
Jan 18 22:04:38.986: INFO: stdout: "\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7"
Jan 18 22:04:38.986: INFO: Received response from host: affinity-clusterip-rrjh7
Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
Jan 18 22:04:38.987: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-484, will wait for the garbage collector to delete the pods 01/18/23 22:04:39.014
Jan 18 22:04:39.086: INFO: Deleting ReplicationController affinity-clusterip took: 13.822646ms
Jan 18 22:04:39.187: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.423577ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 22:04:41.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-484" for this suite. 01/18/23 22:04:41.93
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":180,"skipped":3335,"failed":0}
------------------------------
• [SLOW TEST] [10.030 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:04:31.913
    Jan 18 22:04:31.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename services 01/18/23 22:04:31.916
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:04:31.943
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:04:31.955
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-484 01/18/23 22:04:31.963
    STEP: creating service affinity-clusterip in namespace services-484 01/18/23 22:04:31.964
    STEP: creating replication controller affinity-clusterip in namespace services-484 01/18/23 22:04:31.992
    I0118 22:04:32.014281      19 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-484, replica count: 3
    I0118 22:04:35.069416      19 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 22:04:35.080: INFO: Creating new exec pod
    Jan 18 22:04:35.088: INFO: Waiting up to 5m0s for pod "execpod-affinitycjdkf" in namespace "services-484" to be "running"
    Jan 18 22:04:35.097: INFO: Pod "execpod-affinitycjdkf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.990205ms
    Jan 18 22:04:37.109: INFO: Pod "execpod-affinitycjdkf": Phase="Running", Reason="", readiness=true. Elapsed: 2.020288235s
    Jan 18 22:04:37.110: INFO: Pod "execpod-affinitycjdkf" satisfied condition "running"
    Jan 18 22:04:38.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-484 exec execpod-affinitycjdkf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Jan 18 22:04:38.335: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n+ echo hostName\n"
    Jan 18 22:04:38.335: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 22:04:38.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-484 exec execpod-affinitycjdkf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.59.181 80'
    Jan 18 22:04:38.547: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.59.181 80\nConnection to 10.233.59.181 80 port [tcp/http] succeeded!\n"
    Jan 18 22:04:38.548: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 22:04:38.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-484 exec execpod-affinitycjdkf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.59.181:80/ ; done'
    Jan 18 22:04:38.986: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.181:80/\n"
    Jan 18 22:04:38.986: INFO: stdout: "\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7\naffinity-clusterip-rrjh7"
    Jan 18 22:04:38.986: INFO: Received response from host: affinity-clusterip-rrjh7
    Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
    Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
    Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
    Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
    Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
    Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
    Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
    Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
    Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
    Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
    Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
    Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
    Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
    Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
    Jan 18 22:04:38.987: INFO: Received response from host: affinity-clusterip-rrjh7
    Jan 18 22:04:38.987: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-484, will wait for the garbage collector to delete the pods 01/18/23 22:04:39.014
    Jan 18 22:04:39.086: INFO: Deleting ReplicationController affinity-clusterip took: 13.822646ms
    Jan 18 22:04:39.187: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.423577ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 22:04:41.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-484" for this suite. 01/18/23 22:04:41.93
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:04:41.957
Jan 18 22:04:41.957: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename deployment 01/18/23 22:04:41.96
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:04:41.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:04:41.999
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jan 18 22:04:42.006: INFO: Creating deployment "test-recreate-deployment"
Jan 18 22:04:42.019: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 18 22:04:42.039: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan 18 22:04:44.049: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 18 22:04:44.053: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 18 22:04:44.063: INFO: Updating deployment test-recreate-deployment
Jan 18 22:04:44.063: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 22:04:44.444: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-1896  1b9df8a8-1336-4a6d-b094-838a39e5930b 151584 2 2023-01-18 22:04:42 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 22:04:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:04:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001a09fa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-18 22:04:44 +0000 UTC,LastTransitionTime:2023-01-18 22:04:44 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-01-18 22:04:44 +0000 UTC,LastTransitionTime:2023-01-18 22:04:42 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 18 22:04:44.455: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-1896  c58660ab-307a-4238-8df2-f354e5a7c181 151582 1 2023-01-18 22:04:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 1b9df8a8-1336-4a6d-b094-838a39e5930b 0xc004a26470 0xc004a26471}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:04:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b9df8a8-1336-4a6d-b094-838a39e5930b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:04:44 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a26508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 22:04:44.455: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 18 22:04:44.455: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-1896  19ca5671-15ea-475b-86a0-b7087c075755 151574 2 2023-01-18 22:04:42 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 1b9df8a8-1336-4a6d-b094-838a39e5930b 0xc004a26357 0xc004a26358}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:04:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b9df8a8-1336-4a6d-b094-838a39e5930b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:04:44 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a26408 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 22:04:44.462: INFO: Pod "test-recreate-deployment-9d58999df-8pkvn" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-8pkvn test-recreate-deployment-9d58999df- deployment-1896  6d60b715-4bd2-4fbf-a92d-dc3b06c32eee 151585 0 2023-01-18 22:04:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df c58660ab-307a-4238-8df2-f354e5a7c181 0xc003532390 0xc003532391}] [] [{kube-controller-manager Update v1 2023-01-18 22:04:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c58660ab-307a-4238-8df2-f354e5a7c181\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:04:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l8jcl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l8jcl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:04:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:04:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:04:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:04:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:,StartTime:2023-01-18 22:04:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 18 22:04:44.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1896" for this suite. 01/18/23 22:04:44.467
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":181,"skipped":3358,"failed":0}
------------------------------
• [2.518 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:04:41.957
    Jan 18 22:04:41.957: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename deployment 01/18/23 22:04:41.96
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:04:41.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:04:41.999
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jan 18 22:04:42.006: INFO: Creating deployment "test-recreate-deployment"
    Jan 18 22:04:42.019: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jan 18 22:04:42.039: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Jan 18 22:04:44.049: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jan 18 22:04:44.053: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jan 18 22:04:44.063: INFO: Updating deployment test-recreate-deployment
    Jan 18 22:04:44.063: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 22:04:44.444: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-1896  1b9df8a8-1336-4a6d-b094-838a39e5930b 151584 2 2023-01-18 22:04:42 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 22:04:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:04:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001a09fa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-18 22:04:44 +0000 UTC,LastTransitionTime:2023-01-18 22:04:44 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-01-18 22:04:44 +0000 UTC,LastTransitionTime:2023-01-18 22:04:42 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 18 22:04:44.455: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-1896  c58660ab-307a-4238-8df2-f354e5a7c181 151582 1 2023-01-18 22:04:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 1b9df8a8-1336-4a6d-b094-838a39e5930b 0xc004a26470 0xc004a26471}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:04:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b9df8a8-1336-4a6d-b094-838a39e5930b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:04:44 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a26508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 22:04:44.455: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jan 18 22:04:44.455: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-1896  19ca5671-15ea-475b-86a0-b7087c075755 151574 2 2023-01-18 22:04:42 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 1b9df8a8-1336-4a6d-b094-838a39e5930b 0xc004a26357 0xc004a26358}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:04:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b9df8a8-1336-4a6d-b094-838a39e5930b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:04:44 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a26408 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 22:04:44.462: INFO: Pod "test-recreate-deployment-9d58999df-8pkvn" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-8pkvn test-recreate-deployment-9d58999df- deployment-1896  6d60b715-4bd2-4fbf-a92d-dc3b06c32eee 151585 0 2023-01-18 22:04:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df c58660ab-307a-4238-8df2-f354e5a7c181 0xc003532390 0xc003532391}] [] [{kube-controller-manager Update v1 2023-01-18 22:04:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c58660ab-307a-4238-8df2-f354e5a7c181\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:04:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l8jcl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l8jcl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:04:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:04:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:04:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:04:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:,StartTime:2023-01-18 22:04:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 18 22:04:44.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1896" for this suite. 01/18/23 22:04:44.467
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:04:44.483
Jan 18 22:04:44.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename pods 01/18/23 22:04:44.484
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:04:44.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:04:44.511
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 01/18/23 22:04:44.529
STEP: watching for Pod to be ready 01/18/23 22:04:44.541
Jan 18 22:04:44.544: INFO: observed Pod pod-test in namespace pods-3972 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan 18 22:04:44.549: INFO: observed Pod pod-test in namespace pods-3972 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:04:44 +0000 UTC  }]
Jan 18 22:04:44.692: INFO: observed Pod pod-test in namespace pods-3972 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:04:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:04:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:04:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:04:44 +0000 UTC  }]
Jan 18 22:04:45.445: INFO: observed Pod pod-test in namespace pods-3972 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:04:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:04:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:04:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:04:44 +0000 UTC  }]
Jan 18 22:04:46.963: INFO: Found Pod pod-test in namespace pods-3972 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:04:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:04:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:04:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:04:44 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 01/18/23 22:04:46.978
STEP: getting the Pod and ensuring that it's patched 01/18/23 22:04:47.003
STEP: replacing the Pod's status Ready condition to False 01/18/23 22:04:47.013
STEP: check the Pod again to ensure its Ready conditions are False 01/18/23 22:04:47.036
STEP: deleting the Pod via a Collection with a LabelSelector 01/18/23 22:04:47.036
STEP: watching for the Pod to be deleted 01/18/23 22:04:47.27
Jan 18 22:04:47.274: INFO: observed event type MODIFIED
Jan 18 22:04:48.970: INFO: observed event type MODIFIED
Jan 18 22:04:49.311: INFO: observed event type MODIFIED
Jan 18 22:04:49.983: INFO: observed event type MODIFIED
Jan 18 22:04:50.000: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 22:04:50.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3972" for this suite. 01/18/23 22:04:50.025
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":182,"skipped":3377,"failed":0}
------------------------------
• [SLOW TEST] [5.550 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:04:44.483
    Jan 18 22:04:44.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename pods 01/18/23 22:04:44.484
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:04:44.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:04:44.511
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 01/18/23 22:04:44.529
    STEP: watching for Pod to be ready 01/18/23 22:04:44.541
    Jan 18 22:04:44.544: INFO: observed Pod pod-test in namespace pods-3972 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jan 18 22:04:44.549: INFO: observed Pod pod-test in namespace pods-3972 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:04:44 +0000 UTC  }]
    Jan 18 22:04:44.692: INFO: observed Pod pod-test in namespace pods-3972 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:04:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:04:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:04:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:04:44 +0000 UTC  }]
    Jan 18 22:04:45.445: INFO: observed Pod pod-test in namespace pods-3972 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:04:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:04:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:04:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:04:44 +0000 UTC  }]
    Jan 18 22:04:46.963: INFO: Found Pod pod-test in namespace pods-3972 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:04:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:04:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:04:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:04:44 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 01/18/23 22:04:46.978
    STEP: getting the Pod and ensuring that it's patched 01/18/23 22:04:47.003
    STEP: replacing the Pod's status Ready condition to False 01/18/23 22:04:47.013
    STEP: check the Pod again to ensure its Ready conditions are False 01/18/23 22:04:47.036
    STEP: deleting the Pod via a Collection with a LabelSelector 01/18/23 22:04:47.036
    STEP: watching for the Pod to be deleted 01/18/23 22:04:47.27
    Jan 18 22:04:47.274: INFO: observed event type MODIFIED
    Jan 18 22:04:48.970: INFO: observed event type MODIFIED
    Jan 18 22:04:49.311: INFO: observed event type MODIFIED
    Jan 18 22:04:49.983: INFO: observed event type MODIFIED
    Jan 18 22:04:50.000: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 22:04:50.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3972" for this suite. 01/18/23 22:04:50.025
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:04:50.038
Jan 18 22:04:50.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename replication-controller 01/18/23 22:04:50.04
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:04:50.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:04:50.073
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 01/18/23 22:04:50.076
Jan 18 22:04:50.084: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-2962" to be "running and ready"
Jan 18 22:04:50.094: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 10.352621ms
Jan 18 22:04:50.094: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:04:52.101: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.01733107s
Jan 18 22:04:52.101: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jan 18 22:04:52.101: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 01/18/23 22:04:52.108
STEP: Then the orphan pod is adopted 01/18/23 22:04:52.116
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 18 22:04:53.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2962" for this suite. 01/18/23 22:04:53.141
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":183,"skipped":3394,"failed":0}
------------------------------
• [3.115 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:04:50.038
    Jan 18 22:04:50.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename replication-controller 01/18/23 22:04:50.04
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:04:50.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:04:50.073
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 01/18/23 22:04:50.076
    Jan 18 22:04:50.084: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-2962" to be "running and ready"
    Jan 18 22:04:50.094: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 10.352621ms
    Jan 18 22:04:50.094: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:04:52.101: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.01733107s
    Jan 18 22:04:52.101: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jan 18 22:04:52.101: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 01/18/23 22:04:52.108
    STEP: Then the orphan pod is adopted 01/18/23 22:04:52.116
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 18 22:04:53.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-2962" for this suite. 01/18/23 22:04:53.141
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:04:53.155
Jan 18 22:04:53.155: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename sched-preemption 01/18/23 22:04:53.158
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:04:53.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:04:53.196
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 18 22:04:53.216: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 22:05:53.272: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:05:53.278
Jan 18 22:05:53.278: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename sched-preemption-path 01/18/23 22:05:53.281
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:05:53.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:05:53.321
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 01/18/23 22:05:53.326
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 22:05:53.326
Jan 18 22:05:53.343: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-6296" to be "running"
Jan 18 22:05:53.352: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 9.765676ms
Jan 18 22:05:55.358: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.015804974s
Jan 18 22:05:55.359: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 22:05:55.364
Jan 18 22:05:55.384: INFO: found a healthy node: v1-25-1-18760-w2
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Jan 18 22:06:01.502: INFO: pods created so far: [1 1 1]
Jan 18 22:06:01.503: INFO: length of pods created so far: 3
Jan 18 22:06:07.516: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Jan 18 22:06:14.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-6296" for this suite. 01/18/23 22:06:14.526
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 18 22:06:14.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-255" for this suite. 01/18/23 22:06:14.629
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":184,"skipped":3395,"failed":0}
------------------------------
• [SLOW TEST] [81.547 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:04:53.155
    Jan 18 22:04:53.155: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename sched-preemption 01/18/23 22:04:53.158
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:04:53.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:04:53.196
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 18 22:04:53.216: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 22:05:53.272: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:05:53.278
    Jan 18 22:05:53.278: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename sched-preemption-path 01/18/23 22:05:53.281
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:05:53.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:05:53.321
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 01/18/23 22:05:53.326
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 22:05:53.326
    Jan 18 22:05:53.343: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-6296" to be "running"
    Jan 18 22:05:53.352: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 9.765676ms
    Jan 18 22:05:55.358: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.015804974s
    Jan 18 22:05:55.359: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 22:05:55.364
    Jan 18 22:05:55.384: INFO: found a healthy node: v1-25-1-18760-w2
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Jan 18 22:06:01.502: INFO: pods created so far: [1 1 1]
    Jan 18 22:06:01.503: INFO: length of pods created so far: 3
    Jan 18 22:06:07.516: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Jan 18 22:06:14.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-6296" for this suite. 01/18/23 22:06:14.526
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 22:06:14.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-255" for this suite. 01/18/23 22:06:14.629
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:06:14.709
Jan 18 22:06:14.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 22:06:14.711
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:14.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:14.751
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 01/18/23 22:06:14.756
Jan 18 22:06:14.771: INFO: Waiting up to 5m0s for pod "annotationupdatef05c461c-ce6c-47f1-aad8-325aaa7292f7" in namespace "projected-3743" to be "running and ready"
Jan 18 22:06:14.815: INFO: Pod "annotationupdatef05c461c-ce6c-47f1-aad8-325aaa7292f7": Phase="Pending", Reason="", readiness=false. Elapsed: 43.426771ms
Jan 18 22:06:14.815: INFO: The phase of Pod annotationupdatef05c461c-ce6c-47f1-aad8-325aaa7292f7 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:06:16.835: INFO: Pod "annotationupdatef05c461c-ce6c-47f1-aad8-325aaa7292f7": Phase="Running", Reason="", readiness=true. Elapsed: 2.064082479s
Jan 18 22:06:16.835: INFO: The phase of Pod annotationupdatef05c461c-ce6c-47f1-aad8-325aaa7292f7 is Running (Ready = true)
Jan 18 22:06:16.836: INFO: Pod "annotationupdatef05c461c-ce6c-47f1-aad8-325aaa7292f7" satisfied condition "running and ready"
Jan 18 22:06:17.383: INFO: Successfully updated pod "annotationupdatef05c461c-ce6c-47f1-aad8-325aaa7292f7"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 22:06:21.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3743" for this suite. 01/18/23 22:06:21.458
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":185,"skipped":3414,"failed":0}
------------------------------
• [SLOW TEST] [6.766 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:06:14.709
    Jan 18 22:06:14.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 22:06:14.711
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:14.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:14.751
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 01/18/23 22:06:14.756
    Jan 18 22:06:14.771: INFO: Waiting up to 5m0s for pod "annotationupdatef05c461c-ce6c-47f1-aad8-325aaa7292f7" in namespace "projected-3743" to be "running and ready"
    Jan 18 22:06:14.815: INFO: Pod "annotationupdatef05c461c-ce6c-47f1-aad8-325aaa7292f7": Phase="Pending", Reason="", readiness=false. Elapsed: 43.426771ms
    Jan 18 22:06:14.815: INFO: The phase of Pod annotationupdatef05c461c-ce6c-47f1-aad8-325aaa7292f7 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:06:16.835: INFO: Pod "annotationupdatef05c461c-ce6c-47f1-aad8-325aaa7292f7": Phase="Running", Reason="", readiness=true. Elapsed: 2.064082479s
    Jan 18 22:06:16.835: INFO: The phase of Pod annotationupdatef05c461c-ce6c-47f1-aad8-325aaa7292f7 is Running (Ready = true)
    Jan 18 22:06:16.836: INFO: Pod "annotationupdatef05c461c-ce6c-47f1-aad8-325aaa7292f7" satisfied condition "running and ready"
    Jan 18 22:06:17.383: INFO: Successfully updated pod "annotationupdatef05c461c-ce6c-47f1-aad8-325aaa7292f7"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 22:06:21.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3743" for this suite. 01/18/23 22:06:21.458
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:06:21.477
Jan 18 22:06:21.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename deployment 01/18/23 22:06:21.48
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:21.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:21.576
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 01/18/23 22:06:21.585
STEP: waiting for Deployment to be created 01/18/23 22:06:21.59
STEP: waiting for all Replicas to be Ready 01/18/23 22:06:21.594
Jan 18 22:06:21.599: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 22:06:21.599: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 22:06:21.610: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 22:06:21.610: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 22:06:21.636: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 22:06:21.636: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 22:06:21.673: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 22:06:21.673: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 22:06:23.047: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 18 22:06:23.047: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 18 22:06:23.419: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 01/18/23 22:06:23.419
W0118 22:06:23.430265      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 18 22:06:23.437: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 01/18/23 22:06:23.437
Jan 18 22:06:23.440: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0
Jan 18 22:06:23.441: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0
Jan 18 22:06:23.441: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0
Jan 18 22:06:23.441: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0
Jan 18 22:06:23.441: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0
Jan 18 22:06:23.441: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0
Jan 18 22:06:23.441: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0
Jan 18 22:06:23.441: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0
Jan 18 22:06:23.442: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1
Jan 18 22:06:23.442: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1
Jan 18 22:06:23.442: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2
Jan 18 22:06:23.442: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2
Jan 18 22:06:23.442: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2
Jan 18 22:06:23.442: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2
Jan 18 22:06:23.452: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2
Jan 18 22:06:23.452: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2
Jan 18 22:06:23.484: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2
Jan 18 22:06:23.484: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2
Jan 18 22:06:23.525: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1
Jan 18 22:06:23.525: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1
Jan 18 22:06:23.574: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1
Jan 18 22:06:23.574: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1
Jan 18 22:06:25.527: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2
Jan 18 22:06:25.527: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2
Jan 18 22:06:25.782: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1
STEP: listing Deployments 01/18/23 22:06:25.783
Jan 18 22:06:25.790: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 01/18/23 22:06:25.79
Jan 18 22:06:25.822: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 01/18/23 22:06:25.822
Jan 18 22:06:25.842: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 22:06:25.842: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 22:06:26.083: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 22:06:26.137: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 22:06:26.161: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 22:06:28.065: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 22:06:28.528: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 22:06:28.646: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 22:06:28.656: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 22:06:30.131: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 01/18/23 22:06:30.173
STEP: fetching the DeploymentStatus 01/18/23 22:06:30.184
Jan 18 22:06:30.204: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1
Jan 18 22:06:30.204: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1
Jan 18 22:06:30.204: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1
Jan 18 22:06:30.204: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1
Jan 18 22:06:30.204: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1
Jan 18 22:06:30.204: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2
Jan 18 22:06:30.205: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 3
Jan 18 22:06:30.205: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2
Jan 18 22:06:30.205: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2
Jan 18 22:06:30.205: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 3
STEP: deleting the Deployment 01/18/23 22:06:30.205
Jan 18 22:06:30.220: INFO: observed event type MODIFIED
Jan 18 22:06:30.220: INFO: observed event type MODIFIED
Jan 18 22:06:30.220: INFO: observed event type MODIFIED
Jan 18 22:06:30.221: INFO: observed event type MODIFIED
Jan 18 22:06:30.221: INFO: observed event type MODIFIED
Jan 18 22:06:30.221: INFO: observed event type MODIFIED
Jan 18 22:06:30.221: INFO: observed event type MODIFIED
Jan 18 22:06:30.222: INFO: observed event type MODIFIED
Jan 18 22:06:30.222: INFO: observed event type MODIFIED
Jan 18 22:06:30.222: INFO: observed event type MODIFIED
Jan 18 22:06:30.222: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 22:06:30.228: INFO: Log out all the ReplicaSets if there is no deployment created
Jan 18 22:06:30.236: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-2360  969f8959-b7a7-4d2a-ba65-56074def774b 152518 4 2023-01-18 22:06:23 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 1a526544-7717-4c27-8596-1616aea152b1 0xc000afb267 0xc000afb268}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a526544-7717-4c27-8596-1616aea152b1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:06:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000afb2f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 18 22:06:30.241: INFO: pod: "test-deployment-54cc775c4b-9j44t":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-9j44t test-deployment-54cc775c4b- deployment-2360  f81b21cb-0eb9-4549-b12b-87461eee9ea0 152513 0 2023-01-18 22:06:25 +0000 UTC 2023-01-18 22:06:31 +0000 UTC 0xc0036aa838 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:486409e0d88c8c4b8021e462dbd8077a6ef821b93d897746efcdeebc5bfcbb79 cni.projectcalico.org/podIP:10.233.78.255/32 cni.projectcalico.org/podIPs:10.233.78.255/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 969f8959-b7a7-4d2a-ba65-56074def774b 0xc0036aa887 0xc0036aa888}] [] [{kube-controller-manager Update v1 2023-01-18 22:06:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"969f8959-b7a7-4d2a-ba65-56074def774b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:06:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 22:06:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.255\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k5vxt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k5vxt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:06:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:06:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:06:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:06:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:10.233.78.255,StartTime:2023-01-18 22:06:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:06:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://e4130304f976d4ba535ca57c70281c9f61d59971ff06561ad80ebc77a6402f9d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.255,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 18 22:06:30.241: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-2360  98bfd2c8-a43b-45df-8f3f-ab8a8042009e 152507 2 2023-01-18 22:06:25 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 1a526544-7717-4c27-8596-1616aea152b1 0xc000afb357 0xc000afb358}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:06:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a526544-7717-4c27-8596-1616aea152b1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:06:30 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000afb3e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jan 18 22:06:30.249: INFO: pod: "test-deployment-7c7d8d58c8-cl6f2":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-cl6f2 test-deployment-7c7d8d58c8- deployment-2360  64275381-54ee-43c8-bc94-a645122dea4a 152506 0 2023-01-18 22:06:28 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:b9ee262db3f54cb75b4a0490e271db79c39e946a1959ee1ccc0fe80fc2381286 cni.projectcalico.org/podIP:10.233.78.7/32 cni.projectcalico.org/podIPs:10.233.78.7/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 98bfd2c8-a43b-45df-8f3f-ab8a8042009e 0xc000afb7c7 0xc000afb7c8}] [] [{kube-controller-manager Update v1 2023-01-18 22:06:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"98bfd2c8-a43b-45df-8f3f-ab8a8042009e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:06:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 22:06:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-shdwh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-shdwh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:06:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:06:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:06:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:06:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:10.233.78.7,StartTime:2023-01-18 22:06:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:06:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0b2f1cf761e477061adeedd7d6f11bf87367b5dd7b67321e70004283daf4f524,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 18 22:06:30.250: INFO: pod: "test-deployment-7c7d8d58c8-lsmpj":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-lsmpj test-deployment-7c7d8d58c8- deployment-2360  74e9012c-259d-4767-93e9-82367e59b802 152465 0 2023-01-18 22:06:26 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:62bf758a623bd379f737825f5625532068c963727404ad145c84b4f226c96755 cni.projectcalico.org/podIP:10.233.68.211/32 cni.projectcalico.org/podIPs:10.233.68.211/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 98bfd2c8-a43b-45df-8f3f-ab8a8042009e 0xc000afba07 0xc000afba08}] [] [{calico Update v1 2023-01-18 22:06:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-18 22:06:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"98bfd2c8-a43b-45df-8f3f-ab8a8042009e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:06:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.211\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s9ck5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s9ck5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:06:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:06:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:06:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:06:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.211,StartTime:2023-01-18 22:06:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:06:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2ab9cd2078f545eed7e749fde0dacc7d15cbcea3533280ed73f8d881f7420853,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.211,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 18 22:06:30.257: INFO: ReplicaSet "test-deployment-8594bb6fdd":
&ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-2360  0745af5b-56ea-49e2-83a5-11be7a82469c 152388 3 2023-01-18 22:06:21 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 1a526544-7717-4c27-8596-1616aea152b1 0xc000afb447 0xc000afb448}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:06:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a526544-7717-4c27-8596-1616aea152b1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:06:25 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000afb4d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 18 22:06:30.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2360" for this suite. 01/18/23 22:06:30.279
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":186,"skipped":3417,"failed":0}
------------------------------
• [SLOW TEST] [8.817 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:06:21.477
    Jan 18 22:06:21.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename deployment 01/18/23 22:06:21.48
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:21.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:21.576
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 01/18/23 22:06:21.585
    STEP: waiting for Deployment to be created 01/18/23 22:06:21.59
    STEP: waiting for all Replicas to be Ready 01/18/23 22:06:21.594
    Jan 18 22:06:21.599: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 22:06:21.599: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 22:06:21.610: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 22:06:21.610: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 22:06:21.636: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 22:06:21.636: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 22:06:21.673: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 22:06:21.673: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 22:06:23.047: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 18 22:06:23.047: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 18 22:06:23.419: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 01/18/23 22:06:23.419
    W0118 22:06:23.430265      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 18 22:06:23.437: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 01/18/23 22:06:23.437
    Jan 18 22:06:23.440: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0
    Jan 18 22:06:23.441: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0
    Jan 18 22:06:23.441: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0
    Jan 18 22:06:23.441: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0
    Jan 18 22:06:23.441: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0
    Jan 18 22:06:23.441: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0
    Jan 18 22:06:23.441: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0
    Jan 18 22:06:23.441: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 0
    Jan 18 22:06:23.442: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1
    Jan 18 22:06:23.442: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1
    Jan 18 22:06:23.442: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2
    Jan 18 22:06:23.442: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2
    Jan 18 22:06:23.442: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2
    Jan 18 22:06:23.442: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2
    Jan 18 22:06:23.452: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2
    Jan 18 22:06:23.452: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2
    Jan 18 22:06:23.484: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2
    Jan 18 22:06:23.484: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2
    Jan 18 22:06:23.525: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1
    Jan 18 22:06:23.525: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1
    Jan 18 22:06:23.574: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1
    Jan 18 22:06:23.574: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1
    Jan 18 22:06:25.527: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2
    Jan 18 22:06:25.527: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2
    Jan 18 22:06:25.782: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1
    STEP: listing Deployments 01/18/23 22:06:25.783
    Jan 18 22:06:25.790: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 01/18/23 22:06:25.79
    Jan 18 22:06:25.822: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 01/18/23 22:06:25.822
    Jan 18 22:06:25.842: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 22:06:25.842: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 22:06:26.083: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 22:06:26.137: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 22:06:26.161: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 22:06:28.065: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 22:06:28.528: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 22:06:28.646: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 22:06:28.656: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 22:06:30.131: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 01/18/23 22:06:30.173
    STEP: fetching the DeploymentStatus 01/18/23 22:06:30.184
    Jan 18 22:06:30.204: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1
    Jan 18 22:06:30.204: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1
    Jan 18 22:06:30.204: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1
    Jan 18 22:06:30.204: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1
    Jan 18 22:06:30.204: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 1
    Jan 18 22:06:30.204: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2
    Jan 18 22:06:30.205: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 3
    Jan 18 22:06:30.205: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2
    Jan 18 22:06:30.205: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 2
    Jan 18 22:06:30.205: INFO: observed Deployment test-deployment in namespace deployment-2360 with ReadyReplicas 3
    STEP: deleting the Deployment 01/18/23 22:06:30.205
    Jan 18 22:06:30.220: INFO: observed event type MODIFIED
    Jan 18 22:06:30.220: INFO: observed event type MODIFIED
    Jan 18 22:06:30.220: INFO: observed event type MODIFIED
    Jan 18 22:06:30.221: INFO: observed event type MODIFIED
    Jan 18 22:06:30.221: INFO: observed event type MODIFIED
    Jan 18 22:06:30.221: INFO: observed event type MODIFIED
    Jan 18 22:06:30.221: INFO: observed event type MODIFIED
    Jan 18 22:06:30.222: INFO: observed event type MODIFIED
    Jan 18 22:06:30.222: INFO: observed event type MODIFIED
    Jan 18 22:06:30.222: INFO: observed event type MODIFIED
    Jan 18 22:06:30.222: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 22:06:30.228: INFO: Log out all the ReplicaSets if there is no deployment created
    Jan 18 22:06:30.236: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-2360  969f8959-b7a7-4d2a-ba65-56074def774b 152518 4 2023-01-18 22:06:23 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 1a526544-7717-4c27-8596-1616aea152b1 0xc000afb267 0xc000afb268}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a526544-7717-4c27-8596-1616aea152b1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:06:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000afb2f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Jan 18 22:06:30.241: INFO: pod: "test-deployment-54cc775c4b-9j44t":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-9j44t test-deployment-54cc775c4b- deployment-2360  f81b21cb-0eb9-4549-b12b-87461eee9ea0 152513 0 2023-01-18 22:06:25 +0000 UTC 2023-01-18 22:06:31 +0000 UTC 0xc0036aa838 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:486409e0d88c8c4b8021e462dbd8077a6ef821b93d897746efcdeebc5bfcbb79 cni.projectcalico.org/podIP:10.233.78.255/32 cni.projectcalico.org/podIPs:10.233.78.255/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 969f8959-b7a7-4d2a-ba65-56074def774b 0xc0036aa887 0xc0036aa888}] [] [{kube-controller-manager Update v1 2023-01-18 22:06:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"969f8959-b7a7-4d2a-ba65-56074def774b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:06:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 22:06:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.255\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k5vxt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k5vxt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:06:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:06:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:06:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:06:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:10.233.78.255,StartTime:2023-01-18 22:06:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:06:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://e4130304f976d4ba535ca57c70281c9f61d59971ff06561ad80ebc77a6402f9d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.255,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 18 22:06:30.241: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-2360  98bfd2c8-a43b-45df-8f3f-ab8a8042009e 152507 2 2023-01-18 22:06:25 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 1a526544-7717-4c27-8596-1616aea152b1 0xc000afb357 0xc000afb358}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:06:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a526544-7717-4c27-8596-1616aea152b1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:06:30 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000afb3e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Jan 18 22:06:30.249: INFO: pod: "test-deployment-7c7d8d58c8-cl6f2":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-cl6f2 test-deployment-7c7d8d58c8- deployment-2360  64275381-54ee-43c8-bc94-a645122dea4a 152506 0 2023-01-18 22:06:28 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:b9ee262db3f54cb75b4a0490e271db79c39e946a1959ee1ccc0fe80fc2381286 cni.projectcalico.org/podIP:10.233.78.7/32 cni.projectcalico.org/podIPs:10.233.78.7/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 98bfd2c8-a43b-45df-8f3f-ab8a8042009e 0xc000afb7c7 0xc000afb7c8}] [] [{kube-controller-manager Update v1 2023-01-18 22:06:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"98bfd2c8-a43b-45df-8f3f-ab8a8042009e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:06:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 22:06:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-shdwh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-shdwh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:06:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:06:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:06:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:06:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:10.233.78.7,StartTime:2023-01-18 22:06:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:06:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0b2f1cf761e477061adeedd7d6f11bf87367b5dd7b67321e70004283daf4f524,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 18 22:06:30.250: INFO: pod: "test-deployment-7c7d8d58c8-lsmpj":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-lsmpj test-deployment-7c7d8d58c8- deployment-2360  74e9012c-259d-4767-93e9-82367e59b802 152465 0 2023-01-18 22:06:26 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:62bf758a623bd379f737825f5625532068c963727404ad145c84b4f226c96755 cni.projectcalico.org/podIP:10.233.68.211/32 cni.projectcalico.org/podIPs:10.233.68.211/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 98bfd2c8-a43b-45df-8f3f-ab8a8042009e 0xc000afba07 0xc000afba08}] [] [{calico Update v1 2023-01-18 22:06:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-18 22:06:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"98bfd2c8-a43b-45df-8f3f-ab8a8042009e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:06:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.211\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s9ck5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s9ck5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:06:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:06:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:06:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:06:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.211,StartTime:2023-01-18 22:06:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:06:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2ab9cd2078f545eed7e749fde0dacc7d15cbcea3533280ed73f8d881f7420853,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.211,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 18 22:06:30.257: INFO: ReplicaSet "test-deployment-8594bb6fdd":
    &ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-2360  0745af5b-56ea-49e2-83a5-11be7a82469c 152388 3 2023-01-18 22:06:21 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 1a526544-7717-4c27-8596-1616aea152b1 0xc000afb447 0xc000afb448}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:06:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a526544-7717-4c27-8596-1616aea152b1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:06:25 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000afb4d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 18 22:06:30.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2360" for this suite. 01/18/23 22:06:30.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:06:30.296
Jan 18 22:06:30.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename kubectl 01/18/23 22:06:30.299
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:30.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:30.325
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 01/18/23 22:06:30.33
Jan 18 22:06:30.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4623 create -f -'
Jan 18 22:06:32.086: INFO: stderr: ""
Jan 18 22:06:32.086: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 01/18/23 22:06:32.086
Jan 18 22:06:32.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4623 diff -f -'
Jan 18 22:06:32.500: INFO: rc: 1
Jan 18 22:06:32.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4623 delete -f -'
Jan 18 22:06:32.621: INFO: stderr: ""
Jan 18 22:06:32.621: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 22:06:32.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4623" for this suite. 01/18/23 22:06:32.634
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":187,"skipped":3427,"failed":0}
------------------------------
• [2.348 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:06:30.296
    Jan 18 22:06:30.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename kubectl 01/18/23 22:06:30.299
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:30.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:30.325
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 01/18/23 22:06:30.33
    Jan 18 22:06:30.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4623 create -f -'
    Jan 18 22:06:32.086: INFO: stderr: ""
    Jan 18 22:06:32.086: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 01/18/23 22:06:32.086
    Jan 18 22:06:32.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4623 diff -f -'
    Jan 18 22:06:32.500: INFO: rc: 1
    Jan 18 22:06:32.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4623 delete -f -'
    Jan 18 22:06:32.621: INFO: stderr: ""
    Jan 18 22:06:32.621: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 22:06:32.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4623" for this suite. 01/18/23 22:06:32.634
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:06:32.65
Jan 18 22:06:32.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename pod-network-test 01/18/23 22:06:32.653
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:32.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:32.687
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-8212 01/18/23 22:06:32.693
STEP: creating a selector 01/18/23 22:06:32.694
STEP: Creating the service pods in kubernetes 01/18/23 22:06:32.694
Jan 18 22:06:32.694: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 18 22:06:32.952: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8212" to be "running and ready"
Jan 18 22:06:32.960: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.41891ms
Jan 18 22:06:32.961: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:06:34.969: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017068483s
Jan 18 22:06:34.969: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:06:36.967: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01511814s
Jan 18 22:06:36.967: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:06:38.967: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.014875654s
Jan 18 22:06:38.967: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:06:40.965: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013611038s
Jan 18 22:06:40.966: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:06:42.967: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015021737s
Jan 18 22:06:42.967: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:06:44.968: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015969004s
Jan 18 22:06:44.968: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:06:46.967: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.01553647s
Jan 18 22:06:46.968: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:06:48.967: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.014848459s
Jan 18 22:06:48.967: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:06:50.967: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.015268689s
Jan 18 22:06:50.967: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:06:52.968: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.015842077s
Jan 18 22:06:52.968: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:06:54.966: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.013988553s
Jan 18 22:06:54.966: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 18 22:06:54.966: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 18 22:06:54.970: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8212" to be "running and ready"
Jan 18 22:06:54.975: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.683637ms
Jan 18 22:06:54.975: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 18 22:06:54.975: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/18/23 22:06:54.979
Jan 18 22:06:54.992: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8212" to be "running"
Jan 18 22:06:54.996: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.280952ms
Jan 18 22:06:57.003: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011314196s
Jan 18 22:06:57.004: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 18 22:06:57.009: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8212" to be "running"
Jan 18 22:06:57.014: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.16534ms
Jan 18 22:06:57.014: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 18 22:06:57.019: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 18 22:06:57.019: INFO: Going to poll 10.233.78.8 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 18 22:06:57.024: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.78.8 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8212 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:06:57.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 22:06:57.027: INFO: ExecWithOptions: Clientset creation
Jan 18 22:06:57.028: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8212/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.78.8+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 22:06:58.186: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 18 22:06:58.186: INFO: Going to poll 10.233.68.209 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 18 22:06:58.190: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.68.209 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8212 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:06:58.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 22:06:58.192: INFO: ExecWithOptions: Clientset creation
Jan 18 22:06:58.192: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8212/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.68.209+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 22:06:59.293: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 18 22:06:59.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8212" for this suite. 01/18/23 22:06:59.303
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":188,"skipped":3466,"failed":0}
------------------------------
• [SLOW TEST] [26.668 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:06:32.65
    Jan 18 22:06:32.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename pod-network-test 01/18/23 22:06:32.653
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:32.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:32.687
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-8212 01/18/23 22:06:32.693
    STEP: creating a selector 01/18/23 22:06:32.694
    STEP: Creating the service pods in kubernetes 01/18/23 22:06:32.694
    Jan 18 22:06:32.694: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 18 22:06:32.952: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8212" to be "running and ready"
    Jan 18 22:06:32.960: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.41891ms
    Jan 18 22:06:32.961: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:06:34.969: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017068483s
    Jan 18 22:06:34.969: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:06:36.967: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01511814s
    Jan 18 22:06:36.967: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:06:38.967: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.014875654s
    Jan 18 22:06:38.967: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:06:40.965: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013611038s
    Jan 18 22:06:40.966: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:06:42.967: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015021737s
    Jan 18 22:06:42.967: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:06:44.968: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015969004s
    Jan 18 22:06:44.968: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:06:46.967: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.01553647s
    Jan 18 22:06:46.968: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:06:48.967: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.014848459s
    Jan 18 22:06:48.967: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:06:50.967: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.015268689s
    Jan 18 22:06:50.967: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:06:52.968: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.015842077s
    Jan 18 22:06:52.968: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:06:54.966: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.013988553s
    Jan 18 22:06:54.966: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 18 22:06:54.966: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 18 22:06:54.970: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8212" to be "running and ready"
    Jan 18 22:06:54.975: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.683637ms
    Jan 18 22:06:54.975: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 18 22:06:54.975: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/18/23 22:06:54.979
    Jan 18 22:06:54.992: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8212" to be "running"
    Jan 18 22:06:54.996: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.280952ms
    Jan 18 22:06:57.003: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011314196s
    Jan 18 22:06:57.004: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 18 22:06:57.009: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8212" to be "running"
    Jan 18 22:06:57.014: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.16534ms
    Jan 18 22:06:57.014: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 18 22:06:57.019: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 18 22:06:57.019: INFO: Going to poll 10.233.78.8 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Jan 18 22:06:57.024: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.78.8 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8212 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:06:57.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 22:06:57.027: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:06:57.028: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8212/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.78.8+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 18 22:06:58.186: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 18 22:06:58.186: INFO: Going to poll 10.233.68.209 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Jan 18 22:06:58.190: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.68.209 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8212 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:06:58.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 22:06:58.192: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:06:58.192: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8212/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.68.209+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 18 22:06:59.293: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 18 22:06:59.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-8212" for this suite. 01/18/23 22:06:59.303
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:06:59.323
Jan 18 22:06:59.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename services 01/18/23 22:06:59.326
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:59.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:59.354
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-5964 01/18/23 22:06:59.36
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/18/23 22:06:59.373
STEP: creating service externalsvc in namespace services-5964 01/18/23 22:06:59.374
STEP: creating replication controller externalsvc in namespace services-5964 01/18/23 22:06:59.391
I0118 22:06:59.409500      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5964, replica count: 2
I0118 22:07:02.461626      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 01/18/23 22:07:02.473
Jan 18 22:07:02.522: INFO: Creating new exec pod
Jan 18 22:07:02.560: INFO: Waiting up to 5m0s for pod "execpodnx58p" in namespace "services-5964" to be "running"
Jan 18 22:07:02.600: INFO: Pod "execpodnx58p": Phase="Pending", Reason="", readiness=false. Elapsed: 39.801594ms
Jan 18 22:07:04.632: INFO: Pod "execpodnx58p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071920096s
Jan 18 22:07:06.606: INFO: Pod "execpodnx58p": Phase="Running", Reason="", readiness=true. Elapsed: 4.046171723s
Jan 18 22:07:06.607: INFO: Pod "execpodnx58p" satisfied condition "running"
Jan 18 22:07:06.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-5964 exec execpodnx58p -- /bin/sh -x -c nslookup clusterip-service.services-5964.svc.cluster.local'
Jan 18 22:07:07.132: INFO: stderr: "+ nslookup clusterip-service.services-5964.svc.cluster.local\n"
Jan 18 22:07:07.132: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nclusterip-service.services-5964.svc.cluster.local\tcanonical name = externalsvc.services-5964.svc.cluster.local.\nName:\texternalsvc.services-5964.svc.cluster.local\nAddress: 10.233.42.131\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5964, will wait for the garbage collector to delete the pods 01/18/23 22:07:07.132
Jan 18 22:07:07.195: INFO: Deleting ReplicationController externalsvc took: 8.224214ms
Jan 18 22:07:07.296: INFO: Terminating ReplicationController externalsvc pods took: 100.978166ms
Jan 18 22:07:09.819: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 22:07:09.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5964" for this suite. 01/18/23 22:07:09.867
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":189,"skipped":3467,"failed":0}
------------------------------
• [SLOW TEST] [10.556 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:06:59.323
    Jan 18 22:06:59.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename services 01/18/23 22:06:59.326
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:59.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:59.354
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-5964 01/18/23 22:06:59.36
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/18/23 22:06:59.373
    STEP: creating service externalsvc in namespace services-5964 01/18/23 22:06:59.374
    STEP: creating replication controller externalsvc in namespace services-5964 01/18/23 22:06:59.391
    I0118 22:06:59.409500      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5964, replica count: 2
    I0118 22:07:02.461626      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 01/18/23 22:07:02.473
    Jan 18 22:07:02.522: INFO: Creating new exec pod
    Jan 18 22:07:02.560: INFO: Waiting up to 5m0s for pod "execpodnx58p" in namespace "services-5964" to be "running"
    Jan 18 22:07:02.600: INFO: Pod "execpodnx58p": Phase="Pending", Reason="", readiness=false. Elapsed: 39.801594ms
    Jan 18 22:07:04.632: INFO: Pod "execpodnx58p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071920096s
    Jan 18 22:07:06.606: INFO: Pod "execpodnx58p": Phase="Running", Reason="", readiness=true. Elapsed: 4.046171723s
    Jan 18 22:07:06.607: INFO: Pod "execpodnx58p" satisfied condition "running"
    Jan 18 22:07:06.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-5964 exec execpodnx58p -- /bin/sh -x -c nslookup clusterip-service.services-5964.svc.cluster.local'
    Jan 18 22:07:07.132: INFO: stderr: "+ nslookup clusterip-service.services-5964.svc.cluster.local\n"
    Jan 18 22:07:07.132: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nclusterip-service.services-5964.svc.cluster.local\tcanonical name = externalsvc.services-5964.svc.cluster.local.\nName:\texternalsvc.services-5964.svc.cluster.local\nAddress: 10.233.42.131\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-5964, will wait for the garbage collector to delete the pods 01/18/23 22:07:07.132
    Jan 18 22:07:07.195: INFO: Deleting ReplicationController externalsvc took: 8.224214ms
    Jan 18 22:07:07.296: INFO: Terminating ReplicationController externalsvc pods took: 100.978166ms
    Jan 18 22:07:09.819: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 22:07:09.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5964" for this suite. 01/18/23 22:07:09.867
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:07:09.884
Jan 18 22:07:09.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename resourcequota 01/18/23 22:07:09.886
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:07:09.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:07:09.937
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 01/18/23 22:07:09.95
STEP: Counting existing ResourceQuota 01/18/23 22:07:14.963
STEP: Creating a ResourceQuota 01/18/23 22:07:19.97
STEP: Ensuring resource quota status is calculated 01/18/23 22:07:19.978
STEP: Creating a Secret 01/18/23 22:07:21.985
STEP: Ensuring resource quota status captures secret creation 01/18/23 22:07:22.017
STEP: Deleting a secret 01/18/23 22:07:24.026
STEP: Ensuring resource quota status released usage 01/18/23 22:07:24.033
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 22:07:26.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2460" for this suite. 01/18/23 22:07:26.048
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":190,"skipped":3477,"failed":0}
------------------------------
• [SLOW TEST] [16.172 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:07:09.884
    Jan 18 22:07:09.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename resourcequota 01/18/23 22:07:09.886
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:07:09.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:07:09.937
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 01/18/23 22:07:09.95
    STEP: Counting existing ResourceQuota 01/18/23 22:07:14.963
    STEP: Creating a ResourceQuota 01/18/23 22:07:19.97
    STEP: Ensuring resource quota status is calculated 01/18/23 22:07:19.978
    STEP: Creating a Secret 01/18/23 22:07:21.985
    STEP: Ensuring resource quota status captures secret creation 01/18/23 22:07:22.017
    STEP: Deleting a secret 01/18/23 22:07:24.026
    STEP: Ensuring resource quota status released usage 01/18/23 22:07:24.033
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 22:07:26.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2460" for this suite. 01/18/23 22:07:26.048
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:07:26.061
Jan 18 22:07:26.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename container-runtime 01/18/23 22:07:26.063
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:07:26.084
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:07:26.092
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 01/18/23 22:07:26.097
STEP: wait for the container to reach Failed 01/18/23 22:07:26.107
STEP: get the container status 01/18/23 22:07:30.148
STEP: the container should be terminated 01/18/23 22:07:30.156
STEP: the termination message should be set 01/18/23 22:07:30.156
Jan 18 22:07:30.156: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/18/23 22:07:30.156
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 18 22:07:30.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3108" for this suite. 01/18/23 22:07:30.184
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":191,"skipped":3489,"failed":0}
------------------------------
• [4.338 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:07:26.061
    Jan 18 22:07:26.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename container-runtime 01/18/23 22:07:26.063
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:07:26.084
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:07:26.092
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 01/18/23 22:07:26.097
    STEP: wait for the container to reach Failed 01/18/23 22:07:26.107
    STEP: get the container status 01/18/23 22:07:30.148
    STEP: the container should be terminated 01/18/23 22:07:30.156
    STEP: the termination message should be set 01/18/23 22:07:30.156
    Jan 18 22:07:30.156: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/18/23 22:07:30.156
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 18 22:07:30.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-3108" for this suite. 01/18/23 22:07:30.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:07:30.417
Jan 18 22:07:30.417: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename deployment 01/18/23 22:07:30.42
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:07:30.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:07:30.455
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jan 18 22:07:30.494: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 18 22:07:30.557: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 18 22:07:35.568: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 22:07:35.568
Jan 18 22:07:35.569: INFO: Creating deployment "test-rolling-update-deployment"
Jan 18 22:07:35.578: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 18 22:07:35.632: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 18 22:07:37.645: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 18 22:07:37.650: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 7, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 7, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 7, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 7, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-78f575d8ff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 22:07:39.657: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 22:07:39.677: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4174  857a22c1-e7db-46d6-a866-09d270d088ee 153196 1 2023-01-18 22:07:35 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-18 22:07:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:07:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0031d0c68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 22:07:35 +0000 UTC,LastTransitionTime:2023-01-18 22:07:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-01-18 22:07:37 +0000 UTC,LastTransitionTime:2023-01-18 22:07:35 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 18 22:07:39.684: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-4174  372b6580-d5d0-4822-b4a2-44707a437d19 153186 1 2023-01-18 22:07:35 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 857a22c1-e7db-46d6-a866-09d270d088ee 0xc0031d1167 0xc0031d1168}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:07:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"857a22c1-e7db-46d6-a866-09d270d088ee\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:07:37 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0031d1218 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 22:07:39.684: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 18 22:07:39.684: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4174  35e564a7-ec64-449a-8987-5d4fcb9f2f23 153195 2 2023-01-18 22:07:30 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 857a22c1-e7db-46d6-a866-09d270d088ee 0xc0031d1037 0xc0031d1038}] [] [{e2e.test Update apps/v1 2023-01-18 22:07:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:07:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"857a22c1-e7db-46d6-a866-09d270d088ee\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:07:37 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0031d10f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 22:07:39.692: INFO: Pod "test-rolling-update-deployment-78f575d8ff-gcqjn" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-gcqjn test-rolling-update-deployment-78f575d8ff- deployment-4174  c9a176de-0669-458f-8580-e6acd41abc61 153185 0 2023-01-18 22:07:35 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:9b8a3c2e3c94d70be6e36835c19de159afa059cb007176e959574ca2312d2bd2 cni.projectcalico.org/podIP:10.233.68.239/32 cni.projectcalico.org/podIPs:10.233.68.239/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 372b6580-d5d0-4822-b4a2-44707a437d19 0xc0031d16c7 0xc0031d16c8}] [] [{kube-controller-manager Update v1 2023-01-18 22:07:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"372b6580-d5d0-4822-b4a2-44707a437d19\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:07:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 22:07:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.239\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4xbhk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4xbhk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:07:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:07:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:07:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:07:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.239,StartTime:2023-01-18 22:07:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:07:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://560f836c3fc3c66817452766ceb7324a22408fdd1679c53e0d12a918cccca00a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.239,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 18 22:07:39.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4174" for this suite. 01/18/23 22:07:39.7
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":192,"skipped":3533,"failed":0}
------------------------------
• [SLOW TEST] [9.294 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:07:30.417
    Jan 18 22:07:30.417: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename deployment 01/18/23 22:07:30.42
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:07:30.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:07:30.455
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jan 18 22:07:30.494: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jan 18 22:07:30.557: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 18 22:07:35.568: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 22:07:35.568
    Jan 18 22:07:35.569: INFO: Creating deployment "test-rolling-update-deployment"
    Jan 18 22:07:35.578: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jan 18 22:07:35.632: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jan 18 22:07:37.645: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jan 18 22:07:37.650: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 7, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 7, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 7, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 7, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-78f575d8ff\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 22:07:39.657: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 22:07:39.677: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4174  857a22c1-e7db-46d6-a866-09d270d088ee 153196 1 2023-01-18 22:07:35 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-18 22:07:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:07:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0031d0c68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 22:07:35 +0000 UTC,LastTransitionTime:2023-01-18 22:07:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-01-18 22:07:37 +0000 UTC,LastTransitionTime:2023-01-18 22:07:35 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 18 22:07:39.684: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-4174  372b6580-d5d0-4822-b4a2-44707a437d19 153186 1 2023-01-18 22:07:35 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 857a22c1-e7db-46d6-a866-09d270d088ee 0xc0031d1167 0xc0031d1168}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:07:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"857a22c1-e7db-46d6-a866-09d270d088ee\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:07:37 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0031d1218 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 22:07:39.684: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jan 18 22:07:39.684: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4174  35e564a7-ec64-449a-8987-5d4fcb9f2f23 153195 2 2023-01-18 22:07:30 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 857a22c1-e7db-46d6-a866-09d270d088ee 0xc0031d1037 0xc0031d1038}] [] [{e2e.test Update apps/v1 2023-01-18 22:07:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:07:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"857a22c1-e7db-46d6-a866-09d270d088ee\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:07:37 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0031d10f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 22:07:39.692: INFO: Pod "test-rolling-update-deployment-78f575d8ff-gcqjn" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-gcqjn test-rolling-update-deployment-78f575d8ff- deployment-4174  c9a176de-0669-458f-8580-e6acd41abc61 153185 0 2023-01-18 22:07:35 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:9b8a3c2e3c94d70be6e36835c19de159afa059cb007176e959574ca2312d2bd2 cni.projectcalico.org/podIP:10.233.68.239/32 cni.projectcalico.org/podIPs:10.233.68.239/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 372b6580-d5d0-4822-b4a2-44707a437d19 0xc0031d16c7 0xc0031d16c8}] [] [{kube-controller-manager Update v1 2023-01-18 22:07:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"372b6580-d5d0-4822-b4a2-44707a437d19\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:07:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 22:07:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.239\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4xbhk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4xbhk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:07:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:07:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:07:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:07:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.239,StartTime:2023-01-18 22:07:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:07:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://560f836c3fc3c66817452766ceb7324a22408fdd1679c53e0d12a918cccca00a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.239,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 18 22:07:39.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4174" for this suite. 01/18/23 22:07:39.7
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:07:39.715
Jan 18 22:07:39.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename containers 01/18/23 22:07:39.718
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:07:39.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:07:39.757
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 01/18/23 22:07:39.762
Jan 18 22:07:39.772: INFO: Waiting up to 5m0s for pod "client-containers-0cc57f7f-5a08-459d-9c9e-8c409ce91fc4" in namespace "containers-9102" to be "Succeeded or Failed"
Jan 18 22:07:39.777: INFO: Pod "client-containers-0cc57f7f-5a08-459d-9c9e-8c409ce91fc4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.208658ms
Jan 18 22:07:41.786: INFO: Pod "client-containers-0cc57f7f-5a08-459d-9c9e-8c409ce91fc4": Phase="Running", Reason="", readiness=true. Elapsed: 2.01345063s
Jan 18 22:07:43.783: INFO: Pod "client-containers-0cc57f7f-5a08-459d-9c9e-8c409ce91fc4": Phase="Running", Reason="", readiness=false. Elapsed: 4.010673696s
Jan 18 22:07:45.785: INFO: Pod "client-containers-0cc57f7f-5a08-459d-9c9e-8c409ce91fc4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01278652s
STEP: Saw pod success 01/18/23 22:07:45.785
Jan 18 22:07:45.785: INFO: Pod "client-containers-0cc57f7f-5a08-459d-9c9e-8c409ce91fc4" satisfied condition "Succeeded or Failed"
Jan 18 22:07:45.791: INFO: Trying to get logs from node v1-25-1-18760-w2 pod client-containers-0cc57f7f-5a08-459d-9c9e-8c409ce91fc4 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 22:07:45.805
Jan 18 22:07:45.816: INFO: Waiting for pod client-containers-0cc57f7f-5a08-459d-9c9e-8c409ce91fc4 to disappear
Jan 18 22:07:45.819: INFO: Pod client-containers-0cc57f7f-5a08-459d-9c9e-8c409ce91fc4 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 18 22:07:45.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9102" for this suite. 01/18/23 22:07:45.825
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":193,"skipped":3541,"failed":0}
------------------------------
• [SLOW TEST] [6.124 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:07:39.715
    Jan 18 22:07:39.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename containers 01/18/23 22:07:39.718
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:07:39.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:07:39.757
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 01/18/23 22:07:39.762
    Jan 18 22:07:39.772: INFO: Waiting up to 5m0s for pod "client-containers-0cc57f7f-5a08-459d-9c9e-8c409ce91fc4" in namespace "containers-9102" to be "Succeeded or Failed"
    Jan 18 22:07:39.777: INFO: Pod "client-containers-0cc57f7f-5a08-459d-9c9e-8c409ce91fc4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.208658ms
    Jan 18 22:07:41.786: INFO: Pod "client-containers-0cc57f7f-5a08-459d-9c9e-8c409ce91fc4": Phase="Running", Reason="", readiness=true. Elapsed: 2.01345063s
    Jan 18 22:07:43.783: INFO: Pod "client-containers-0cc57f7f-5a08-459d-9c9e-8c409ce91fc4": Phase="Running", Reason="", readiness=false. Elapsed: 4.010673696s
    Jan 18 22:07:45.785: INFO: Pod "client-containers-0cc57f7f-5a08-459d-9c9e-8c409ce91fc4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01278652s
    STEP: Saw pod success 01/18/23 22:07:45.785
    Jan 18 22:07:45.785: INFO: Pod "client-containers-0cc57f7f-5a08-459d-9c9e-8c409ce91fc4" satisfied condition "Succeeded or Failed"
    Jan 18 22:07:45.791: INFO: Trying to get logs from node v1-25-1-18760-w2 pod client-containers-0cc57f7f-5a08-459d-9c9e-8c409ce91fc4 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 22:07:45.805
    Jan 18 22:07:45.816: INFO: Waiting for pod client-containers-0cc57f7f-5a08-459d-9c9e-8c409ce91fc4 to disappear
    Jan 18 22:07:45.819: INFO: Pod client-containers-0cc57f7f-5a08-459d-9c9e-8c409ce91fc4 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 18 22:07:45.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-9102" for this suite. 01/18/23 22:07:45.825
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:07:45.84
Jan 18 22:07:45.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 22:07:45.841
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:07:45.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:07:45.88
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/18/23 22:07:45.887
Jan 18 22:07:45.889: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 22:07:50.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 22:08:08.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2944" for this suite. 01/18/23 22:08:08.871
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":194,"skipped":3552,"failed":0}
------------------------------
• [SLOW TEST] [23.041 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:07:45.84
    Jan 18 22:07:45.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 22:07:45.841
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:07:45.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:07:45.88
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/18/23 22:07:45.887
    Jan 18 22:07:45.889: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 22:07:50.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 22:08:08.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2944" for this suite. 01/18/23 22:08:08.871
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:08:08.892
Jan 18 22:08:08.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename container-probe 01/18/23 22:08:08.895
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:08:08.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:08:08.932
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-e6b0e1d0-e21d-463d-80c1-20cdac9a67f9 in namespace container-probe-2086 01/18/23 22:08:08.938
Jan 18 22:08:08.952: INFO: Waiting up to 5m0s for pod "liveness-e6b0e1d0-e21d-463d-80c1-20cdac9a67f9" in namespace "container-probe-2086" to be "not pending"
Jan 18 22:08:09.003: INFO: Pod "liveness-e6b0e1d0-e21d-463d-80c1-20cdac9a67f9": Phase="Pending", Reason="", readiness=false. Elapsed: 50.176731ms
Jan 18 22:08:11.009: INFO: Pod "liveness-e6b0e1d0-e21d-463d-80c1-20cdac9a67f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056343948s
Jan 18 22:08:13.009: INFO: Pod "liveness-e6b0e1d0-e21d-463d-80c1-20cdac9a67f9": Phase="Running", Reason="", readiness=true. Elapsed: 4.056324603s
Jan 18 22:08:13.009: INFO: Pod "liveness-e6b0e1d0-e21d-463d-80c1-20cdac9a67f9" satisfied condition "not pending"
Jan 18 22:08:13.009: INFO: Started pod liveness-e6b0e1d0-e21d-463d-80c1-20cdac9a67f9 in namespace container-probe-2086
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 22:08:13.009
Jan 18 22:08:13.014: INFO: Initial restart count of pod liveness-e6b0e1d0-e21d-463d-80c1-20cdac9a67f9 is 0
Jan 18 22:08:31.076: INFO: Restart count of pod container-probe-2086/liveness-e6b0e1d0-e21d-463d-80c1-20cdac9a67f9 is now 1 (18.062349948s elapsed)
STEP: deleting the pod 01/18/23 22:08:31.077
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 18 22:08:31.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2086" for this suite. 01/18/23 22:08:31.115
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":195,"skipped":3558,"failed":0}
------------------------------
• [SLOW TEST] [22.247 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:08:08.892
    Jan 18 22:08:08.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename container-probe 01/18/23 22:08:08.895
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:08:08.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:08:08.932
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-e6b0e1d0-e21d-463d-80c1-20cdac9a67f9 in namespace container-probe-2086 01/18/23 22:08:08.938
    Jan 18 22:08:08.952: INFO: Waiting up to 5m0s for pod "liveness-e6b0e1d0-e21d-463d-80c1-20cdac9a67f9" in namespace "container-probe-2086" to be "not pending"
    Jan 18 22:08:09.003: INFO: Pod "liveness-e6b0e1d0-e21d-463d-80c1-20cdac9a67f9": Phase="Pending", Reason="", readiness=false. Elapsed: 50.176731ms
    Jan 18 22:08:11.009: INFO: Pod "liveness-e6b0e1d0-e21d-463d-80c1-20cdac9a67f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056343948s
    Jan 18 22:08:13.009: INFO: Pod "liveness-e6b0e1d0-e21d-463d-80c1-20cdac9a67f9": Phase="Running", Reason="", readiness=true. Elapsed: 4.056324603s
    Jan 18 22:08:13.009: INFO: Pod "liveness-e6b0e1d0-e21d-463d-80c1-20cdac9a67f9" satisfied condition "not pending"
    Jan 18 22:08:13.009: INFO: Started pod liveness-e6b0e1d0-e21d-463d-80c1-20cdac9a67f9 in namespace container-probe-2086
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 22:08:13.009
    Jan 18 22:08:13.014: INFO: Initial restart count of pod liveness-e6b0e1d0-e21d-463d-80c1-20cdac9a67f9 is 0
    Jan 18 22:08:31.076: INFO: Restart count of pod container-probe-2086/liveness-e6b0e1d0-e21d-463d-80c1-20cdac9a67f9 is now 1 (18.062349948s elapsed)
    STEP: deleting the pod 01/18/23 22:08:31.077
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 18 22:08:31.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-2086" for this suite. 01/18/23 22:08:31.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:08:31.146
Jan 18 22:08:31.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename subpath 01/18/23 22:08:31.148
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:08:31.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:08:31.185
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/18/23 22:08:31.195
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-6r5z 01/18/23 22:08:31.211
STEP: Creating a pod to test atomic-volume-subpath 01/18/23 22:08:31.211
Jan 18 22:08:31.222: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-6r5z" in namespace "subpath-9559" to be "Succeeded or Failed"
Jan 18 22:08:31.447: INFO: Pod "pod-subpath-test-downwardapi-6r5z": Phase="Pending", Reason="", readiness=false. Elapsed: 225.292679ms
Jan 18 22:08:33.455: INFO: Pod "pod-subpath-test-downwardapi-6r5z": Phase="Running", Reason="", readiness=true. Elapsed: 2.23299445s
Jan 18 22:08:35.454: INFO: Pod "pod-subpath-test-downwardapi-6r5z": Phase="Running", Reason="", readiness=true. Elapsed: 4.231694726s
Jan 18 22:08:37.453: INFO: Pod "pod-subpath-test-downwardapi-6r5z": Phase="Running", Reason="", readiness=true. Elapsed: 6.230970036s
Jan 18 22:08:39.455: INFO: Pod "pod-subpath-test-downwardapi-6r5z": Phase="Running", Reason="", readiness=true. Elapsed: 8.232963921s
Jan 18 22:08:41.454: INFO: Pod "pod-subpath-test-downwardapi-6r5z": Phase="Running", Reason="", readiness=true. Elapsed: 10.231480346s
Jan 18 22:08:43.454: INFO: Pod "pod-subpath-test-downwardapi-6r5z": Phase="Running", Reason="", readiness=true. Elapsed: 12.231433075s
Jan 18 22:08:45.456: INFO: Pod "pod-subpath-test-downwardapi-6r5z": Phase="Running", Reason="", readiness=true. Elapsed: 14.233980461s
Jan 18 22:08:47.452: INFO: Pod "pod-subpath-test-downwardapi-6r5z": Phase="Running", Reason="", readiness=true. Elapsed: 16.230392071s
Jan 18 22:08:49.455: INFO: Pod "pod-subpath-test-downwardapi-6r5z": Phase="Running", Reason="", readiness=true. Elapsed: 18.233164977s
Jan 18 22:08:51.454: INFO: Pod "pod-subpath-test-downwardapi-6r5z": Phase="Running", Reason="", readiness=true. Elapsed: 20.231530342s
Jan 18 22:08:53.455: INFO: Pod "pod-subpath-test-downwardapi-6r5z": Phase="Running", Reason="", readiness=false. Elapsed: 22.232475656s
Jan 18 22:08:55.458: INFO: Pod "pod-subpath-test-downwardapi-6r5z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.236345791s
STEP: Saw pod success 01/18/23 22:08:55.459
Jan 18 22:08:55.459: INFO: Pod "pod-subpath-test-downwardapi-6r5z" satisfied condition "Succeeded or Failed"
Jan 18 22:08:55.466: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-subpath-test-downwardapi-6r5z container test-container-subpath-downwardapi-6r5z: <nil>
STEP: delete the pod 01/18/23 22:08:55.483
Jan 18 22:08:55.498: INFO: Waiting for pod pod-subpath-test-downwardapi-6r5z to disappear
Jan 18 22:08:55.502: INFO: Pod pod-subpath-test-downwardapi-6r5z no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-6r5z 01/18/23 22:08:55.502
Jan 18 22:08:55.502: INFO: Deleting pod "pod-subpath-test-downwardapi-6r5z" in namespace "subpath-9559"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 18 22:08:55.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9559" for this suite. 01/18/23 22:08:55.52
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":196,"skipped":3573,"failed":0}
------------------------------
• [SLOW TEST] [24.383 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:08:31.146
    Jan 18 22:08:31.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename subpath 01/18/23 22:08:31.148
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:08:31.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:08:31.185
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/18/23 22:08:31.195
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-6r5z 01/18/23 22:08:31.211
    STEP: Creating a pod to test atomic-volume-subpath 01/18/23 22:08:31.211
    Jan 18 22:08:31.222: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-6r5z" in namespace "subpath-9559" to be "Succeeded or Failed"
    Jan 18 22:08:31.447: INFO: Pod "pod-subpath-test-downwardapi-6r5z": Phase="Pending", Reason="", readiness=false. Elapsed: 225.292679ms
    Jan 18 22:08:33.455: INFO: Pod "pod-subpath-test-downwardapi-6r5z": Phase="Running", Reason="", readiness=true. Elapsed: 2.23299445s
    Jan 18 22:08:35.454: INFO: Pod "pod-subpath-test-downwardapi-6r5z": Phase="Running", Reason="", readiness=true. Elapsed: 4.231694726s
    Jan 18 22:08:37.453: INFO: Pod "pod-subpath-test-downwardapi-6r5z": Phase="Running", Reason="", readiness=true. Elapsed: 6.230970036s
    Jan 18 22:08:39.455: INFO: Pod "pod-subpath-test-downwardapi-6r5z": Phase="Running", Reason="", readiness=true. Elapsed: 8.232963921s
    Jan 18 22:08:41.454: INFO: Pod "pod-subpath-test-downwardapi-6r5z": Phase="Running", Reason="", readiness=true. Elapsed: 10.231480346s
    Jan 18 22:08:43.454: INFO: Pod "pod-subpath-test-downwardapi-6r5z": Phase="Running", Reason="", readiness=true. Elapsed: 12.231433075s
    Jan 18 22:08:45.456: INFO: Pod "pod-subpath-test-downwardapi-6r5z": Phase="Running", Reason="", readiness=true. Elapsed: 14.233980461s
    Jan 18 22:08:47.452: INFO: Pod "pod-subpath-test-downwardapi-6r5z": Phase="Running", Reason="", readiness=true. Elapsed: 16.230392071s
    Jan 18 22:08:49.455: INFO: Pod "pod-subpath-test-downwardapi-6r5z": Phase="Running", Reason="", readiness=true. Elapsed: 18.233164977s
    Jan 18 22:08:51.454: INFO: Pod "pod-subpath-test-downwardapi-6r5z": Phase="Running", Reason="", readiness=true. Elapsed: 20.231530342s
    Jan 18 22:08:53.455: INFO: Pod "pod-subpath-test-downwardapi-6r5z": Phase="Running", Reason="", readiness=false. Elapsed: 22.232475656s
    Jan 18 22:08:55.458: INFO: Pod "pod-subpath-test-downwardapi-6r5z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.236345791s
    STEP: Saw pod success 01/18/23 22:08:55.459
    Jan 18 22:08:55.459: INFO: Pod "pod-subpath-test-downwardapi-6r5z" satisfied condition "Succeeded or Failed"
    Jan 18 22:08:55.466: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-subpath-test-downwardapi-6r5z container test-container-subpath-downwardapi-6r5z: <nil>
    STEP: delete the pod 01/18/23 22:08:55.483
    Jan 18 22:08:55.498: INFO: Waiting for pod pod-subpath-test-downwardapi-6r5z to disappear
    Jan 18 22:08:55.502: INFO: Pod pod-subpath-test-downwardapi-6r5z no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-6r5z 01/18/23 22:08:55.502
    Jan 18 22:08:55.502: INFO: Deleting pod "pod-subpath-test-downwardapi-6r5z" in namespace "subpath-9559"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 18 22:08:55.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-9559" for this suite. 01/18/23 22:08:55.52
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:08:55.536
Jan 18 22:08:55.536: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 22:08:55.538
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:08:55.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:08:55.579
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jan 18 22:08:55.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 22:08:56.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7334" for this suite. 01/18/23 22:08:56.623
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":197,"skipped":3587,"failed":0}
------------------------------
• [1.096 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:08:55.536
    Jan 18 22:08:55.536: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 22:08:55.538
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:08:55.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:08:55.579
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jan 18 22:08:55.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 22:08:56.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-7334" for this suite. 01/18/23 22:08:56.623
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:08:56.638
Jan 18 22:08:56.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename kubectl 01/18/23 22:08:56.639
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:08:56.662
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:08:56.668
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 01/18/23 22:08:56.674
Jan 18 22:08:56.674: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-1343 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 01/18/23 22:08:56.759
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 22:08:56.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1343" for this suite. 01/18/23 22:08:56.776
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":198,"skipped":3619,"failed":0}
------------------------------
• [0.147 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:08:56.638
    Jan 18 22:08:56.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename kubectl 01/18/23 22:08:56.639
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:08:56.662
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:08:56.668
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 01/18/23 22:08:56.674
    Jan 18 22:08:56.674: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-1343 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 01/18/23 22:08:56.759
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 22:08:56.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1343" for this suite. 01/18/23 22:08:56.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:08:56.787
Jan 18 22:08:56.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename cronjob 01/18/23 22:08:56.788
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:08:56.806
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:08:56.813
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 01/18/23 22:08:56.819
STEP: Ensuring a job is scheduled 01/18/23 22:08:56.826
STEP: Ensuring exactly one is scheduled 01/18/23 22:09:00.84
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/18/23 22:09:00.857
STEP: Ensuring the job is replaced with a new one 01/18/23 22:09:00.864
STEP: Removing cronjob 01/18/23 22:10:00.87
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 18 22:10:00.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3884" for this suite. 01/18/23 22:10:00.914
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":199,"skipped":3643,"failed":0}
------------------------------
• [SLOW TEST] [64.190 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:08:56.787
    Jan 18 22:08:56.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename cronjob 01/18/23 22:08:56.788
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:08:56.806
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:08:56.813
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 01/18/23 22:08:56.819
    STEP: Ensuring a job is scheduled 01/18/23 22:08:56.826
    STEP: Ensuring exactly one is scheduled 01/18/23 22:09:00.84
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/18/23 22:09:00.857
    STEP: Ensuring the job is replaced with a new one 01/18/23 22:09:00.864
    STEP: Removing cronjob 01/18/23 22:10:00.87
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 18 22:10:00.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-3884" for this suite. 01/18/23 22:10:00.914
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:10:00.984
Jan 18 22:10:00.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename webhook 01/18/23 22:10:00.987
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:10:01.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:10:01.036
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 22:10:01.064
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:10:02.058
STEP: Deploying the webhook pod 01/18/23 22:10:02.07
STEP: Wait for the deployment to be ready 01/18/23 22:10:02.089
Jan 18 22:10:02.109: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 22:10:04.122
STEP: Verifying the service has paired with the endpoint 01/18/23 22:10:04.138
Jan 18 22:10:05.151: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/18/23 22:10:05.156
STEP: create a configmap that should be updated by the webhook 01/18/23 22:10:05.179
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 22:10:05.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5108" for this suite. 01/18/23 22:10:05.21
STEP: Destroying namespace "webhook-5108-markers" for this suite. 01/18/23 22:10:05.221
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":200,"skipped":3667,"failed":0}
------------------------------
• [4.347 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:10:00.984
    Jan 18 22:10:00.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename webhook 01/18/23 22:10:00.987
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:10:01.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:10:01.036
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 22:10:01.064
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:10:02.058
    STEP: Deploying the webhook pod 01/18/23 22:10:02.07
    STEP: Wait for the deployment to be ready 01/18/23 22:10:02.089
    Jan 18 22:10:02.109: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 22:10:04.122
    STEP: Verifying the service has paired with the endpoint 01/18/23 22:10:04.138
    Jan 18 22:10:05.151: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/18/23 22:10:05.156
    STEP: create a configmap that should be updated by the webhook 01/18/23 22:10:05.179
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 22:10:05.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5108" for this suite. 01/18/23 22:10:05.21
    STEP: Destroying namespace "webhook-5108-markers" for this suite. 01/18/23 22:10:05.221
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:10:05.347
Jan 18 22:10:05.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename endpointslicemirroring 01/18/23 22:10:05.349
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:10:05.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:10:05.441
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 01/18/23 22:10:05.474
Jan 18 22:10:05.488: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 01/18/23 22:10:07.493
Jan 18 22:10:07.513: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 01/18/23 22:10:09.521
Jan 18 22:10:09.558: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Jan 18 22:10:11.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-6205" for this suite. 01/18/23 22:10:11.576
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":201,"skipped":3695,"failed":0}
------------------------------
• [SLOW TEST] [6.248 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:10:05.347
    Jan 18 22:10:05.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename endpointslicemirroring 01/18/23 22:10:05.349
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:10:05.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:10:05.441
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 01/18/23 22:10:05.474
    Jan 18 22:10:05.488: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 01/18/23 22:10:07.493
    Jan 18 22:10:07.513: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 01/18/23 22:10:09.521
    Jan 18 22:10:09.558: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Jan 18 22:10:11.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-6205" for this suite. 01/18/23 22:10:11.576
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:10:11.598
Jan 18 22:10:11.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename container-probe 01/18/23 22:10:11.599
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:10:11.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:10:11.649
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-253504c6-d8ab-4684-922b-d3c246329643 in namespace container-probe-3028 01/18/23 22:10:11.657
Jan 18 22:10:11.669: INFO: Waiting up to 5m0s for pod "liveness-253504c6-d8ab-4684-922b-d3c246329643" in namespace "container-probe-3028" to be "not pending"
Jan 18 22:10:11.675: INFO: Pod "liveness-253504c6-d8ab-4684-922b-d3c246329643": Phase="Pending", Reason="", readiness=false. Elapsed: 5.714772ms
Jan 18 22:10:13.724: INFO: Pod "liveness-253504c6-d8ab-4684-922b-d3c246329643": Phase="Running", Reason="", readiness=true. Elapsed: 2.054786364s
Jan 18 22:10:13.724: INFO: Pod "liveness-253504c6-d8ab-4684-922b-d3c246329643" satisfied condition "not pending"
Jan 18 22:10:13.724: INFO: Started pod liveness-253504c6-d8ab-4684-922b-d3c246329643 in namespace container-probe-3028
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 22:10:13.724
Jan 18 22:10:13.729: INFO: Initial restart count of pod liveness-253504c6-d8ab-4684-922b-d3c246329643 is 0
Jan 18 22:10:33.847: INFO: Restart count of pod container-probe-3028/liveness-253504c6-d8ab-4684-922b-d3c246329643 is now 1 (20.117303659s elapsed)
Jan 18 22:10:53.911: INFO: Restart count of pod container-probe-3028/liveness-253504c6-d8ab-4684-922b-d3c246329643 is now 2 (40.181645982s elapsed)
Jan 18 22:11:13.976: INFO: Restart count of pod container-probe-3028/liveness-253504c6-d8ab-4684-922b-d3c246329643 is now 3 (1m0.246411837s elapsed)
Jan 18 22:11:34.040: INFO: Restart count of pod container-probe-3028/liveness-253504c6-d8ab-4684-922b-d3c246329643 is now 4 (1m20.310645403s elapsed)
Jan 18 22:12:46.291: INFO: Restart count of pod container-probe-3028/liveness-253504c6-d8ab-4684-922b-d3c246329643 is now 5 (2m32.561110419s elapsed)
STEP: deleting the pod 01/18/23 22:12:46.291
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 18 22:12:46.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3028" for this suite. 01/18/23 22:12:46.342
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":202,"skipped":3704,"failed":0}
------------------------------
• [SLOW TEST] [154.761 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:10:11.598
    Jan 18 22:10:11.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename container-probe 01/18/23 22:10:11.599
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:10:11.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:10:11.649
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-253504c6-d8ab-4684-922b-d3c246329643 in namespace container-probe-3028 01/18/23 22:10:11.657
    Jan 18 22:10:11.669: INFO: Waiting up to 5m0s for pod "liveness-253504c6-d8ab-4684-922b-d3c246329643" in namespace "container-probe-3028" to be "not pending"
    Jan 18 22:10:11.675: INFO: Pod "liveness-253504c6-d8ab-4684-922b-d3c246329643": Phase="Pending", Reason="", readiness=false. Elapsed: 5.714772ms
    Jan 18 22:10:13.724: INFO: Pod "liveness-253504c6-d8ab-4684-922b-d3c246329643": Phase="Running", Reason="", readiness=true. Elapsed: 2.054786364s
    Jan 18 22:10:13.724: INFO: Pod "liveness-253504c6-d8ab-4684-922b-d3c246329643" satisfied condition "not pending"
    Jan 18 22:10:13.724: INFO: Started pod liveness-253504c6-d8ab-4684-922b-d3c246329643 in namespace container-probe-3028
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 22:10:13.724
    Jan 18 22:10:13.729: INFO: Initial restart count of pod liveness-253504c6-d8ab-4684-922b-d3c246329643 is 0
    Jan 18 22:10:33.847: INFO: Restart count of pod container-probe-3028/liveness-253504c6-d8ab-4684-922b-d3c246329643 is now 1 (20.117303659s elapsed)
    Jan 18 22:10:53.911: INFO: Restart count of pod container-probe-3028/liveness-253504c6-d8ab-4684-922b-d3c246329643 is now 2 (40.181645982s elapsed)
    Jan 18 22:11:13.976: INFO: Restart count of pod container-probe-3028/liveness-253504c6-d8ab-4684-922b-d3c246329643 is now 3 (1m0.246411837s elapsed)
    Jan 18 22:11:34.040: INFO: Restart count of pod container-probe-3028/liveness-253504c6-d8ab-4684-922b-d3c246329643 is now 4 (1m20.310645403s elapsed)
    Jan 18 22:12:46.291: INFO: Restart count of pod container-probe-3028/liveness-253504c6-d8ab-4684-922b-d3c246329643 is now 5 (2m32.561110419s elapsed)
    STEP: deleting the pod 01/18/23 22:12:46.291
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 18 22:12:46.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-3028" for this suite. 01/18/23 22:12:46.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:12:46.361
Jan 18 22:12:46.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename certificates 01/18/23 22:12:46.364
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:12:46.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:12:46.422
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 01/18/23 22:12:47.321
STEP: getting /apis/certificates.k8s.io 01/18/23 22:12:47.325
STEP: getting /apis/certificates.k8s.io/v1 01/18/23 22:12:47.327
STEP: creating 01/18/23 22:12:47.329
STEP: getting 01/18/23 22:12:47.351
STEP: listing 01/18/23 22:12:47.355
STEP: watching 01/18/23 22:12:47.359
Jan 18 22:12:47.359: INFO: starting watch
STEP: patching 01/18/23 22:12:47.361
STEP: updating 01/18/23 22:12:47.371
Jan 18 22:12:47.388: INFO: waiting for watch events with expected annotations
Jan 18 22:12:47.389: INFO: saw patched and updated annotations
STEP: getting /approval 01/18/23 22:12:47.389
STEP: patching /approval 01/18/23 22:12:47.392
STEP: updating /approval 01/18/23 22:12:47.398
STEP: getting /status 01/18/23 22:12:47.406
STEP: patching /status 01/18/23 22:12:47.412
STEP: updating /status 01/18/23 22:12:47.421
STEP: deleting 01/18/23 22:12:47.429
STEP: deleting a collection 01/18/23 22:12:47.451
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 22:12:47.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-9676" for this suite. 01/18/23 22:12:47.475
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":203,"skipped":3717,"failed":0}
------------------------------
• [1.127 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:12:46.361
    Jan 18 22:12:46.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename certificates 01/18/23 22:12:46.364
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:12:46.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:12:46.422
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 01/18/23 22:12:47.321
    STEP: getting /apis/certificates.k8s.io 01/18/23 22:12:47.325
    STEP: getting /apis/certificates.k8s.io/v1 01/18/23 22:12:47.327
    STEP: creating 01/18/23 22:12:47.329
    STEP: getting 01/18/23 22:12:47.351
    STEP: listing 01/18/23 22:12:47.355
    STEP: watching 01/18/23 22:12:47.359
    Jan 18 22:12:47.359: INFO: starting watch
    STEP: patching 01/18/23 22:12:47.361
    STEP: updating 01/18/23 22:12:47.371
    Jan 18 22:12:47.388: INFO: waiting for watch events with expected annotations
    Jan 18 22:12:47.389: INFO: saw patched and updated annotations
    STEP: getting /approval 01/18/23 22:12:47.389
    STEP: patching /approval 01/18/23 22:12:47.392
    STEP: updating /approval 01/18/23 22:12:47.398
    STEP: getting /status 01/18/23 22:12:47.406
    STEP: patching /status 01/18/23 22:12:47.412
    STEP: updating /status 01/18/23 22:12:47.421
    STEP: deleting 01/18/23 22:12:47.429
    STEP: deleting a collection 01/18/23 22:12:47.451
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 22:12:47.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-9676" for this suite. 01/18/23 22:12:47.475
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:12:47.505
Jan 18 22:12:47.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename services 01/18/23 22:12:47.506
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:12:47.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:12:47.539
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-3222 01/18/23 22:12:47.566
Jan 18 22:12:47.579: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-3222" to be "running and ready"
Jan 18 22:12:47.602: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 22.877611ms
Jan 18 22:12:47.602: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:12:49.608: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.029687581s
Jan 18 22:12:49.609: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 18 22:12:49.609: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Jan 18 22:12:49.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3222 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 18 22:12:49.861: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 18 22:12:49.861: INFO: stdout: "ipvs"
Jan 18 22:12:49.861: INFO: proxyMode: ipvs
Jan 18 22:12:49.879: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 18 22:12:49.884: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-3222 01/18/23 22:12:49.885
STEP: creating replication controller affinity-nodeport-timeout in namespace services-3222 01/18/23 22:12:49.91
I0118 22:12:49.925310      19 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-3222, replica count: 3
I0118 22:12:52.977460      19 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 22:12:53.003: INFO: Creating new exec pod
Jan 18 22:12:53.012: INFO: Waiting up to 5m0s for pod "execpod-affinitysvfrp" in namespace "services-3222" to be "running"
Jan 18 22:12:53.025: INFO: Pod "execpod-affinitysvfrp": Phase="Pending", Reason="", readiness=false. Elapsed: 12.344581ms
Jan 18 22:12:55.036: INFO: Pod "execpod-affinitysvfrp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022816654s
Jan 18 22:12:57.030: INFO: Pod "execpod-affinitysvfrp": Phase="Running", Reason="", readiness=true. Elapsed: 4.017501433s
Jan 18 22:12:57.031: INFO: Pod "execpod-affinitysvfrp" satisfied condition "running"
Jan 18 22:12:58.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3222 exec execpod-affinitysvfrp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jan 18 22:12:58.326: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jan 18 22:12:58.326: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 22:12:58.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3222 exec execpod-affinitysvfrp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.54.63 80'
Jan 18 22:12:58.525: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.54.63 80\nConnection to 10.233.54.63 80 port [tcp/http] succeeded!\n"
Jan 18 22:12:58.525: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 22:12:58.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3222 exec execpod-affinitysvfrp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.168 30100'
Jan 18 22:12:58.780: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.168 30100\nConnection to 192.168.101.168 30100 port [tcp/*] succeeded!\n"
Jan 18 22:12:58.780: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 22:12:58.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3222 exec execpod-affinitysvfrp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 30100'
Jan 18 22:12:58.973: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.216 30100\nConnection to 192.168.101.216 30100 port [tcp/*] succeeded!\n"
Jan 18 22:12:58.973: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 22:12:58.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3222 exec execpod-affinitysvfrp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.101.168:30100/ ; done'
Jan 18 22:12:59.369: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n"
Jan 18 22:12:59.369: INFO: stdout: "\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf"
Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
Jan 18 22:12:59.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3222 exec execpod-affinitysvfrp -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.101.168:30100/'
Jan 18 22:12:59.567: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n"
Jan 18 22:12:59.567: INFO: stdout: "affinity-nodeport-timeout-6wvbf"
Jan 18 22:15:09.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3222 exec execpod-affinitysvfrp -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.101.168:30100/'
Jan 18 22:15:09.786: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n"
Jan 18 22:15:09.787: INFO: stdout: "affinity-nodeport-timeout-6b22m"
Jan 18 22:15:09.787: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-3222, will wait for the garbage collector to delete the pods 01/18/23 22:15:09.819
Jan 18 22:15:09.895: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 17.589592ms
Jan 18 22:15:09.995: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.541626ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 22:15:12.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3222" for this suite. 01/18/23 22:15:12.838
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":204,"skipped":3739,"failed":0}
------------------------------
• [SLOW TEST] [145.344 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:12:47.505
    Jan 18 22:12:47.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename services 01/18/23 22:12:47.506
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:12:47.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:12:47.539
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-3222 01/18/23 22:12:47.566
    Jan 18 22:12:47.579: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-3222" to be "running and ready"
    Jan 18 22:12:47.602: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 22.877611ms
    Jan 18 22:12:47.602: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:12:49.608: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.029687581s
    Jan 18 22:12:49.609: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Jan 18 22:12:49.609: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Jan 18 22:12:49.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3222 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Jan 18 22:12:49.861: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Jan 18 22:12:49.861: INFO: stdout: "ipvs"
    Jan 18 22:12:49.861: INFO: proxyMode: ipvs
    Jan 18 22:12:49.879: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Jan 18 22:12:49.884: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-3222 01/18/23 22:12:49.885
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-3222 01/18/23 22:12:49.91
    I0118 22:12:49.925310      19 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-3222, replica count: 3
    I0118 22:12:52.977460      19 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 22:12:53.003: INFO: Creating new exec pod
    Jan 18 22:12:53.012: INFO: Waiting up to 5m0s for pod "execpod-affinitysvfrp" in namespace "services-3222" to be "running"
    Jan 18 22:12:53.025: INFO: Pod "execpod-affinitysvfrp": Phase="Pending", Reason="", readiness=false. Elapsed: 12.344581ms
    Jan 18 22:12:55.036: INFO: Pod "execpod-affinitysvfrp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022816654s
    Jan 18 22:12:57.030: INFO: Pod "execpod-affinitysvfrp": Phase="Running", Reason="", readiness=true. Elapsed: 4.017501433s
    Jan 18 22:12:57.031: INFO: Pod "execpod-affinitysvfrp" satisfied condition "running"
    Jan 18 22:12:58.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3222 exec execpod-affinitysvfrp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Jan 18 22:12:58.326: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Jan 18 22:12:58.326: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 22:12:58.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3222 exec execpod-affinitysvfrp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.54.63 80'
    Jan 18 22:12:58.525: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.54.63 80\nConnection to 10.233.54.63 80 port [tcp/http] succeeded!\n"
    Jan 18 22:12:58.525: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 22:12:58.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3222 exec execpod-affinitysvfrp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.168 30100'
    Jan 18 22:12:58.780: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.168 30100\nConnection to 192.168.101.168 30100 port [tcp/*] succeeded!\n"
    Jan 18 22:12:58.780: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 22:12:58.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3222 exec execpod-affinitysvfrp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 30100'
    Jan 18 22:12:58.973: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.216 30100\nConnection to 192.168.101.216 30100 port [tcp/*] succeeded!\n"
    Jan 18 22:12:58.973: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 22:12:58.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3222 exec execpod-affinitysvfrp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.101.168:30100/ ; done'
    Jan 18 22:12:59.369: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n"
    Jan 18 22:12:59.369: INFO: stdout: "\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf\naffinity-nodeport-timeout-6wvbf"
    Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
    Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
    Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
    Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
    Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
    Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
    Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
    Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
    Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
    Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
    Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
    Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
    Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
    Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
    Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
    Jan 18 22:12:59.369: INFO: Received response from host: affinity-nodeport-timeout-6wvbf
    Jan 18 22:12:59.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3222 exec execpod-affinitysvfrp -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.101.168:30100/'
    Jan 18 22:12:59.567: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n"
    Jan 18 22:12:59.567: INFO: stdout: "affinity-nodeport-timeout-6wvbf"
    Jan 18 22:15:09.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-3222 exec execpod-affinitysvfrp -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.101.168:30100/'
    Jan 18 22:15:09.786: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.101.168:30100/\n"
    Jan 18 22:15:09.787: INFO: stdout: "affinity-nodeport-timeout-6b22m"
    Jan 18 22:15:09.787: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-3222, will wait for the garbage collector to delete the pods 01/18/23 22:15:09.819
    Jan 18 22:15:09.895: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 17.589592ms
    Jan 18 22:15:09.995: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.541626ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 22:15:12.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3222" for this suite. 01/18/23 22:15:12.838
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:15:12.855
Jan 18 22:15:12.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename sched-pred 01/18/23 22:15:12.858
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:12.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:12.882
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 18 22:15:12.893: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 18 22:15:12.910: INFO: Waiting for terminating namespaces to be deleted...
Jan 18 22:15:12.918: INFO: 
Logging pods the apiserver thinks is on node v1-25-1-18760-w before test
Jan 18 22:15:12.932: INFO: helm-controller-578bd944df-dzr2c from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:15:12.932: INFO: 	Container manager ready: true, restart count 0
Jan 18 22:15:12.932: INFO: notification-controller-6459696d4f-gdmj8 from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:15:12.932: INFO: 	Container manager ready: true, restart count 0
Jan 18 22:15:12.932: INFO: source-controller-84f6bcbfb8-ljjjm from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:15:12.932: INFO: 	Container manager ready: true, restart count 0
Jan 18 22:15:12.932: INFO: calico-node-59tts from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
Jan 18 22:15:12.932: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 22:15:12.932: INFO: coredns-coredns-5f9b955d9-pjqxb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:15:12.932: INFO: 	Container coredns ready: true, restart count 0
Jan 18 22:15:12.932: INFO: coredns-coredns-autoscaler-7d98f7496d-7zbwb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:15:12.932: INFO: 	Container autoscaler ready: true, restart count 0
Jan 18 22:15:12.932: INFO: kube-proxy-rzflq from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
Jan 18 22:15:12.932: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 22:15:12.932: INFO: metrics-server-58498b9c56-q88cp from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:15:12.933: INFO: 	Container metrics-server ready: true, restart count 0
Jan 18 22:15:12.933: INFO: nginx-proxy-v1-25-1-18760-w from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
Jan 18 22:15:12.933: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 18 22:15:12.933: INFO: nodelocaldns-skdxm from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
Jan 18 22:15:12.933: INFO: 	Container node-cache ready: true, restart count 0
Jan 18 22:15:12.933: INFO: openstack-cinder-csi-controllerplugin-55d57dd799-6496v from kube-system started at 2023-01-18 14:39:48 +0000 UTC (6 container statuses recorded)
Jan 18 22:15:12.933: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 18 22:15:12.933: INFO: 	Container csi-attacher ready: true, restart count 0
Jan 18 22:15:12.933: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan 18 22:15:12.933: INFO: 	Container csi-resizer ready: true, restart count 0
Jan 18 22:15:12.933: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan 18 22:15:12.933: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 18 22:15:12.933: INFO: openstack-cinder-csi-nodeplugin-w7sxz from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
Jan 18 22:15:12.933: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 18 22:15:12.933: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 18 22:15:12.933: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 18 22:15:12.933: INFO: sonobuoy-systemd-logs-daemon-set-c355d4758f184dbb-d68zx from sonobuoy started at 2023-01-18 21:29:30 +0000 UTC (2 container statuses recorded)
Jan 18 22:15:12.933: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:15:12.933: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 22:15:12.933: INFO: 
Logging pods the apiserver thinks is on node v1-25-1-18760-w2 before test
Jan 18 22:15:12.947: INFO: calico-node-qx4f9 from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
Jan 18 22:15:12.947: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 22:15:12.947: INFO: coredns-coredns-5f9b955d9-qggng from kube-system started at 2023-01-18 14:40:34 +0000 UTC (1 container statuses recorded)
Jan 18 22:15:12.948: INFO: 	Container coredns ready: true, restart count 0
Jan 18 22:15:12.948: INFO: kube-proxy-8r2pt from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
Jan 18 22:15:12.948: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 22:15:12.948: INFO: nginx-proxy-v1-25-1-18760-w2 from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
Jan 18 22:15:12.948: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 18 22:15:12.949: INFO: nodelocaldns-dzwbj from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
Jan 18 22:15:12.949: INFO: 	Container node-cache ready: true, restart count 0
Jan 18 22:15:12.949: INFO: openstack-cinder-csi-nodeplugin-qm575 from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
Jan 18 22:15:12.949: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 18 22:15:12.949: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 18 22:15:12.949: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 18 22:15:12.949: INFO: sonobuoy from sonobuoy started at 2023-01-18 21:29:28 +0000 UTC (1 container statuses recorded)
Jan 18 22:15:12.950: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 18 22:15:12.950: INFO: sonobuoy-e2e-job-2f3390b6578e4b8c from sonobuoy started at 2023-01-18 21:29:29 +0000 UTC (2 container statuses recorded)
Jan 18 22:15:12.950: INFO: 	Container e2e ready: true, restart count 0
Jan 18 22:15:12.950: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:15:12.950: INFO: sonobuoy-systemd-logs-daemon-set-c355d4758f184dbb-svdn4 from sonobuoy started at 2023-01-18 21:29:29 +0000 UTC (2 container statuses recorded)
Jan 18 22:15:12.952: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:15:12.952: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 22:15:12.953
Jan 18 22:15:12.967: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4637" to be "running"
Jan 18 22:15:12.973: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052446ms
Jan 18 22:15:14.980: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.012639452s
Jan 18 22:15:14.981: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 22:15:14.987
STEP: Trying to apply a random label on the found node. 01/18/23 22:15:15.016
STEP: verifying the node has the label kubernetes.io/e2e-19d1872b-31d8-45a7-bbef-4a5fe5bc0fcb 42 01/18/23 22:15:15.033
STEP: Trying to relaunch the pod, now with labels. 01/18/23 22:15:15.042
Jan 18 22:15:15.049: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-4637" to be "not pending"
Jan 18 22:15:15.053: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 4.255323ms
Jan 18 22:15:17.060: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.010352471s
Jan 18 22:15:17.060: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-19d1872b-31d8-45a7-bbef-4a5fe5bc0fcb off the node v1-25-1-18760-w2 01/18/23 22:15:17.065
STEP: verifying the node doesn't have the label kubernetes.io/e2e-19d1872b-31d8-45a7-bbef-4a5fe5bc0fcb 01/18/23 22:15:17.086
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 18 22:15:17.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4637" for this suite. 01/18/23 22:15:17.107
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":205,"skipped":3742,"failed":0}
------------------------------
• [4.264 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:15:12.855
    Jan 18 22:15:12.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename sched-pred 01/18/23 22:15:12.858
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:12.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:12.882
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 18 22:15:12.893: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 18 22:15:12.910: INFO: Waiting for terminating namespaces to be deleted...
    Jan 18 22:15:12.918: INFO: 
    Logging pods the apiserver thinks is on node v1-25-1-18760-w before test
    Jan 18 22:15:12.932: INFO: helm-controller-578bd944df-dzr2c from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 22:15:12.932: INFO: 	Container manager ready: true, restart count 0
    Jan 18 22:15:12.932: INFO: notification-controller-6459696d4f-gdmj8 from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 22:15:12.932: INFO: 	Container manager ready: true, restart count 0
    Jan 18 22:15:12.932: INFO: source-controller-84f6bcbfb8-ljjjm from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 22:15:12.932: INFO: 	Container manager ready: true, restart count 0
    Jan 18 22:15:12.932: INFO: calico-node-59tts from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
    Jan 18 22:15:12.932: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 22:15:12.932: INFO: coredns-coredns-5f9b955d9-pjqxb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 22:15:12.932: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 22:15:12.932: INFO: coredns-coredns-autoscaler-7d98f7496d-7zbwb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 22:15:12.932: INFO: 	Container autoscaler ready: true, restart count 0
    Jan 18 22:15:12.932: INFO: kube-proxy-rzflq from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
    Jan 18 22:15:12.932: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 22:15:12.932: INFO: metrics-server-58498b9c56-q88cp from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 22:15:12.933: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 18 22:15:12.933: INFO: nginx-proxy-v1-25-1-18760-w from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
    Jan 18 22:15:12.933: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 18 22:15:12.933: INFO: nodelocaldns-skdxm from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
    Jan 18 22:15:12.933: INFO: 	Container node-cache ready: true, restart count 0
    Jan 18 22:15:12.933: INFO: openstack-cinder-csi-controllerplugin-55d57dd799-6496v from kube-system started at 2023-01-18 14:39:48 +0000 UTC (6 container statuses recorded)
    Jan 18 22:15:12.933: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 18 22:15:12.933: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan 18 22:15:12.933: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan 18 22:15:12.933: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan 18 22:15:12.933: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan 18 22:15:12.933: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 18 22:15:12.933: INFO: openstack-cinder-csi-nodeplugin-w7sxz from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
    Jan 18 22:15:12.933: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 18 22:15:12.933: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 18 22:15:12.933: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 18 22:15:12.933: INFO: sonobuoy-systemd-logs-daemon-set-c355d4758f184dbb-d68zx from sonobuoy started at 2023-01-18 21:29:30 +0000 UTC (2 container statuses recorded)
    Jan 18 22:15:12.933: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 22:15:12.933: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 18 22:15:12.933: INFO: 
    Logging pods the apiserver thinks is on node v1-25-1-18760-w2 before test
    Jan 18 22:15:12.947: INFO: calico-node-qx4f9 from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
    Jan 18 22:15:12.947: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 22:15:12.947: INFO: coredns-coredns-5f9b955d9-qggng from kube-system started at 2023-01-18 14:40:34 +0000 UTC (1 container statuses recorded)
    Jan 18 22:15:12.948: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 22:15:12.948: INFO: kube-proxy-8r2pt from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
    Jan 18 22:15:12.948: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 22:15:12.948: INFO: nginx-proxy-v1-25-1-18760-w2 from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
    Jan 18 22:15:12.948: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 18 22:15:12.949: INFO: nodelocaldns-dzwbj from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
    Jan 18 22:15:12.949: INFO: 	Container node-cache ready: true, restart count 0
    Jan 18 22:15:12.949: INFO: openstack-cinder-csi-nodeplugin-qm575 from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
    Jan 18 22:15:12.949: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 18 22:15:12.949: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 18 22:15:12.949: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 18 22:15:12.949: INFO: sonobuoy from sonobuoy started at 2023-01-18 21:29:28 +0000 UTC (1 container statuses recorded)
    Jan 18 22:15:12.950: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 18 22:15:12.950: INFO: sonobuoy-e2e-job-2f3390b6578e4b8c from sonobuoy started at 2023-01-18 21:29:29 +0000 UTC (2 container statuses recorded)
    Jan 18 22:15:12.950: INFO: 	Container e2e ready: true, restart count 0
    Jan 18 22:15:12.950: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 22:15:12.950: INFO: sonobuoy-systemd-logs-daemon-set-c355d4758f184dbb-svdn4 from sonobuoy started at 2023-01-18 21:29:29 +0000 UTC (2 container statuses recorded)
    Jan 18 22:15:12.952: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 22:15:12.952: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 22:15:12.953
    Jan 18 22:15:12.967: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4637" to be "running"
    Jan 18 22:15:12.973: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052446ms
    Jan 18 22:15:14.980: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.012639452s
    Jan 18 22:15:14.981: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 22:15:14.987
    STEP: Trying to apply a random label on the found node. 01/18/23 22:15:15.016
    STEP: verifying the node has the label kubernetes.io/e2e-19d1872b-31d8-45a7-bbef-4a5fe5bc0fcb 42 01/18/23 22:15:15.033
    STEP: Trying to relaunch the pod, now with labels. 01/18/23 22:15:15.042
    Jan 18 22:15:15.049: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-4637" to be "not pending"
    Jan 18 22:15:15.053: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 4.255323ms
    Jan 18 22:15:17.060: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.010352471s
    Jan 18 22:15:17.060: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-19d1872b-31d8-45a7-bbef-4a5fe5bc0fcb off the node v1-25-1-18760-w2 01/18/23 22:15:17.065
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-19d1872b-31d8-45a7-bbef-4a5fe5bc0fcb 01/18/23 22:15:17.086
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 22:15:17.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-4637" for this suite. 01/18/23 22:15:17.107
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:15:17.124
Jan 18 22:15:17.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename cronjob 01/18/23 22:15:17.127
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:17.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:17.175
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 01/18/23 22:15:17.18
STEP: Ensuring no jobs are scheduled 01/18/23 22:15:17.19
STEP: Ensuring no job exists by listing jobs explicitly 01/18/23 22:20:17.2
STEP: Removing cronjob 01/18/23 22:20:17.204
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 18 22:20:17.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6453" for this suite. 01/18/23 22:20:17.22
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":206,"skipped":3755,"failed":0}
------------------------------
• [SLOW TEST] [300.106 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:15:17.124
    Jan 18 22:15:17.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename cronjob 01/18/23 22:15:17.127
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:17.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:17.175
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 01/18/23 22:15:17.18
    STEP: Ensuring no jobs are scheduled 01/18/23 22:15:17.19
    STEP: Ensuring no job exists by listing jobs explicitly 01/18/23 22:20:17.2
    STEP: Removing cronjob 01/18/23 22:20:17.204
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 18 22:20:17.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-6453" for this suite. 01/18/23 22:20:17.22
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:20:17.235
Jan 18 22:20:17.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename job 01/18/23 22:20:17.236
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:20:17.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:20:17.281
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 01/18/23 22:20:17.285
STEP: Ensure pods equal to paralellism count is attached to the job 01/18/23 22:20:17.292
STEP: patching /status 01/18/23 22:20:21.301
STEP: updating /status 01/18/23 22:20:21.315
STEP: get /status 01/18/23 22:20:21.328
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 18 22:20:21.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3221" for this suite. 01/18/23 22:20:21.339
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":207,"skipped":3755,"failed":0}
------------------------------
• [4.114 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:20:17.235
    Jan 18 22:20:17.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename job 01/18/23 22:20:17.236
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:20:17.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:20:17.281
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 01/18/23 22:20:17.285
    STEP: Ensure pods equal to paralellism count is attached to the job 01/18/23 22:20:17.292
    STEP: patching /status 01/18/23 22:20:21.301
    STEP: updating /status 01/18/23 22:20:21.315
    STEP: get /status 01/18/23 22:20:21.328
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 18 22:20:21.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-3221" for this suite. 01/18/23 22:20:21.339
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:20:21.353
Jan 18 22:20:21.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename sched-pred 01/18/23 22:20:21.357
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:20:21.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:20:21.383
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 18 22:20:21.391: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 18 22:20:21.402: INFO: Waiting for terminating namespaces to be deleted...
Jan 18 22:20:21.407: INFO: 
Logging pods the apiserver thinks is on node v1-25-1-18760-w before test
Jan 18 22:20:21.422: INFO: helm-controller-578bd944df-dzr2c from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:20:21.422: INFO: 	Container manager ready: true, restart count 0
Jan 18 22:20:21.422: INFO: notification-controller-6459696d4f-gdmj8 from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:20:21.422: INFO: 	Container manager ready: true, restart count 0
Jan 18 22:20:21.422: INFO: source-controller-84f6bcbfb8-ljjjm from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:20:21.422: INFO: 	Container manager ready: true, restart count 0
Jan 18 22:20:21.422: INFO: calico-node-59tts from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
Jan 18 22:20:21.422: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 22:20:21.422: INFO: coredns-coredns-5f9b955d9-pjqxb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:20:21.422: INFO: 	Container coredns ready: true, restart count 0
Jan 18 22:20:21.422: INFO: coredns-coredns-autoscaler-7d98f7496d-7zbwb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:20:21.422: INFO: 	Container autoscaler ready: true, restart count 0
Jan 18 22:20:21.422: INFO: kube-proxy-rzflq from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
Jan 18 22:20:21.422: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 22:20:21.422: INFO: metrics-server-58498b9c56-q88cp from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:20:21.422: INFO: 	Container metrics-server ready: true, restart count 0
Jan 18 22:20:21.422: INFO: nginx-proxy-v1-25-1-18760-w from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
Jan 18 22:20:21.422: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 18 22:20:21.422: INFO: nodelocaldns-skdxm from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
Jan 18 22:20:21.422: INFO: 	Container node-cache ready: true, restart count 0
Jan 18 22:20:21.422: INFO: openstack-cinder-csi-controllerplugin-55d57dd799-6496v from kube-system started at 2023-01-18 14:39:48 +0000 UTC (6 container statuses recorded)
Jan 18 22:20:21.422: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 18 22:20:21.422: INFO: 	Container csi-attacher ready: true, restart count 0
Jan 18 22:20:21.422: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan 18 22:20:21.422: INFO: 	Container csi-resizer ready: true, restart count 0
Jan 18 22:20:21.422: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan 18 22:20:21.422: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 18 22:20:21.422: INFO: openstack-cinder-csi-nodeplugin-w7sxz from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
Jan 18 22:20:21.422: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 18 22:20:21.422: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 18 22:20:21.422: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 18 22:20:21.422: INFO: sonobuoy-systemd-logs-daemon-set-c355d4758f184dbb-d68zx from sonobuoy started at 2023-01-18 21:29:30 +0000 UTC (2 container statuses recorded)
Jan 18 22:20:21.422: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:20:21.422: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 22:20:21.422: INFO: 
Logging pods the apiserver thinks is on node v1-25-1-18760-w2 before test
Jan 18 22:20:21.434: INFO: suspend-false-to-true-6987q from job-3221 started at 2023-01-18 22:20:17 +0000 UTC (1 container statuses recorded)
Jan 18 22:20:21.435: INFO: 	Container c ready: true, restart count 0
Jan 18 22:20:21.435: INFO: suspend-false-to-true-qz49n from job-3221 started at 2023-01-18 22:20:17 +0000 UTC (1 container statuses recorded)
Jan 18 22:20:21.435: INFO: 	Container c ready: true, restart count 0
Jan 18 22:20:21.435: INFO: calico-node-qx4f9 from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
Jan 18 22:20:21.435: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 22:20:21.435: INFO: coredns-coredns-5f9b955d9-qggng from kube-system started at 2023-01-18 14:40:34 +0000 UTC (1 container statuses recorded)
Jan 18 22:20:21.436: INFO: 	Container coredns ready: true, restart count 0
Jan 18 22:20:21.436: INFO: kube-proxy-8r2pt from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
Jan 18 22:20:21.436: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 22:20:21.436: INFO: nginx-proxy-v1-25-1-18760-w2 from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
Jan 18 22:20:21.436: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 18 22:20:21.436: INFO: nodelocaldns-dzwbj from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
Jan 18 22:20:21.436: INFO: 	Container node-cache ready: true, restart count 0
Jan 18 22:20:21.437: INFO: openstack-cinder-csi-nodeplugin-qm575 from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
Jan 18 22:20:21.437: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 18 22:20:21.437: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 18 22:20:21.437: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 18 22:20:21.437: INFO: sonobuoy from sonobuoy started at 2023-01-18 21:29:28 +0000 UTC (1 container statuses recorded)
Jan 18 22:20:21.437: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 18 22:20:21.437: INFO: sonobuoy-e2e-job-2f3390b6578e4b8c from sonobuoy started at 2023-01-18 21:29:29 +0000 UTC (2 container statuses recorded)
Jan 18 22:20:21.437: INFO: 	Container e2e ready: true, restart count 0
Jan 18 22:20:21.438: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:20:21.438: INFO: sonobuoy-systemd-logs-daemon-set-c355d4758f184dbb-svdn4 from sonobuoy started at 2023-01-18 21:29:29 +0000 UTC (2 container statuses recorded)
Jan 18 22:20:21.438: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:20:21.438: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 22:20:21.438
Jan 18 22:20:21.450: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-684" to be "running"
Jan 18 22:20:21.459: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 9.480533ms
Jan 18 22:20:23.466: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.016028277s
Jan 18 22:20:23.466: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 22:20:23.472
STEP: Trying to apply a random label on the found node. 01/18/23 22:20:23.504
STEP: verifying the node has the label kubernetes.io/e2e-205b1b8c-2cc2-4bee-97ac-0c55a67ffefe 95 01/18/23 22:20:23.52
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/18/23 22:20:23.527
Jan 18 22:20:23.537: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-684" to be "not pending"
Jan 18 22:20:23.555: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.024789ms
Jan 18 22:20:25.563: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.025555975s
Jan 18 22:20:25.563: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.101.216 on the node which pod4 resides and expect not scheduled 01/18/23 22:20:25.563
Jan 18 22:20:25.576: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-684" to be "not pending"
Jan 18 22:20:25.877: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 299.687314ms
Jan 18 22:20:27.887: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.309433902s
Jan 18 22:20:29.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.30500161s
Jan 18 22:20:31.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.305528127s
Jan 18 22:20:33.889: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.3110943s
Jan 18 22:20:35.890: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.31189345s
Jan 18 22:20:37.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.307232435s
Jan 18 22:20:39.887: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.308781645s
Jan 18 22:20:41.886: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.308287643s
Jan 18 22:20:43.882: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.304719344s
Jan 18 22:20:45.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.30545202s
Jan 18 22:20:47.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.305330442s
Jan 18 22:20:49.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.307450802s
Jan 18 22:20:51.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.306035918s
Jan 18 22:20:53.886: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.308663486s
Jan 18 22:20:55.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.307093449s
Jan 18 22:20:57.889: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.310804481s
Jan 18 22:20:59.887: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.309386744s
Jan 18 22:21:01.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.307320887s
Jan 18 22:21:03.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.306251302s
Jan 18 22:21:05.882: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.304572979s
Jan 18 22:21:07.888: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.310083118s
Jan 18 22:21:09.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.305258676s
Jan 18 22:21:11.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.306183495s
Jan 18 22:21:13.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.306795389s
Jan 18 22:21:15.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.307343426s
Jan 18 22:21:17.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.306965316s
Jan 18 22:21:19.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.305942233s
Jan 18 22:21:21.886: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.308644729s
Jan 18 22:21:23.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.306785332s
Jan 18 22:21:25.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.30567087s
Jan 18 22:21:27.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.307450467s
Jan 18 22:21:29.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.305689798s
Jan 18 22:21:31.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.306659027s
Jan 18 22:21:33.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.305424786s
Jan 18 22:21:35.888: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.310027661s
Jan 18 22:21:37.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.307124485s
Jan 18 22:21:39.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.305207455s
Jan 18 22:21:41.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.305471573s
Jan 18 22:21:43.888: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.310384344s
Jan 18 22:21:45.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.306383832s
Jan 18 22:21:47.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.306916789s
Jan 18 22:21:49.886: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.307849122s
Jan 18 22:21:51.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.306171342s
Jan 18 22:21:53.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.306866603s
Jan 18 22:21:55.882: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.304668165s
Jan 18 22:21:57.887: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.309375457s
Jan 18 22:21:59.905: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.327102398s
Jan 18 22:22:01.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.305521042s
Jan 18 22:22:03.889: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.311053822s
Jan 18 22:22:05.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.306215281s
Jan 18 22:22:07.895: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.317297797s
Jan 18 22:22:09.888: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.310215056s
Jan 18 22:22:11.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.306334169s
Jan 18 22:22:13.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.305732612s
Jan 18 22:22:15.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.305813992s
Jan 18 22:22:17.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.305828683s
Jan 18 22:22:19.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.304846749s
Jan 18 22:22:21.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.307204095s
Jan 18 22:22:23.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.305659954s
Jan 18 22:22:25.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.306621557s
Jan 18 22:22:27.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.306022409s
Jan 18 22:22:29.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.305089123s
Jan 18 22:22:31.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.305107654s
Jan 18 22:22:33.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.306805835s
Jan 18 22:22:35.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.306417439s
Jan 18 22:22:37.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.307216124s
Jan 18 22:22:39.882: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.304183289s
Jan 18 22:22:41.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.30631356s
Jan 18 22:22:43.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.305424538s
Jan 18 22:22:45.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.306114814s
Jan 18 22:22:47.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.307541964s
Jan 18 22:22:49.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.305732338s
Jan 18 22:22:51.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.307524746s
Jan 18 22:22:53.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.306127475s
Jan 18 22:22:55.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.307072447s
Jan 18 22:22:57.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.304788851s
Jan 18 22:22:59.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.305054333s
Jan 18 22:23:01.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.30612206s
Jan 18 22:23:03.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.306741391s
Jan 18 22:23:05.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.305719675s
Jan 18 22:23:07.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.305293022s
Jan 18 22:23:09.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.307090525s
Jan 18 22:23:11.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.306886162s
Jan 18 22:23:13.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.305739946s
Jan 18 22:23:15.886: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.307808002s
Jan 18 22:23:17.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.306660668s
Jan 18 22:23:19.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.306904686s
Jan 18 22:23:21.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.306120557s
Jan 18 22:23:23.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.306699213s
Jan 18 22:23:25.888: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.310234229s
Jan 18 22:23:27.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.30531738s
Jan 18 22:23:29.886: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.307916495s
Jan 18 22:23:31.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.306665056s
Jan 18 22:23:33.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.30607365s
Jan 18 22:23:35.888: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.310313023s
Jan 18 22:23:37.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.305511026s
Jan 18 22:23:39.886: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.308523591s
Jan 18 22:23:41.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.306057667s
Jan 18 22:23:43.882: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.304584216s
Jan 18 22:23:45.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.30662259s
Jan 18 22:23:47.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.307045737s
Jan 18 22:23:49.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.306190525s
Jan 18 22:23:51.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.305474701s
Jan 18 22:23:53.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.305446061s
Jan 18 22:23:55.886: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.308693623s
Jan 18 22:23:57.888: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.309882143s
Jan 18 22:23:59.887: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.309269217s
Jan 18 22:24:01.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.304919876s
Jan 18 22:24:03.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.305288363s
Jan 18 22:24:05.890: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.312371932s
Jan 18 22:24:07.886: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.308387037s
Jan 18 22:24:09.888: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.310632976s
Jan 18 22:24:11.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.30770981s
Jan 18 22:24:13.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.30480662s
Jan 18 22:24:15.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.307183398s
Jan 18 22:24:17.889: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.311081409s
Jan 18 22:24:19.886: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.307836383s
Jan 18 22:24:21.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.305994815s
Jan 18 22:24:23.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.304967886s
Jan 18 22:24:25.886: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.307739694s
Jan 18 22:24:27.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.306110133s
Jan 18 22:24:29.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.30583249s
Jan 18 22:24:31.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.305515917s
Jan 18 22:24:33.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.305906484s
Jan 18 22:24:35.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.305386729s
Jan 18 22:24:37.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.307042836s
Jan 18 22:24:39.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.306709214s
Jan 18 22:24:41.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.30638721s
Jan 18 22:24:43.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.30551375s
Jan 18 22:24:45.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.305060136s
Jan 18 22:24:47.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.305774392s
Jan 18 22:24:49.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.30706454s
Jan 18 22:24:51.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.305528819s
Jan 18 22:24:53.887: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.309426933s
Jan 18 22:24:55.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.307148223s
Jan 18 22:24:57.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.306599909s
Jan 18 22:24:59.886: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.30837542s
Jan 18 22:25:01.886: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.307936023s
Jan 18 22:25:03.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.305569765s
Jan 18 22:25:05.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.306935527s
Jan 18 22:25:07.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.305708567s
Jan 18 22:25:09.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.305136032s
Jan 18 22:25:11.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.30556911s
Jan 18 22:25:13.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.306785824s
Jan 18 22:25:15.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.306392574s
Jan 18 22:25:17.887: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.309692398s
Jan 18 22:25:19.882: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.304574837s
Jan 18 22:25:21.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.306122113s
Jan 18 22:25:23.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.305881894s
Jan 18 22:25:25.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.307212772s
Jan 18 22:25:25.890: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.311839795s
STEP: removing the label kubernetes.io/e2e-205b1b8c-2cc2-4bee-97ac-0c55a67ffefe off the node v1-25-1-18760-w2 01/18/23 22:25:25.89
STEP: verifying the node doesn't have the label kubernetes.io/e2e-205b1b8c-2cc2-4bee-97ac-0c55a67ffefe 01/18/23 22:25:25.91
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 18 22:25:25.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-684" for this suite. 01/18/23 22:25:25.931
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":208,"skipped":3756,"failed":0}
------------------------------
• [SLOW TEST] [304.597 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:20:21.353
    Jan 18 22:20:21.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename sched-pred 01/18/23 22:20:21.357
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:20:21.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:20:21.383
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 18 22:20:21.391: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 18 22:20:21.402: INFO: Waiting for terminating namespaces to be deleted...
    Jan 18 22:20:21.407: INFO: 
    Logging pods the apiserver thinks is on node v1-25-1-18760-w before test
    Jan 18 22:20:21.422: INFO: helm-controller-578bd944df-dzr2c from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 22:20:21.422: INFO: 	Container manager ready: true, restart count 0
    Jan 18 22:20:21.422: INFO: notification-controller-6459696d4f-gdmj8 from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 22:20:21.422: INFO: 	Container manager ready: true, restart count 0
    Jan 18 22:20:21.422: INFO: source-controller-84f6bcbfb8-ljjjm from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 22:20:21.422: INFO: 	Container manager ready: true, restart count 0
    Jan 18 22:20:21.422: INFO: calico-node-59tts from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
    Jan 18 22:20:21.422: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 22:20:21.422: INFO: coredns-coredns-5f9b955d9-pjqxb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 22:20:21.422: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 22:20:21.422: INFO: coredns-coredns-autoscaler-7d98f7496d-7zbwb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 22:20:21.422: INFO: 	Container autoscaler ready: true, restart count 0
    Jan 18 22:20:21.422: INFO: kube-proxy-rzflq from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
    Jan 18 22:20:21.422: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 22:20:21.422: INFO: metrics-server-58498b9c56-q88cp from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 22:20:21.422: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 18 22:20:21.422: INFO: nginx-proxy-v1-25-1-18760-w from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
    Jan 18 22:20:21.422: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 18 22:20:21.422: INFO: nodelocaldns-skdxm from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
    Jan 18 22:20:21.422: INFO: 	Container node-cache ready: true, restart count 0
    Jan 18 22:20:21.422: INFO: openstack-cinder-csi-controllerplugin-55d57dd799-6496v from kube-system started at 2023-01-18 14:39:48 +0000 UTC (6 container statuses recorded)
    Jan 18 22:20:21.422: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 18 22:20:21.422: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan 18 22:20:21.422: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan 18 22:20:21.422: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan 18 22:20:21.422: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan 18 22:20:21.422: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 18 22:20:21.422: INFO: openstack-cinder-csi-nodeplugin-w7sxz from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
    Jan 18 22:20:21.422: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 18 22:20:21.422: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 18 22:20:21.422: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 18 22:20:21.422: INFO: sonobuoy-systemd-logs-daemon-set-c355d4758f184dbb-d68zx from sonobuoy started at 2023-01-18 21:29:30 +0000 UTC (2 container statuses recorded)
    Jan 18 22:20:21.422: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 22:20:21.422: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 18 22:20:21.422: INFO: 
    Logging pods the apiserver thinks is on node v1-25-1-18760-w2 before test
    Jan 18 22:20:21.434: INFO: suspend-false-to-true-6987q from job-3221 started at 2023-01-18 22:20:17 +0000 UTC (1 container statuses recorded)
    Jan 18 22:20:21.435: INFO: 	Container c ready: true, restart count 0
    Jan 18 22:20:21.435: INFO: suspend-false-to-true-qz49n from job-3221 started at 2023-01-18 22:20:17 +0000 UTC (1 container statuses recorded)
    Jan 18 22:20:21.435: INFO: 	Container c ready: true, restart count 0
    Jan 18 22:20:21.435: INFO: calico-node-qx4f9 from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
    Jan 18 22:20:21.435: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 22:20:21.435: INFO: coredns-coredns-5f9b955d9-qggng from kube-system started at 2023-01-18 14:40:34 +0000 UTC (1 container statuses recorded)
    Jan 18 22:20:21.436: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 22:20:21.436: INFO: kube-proxy-8r2pt from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
    Jan 18 22:20:21.436: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 22:20:21.436: INFO: nginx-proxy-v1-25-1-18760-w2 from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
    Jan 18 22:20:21.436: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 18 22:20:21.436: INFO: nodelocaldns-dzwbj from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
    Jan 18 22:20:21.436: INFO: 	Container node-cache ready: true, restart count 0
    Jan 18 22:20:21.437: INFO: openstack-cinder-csi-nodeplugin-qm575 from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
    Jan 18 22:20:21.437: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 18 22:20:21.437: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 18 22:20:21.437: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 18 22:20:21.437: INFO: sonobuoy from sonobuoy started at 2023-01-18 21:29:28 +0000 UTC (1 container statuses recorded)
    Jan 18 22:20:21.437: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 18 22:20:21.437: INFO: sonobuoy-e2e-job-2f3390b6578e4b8c from sonobuoy started at 2023-01-18 21:29:29 +0000 UTC (2 container statuses recorded)
    Jan 18 22:20:21.437: INFO: 	Container e2e ready: true, restart count 0
    Jan 18 22:20:21.438: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 22:20:21.438: INFO: sonobuoy-systemd-logs-daemon-set-c355d4758f184dbb-svdn4 from sonobuoy started at 2023-01-18 21:29:29 +0000 UTC (2 container statuses recorded)
    Jan 18 22:20:21.438: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 22:20:21.438: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 22:20:21.438
    Jan 18 22:20:21.450: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-684" to be "running"
    Jan 18 22:20:21.459: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 9.480533ms
    Jan 18 22:20:23.466: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.016028277s
    Jan 18 22:20:23.466: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 22:20:23.472
    STEP: Trying to apply a random label on the found node. 01/18/23 22:20:23.504
    STEP: verifying the node has the label kubernetes.io/e2e-205b1b8c-2cc2-4bee-97ac-0c55a67ffefe 95 01/18/23 22:20:23.52
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/18/23 22:20:23.527
    Jan 18 22:20:23.537: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-684" to be "not pending"
    Jan 18 22:20:23.555: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.024789ms
    Jan 18 22:20:25.563: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.025555975s
    Jan 18 22:20:25.563: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.101.216 on the node which pod4 resides and expect not scheduled 01/18/23 22:20:25.563
    Jan 18 22:20:25.576: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-684" to be "not pending"
    Jan 18 22:20:25.877: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 299.687314ms
    Jan 18 22:20:27.887: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.309433902s
    Jan 18 22:20:29.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.30500161s
    Jan 18 22:20:31.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.305528127s
    Jan 18 22:20:33.889: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.3110943s
    Jan 18 22:20:35.890: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.31189345s
    Jan 18 22:20:37.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.307232435s
    Jan 18 22:20:39.887: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.308781645s
    Jan 18 22:20:41.886: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.308287643s
    Jan 18 22:20:43.882: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.304719344s
    Jan 18 22:20:45.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.30545202s
    Jan 18 22:20:47.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.305330442s
    Jan 18 22:20:49.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.307450802s
    Jan 18 22:20:51.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.306035918s
    Jan 18 22:20:53.886: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.308663486s
    Jan 18 22:20:55.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.307093449s
    Jan 18 22:20:57.889: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.310804481s
    Jan 18 22:20:59.887: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.309386744s
    Jan 18 22:21:01.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.307320887s
    Jan 18 22:21:03.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.306251302s
    Jan 18 22:21:05.882: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.304572979s
    Jan 18 22:21:07.888: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.310083118s
    Jan 18 22:21:09.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.305258676s
    Jan 18 22:21:11.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.306183495s
    Jan 18 22:21:13.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.306795389s
    Jan 18 22:21:15.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.307343426s
    Jan 18 22:21:17.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.306965316s
    Jan 18 22:21:19.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.305942233s
    Jan 18 22:21:21.886: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.308644729s
    Jan 18 22:21:23.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.306785332s
    Jan 18 22:21:25.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.30567087s
    Jan 18 22:21:27.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.307450467s
    Jan 18 22:21:29.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.305689798s
    Jan 18 22:21:31.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.306659027s
    Jan 18 22:21:33.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.305424786s
    Jan 18 22:21:35.888: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.310027661s
    Jan 18 22:21:37.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.307124485s
    Jan 18 22:21:39.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.305207455s
    Jan 18 22:21:41.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.305471573s
    Jan 18 22:21:43.888: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.310384344s
    Jan 18 22:21:45.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.306383832s
    Jan 18 22:21:47.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.306916789s
    Jan 18 22:21:49.886: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.307849122s
    Jan 18 22:21:51.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.306171342s
    Jan 18 22:21:53.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.306866603s
    Jan 18 22:21:55.882: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.304668165s
    Jan 18 22:21:57.887: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.309375457s
    Jan 18 22:21:59.905: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.327102398s
    Jan 18 22:22:01.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.305521042s
    Jan 18 22:22:03.889: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.311053822s
    Jan 18 22:22:05.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.306215281s
    Jan 18 22:22:07.895: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.317297797s
    Jan 18 22:22:09.888: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.310215056s
    Jan 18 22:22:11.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.306334169s
    Jan 18 22:22:13.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.305732612s
    Jan 18 22:22:15.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.305813992s
    Jan 18 22:22:17.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.305828683s
    Jan 18 22:22:19.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.304846749s
    Jan 18 22:22:21.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.307204095s
    Jan 18 22:22:23.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.305659954s
    Jan 18 22:22:25.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.306621557s
    Jan 18 22:22:27.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.306022409s
    Jan 18 22:22:29.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.305089123s
    Jan 18 22:22:31.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.305107654s
    Jan 18 22:22:33.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.306805835s
    Jan 18 22:22:35.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.306417439s
    Jan 18 22:22:37.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.307216124s
    Jan 18 22:22:39.882: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.304183289s
    Jan 18 22:22:41.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.30631356s
    Jan 18 22:22:43.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.305424538s
    Jan 18 22:22:45.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.306114814s
    Jan 18 22:22:47.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.307541964s
    Jan 18 22:22:49.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.305732338s
    Jan 18 22:22:51.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.307524746s
    Jan 18 22:22:53.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.306127475s
    Jan 18 22:22:55.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.307072447s
    Jan 18 22:22:57.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.304788851s
    Jan 18 22:22:59.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.305054333s
    Jan 18 22:23:01.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.30612206s
    Jan 18 22:23:03.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.306741391s
    Jan 18 22:23:05.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.305719675s
    Jan 18 22:23:07.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.305293022s
    Jan 18 22:23:09.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.307090525s
    Jan 18 22:23:11.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.306886162s
    Jan 18 22:23:13.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.305739946s
    Jan 18 22:23:15.886: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.307808002s
    Jan 18 22:23:17.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.306660668s
    Jan 18 22:23:19.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.306904686s
    Jan 18 22:23:21.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.306120557s
    Jan 18 22:23:23.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.306699213s
    Jan 18 22:23:25.888: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.310234229s
    Jan 18 22:23:27.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.30531738s
    Jan 18 22:23:29.886: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.307916495s
    Jan 18 22:23:31.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.306665056s
    Jan 18 22:23:33.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.30607365s
    Jan 18 22:23:35.888: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.310313023s
    Jan 18 22:23:37.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.305511026s
    Jan 18 22:23:39.886: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.308523591s
    Jan 18 22:23:41.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.306057667s
    Jan 18 22:23:43.882: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.304584216s
    Jan 18 22:23:45.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.30662259s
    Jan 18 22:23:47.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.307045737s
    Jan 18 22:23:49.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.306190525s
    Jan 18 22:23:51.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.305474701s
    Jan 18 22:23:53.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.305446061s
    Jan 18 22:23:55.886: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.308693623s
    Jan 18 22:23:57.888: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.309882143s
    Jan 18 22:23:59.887: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.309269217s
    Jan 18 22:24:01.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.304919876s
    Jan 18 22:24:03.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.305288363s
    Jan 18 22:24:05.890: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.312371932s
    Jan 18 22:24:07.886: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.308387037s
    Jan 18 22:24:09.888: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.310632976s
    Jan 18 22:24:11.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.30770981s
    Jan 18 22:24:13.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.30480662s
    Jan 18 22:24:15.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.307183398s
    Jan 18 22:24:17.889: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.311081409s
    Jan 18 22:24:19.886: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.307836383s
    Jan 18 22:24:21.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.305994815s
    Jan 18 22:24:23.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.304967886s
    Jan 18 22:24:25.886: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.307739694s
    Jan 18 22:24:27.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.306110133s
    Jan 18 22:24:29.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.30583249s
    Jan 18 22:24:31.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.305515917s
    Jan 18 22:24:33.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.305906484s
    Jan 18 22:24:35.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.305386729s
    Jan 18 22:24:37.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.307042836s
    Jan 18 22:24:39.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.306709214s
    Jan 18 22:24:41.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.30638721s
    Jan 18 22:24:43.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.30551375s
    Jan 18 22:24:45.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.305060136s
    Jan 18 22:24:47.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.305774392s
    Jan 18 22:24:49.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.30706454s
    Jan 18 22:24:51.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.305528819s
    Jan 18 22:24:53.887: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.309426933s
    Jan 18 22:24:55.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.307148223s
    Jan 18 22:24:57.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.306599909s
    Jan 18 22:24:59.886: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.30837542s
    Jan 18 22:25:01.886: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.307936023s
    Jan 18 22:25:03.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.305569765s
    Jan 18 22:25:05.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.306935527s
    Jan 18 22:25:07.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.305708567s
    Jan 18 22:25:09.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.305136032s
    Jan 18 22:25:11.883: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.30556911s
    Jan 18 22:25:13.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.306785824s
    Jan 18 22:25:15.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.306392574s
    Jan 18 22:25:17.887: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.309692398s
    Jan 18 22:25:19.882: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.304574837s
    Jan 18 22:25:21.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.306122113s
    Jan 18 22:25:23.884: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.305881894s
    Jan 18 22:25:25.885: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.307212772s
    Jan 18 22:25:25.890: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.311839795s
    STEP: removing the label kubernetes.io/e2e-205b1b8c-2cc2-4bee-97ac-0c55a67ffefe off the node v1-25-1-18760-w2 01/18/23 22:25:25.89
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-205b1b8c-2cc2-4bee-97ac-0c55a67ffefe 01/18/23 22:25:25.91
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 22:25:25.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-684" for this suite. 01/18/23 22:25:25.931
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:25:25.952
Jan 18 22:25:25.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename job 01/18/23 22:25:25.955
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:25.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:25.997
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 01/18/23 22:25:26.023
STEP: Patching the Job 01/18/23 22:25:26.033
STEP: Watching for Job to be patched 01/18/23 22:25:26.058
Jan 18 22:25:26.061: INFO: Event ADDED observed for Job e2e-vjnks in namespace job-8866 with labels: map[e2e-job-label:e2e-vjnks] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 18 22:25:26.061: INFO: Event MODIFIED observed for Job e2e-vjnks in namespace job-8866 with labels: map[e2e-job-label:e2e-vjnks] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 18 22:25:26.061: INFO: Event MODIFIED found for Job e2e-vjnks in namespace job-8866 with labels: map[e2e-job-label:e2e-vjnks e2e-vjnks:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 01/18/23 22:25:26.061
STEP: Watching for Job to be updated 01/18/23 22:25:26.096
Jan 18 22:25:26.099: INFO: Event MODIFIED found for Job e2e-vjnks in namespace job-8866 with labels: map[e2e-job-label:e2e-vjnks e2e-vjnks:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 22:25:26.099: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 01/18/23 22:25:26.099
Jan 18 22:25:26.105: INFO: Job: e2e-vjnks as labels: map[e2e-job-label:e2e-vjnks e2e-vjnks:patched]
STEP: Waiting for job to complete 01/18/23 22:25:26.105
STEP: Delete a job collection with a labelselector 01/18/23 22:25:34.111
STEP: Watching for Job to be deleted 01/18/23 22:25:34.117
Jan 18 22:25:34.121: INFO: Event MODIFIED observed for Job e2e-vjnks in namespace job-8866 with labels: map[e2e-job-label:e2e-vjnks e2e-vjnks:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 22:25:34.121: INFO: Event MODIFIED observed for Job e2e-vjnks in namespace job-8866 with labels: map[e2e-job-label:e2e-vjnks e2e-vjnks:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 22:25:34.122: INFO: Event MODIFIED observed for Job e2e-vjnks in namespace job-8866 with labels: map[e2e-job-label:e2e-vjnks e2e-vjnks:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 22:25:34.122: INFO: Event MODIFIED observed for Job e2e-vjnks in namespace job-8866 with labels: map[e2e-job-label:e2e-vjnks e2e-vjnks:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 22:25:34.122: INFO: Event MODIFIED observed for Job e2e-vjnks in namespace job-8866 with labels: map[e2e-job-label:e2e-vjnks e2e-vjnks:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 22:25:34.122: INFO: Event DELETED found for Job e2e-vjnks in namespace job-8866 with labels: map[e2e-job-label:e2e-vjnks e2e-vjnks:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 01/18/23 22:25:34.122
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 18 22:25:34.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8866" for this suite. 01/18/23 22:25:34.134
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":209,"skipped":3756,"failed":0}
------------------------------
• [SLOW TEST] [8.207 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:25:25.952
    Jan 18 22:25:25.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename job 01/18/23 22:25:25.955
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:25.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:25.997
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 01/18/23 22:25:26.023
    STEP: Patching the Job 01/18/23 22:25:26.033
    STEP: Watching for Job to be patched 01/18/23 22:25:26.058
    Jan 18 22:25:26.061: INFO: Event ADDED observed for Job e2e-vjnks in namespace job-8866 with labels: map[e2e-job-label:e2e-vjnks] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 18 22:25:26.061: INFO: Event MODIFIED observed for Job e2e-vjnks in namespace job-8866 with labels: map[e2e-job-label:e2e-vjnks] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 18 22:25:26.061: INFO: Event MODIFIED found for Job e2e-vjnks in namespace job-8866 with labels: map[e2e-job-label:e2e-vjnks e2e-vjnks:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 01/18/23 22:25:26.061
    STEP: Watching for Job to be updated 01/18/23 22:25:26.096
    Jan 18 22:25:26.099: INFO: Event MODIFIED found for Job e2e-vjnks in namespace job-8866 with labels: map[e2e-job-label:e2e-vjnks e2e-vjnks:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 22:25:26.099: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 01/18/23 22:25:26.099
    Jan 18 22:25:26.105: INFO: Job: e2e-vjnks as labels: map[e2e-job-label:e2e-vjnks e2e-vjnks:patched]
    STEP: Waiting for job to complete 01/18/23 22:25:26.105
    STEP: Delete a job collection with a labelselector 01/18/23 22:25:34.111
    STEP: Watching for Job to be deleted 01/18/23 22:25:34.117
    Jan 18 22:25:34.121: INFO: Event MODIFIED observed for Job e2e-vjnks in namespace job-8866 with labels: map[e2e-job-label:e2e-vjnks e2e-vjnks:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 22:25:34.121: INFO: Event MODIFIED observed for Job e2e-vjnks in namespace job-8866 with labels: map[e2e-job-label:e2e-vjnks e2e-vjnks:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 22:25:34.122: INFO: Event MODIFIED observed for Job e2e-vjnks in namespace job-8866 with labels: map[e2e-job-label:e2e-vjnks e2e-vjnks:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 22:25:34.122: INFO: Event MODIFIED observed for Job e2e-vjnks in namespace job-8866 with labels: map[e2e-job-label:e2e-vjnks e2e-vjnks:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 22:25:34.122: INFO: Event MODIFIED observed for Job e2e-vjnks in namespace job-8866 with labels: map[e2e-job-label:e2e-vjnks e2e-vjnks:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 22:25:34.122: INFO: Event DELETED found for Job e2e-vjnks in namespace job-8866 with labels: map[e2e-job-label:e2e-vjnks e2e-vjnks:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 01/18/23 22:25:34.122
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 18 22:25:34.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-8866" for this suite. 01/18/23 22:25:34.134
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:25:34.166
Jan 18 22:25:34.166: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename statefulset 01/18/23 22:25:34.168
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:34.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:34.213
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-380 01/18/23 22:25:34.218
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-380 01/18/23 22:25:34.224
Jan 18 22:25:34.249: INFO: Found 0 stateful pods, waiting for 1
Jan 18 22:25:44.256: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 01/18/23 22:25:44.269
STEP: updating a scale subresource 01/18/23 22:25:44.273
STEP: verifying the statefulset Spec.Replicas was modified 01/18/23 22:25:44.287
STEP: Patch a scale subresource 01/18/23 22:25:44.315
STEP: verifying the statefulset Spec.Replicas was modified 01/18/23 22:25:44.334
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 22:25:44.362: INFO: Deleting all statefulset in ns statefulset-380
Jan 18 22:25:44.367: INFO: Scaling statefulset ss to 0
Jan 18 22:25:54.390: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 22:25:54.395: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 18 22:25:54.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-380" for this suite. 01/18/23 22:25:54.452
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":210,"skipped":3771,"failed":0}
------------------------------
• [SLOW TEST] [20.295 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:25:34.166
    Jan 18 22:25:34.166: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename statefulset 01/18/23 22:25:34.168
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:34.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:34.213
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-380 01/18/23 22:25:34.218
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-380 01/18/23 22:25:34.224
    Jan 18 22:25:34.249: INFO: Found 0 stateful pods, waiting for 1
    Jan 18 22:25:44.256: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 01/18/23 22:25:44.269
    STEP: updating a scale subresource 01/18/23 22:25:44.273
    STEP: verifying the statefulset Spec.Replicas was modified 01/18/23 22:25:44.287
    STEP: Patch a scale subresource 01/18/23 22:25:44.315
    STEP: verifying the statefulset Spec.Replicas was modified 01/18/23 22:25:44.334
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 18 22:25:44.362: INFO: Deleting all statefulset in ns statefulset-380
    Jan 18 22:25:44.367: INFO: Scaling statefulset ss to 0
    Jan 18 22:25:54.390: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 22:25:54.395: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 18 22:25:54.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-380" for this suite. 01/18/23 22:25:54.452
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:25:54.48
Jan 18 22:25:54.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename svc-latency 01/18/23 22:25:54.482
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:54.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:54.521
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jan 18 22:25:54.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3779 01/18/23 22:25:54.544
I0118 22:25:54.556263      19 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3779, replica count: 1
I0118 22:25:55.607764      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 22:25:56.608150      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 22:25:56.727: INFO: Created: latency-svc-q5bvq
Jan 18 22:25:56.733: INFO: Got endpoints: latency-svc-q5bvq [23.883031ms]
Jan 18 22:25:56.764: INFO: Created: latency-svc-4fr4s
Jan 18 22:25:56.782: INFO: Got endpoints: latency-svc-4fr4s [47.94756ms]
Jan 18 22:25:56.782: INFO: Created: latency-svc-8w7tt
Jan 18 22:25:56.787: INFO: Created: latency-svc-qhf67
Jan 18 22:25:56.809: INFO: Got endpoints: latency-svc-8w7tt [73.736731ms]
Jan 18 22:25:56.812: INFO: Created: latency-svc-tmj48
Jan 18 22:25:56.820: INFO: Created: latency-svc-nskcs
Jan 18 22:25:56.853: INFO: Got endpoints: latency-svc-nskcs [116.712428ms]
Jan 18 22:25:56.853: INFO: Got endpoints: latency-svc-qhf67 [116.803682ms]
Jan 18 22:25:56.853: INFO: Got endpoints: latency-svc-tmj48 [117.405361ms]
Jan 18 22:25:56.861: INFO: Created: latency-svc-rh4pn
Jan 18 22:25:56.883: INFO: Got endpoints: latency-svc-rh4pn [146.119135ms]
Jan 18 22:25:56.884: INFO: Created: latency-svc-4mh74
Jan 18 22:25:56.900: INFO: Created: latency-svc-dhm9f
Jan 18 22:25:56.932: INFO: Got endpoints: latency-svc-dhm9f [194.704755ms]
Jan 18 22:25:56.933: INFO: Got endpoints: latency-svc-4mh74 [194.879379ms]
Jan 18 22:25:56.939: INFO: Created: latency-svc-j2ckt
Jan 18 22:25:56.947: INFO: Created: latency-svc-wzzf2
Jan 18 22:25:56.955: INFO: Got endpoints: latency-svc-j2ckt [217.395787ms]
Jan 18 22:25:56.962: INFO: Created: latency-svc-cph2s
Jan 18 22:25:56.966: INFO: Got endpoints: latency-svc-wzzf2 [228.58307ms]
Jan 18 22:25:56.981: INFO: Got endpoints: latency-svc-cph2s [243.572511ms]
Jan 18 22:25:56.982: INFO: Created: latency-svc-xj87z
Jan 18 22:25:56.989: INFO: Created: latency-svc-jsz5b
Jan 18 22:25:56.997: INFO: Got endpoints: latency-svc-xj87z [260.185188ms]
Jan 18 22:25:57.002: INFO: Got endpoints: latency-svc-jsz5b [264.701677ms]
Jan 18 22:25:57.004: INFO: Created: latency-svc-ccjm6
Jan 18 22:25:57.027: INFO: Created: latency-svc-9bn6f
Jan 18 22:25:57.032: INFO: Got endpoints: latency-svc-ccjm6 [294.388983ms]
Jan 18 22:25:57.046: INFO: Got endpoints: latency-svc-9bn6f [307.819572ms]
Jan 18 22:25:57.047: INFO: Created: latency-svc-vx2qh
Jan 18 22:25:57.064: INFO: Got endpoints: latency-svc-vx2qh [281.955904ms]
Jan 18 22:25:57.065: INFO: Created: latency-svc-7qc54
Jan 18 22:25:57.080: INFO: Got endpoints: latency-svc-7qc54 [271.460711ms]
Jan 18 22:25:57.181: INFO: Created: latency-svc-p89xw
Jan 18 22:25:57.188: INFO: Created: latency-svc-zn5pv
Jan 18 22:25:57.188: INFO: Created: latency-svc-9jrr2
Jan 18 22:25:57.188: INFO: Created: latency-svc-8kzkh
Jan 18 22:25:57.189: INFO: Created: latency-svc-mfvb9
Jan 18 22:25:57.189: INFO: Created: latency-svc-cxs7p
Jan 18 22:25:57.189: INFO: Created: latency-svc-ntl7h
Jan 18 22:25:57.189: INFO: Created: latency-svc-kwt8j
Jan 18 22:25:57.189: INFO: Created: latency-svc-jw5pl
Jan 18 22:25:57.190: INFO: Created: latency-svc-zm5d8
Jan 18 22:25:57.190: INFO: Created: latency-svc-729wh
Jan 18 22:25:57.190: INFO: Created: latency-svc-kswl9
Jan 18 22:25:57.190: INFO: Created: latency-svc-nghlw
Jan 18 22:25:57.191: INFO: Created: latency-svc-pwjxr
Jan 18 22:25:57.191: INFO: Created: latency-svc-x9bq4
Jan 18 22:25:57.232: INFO: Got endpoints: latency-svc-ntl7h [299.446605ms]
Jan 18 22:25:57.270: INFO: Got endpoints: latency-svc-kwt8j [303.840805ms]
Jan 18 22:25:57.271: INFO: Got endpoints: latency-svc-pwjxr [189.92768ms]
Jan 18 22:25:57.271: INFO: Got endpoints: latency-svc-zm5d8 [289.543128ms]
Jan 18 22:25:57.276: INFO: Got endpoints: latency-svc-mfvb9 [342.42446ms]
Jan 18 22:25:57.278: INFO: Got endpoints: latency-svc-kswl9 [279.984138ms]
Jan 18 22:25:57.299: INFO: Created: latency-svc-xqcwl
Jan 18 22:25:57.306: INFO: Got endpoints: latency-svc-nghlw [350.303641ms]
Jan 18 22:25:57.307: INFO: Got endpoints: latency-svc-9jrr2 [241.698437ms]
Jan 18 22:25:57.319: INFO: Got endpoints: latency-svc-zn5pv [272.535784ms]
Jan 18 22:25:57.319: INFO: Got endpoints: latency-svc-p89xw [316.101298ms]
Jan 18 22:25:57.326: INFO: Got endpoints: latency-svc-729wh [293.94223ms]
Jan 18 22:25:57.337: INFO: Created: latency-svc-xdc7p
Jan 18 22:25:57.359: INFO: Got endpoints: latency-svc-x9bq4 [475.769669ms]
Jan 18 22:25:57.361: INFO: Created: latency-svc-kr6pr
Jan 18 22:25:57.359: INFO: Got endpoints: latency-svc-cxs7p [504.439625ms]
Jan 18 22:25:57.359: INFO: Got endpoints: latency-svc-8kzkh [505.072455ms]
Jan 18 22:25:57.359: INFO: Got endpoints: latency-svc-xqcwl [126.206893ms]
Jan 18 22:25:57.359: INFO: Got endpoints: latency-svc-jw5pl [503.83151ms]
Jan 18 22:25:57.382: INFO: Got endpoints: latency-svc-xdc7p [111.214189ms]
Jan 18 22:25:57.418: INFO: Got endpoints: latency-svc-kr6pr [141.17134ms]
Jan 18 22:25:57.419: INFO: Created: latency-svc-rcpwt
Jan 18 22:25:57.441: INFO: Got endpoints: latency-svc-rcpwt [163.223197ms]
Jan 18 22:25:57.447: INFO: Created: latency-svc-86gws
Jan 18 22:25:57.500: INFO: Got endpoints: latency-svc-86gws [229.557228ms]
Jan 18 22:25:57.501: INFO: Created: latency-svc-p2htv
Jan 18 22:25:57.524: INFO: Created: latency-svc-sg2nf
Jan 18 22:25:57.529: INFO: Got endpoints: latency-svc-p2htv [257.570637ms]
Jan 18 22:25:57.545: INFO: Got endpoints: latency-svc-sg2nf [237.965044ms]
Jan 18 22:25:57.546: INFO: Created: latency-svc-l8mvq
Jan 18 22:25:57.579: INFO: Got endpoints: latency-svc-l8mvq [270.974785ms]
Jan 18 22:25:57.580: INFO: Created: latency-svc-5dzsp
Jan 18 22:25:57.585: INFO: Created: latency-svc-b8kjq
Jan 18 22:25:57.608: INFO: Got endpoints: latency-svc-5dzsp [289.751088ms]
Jan 18 22:25:57.624: INFO: Created: latency-svc-pr2dn
Jan 18 22:25:57.630: INFO: Created: latency-svc-2hvsr
Jan 18 22:25:57.637: INFO: Got endpoints: latency-svc-b8kjq [318.180201ms]
Jan 18 22:25:57.659: INFO: Got endpoints: latency-svc-pr2dn [331.564847ms]
Jan 18 22:25:57.674: INFO: Created: latency-svc-ptq8n
Jan 18 22:25:57.674: INFO: Got endpoints: latency-svc-2hvsr [313.235915ms]
Jan 18 22:25:57.691: INFO: Created: latency-svc-x4572
Jan 18 22:25:57.719: INFO: Got endpoints: latency-svc-ptq8n [355.262601ms]
Jan 18 22:25:57.719: INFO: Got endpoints: latency-svc-x4572 [356.667467ms]
Jan 18 22:25:58.051: INFO: Created: latency-svc-rrk7f
Jan 18 22:25:58.067: INFO: Created: latency-svc-jsx5l
Jan 18 22:25:58.068: INFO: Created: latency-svc-gs8hj
Jan 18 22:25:58.070: INFO: Created: latency-svc-nv6hl
Jan 18 22:25:58.070: INFO: Created: latency-svc-hpdzh
Jan 18 22:25:58.073: INFO: Created: latency-svc-x6mdr
Jan 18 22:25:58.073: INFO: Created: latency-svc-fv8mx
Jan 18 22:25:58.074: INFO: Created: latency-svc-j8cqf
Jan 18 22:25:58.076: INFO: Created: latency-svc-r9xqz
Jan 18 22:25:58.077: INFO: Created: latency-svc-7jgg6
Jan 18 22:25:58.078: INFO: Created: latency-svc-2g42g
Jan 18 22:25:58.078: INFO: Created: latency-svc-n9994
Jan 18 22:25:58.078: INFO: Created: latency-svc-pzdkr
Jan 18 22:25:58.079: INFO: Created: latency-svc-76ljs
Jan 18 22:25:58.079: INFO: Created: latency-svc-79r6q
Jan 18 22:25:58.081: INFO: Got endpoints: latency-svc-rrk7f [359.98239ms]
Jan 18 22:25:58.095: INFO: Got endpoints: latency-svc-79r6q [711.731532ms]
Jan 18 22:25:58.121: INFO: Got endpoints: latency-svc-pzdkr [399.841001ms]
Jan 18 22:25:58.125: INFO: Created: latency-svc-jbwb5
Jan 18 22:25:58.136: INFO: Got endpoints: latency-svc-nv6hl [717.804222ms]
Jan 18 22:25:58.171: INFO: Created: latency-svc-qgcdj
Jan 18 22:25:58.171: INFO: Got endpoints: latency-svc-x6mdr [562.481691ms]
Jan 18 22:25:58.172: INFO: Got endpoints: latency-svc-r9xqz [811.295668ms]
Jan 18 22:25:58.173: INFO: Got endpoints: latency-svc-hpdzh [514.224312ms]
Jan 18 22:25:58.174: INFO: Got endpoints: latency-svc-gs8hj [498.793173ms]
Jan 18 22:25:58.193: INFO: Got endpoints: latency-svc-jsx5l [553.690205ms]
Jan 18 22:25:58.195: INFO: Created: latency-svc-d6rf4
Jan 18 22:25:58.203: INFO: Got endpoints: latency-svc-n9994 [702.21732ms]
Jan 18 22:25:58.203: INFO: Got endpoints: latency-svc-j8cqf [840.75235ms]
Jan 18 22:25:58.203: INFO: Got endpoints: latency-svc-2g42g [674.021086ms]
Jan 18 22:25:58.239: INFO: Got endpoints: latency-svc-7jgg6 [693.701239ms]
Jan 18 22:25:58.303: INFO: Got endpoints: latency-svc-fv8mx [724.051462ms]
Jan 18 22:25:58.327: INFO: Created: latency-svc-mtzjk
Jan 18 22:25:58.337: INFO: Created: latency-svc-kgwc5
Jan 18 22:25:58.344: INFO: Got endpoints: latency-svc-76ljs [902.153823ms]
Jan 18 22:25:58.363: INFO: Created: latency-svc-bk67t
Jan 18 22:25:58.364: INFO: Created: latency-svc-4rtfm
Jan 18 22:25:58.364: INFO: Created: latency-svc-4xq9l
Jan 18 22:25:58.364: INFO: Created: latency-svc-p7r7j
Jan 18 22:25:58.364: INFO: Created: latency-svc-gqzwj
Jan 18 22:25:58.365: INFO: Created: latency-svc-dnr4h
Jan 18 22:25:58.365: INFO: Created: latency-svc-ffdfh
Jan 18 22:25:58.365: INFO: Created: latency-svc-njjpx
Jan 18 22:25:58.365: INFO: Created: latency-svc-g84jx
Jan 18 22:25:58.407: INFO: Created: latency-svc-zhdnh
Jan 18 22:25:58.407: INFO: Got endpoints: latency-svc-jbwb5 [326.432228ms]
Jan 18 22:25:58.427: INFO: Created: latency-svc-sp6qh
Jan 18 22:25:58.435: INFO: Got endpoints: latency-svc-qgcdj [340.074043ms]
Jan 18 22:25:58.449: INFO: Created: latency-svc-w8f9f
Jan 18 22:25:58.498: INFO: Got endpoints: latency-svc-d6rf4 [361.574686ms]
Jan 18 22:25:58.526: INFO: Created: latency-svc-s4jgm
Jan 18 22:25:58.528: INFO: Got endpoints: latency-svc-mtzjk [357.079267ms]
Jan 18 22:25:58.542: INFO: Created: latency-svc-7n7qq
Jan 18 22:25:58.586: INFO: Got endpoints: latency-svc-kgwc5 [461.569939ms]
Jan 18 22:25:58.626: INFO: Created: latency-svc-b746t
Jan 18 22:25:58.631: INFO: Got endpoints: latency-svc-njjpx [427.17753ms]
Jan 18 22:25:58.647: INFO: Created: latency-svc-t6c4p
Jan 18 22:25:58.679: INFO: Got endpoints: latency-svc-ffdfh [475.104815ms]
Jan 18 22:25:58.690: INFO: Created: latency-svc-76rpm
Jan 18 22:25:58.731: INFO: Got endpoints: latency-svc-bk67t [558.734215ms]
Jan 18 22:25:58.741: INFO: Created: latency-svc-6kb7x
Jan 18 22:25:58.781: INFO: Got endpoints: latency-svc-4rtfm [477.978135ms]
Jan 18 22:25:58.797: INFO: Created: latency-svc-jzqdv
Jan 18 22:25:58.834: INFO: Got endpoints: latency-svc-g84jx [595.386516ms]
Jan 18 22:25:58.860: INFO: Created: latency-svc-r55hr
Jan 18 22:25:58.893: INFO: Got endpoints: latency-svc-dnr4h [719.858297ms]
Jan 18 22:25:58.911: INFO: Created: latency-svc-gp5g7
Jan 18 22:25:58.927: INFO: Got endpoints: latency-svc-p7r7j [724.591443ms]
Jan 18 22:25:58.947: INFO: Created: latency-svc-gjj87
Jan 18 22:25:58.979: INFO: Got endpoints: latency-svc-gqzwj [805.563857ms]
Jan 18 22:25:59.002: INFO: Created: latency-svc-cktsp
Jan 18 22:25:59.031: INFO: Got endpoints: latency-svc-4xq9l [838.147771ms]
Jan 18 22:25:59.043: INFO: Created: latency-svc-frfkp
Jan 18 22:25:59.084: INFO: Got endpoints: latency-svc-zhdnh [739.934843ms]
Jan 18 22:25:59.107: INFO: Created: latency-svc-dtxrd
Jan 18 22:25:59.128: INFO: Got endpoints: latency-svc-sp6qh [721.013668ms]
Jan 18 22:25:59.143: INFO: Created: latency-svc-kq559
Jan 18 22:25:59.181: INFO: Got endpoints: latency-svc-w8f9f [745.419607ms]
Jan 18 22:25:59.192: INFO: Created: latency-svc-s8gwq
Jan 18 22:25:59.237: INFO: Got endpoints: latency-svc-s4jgm [738.853123ms]
Jan 18 22:25:59.249: INFO: Created: latency-svc-sx6gk
Jan 18 22:25:59.281: INFO: Got endpoints: latency-svc-7n7qq [752.194522ms]
Jan 18 22:25:59.293: INFO: Created: latency-svc-5bb57
Jan 18 22:25:59.327: INFO: Got endpoints: latency-svc-b746t [741.819924ms]
Jan 18 22:25:59.338: INFO: Created: latency-svc-ztwvc
Jan 18 22:25:59.378: INFO: Got endpoints: latency-svc-t6c4p [747.076696ms]
Jan 18 22:25:59.391: INFO: Created: latency-svc-gg2hk
Jan 18 22:25:59.426: INFO: Got endpoints: latency-svc-76rpm [747.578845ms]
Jan 18 22:25:59.445: INFO: Created: latency-svc-hl8mt
Jan 18 22:25:59.493: INFO: Got endpoints: latency-svc-6kb7x [761.907431ms]
Jan 18 22:25:59.518: INFO: Created: latency-svc-pvsf5
Jan 18 22:25:59.530: INFO: Got endpoints: latency-svc-jzqdv [748.994275ms]
Jan 18 22:25:59.561: INFO: Created: latency-svc-h4mr9
Jan 18 22:25:59.581: INFO: Got endpoints: latency-svc-r55hr [746.906964ms]
Jan 18 22:25:59.599: INFO: Created: latency-svc-mhm7m
Jan 18 22:25:59.635: INFO: Got endpoints: latency-svc-gp5g7 [741.532989ms]
Jan 18 22:25:59.658: INFO: Created: latency-svc-wdmkc
Jan 18 22:25:59.677: INFO: Got endpoints: latency-svc-gjj87 [749.671837ms]
Jan 18 22:25:59.695: INFO: Created: latency-svc-xn5l2
Jan 18 22:25:59.729: INFO: Got endpoints: latency-svc-cktsp [749.479624ms]
Jan 18 22:25:59.747: INFO: Created: latency-svc-j7ks6
Jan 18 22:25:59.782: INFO: Got endpoints: latency-svc-frfkp [750.611459ms]
Jan 18 22:25:59.801: INFO: Created: latency-svc-g5tff
Jan 18 22:25:59.833: INFO: Got endpoints: latency-svc-dtxrd [748.282575ms]
Jan 18 22:25:59.858: INFO: Created: latency-svc-9tps9
Jan 18 22:25:59.892: INFO: Got endpoints: latency-svc-kq559 [763.648887ms]
Jan 18 22:25:59.908: INFO: Created: latency-svc-6j9ml
Jan 18 22:25:59.941: INFO: Got endpoints: latency-svc-s8gwq [760.072884ms]
Jan 18 22:25:59.965: INFO: Created: latency-svc-4qghp
Jan 18 22:25:59.990: INFO: Got endpoints: latency-svc-sx6gk [753.177438ms]
Jan 18 22:26:00.041: INFO: Created: latency-svc-tcgbc
Jan 18 22:26:00.043: INFO: Got endpoints: latency-svc-5bb57 [761.633851ms]
Jan 18 22:26:00.063: INFO: Created: latency-svc-jd5n9
Jan 18 22:26:00.098: INFO: Got endpoints: latency-svc-ztwvc [770.884644ms]
Jan 18 22:26:00.119: INFO: Created: latency-svc-p7p9b
Jan 18 22:26:00.142: INFO: Got endpoints: latency-svc-gg2hk [764.690282ms]
Jan 18 22:26:00.169: INFO: Created: latency-svc-lrljd
Jan 18 22:26:00.186: INFO: Got endpoints: latency-svc-hl8mt [759.922518ms]
Jan 18 22:26:00.201: INFO: Created: latency-svc-ts5l5
Jan 18 22:26:00.230: INFO: Got endpoints: latency-svc-pvsf5 [735.749724ms]
Jan 18 22:26:00.245: INFO: Created: latency-svc-srhcg
Jan 18 22:26:00.283: INFO: Got endpoints: latency-svc-h4mr9 [752.363064ms]
Jan 18 22:26:00.307: INFO: Created: latency-svc-897bx
Jan 18 22:26:00.329: INFO: Got endpoints: latency-svc-mhm7m [747.454698ms]
Jan 18 22:26:00.382: INFO: Got endpoints: latency-svc-wdmkc [746.430419ms]
Jan 18 22:26:00.391: INFO: Created: latency-svc-nvzfz
Jan 18 22:26:00.400: INFO: Created: latency-svc-lwswz
Jan 18 22:26:00.428: INFO: Got endpoints: latency-svc-xn5l2 [750.434636ms]
Jan 18 22:26:00.453: INFO: Created: latency-svc-59klk
Jan 18 22:26:00.479: INFO: Got endpoints: latency-svc-j7ks6 [750.439316ms]
Jan 18 22:26:00.491: INFO: Created: latency-svc-jjnbx
Jan 18 22:26:00.592: INFO: Got endpoints: latency-svc-g5tff [810.344291ms]
Jan 18 22:26:00.619: INFO: Created: latency-svc-7pn7v
Jan 18 22:26:00.632: INFO: Got endpoints: latency-svc-9tps9 [798.695155ms]
Jan 18 22:26:00.648: INFO: Created: latency-svc-5qxgw
Jan 18 22:26:00.678: INFO: Got endpoints: latency-svc-6j9ml [786.005928ms]
Jan 18 22:26:00.691: INFO: Created: latency-svc-5chg2
Jan 18 22:26:00.731: INFO: Got endpoints: latency-svc-4qghp [788.799875ms]
Jan 18 22:26:00.769: INFO: Created: latency-svc-pv9p8
Jan 18 22:26:00.777: INFO: Got endpoints: latency-svc-tcgbc [786.566668ms]
Jan 18 22:26:00.813: INFO: Created: latency-svc-26bnd
Jan 18 22:26:00.832: INFO: Got endpoints: latency-svc-jd5n9 [788.360483ms]
Jan 18 22:26:00.850: INFO: Created: latency-svc-z6vsg
Jan 18 22:26:00.877: INFO: Got endpoints: latency-svc-p7p9b [778.360755ms]
Jan 18 22:26:00.889: INFO: Created: latency-svc-lttgb
Jan 18 22:26:00.932: INFO: Got endpoints: latency-svc-lrljd [788.756121ms]
Jan 18 22:26:00.946: INFO: Created: latency-svc-m69xw
Jan 18 22:26:00.984: INFO: Got endpoints: latency-svc-ts5l5 [797.257398ms]
Jan 18 22:26:01.012: INFO: Created: latency-svc-j2dqb
Jan 18 22:26:01.032: INFO: Got endpoints: latency-svc-srhcg [801.588742ms]
Jan 18 22:26:01.044: INFO: Created: latency-svc-9tgdp
Jan 18 22:26:01.088: INFO: Got endpoints: latency-svc-897bx [805.551272ms]
Jan 18 22:26:01.107: INFO: Created: latency-svc-p8flt
Jan 18 22:26:01.133: INFO: Got endpoints: latency-svc-nvzfz [803.308843ms]
Jan 18 22:26:01.151: INFO: Created: latency-svc-8gjgd
Jan 18 22:26:01.181: INFO: Got endpoints: latency-svc-lwswz [799.240955ms]
Jan 18 22:26:01.192: INFO: Created: latency-svc-9w6pm
Jan 18 22:26:01.226: INFO: Got endpoints: latency-svc-59klk [797.252052ms]
Jan 18 22:26:01.243: INFO: Created: latency-svc-tjfwl
Jan 18 22:26:01.285: INFO: Got endpoints: latency-svc-jjnbx [805.615963ms]
Jan 18 22:26:01.298: INFO: Created: latency-svc-6zbf7
Jan 18 22:26:01.329: INFO: Got endpoints: latency-svc-7pn7v [736.830691ms]
Jan 18 22:26:01.343: INFO: Created: latency-svc-v8zp6
Jan 18 22:26:01.386: INFO: Got endpoints: latency-svc-5qxgw [753.659005ms]
Jan 18 22:26:01.399: INFO: Created: latency-svc-zdn2s
Jan 18 22:26:01.426: INFO: Got endpoints: latency-svc-5chg2 [747.8267ms]
Jan 18 22:26:01.445: INFO: Created: latency-svc-nklsq
Jan 18 22:26:01.480: INFO: Got endpoints: latency-svc-pv9p8 [749.313093ms]
Jan 18 22:26:01.496: INFO: Created: latency-svc-5vhjh
Jan 18 22:26:01.530: INFO: Got endpoints: latency-svc-26bnd [752.941398ms]
Jan 18 22:26:01.542: INFO: Created: latency-svc-k9n6d
Jan 18 22:26:01.581: INFO: Got endpoints: latency-svc-z6vsg [748.790577ms]
Jan 18 22:26:01.597: INFO: Created: latency-svc-6k7t6
Jan 18 22:26:01.630: INFO: Got endpoints: latency-svc-lttgb [752.861936ms]
Jan 18 22:26:01.658: INFO: Created: latency-svc-xdwdw
Jan 18 22:26:01.695: INFO: Got endpoints: latency-svc-m69xw [762.832382ms]
Jan 18 22:26:01.707: INFO: Created: latency-svc-t6hm2
Jan 18 22:26:01.727: INFO: Got endpoints: latency-svc-j2dqb [743.294122ms]
Jan 18 22:26:01.747: INFO: Created: latency-svc-w7wbz
Jan 18 22:26:01.777: INFO: Got endpoints: latency-svc-9tgdp [745.409305ms]
Jan 18 22:26:01.797: INFO: Created: latency-svc-7pr6f
Jan 18 22:26:01.835: INFO: Got endpoints: latency-svc-p8flt [746.186652ms]
Jan 18 22:26:01.847: INFO: Created: latency-svc-fk7p8
Jan 18 22:26:01.878: INFO: Got endpoints: latency-svc-8gjgd [744.533465ms]
Jan 18 22:26:01.899: INFO: Created: latency-svc-x22mv
Jan 18 22:26:01.938: INFO: Got endpoints: latency-svc-9w6pm [757.350753ms]
Jan 18 22:26:01.969: INFO: Created: latency-svc-l7hrz
Jan 18 22:26:01.988: INFO: Got endpoints: latency-svc-tjfwl [762.161582ms]
Jan 18 22:26:02.005: INFO: Created: latency-svc-9x5sw
Jan 18 22:26:02.029: INFO: Got endpoints: latency-svc-6zbf7 [743.991805ms]
Jan 18 22:26:02.050: INFO: Created: latency-svc-t2lw5
Jan 18 22:26:02.084: INFO: Got endpoints: latency-svc-v8zp6 [754.843676ms]
Jan 18 22:26:02.112: INFO: Created: latency-svc-f6vx5
Jan 18 22:26:02.131: INFO: Got endpoints: latency-svc-zdn2s [745.423357ms]
Jan 18 22:26:02.142: INFO: Created: latency-svc-bpzsj
Jan 18 22:26:02.177: INFO: Got endpoints: latency-svc-nklsq [750.754189ms]
Jan 18 22:26:02.209: INFO: Created: latency-svc-v76c9
Jan 18 22:26:02.234: INFO: Got endpoints: latency-svc-5vhjh [753.647066ms]
Jan 18 22:26:02.262: INFO: Created: latency-svc-9bpsx
Jan 18 22:26:02.282: INFO: Got endpoints: latency-svc-k9n6d [752.01055ms]
Jan 18 22:26:02.306: INFO: Created: latency-svc-xl5sf
Jan 18 22:26:02.334: INFO: Got endpoints: latency-svc-6k7t6 [753.519308ms]
Jan 18 22:26:02.358: INFO: Created: latency-svc-45f9s
Jan 18 22:26:02.388: INFO: Got endpoints: latency-svc-xdwdw [757.451712ms]
Jan 18 22:26:02.463: INFO: Created: latency-svc-v7hrc
Jan 18 22:26:02.481: INFO: Got endpoints: latency-svc-t6hm2 [786.066889ms]
Jan 18 22:26:02.509: INFO: Got endpoints: latency-svc-w7wbz [781.903067ms]
Jan 18 22:26:02.511: INFO: Created: latency-svc-tzg2z
Jan 18 22:26:02.561: INFO: Got endpoints: latency-svc-7pr6f [783.25144ms]
Jan 18 22:26:02.568: INFO: Created: latency-svc-hxn9m
Jan 18 22:26:02.587: INFO: Got endpoints: latency-svc-fk7p8 [751.875618ms]
Jan 18 22:26:02.612: INFO: Created: latency-svc-rmdsn
Jan 18 22:26:02.613: INFO: Created: latency-svc-75dlh
Jan 18 22:26:02.662: INFO: Got endpoints: latency-svc-x22mv [784.588125ms]
Jan 18 22:26:02.688: INFO: Got endpoints: latency-svc-l7hrz [749.352361ms]
Jan 18 22:26:02.691: INFO: Created: latency-svc-mrzzz
Jan 18 22:26:02.707: INFO: Created: latency-svc-wcbwg
Jan 18 22:26:02.734: INFO: Got endpoints: latency-svc-9x5sw [745.460491ms]
Jan 18 22:26:02.757: INFO: Created: latency-svc-krhlf
Jan 18 22:26:02.791: INFO: Got endpoints: latency-svc-t2lw5 [761.477772ms]
Jan 18 22:26:02.833: INFO: Got endpoints: latency-svc-f6vx5 [748.78448ms]
Jan 18 22:26:02.834: INFO: Created: latency-svc-2hs5p
Jan 18 22:26:02.862: INFO: Created: latency-svc-z97fb
Jan 18 22:26:02.879: INFO: Got endpoints: latency-svc-bpzsj [748.037385ms]
Jan 18 22:26:02.892: INFO: Created: latency-svc-ddfs6
Jan 18 22:26:02.929: INFO: Got endpoints: latency-svc-v76c9 [751.14548ms]
Jan 18 22:26:02.940: INFO: Created: latency-svc-f6zxz
Jan 18 22:26:02.980: INFO: Got endpoints: latency-svc-9bpsx [745.687056ms]
Jan 18 22:26:02.999: INFO: Created: latency-svc-j2gh2
Jan 18 22:26:03.039: INFO: Got endpoints: latency-svc-xl5sf [756.308346ms]
Jan 18 22:26:03.056: INFO: Created: latency-svc-lbqr5
Jan 18 22:26:03.094: INFO: Got endpoints: latency-svc-45f9s [759.362279ms]
Jan 18 22:26:03.111: INFO: Created: latency-svc-84gnk
Jan 18 22:26:03.131: INFO: Got endpoints: latency-svc-v7hrc [742.84024ms]
Jan 18 22:26:03.144: INFO: Created: latency-svc-8jbbc
Jan 18 22:26:03.179: INFO: Got endpoints: latency-svc-tzg2z [698.66994ms]
Jan 18 22:26:03.196: INFO: Created: latency-svc-28lmq
Jan 18 22:26:03.226: INFO: Got endpoints: latency-svc-hxn9m [717.002784ms]
Jan 18 22:26:03.242: INFO: Created: latency-svc-6m527
Jan 18 22:26:03.281: INFO: Got endpoints: latency-svc-rmdsn [719.793422ms]
Jan 18 22:26:03.322: INFO: Created: latency-svc-2gscp
Jan 18 22:26:03.328: INFO: Got endpoints: latency-svc-75dlh [740.828584ms]
Jan 18 22:26:03.346: INFO: Created: latency-svc-snw6q
Jan 18 22:26:03.394: INFO: Got endpoints: latency-svc-mrzzz [731.225691ms]
Jan 18 22:26:03.413: INFO: Created: latency-svc-6k687
Jan 18 22:26:03.433: INFO: Got endpoints: latency-svc-wcbwg [745.651684ms]
Jan 18 22:26:03.448: INFO: Created: latency-svc-5jvjl
Jan 18 22:26:03.486: INFO: Got endpoints: latency-svc-krhlf [752.359983ms]
Jan 18 22:26:03.513: INFO: Created: latency-svc-cgbbf
Jan 18 22:26:03.539: INFO: Got endpoints: latency-svc-2hs5p [747.743402ms]
Jan 18 22:26:03.549: INFO: Created: latency-svc-fvwjm
Jan 18 22:26:03.579: INFO: Got endpoints: latency-svc-z97fb [744.68197ms]
Jan 18 22:26:03.606: INFO: Created: latency-svc-qccf2
Jan 18 22:26:03.630: INFO: Got endpoints: latency-svc-ddfs6 [750.798837ms]
Jan 18 22:26:03.658: INFO: Created: latency-svc-48fkh
Jan 18 22:26:03.684: INFO: Got endpoints: latency-svc-f6zxz [755.319256ms]
Jan 18 22:26:03.698: INFO: Created: latency-svc-bkgzw
Jan 18 22:26:03.738: INFO: Got endpoints: latency-svc-j2gh2 [758.077789ms]
Jan 18 22:26:03.754: INFO: Created: latency-svc-7sjvr
Jan 18 22:26:03.782: INFO: Got endpoints: latency-svc-lbqr5 [743.095646ms]
Jan 18 22:26:03.804: INFO: Created: latency-svc-vdlx2
Jan 18 22:26:03.826: INFO: Got endpoints: latency-svc-84gnk [732.455154ms]
Jan 18 22:26:03.854: INFO: Created: latency-svc-wtm79
Jan 18 22:26:03.878: INFO: Got endpoints: latency-svc-8jbbc [746.824257ms]
Jan 18 22:26:03.893: INFO: Created: latency-svc-94jl2
Jan 18 22:26:03.930: INFO: Got endpoints: latency-svc-28lmq [750.921519ms]
Jan 18 22:26:03.959: INFO: Created: latency-svc-85nsl
Jan 18 22:26:03.980: INFO: Got endpoints: latency-svc-6m527 [753.247373ms]
Jan 18 22:26:03.994: INFO: Created: latency-svc-n9fr7
Jan 18 22:26:04.028: INFO: Got endpoints: latency-svc-2gscp [746.66715ms]
Jan 18 22:26:04.047: INFO: Created: latency-svc-p2n2m
Jan 18 22:26:04.078: INFO: Got endpoints: latency-svc-snw6q [749.883642ms]
Jan 18 22:26:04.094: INFO: Created: latency-svc-t78b8
Jan 18 22:26:04.141: INFO: Got endpoints: latency-svc-6k687 [746.978963ms]
Jan 18 22:26:04.173: INFO: Created: latency-svc-5xf69
Jan 18 22:26:04.181: INFO: Got endpoints: latency-svc-5jvjl [747.322649ms]
Jan 18 22:26:04.196: INFO: Created: latency-svc-7snrz
Jan 18 22:26:04.235: INFO: Got endpoints: latency-svc-cgbbf [748.432956ms]
Jan 18 22:26:04.250: INFO: Created: latency-svc-tvf6t
Jan 18 22:26:04.288: INFO: Got endpoints: latency-svc-fvwjm [749.345819ms]
Jan 18 22:26:04.301: INFO: Created: latency-svc-sjcf9
Jan 18 22:26:04.327: INFO: Got endpoints: latency-svc-qccf2 [747.051572ms]
Jan 18 22:26:04.346: INFO: Created: latency-svc-j8hlp
Jan 18 22:26:04.376: INFO: Got endpoints: latency-svc-48fkh [735.793447ms]
Jan 18 22:26:04.392: INFO: Created: latency-svc-g4qzx
Jan 18 22:26:04.430: INFO: Got endpoints: latency-svc-bkgzw [745.867061ms]
Jan 18 22:26:04.445: INFO: Created: latency-svc-46sd8
Jan 18 22:26:04.481: INFO: Got endpoints: latency-svc-7sjvr [742.426608ms]
Jan 18 22:26:04.512: INFO: Created: latency-svc-tnqlc
Jan 18 22:26:04.531: INFO: Got endpoints: latency-svc-vdlx2 [748.508094ms]
Jan 18 22:26:04.550: INFO: Created: latency-svc-q26zs
Jan 18 22:26:04.598: INFO: Got endpoints: latency-svc-wtm79 [772.079649ms]
Jan 18 22:26:04.623: INFO: Created: latency-svc-mngp8
Jan 18 22:26:04.629: INFO: Got endpoints: latency-svc-94jl2 [751.119803ms]
Jan 18 22:26:04.682: INFO: Got endpoints: latency-svc-85nsl [751.380145ms]
Jan 18 22:26:04.738: INFO: Got endpoints: latency-svc-n9fr7 [757.916067ms]
Jan 18 22:26:04.779: INFO: Got endpoints: latency-svc-p2n2m [750.845038ms]
Jan 18 22:26:04.826: INFO: Got endpoints: latency-svc-t78b8 [747.707836ms]
Jan 18 22:26:04.887: INFO: Got endpoints: latency-svc-5xf69 [745.858631ms]
Jan 18 22:26:04.936: INFO: Got endpoints: latency-svc-7snrz [755.405643ms]
Jan 18 22:26:04.979: INFO: Got endpoints: latency-svc-tvf6t [744.631798ms]
Jan 18 22:26:05.030: INFO: Got endpoints: latency-svc-sjcf9 [741.762059ms]
Jan 18 22:26:05.078: INFO: Got endpoints: latency-svc-j8hlp [749.128102ms]
Jan 18 22:26:05.131: INFO: Got endpoints: latency-svc-g4qzx [754.392551ms]
Jan 18 22:26:05.176: INFO: Got endpoints: latency-svc-46sd8 [745.324125ms]
Jan 18 22:26:05.235: INFO: Got endpoints: latency-svc-tnqlc [753.857063ms]
Jan 18 22:26:05.279: INFO: Got endpoints: latency-svc-q26zs [748.036663ms]
Jan 18 22:26:05.326: INFO: Got endpoints: latency-svc-mngp8 [727.501662ms]
Jan 18 22:26:05.327: INFO: Latencies: [47.94756ms 73.736731ms 111.214189ms 116.712428ms 116.803682ms 117.405361ms 126.206893ms 141.17134ms 146.119135ms 163.223197ms 189.92768ms 194.704755ms 194.879379ms 217.395787ms 228.58307ms 229.557228ms 237.965044ms 241.698437ms 243.572511ms 257.570637ms 260.185188ms 264.701677ms 270.974785ms 271.460711ms 272.535784ms 279.984138ms 281.955904ms 289.543128ms 289.751088ms 293.94223ms 294.388983ms 299.446605ms 303.840805ms 307.819572ms 313.235915ms 316.101298ms 318.180201ms 326.432228ms 331.564847ms 340.074043ms 342.42446ms 350.303641ms 355.262601ms 356.667467ms 357.079267ms 359.98239ms 361.574686ms 399.841001ms 427.17753ms 461.569939ms 475.104815ms 475.769669ms 477.978135ms 498.793173ms 503.83151ms 504.439625ms 505.072455ms 514.224312ms 553.690205ms 558.734215ms 562.481691ms 595.386516ms 674.021086ms 693.701239ms 698.66994ms 702.21732ms 711.731532ms 717.002784ms 717.804222ms 719.793422ms 719.858297ms 721.013668ms 724.051462ms 724.591443ms 727.501662ms 731.225691ms 732.455154ms 735.749724ms 735.793447ms 736.830691ms 738.853123ms 739.934843ms 740.828584ms 741.532989ms 741.762059ms 741.819924ms 742.426608ms 742.84024ms 743.095646ms 743.294122ms 743.991805ms 744.533465ms 744.631798ms 744.68197ms 745.324125ms 745.409305ms 745.419607ms 745.423357ms 745.460491ms 745.651684ms 745.687056ms 745.858631ms 745.867061ms 746.186652ms 746.430419ms 746.66715ms 746.824257ms 746.906964ms 746.978963ms 747.051572ms 747.076696ms 747.322649ms 747.454698ms 747.578845ms 747.707836ms 747.743402ms 747.8267ms 748.036663ms 748.037385ms 748.282575ms 748.432956ms 748.508094ms 748.78448ms 748.790577ms 748.994275ms 749.128102ms 749.313093ms 749.345819ms 749.352361ms 749.479624ms 749.671837ms 749.883642ms 750.434636ms 750.439316ms 750.611459ms 750.754189ms 750.798837ms 750.845038ms 750.921519ms 751.119803ms 751.14548ms 751.380145ms 751.875618ms 752.01055ms 752.194522ms 752.359983ms 752.363064ms 752.861936ms 752.941398ms 753.177438ms 753.247373ms 753.519308ms 753.647066ms 753.659005ms 753.857063ms 754.392551ms 754.843676ms 755.319256ms 755.405643ms 756.308346ms 757.350753ms 757.451712ms 757.916067ms 758.077789ms 759.362279ms 759.922518ms 760.072884ms 761.477772ms 761.633851ms 761.907431ms 762.161582ms 762.832382ms 763.648887ms 764.690282ms 770.884644ms 772.079649ms 778.360755ms 781.903067ms 783.25144ms 784.588125ms 786.005928ms 786.066889ms 786.566668ms 788.360483ms 788.756121ms 788.799875ms 797.252052ms 797.257398ms 798.695155ms 799.240955ms 801.588742ms 803.308843ms 805.551272ms 805.563857ms 805.615963ms 810.344291ms 811.295668ms 838.147771ms 840.75235ms 902.153823ms]
Jan 18 22:26:05.327: INFO: 50 %ile: 745.687056ms
Jan 18 22:26:05.327: INFO: 90 %ile: 786.005928ms
Jan 18 22:26:05.327: INFO: 99 %ile: 840.75235ms
Jan 18 22:26:05.327: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Jan 18 22:26:05.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3779" for this suite. 01/18/23 22:26:05.337
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":211,"skipped":3798,"failed":0}
------------------------------
• [SLOW TEST] [10.867 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:25:54.48
    Jan 18 22:25:54.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename svc-latency 01/18/23 22:25:54.482
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:54.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:54.521
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jan 18 22:25:54.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-3779 01/18/23 22:25:54.544
    I0118 22:25:54.556263      19 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3779, replica count: 1
    I0118 22:25:55.607764      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0118 22:25:56.608150      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 22:25:56.727: INFO: Created: latency-svc-q5bvq
    Jan 18 22:25:56.733: INFO: Got endpoints: latency-svc-q5bvq [23.883031ms]
    Jan 18 22:25:56.764: INFO: Created: latency-svc-4fr4s
    Jan 18 22:25:56.782: INFO: Got endpoints: latency-svc-4fr4s [47.94756ms]
    Jan 18 22:25:56.782: INFO: Created: latency-svc-8w7tt
    Jan 18 22:25:56.787: INFO: Created: latency-svc-qhf67
    Jan 18 22:25:56.809: INFO: Got endpoints: latency-svc-8w7tt [73.736731ms]
    Jan 18 22:25:56.812: INFO: Created: latency-svc-tmj48
    Jan 18 22:25:56.820: INFO: Created: latency-svc-nskcs
    Jan 18 22:25:56.853: INFO: Got endpoints: latency-svc-nskcs [116.712428ms]
    Jan 18 22:25:56.853: INFO: Got endpoints: latency-svc-qhf67 [116.803682ms]
    Jan 18 22:25:56.853: INFO: Got endpoints: latency-svc-tmj48 [117.405361ms]
    Jan 18 22:25:56.861: INFO: Created: latency-svc-rh4pn
    Jan 18 22:25:56.883: INFO: Got endpoints: latency-svc-rh4pn [146.119135ms]
    Jan 18 22:25:56.884: INFO: Created: latency-svc-4mh74
    Jan 18 22:25:56.900: INFO: Created: latency-svc-dhm9f
    Jan 18 22:25:56.932: INFO: Got endpoints: latency-svc-dhm9f [194.704755ms]
    Jan 18 22:25:56.933: INFO: Got endpoints: latency-svc-4mh74 [194.879379ms]
    Jan 18 22:25:56.939: INFO: Created: latency-svc-j2ckt
    Jan 18 22:25:56.947: INFO: Created: latency-svc-wzzf2
    Jan 18 22:25:56.955: INFO: Got endpoints: latency-svc-j2ckt [217.395787ms]
    Jan 18 22:25:56.962: INFO: Created: latency-svc-cph2s
    Jan 18 22:25:56.966: INFO: Got endpoints: latency-svc-wzzf2 [228.58307ms]
    Jan 18 22:25:56.981: INFO: Got endpoints: latency-svc-cph2s [243.572511ms]
    Jan 18 22:25:56.982: INFO: Created: latency-svc-xj87z
    Jan 18 22:25:56.989: INFO: Created: latency-svc-jsz5b
    Jan 18 22:25:56.997: INFO: Got endpoints: latency-svc-xj87z [260.185188ms]
    Jan 18 22:25:57.002: INFO: Got endpoints: latency-svc-jsz5b [264.701677ms]
    Jan 18 22:25:57.004: INFO: Created: latency-svc-ccjm6
    Jan 18 22:25:57.027: INFO: Created: latency-svc-9bn6f
    Jan 18 22:25:57.032: INFO: Got endpoints: latency-svc-ccjm6 [294.388983ms]
    Jan 18 22:25:57.046: INFO: Got endpoints: latency-svc-9bn6f [307.819572ms]
    Jan 18 22:25:57.047: INFO: Created: latency-svc-vx2qh
    Jan 18 22:25:57.064: INFO: Got endpoints: latency-svc-vx2qh [281.955904ms]
    Jan 18 22:25:57.065: INFO: Created: latency-svc-7qc54
    Jan 18 22:25:57.080: INFO: Got endpoints: latency-svc-7qc54 [271.460711ms]
    Jan 18 22:25:57.181: INFO: Created: latency-svc-p89xw
    Jan 18 22:25:57.188: INFO: Created: latency-svc-zn5pv
    Jan 18 22:25:57.188: INFO: Created: latency-svc-9jrr2
    Jan 18 22:25:57.188: INFO: Created: latency-svc-8kzkh
    Jan 18 22:25:57.189: INFO: Created: latency-svc-mfvb9
    Jan 18 22:25:57.189: INFO: Created: latency-svc-cxs7p
    Jan 18 22:25:57.189: INFO: Created: latency-svc-ntl7h
    Jan 18 22:25:57.189: INFO: Created: latency-svc-kwt8j
    Jan 18 22:25:57.189: INFO: Created: latency-svc-jw5pl
    Jan 18 22:25:57.190: INFO: Created: latency-svc-zm5d8
    Jan 18 22:25:57.190: INFO: Created: latency-svc-729wh
    Jan 18 22:25:57.190: INFO: Created: latency-svc-kswl9
    Jan 18 22:25:57.190: INFO: Created: latency-svc-nghlw
    Jan 18 22:25:57.191: INFO: Created: latency-svc-pwjxr
    Jan 18 22:25:57.191: INFO: Created: latency-svc-x9bq4
    Jan 18 22:25:57.232: INFO: Got endpoints: latency-svc-ntl7h [299.446605ms]
    Jan 18 22:25:57.270: INFO: Got endpoints: latency-svc-kwt8j [303.840805ms]
    Jan 18 22:25:57.271: INFO: Got endpoints: latency-svc-pwjxr [189.92768ms]
    Jan 18 22:25:57.271: INFO: Got endpoints: latency-svc-zm5d8 [289.543128ms]
    Jan 18 22:25:57.276: INFO: Got endpoints: latency-svc-mfvb9 [342.42446ms]
    Jan 18 22:25:57.278: INFO: Got endpoints: latency-svc-kswl9 [279.984138ms]
    Jan 18 22:25:57.299: INFO: Created: latency-svc-xqcwl
    Jan 18 22:25:57.306: INFO: Got endpoints: latency-svc-nghlw [350.303641ms]
    Jan 18 22:25:57.307: INFO: Got endpoints: latency-svc-9jrr2 [241.698437ms]
    Jan 18 22:25:57.319: INFO: Got endpoints: latency-svc-zn5pv [272.535784ms]
    Jan 18 22:25:57.319: INFO: Got endpoints: latency-svc-p89xw [316.101298ms]
    Jan 18 22:25:57.326: INFO: Got endpoints: latency-svc-729wh [293.94223ms]
    Jan 18 22:25:57.337: INFO: Created: latency-svc-xdc7p
    Jan 18 22:25:57.359: INFO: Got endpoints: latency-svc-x9bq4 [475.769669ms]
    Jan 18 22:25:57.361: INFO: Created: latency-svc-kr6pr
    Jan 18 22:25:57.359: INFO: Got endpoints: latency-svc-cxs7p [504.439625ms]
    Jan 18 22:25:57.359: INFO: Got endpoints: latency-svc-8kzkh [505.072455ms]
    Jan 18 22:25:57.359: INFO: Got endpoints: latency-svc-xqcwl [126.206893ms]
    Jan 18 22:25:57.359: INFO: Got endpoints: latency-svc-jw5pl [503.83151ms]
    Jan 18 22:25:57.382: INFO: Got endpoints: latency-svc-xdc7p [111.214189ms]
    Jan 18 22:25:57.418: INFO: Got endpoints: latency-svc-kr6pr [141.17134ms]
    Jan 18 22:25:57.419: INFO: Created: latency-svc-rcpwt
    Jan 18 22:25:57.441: INFO: Got endpoints: latency-svc-rcpwt [163.223197ms]
    Jan 18 22:25:57.447: INFO: Created: latency-svc-86gws
    Jan 18 22:25:57.500: INFO: Got endpoints: latency-svc-86gws [229.557228ms]
    Jan 18 22:25:57.501: INFO: Created: latency-svc-p2htv
    Jan 18 22:25:57.524: INFO: Created: latency-svc-sg2nf
    Jan 18 22:25:57.529: INFO: Got endpoints: latency-svc-p2htv [257.570637ms]
    Jan 18 22:25:57.545: INFO: Got endpoints: latency-svc-sg2nf [237.965044ms]
    Jan 18 22:25:57.546: INFO: Created: latency-svc-l8mvq
    Jan 18 22:25:57.579: INFO: Got endpoints: latency-svc-l8mvq [270.974785ms]
    Jan 18 22:25:57.580: INFO: Created: latency-svc-5dzsp
    Jan 18 22:25:57.585: INFO: Created: latency-svc-b8kjq
    Jan 18 22:25:57.608: INFO: Got endpoints: latency-svc-5dzsp [289.751088ms]
    Jan 18 22:25:57.624: INFO: Created: latency-svc-pr2dn
    Jan 18 22:25:57.630: INFO: Created: latency-svc-2hvsr
    Jan 18 22:25:57.637: INFO: Got endpoints: latency-svc-b8kjq [318.180201ms]
    Jan 18 22:25:57.659: INFO: Got endpoints: latency-svc-pr2dn [331.564847ms]
    Jan 18 22:25:57.674: INFO: Created: latency-svc-ptq8n
    Jan 18 22:25:57.674: INFO: Got endpoints: latency-svc-2hvsr [313.235915ms]
    Jan 18 22:25:57.691: INFO: Created: latency-svc-x4572
    Jan 18 22:25:57.719: INFO: Got endpoints: latency-svc-ptq8n [355.262601ms]
    Jan 18 22:25:57.719: INFO: Got endpoints: latency-svc-x4572 [356.667467ms]
    Jan 18 22:25:58.051: INFO: Created: latency-svc-rrk7f
    Jan 18 22:25:58.067: INFO: Created: latency-svc-jsx5l
    Jan 18 22:25:58.068: INFO: Created: latency-svc-gs8hj
    Jan 18 22:25:58.070: INFO: Created: latency-svc-nv6hl
    Jan 18 22:25:58.070: INFO: Created: latency-svc-hpdzh
    Jan 18 22:25:58.073: INFO: Created: latency-svc-x6mdr
    Jan 18 22:25:58.073: INFO: Created: latency-svc-fv8mx
    Jan 18 22:25:58.074: INFO: Created: latency-svc-j8cqf
    Jan 18 22:25:58.076: INFO: Created: latency-svc-r9xqz
    Jan 18 22:25:58.077: INFO: Created: latency-svc-7jgg6
    Jan 18 22:25:58.078: INFO: Created: latency-svc-2g42g
    Jan 18 22:25:58.078: INFO: Created: latency-svc-n9994
    Jan 18 22:25:58.078: INFO: Created: latency-svc-pzdkr
    Jan 18 22:25:58.079: INFO: Created: latency-svc-76ljs
    Jan 18 22:25:58.079: INFO: Created: latency-svc-79r6q
    Jan 18 22:25:58.081: INFO: Got endpoints: latency-svc-rrk7f [359.98239ms]
    Jan 18 22:25:58.095: INFO: Got endpoints: latency-svc-79r6q [711.731532ms]
    Jan 18 22:25:58.121: INFO: Got endpoints: latency-svc-pzdkr [399.841001ms]
    Jan 18 22:25:58.125: INFO: Created: latency-svc-jbwb5
    Jan 18 22:25:58.136: INFO: Got endpoints: latency-svc-nv6hl [717.804222ms]
    Jan 18 22:25:58.171: INFO: Created: latency-svc-qgcdj
    Jan 18 22:25:58.171: INFO: Got endpoints: latency-svc-x6mdr [562.481691ms]
    Jan 18 22:25:58.172: INFO: Got endpoints: latency-svc-r9xqz [811.295668ms]
    Jan 18 22:25:58.173: INFO: Got endpoints: latency-svc-hpdzh [514.224312ms]
    Jan 18 22:25:58.174: INFO: Got endpoints: latency-svc-gs8hj [498.793173ms]
    Jan 18 22:25:58.193: INFO: Got endpoints: latency-svc-jsx5l [553.690205ms]
    Jan 18 22:25:58.195: INFO: Created: latency-svc-d6rf4
    Jan 18 22:25:58.203: INFO: Got endpoints: latency-svc-n9994 [702.21732ms]
    Jan 18 22:25:58.203: INFO: Got endpoints: latency-svc-j8cqf [840.75235ms]
    Jan 18 22:25:58.203: INFO: Got endpoints: latency-svc-2g42g [674.021086ms]
    Jan 18 22:25:58.239: INFO: Got endpoints: latency-svc-7jgg6 [693.701239ms]
    Jan 18 22:25:58.303: INFO: Got endpoints: latency-svc-fv8mx [724.051462ms]
    Jan 18 22:25:58.327: INFO: Created: latency-svc-mtzjk
    Jan 18 22:25:58.337: INFO: Created: latency-svc-kgwc5
    Jan 18 22:25:58.344: INFO: Got endpoints: latency-svc-76ljs [902.153823ms]
    Jan 18 22:25:58.363: INFO: Created: latency-svc-bk67t
    Jan 18 22:25:58.364: INFO: Created: latency-svc-4rtfm
    Jan 18 22:25:58.364: INFO: Created: latency-svc-4xq9l
    Jan 18 22:25:58.364: INFO: Created: latency-svc-p7r7j
    Jan 18 22:25:58.364: INFO: Created: latency-svc-gqzwj
    Jan 18 22:25:58.365: INFO: Created: latency-svc-dnr4h
    Jan 18 22:25:58.365: INFO: Created: latency-svc-ffdfh
    Jan 18 22:25:58.365: INFO: Created: latency-svc-njjpx
    Jan 18 22:25:58.365: INFO: Created: latency-svc-g84jx
    Jan 18 22:25:58.407: INFO: Created: latency-svc-zhdnh
    Jan 18 22:25:58.407: INFO: Got endpoints: latency-svc-jbwb5 [326.432228ms]
    Jan 18 22:25:58.427: INFO: Created: latency-svc-sp6qh
    Jan 18 22:25:58.435: INFO: Got endpoints: latency-svc-qgcdj [340.074043ms]
    Jan 18 22:25:58.449: INFO: Created: latency-svc-w8f9f
    Jan 18 22:25:58.498: INFO: Got endpoints: latency-svc-d6rf4 [361.574686ms]
    Jan 18 22:25:58.526: INFO: Created: latency-svc-s4jgm
    Jan 18 22:25:58.528: INFO: Got endpoints: latency-svc-mtzjk [357.079267ms]
    Jan 18 22:25:58.542: INFO: Created: latency-svc-7n7qq
    Jan 18 22:25:58.586: INFO: Got endpoints: latency-svc-kgwc5 [461.569939ms]
    Jan 18 22:25:58.626: INFO: Created: latency-svc-b746t
    Jan 18 22:25:58.631: INFO: Got endpoints: latency-svc-njjpx [427.17753ms]
    Jan 18 22:25:58.647: INFO: Created: latency-svc-t6c4p
    Jan 18 22:25:58.679: INFO: Got endpoints: latency-svc-ffdfh [475.104815ms]
    Jan 18 22:25:58.690: INFO: Created: latency-svc-76rpm
    Jan 18 22:25:58.731: INFO: Got endpoints: latency-svc-bk67t [558.734215ms]
    Jan 18 22:25:58.741: INFO: Created: latency-svc-6kb7x
    Jan 18 22:25:58.781: INFO: Got endpoints: latency-svc-4rtfm [477.978135ms]
    Jan 18 22:25:58.797: INFO: Created: latency-svc-jzqdv
    Jan 18 22:25:58.834: INFO: Got endpoints: latency-svc-g84jx [595.386516ms]
    Jan 18 22:25:58.860: INFO: Created: latency-svc-r55hr
    Jan 18 22:25:58.893: INFO: Got endpoints: latency-svc-dnr4h [719.858297ms]
    Jan 18 22:25:58.911: INFO: Created: latency-svc-gp5g7
    Jan 18 22:25:58.927: INFO: Got endpoints: latency-svc-p7r7j [724.591443ms]
    Jan 18 22:25:58.947: INFO: Created: latency-svc-gjj87
    Jan 18 22:25:58.979: INFO: Got endpoints: latency-svc-gqzwj [805.563857ms]
    Jan 18 22:25:59.002: INFO: Created: latency-svc-cktsp
    Jan 18 22:25:59.031: INFO: Got endpoints: latency-svc-4xq9l [838.147771ms]
    Jan 18 22:25:59.043: INFO: Created: latency-svc-frfkp
    Jan 18 22:25:59.084: INFO: Got endpoints: latency-svc-zhdnh [739.934843ms]
    Jan 18 22:25:59.107: INFO: Created: latency-svc-dtxrd
    Jan 18 22:25:59.128: INFO: Got endpoints: latency-svc-sp6qh [721.013668ms]
    Jan 18 22:25:59.143: INFO: Created: latency-svc-kq559
    Jan 18 22:25:59.181: INFO: Got endpoints: latency-svc-w8f9f [745.419607ms]
    Jan 18 22:25:59.192: INFO: Created: latency-svc-s8gwq
    Jan 18 22:25:59.237: INFO: Got endpoints: latency-svc-s4jgm [738.853123ms]
    Jan 18 22:25:59.249: INFO: Created: latency-svc-sx6gk
    Jan 18 22:25:59.281: INFO: Got endpoints: latency-svc-7n7qq [752.194522ms]
    Jan 18 22:25:59.293: INFO: Created: latency-svc-5bb57
    Jan 18 22:25:59.327: INFO: Got endpoints: latency-svc-b746t [741.819924ms]
    Jan 18 22:25:59.338: INFO: Created: latency-svc-ztwvc
    Jan 18 22:25:59.378: INFO: Got endpoints: latency-svc-t6c4p [747.076696ms]
    Jan 18 22:25:59.391: INFO: Created: latency-svc-gg2hk
    Jan 18 22:25:59.426: INFO: Got endpoints: latency-svc-76rpm [747.578845ms]
    Jan 18 22:25:59.445: INFO: Created: latency-svc-hl8mt
    Jan 18 22:25:59.493: INFO: Got endpoints: latency-svc-6kb7x [761.907431ms]
    Jan 18 22:25:59.518: INFO: Created: latency-svc-pvsf5
    Jan 18 22:25:59.530: INFO: Got endpoints: latency-svc-jzqdv [748.994275ms]
    Jan 18 22:25:59.561: INFO: Created: latency-svc-h4mr9
    Jan 18 22:25:59.581: INFO: Got endpoints: latency-svc-r55hr [746.906964ms]
    Jan 18 22:25:59.599: INFO: Created: latency-svc-mhm7m
    Jan 18 22:25:59.635: INFO: Got endpoints: latency-svc-gp5g7 [741.532989ms]
    Jan 18 22:25:59.658: INFO: Created: latency-svc-wdmkc
    Jan 18 22:25:59.677: INFO: Got endpoints: latency-svc-gjj87 [749.671837ms]
    Jan 18 22:25:59.695: INFO: Created: latency-svc-xn5l2
    Jan 18 22:25:59.729: INFO: Got endpoints: latency-svc-cktsp [749.479624ms]
    Jan 18 22:25:59.747: INFO: Created: latency-svc-j7ks6
    Jan 18 22:25:59.782: INFO: Got endpoints: latency-svc-frfkp [750.611459ms]
    Jan 18 22:25:59.801: INFO: Created: latency-svc-g5tff
    Jan 18 22:25:59.833: INFO: Got endpoints: latency-svc-dtxrd [748.282575ms]
    Jan 18 22:25:59.858: INFO: Created: latency-svc-9tps9
    Jan 18 22:25:59.892: INFO: Got endpoints: latency-svc-kq559 [763.648887ms]
    Jan 18 22:25:59.908: INFO: Created: latency-svc-6j9ml
    Jan 18 22:25:59.941: INFO: Got endpoints: latency-svc-s8gwq [760.072884ms]
    Jan 18 22:25:59.965: INFO: Created: latency-svc-4qghp
    Jan 18 22:25:59.990: INFO: Got endpoints: latency-svc-sx6gk [753.177438ms]
    Jan 18 22:26:00.041: INFO: Created: latency-svc-tcgbc
    Jan 18 22:26:00.043: INFO: Got endpoints: latency-svc-5bb57 [761.633851ms]
    Jan 18 22:26:00.063: INFO: Created: latency-svc-jd5n9
    Jan 18 22:26:00.098: INFO: Got endpoints: latency-svc-ztwvc [770.884644ms]
    Jan 18 22:26:00.119: INFO: Created: latency-svc-p7p9b
    Jan 18 22:26:00.142: INFO: Got endpoints: latency-svc-gg2hk [764.690282ms]
    Jan 18 22:26:00.169: INFO: Created: latency-svc-lrljd
    Jan 18 22:26:00.186: INFO: Got endpoints: latency-svc-hl8mt [759.922518ms]
    Jan 18 22:26:00.201: INFO: Created: latency-svc-ts5l5
    Jan 18 22:26:00.230: INFO: Got endpoints: latency-svc-pvsf5 [735.749724ms]
    Jan 18 22:26:00.245: INFO: Created: latency-svc-srhcg
    Jan 18 22:26:00.283: INFO: Got endpoints: latency-svc-h4mr9 [752.363064ms]
    Jan 18 22:26:00.307: INFO: Created: latency-svc-897bx
    Jan 18 22:26:00.329: INFO: Got endpoints: latency-svc-mhm7m [747.454698ms]
    Jan 18 22:26:00.382: INFO: Got endpoints: latency-svc-wdmkc [746.430419ms]
    Jan 18 22:26:00.391: INFO: Created: latency-svc-nvzfz
    Jan 18 22:26:00.400: INFO: Created: latency-svc-lwswz
    Jan 18 22:26:00.428: INFO: Got endpoints: latency-svc-xn5l2 [750.434636ms]
    Jan 18 22:26:00.453: INFO: Created: latency-svc-59klk
    Jan 18 22:26:00.479: INFO: Got endpoints: latency-svc-j7ks6 [750.439316ms]
    Jan 18 22:26:00.491: INFO: Created: latency-svc-jjnbx
    Jan 18 22:26:00.592: INFO: Got endpoints: latency-svc-g5tff [810.344291ms]
    Jan 18 22:26:00.619: INFO: Created: latency-svc-7pn7v
    Jan 18 22:26:00.632: INFO: Got endpoints: latency-svc-9tps9 [798.695155ms]
    Jan 18 22:26:00.648: INFO: Created: latency-svc-5qxgw
    Jan 18 22:26:00.678: INFO: Got endpoints: latency-svc-6j9ml [786.005928ms]
    Jan 18 22:26:00.691: INFO: Created: latency-svc-5chg2
    Jan 18 22:26:00.731: INFO: Got endpoints: latency-svc-4qghp [788.799875ms]
    Jan 18 22:26:00.769: INFO: Created: latency-svc-pv9p8
    Jan 18 22:26:00.777: INFO: Got endpoints: latency-svc-tcgbc [786.566668ms]
    Jan 18 22:26:00.813: INFO: Created: latency-svc-26bnd
    Jan 18 22:26:00.832: INFO: Got endpoints: latency-svc-jd5n9 [788.360483ms]
    Jan 18 22:26:00.850: INFO: Created: latency-svc-z6vsg
    Jan 18 22:26:00.877: INFO: Got endpoints: latency-svc-p7p9b [778.360755ms]
    Jan 18 22:26:00.889: INFO: Created: latency-svc-lttgb
    Jan 18 22:26:00.932: INFO: Got endpoints: latency-svc-lrljd [788.756121ms]
    Jan 18 22:26:00.946: INFO: Created: latency-svc-m69xw
    Jan 18 22:26:00.984: INFO: Got endpoints: latency-svc-ts5l5 [797.257398ms]
    Jan 18 22:26:01.012: INFO: Created: latency-svc-j2dqb
    Jan 18 22:26:01.032: INFO: Got endpoints: latency-svc-srhcg [801.588742ms]
    Jan 18 22:26:01.044: INFO: Created: latency-svc-9tgdp
    Jan 18 22:26:01.088: INFO: Got endpoints: latency-svc-897bx [805.551272ms]
    Jan 18 22:26:01.107: INFO: Created: latency-svc-p8flt
    Jan 18 22:26:01.133: INFO: Got endpoints: latency-svc-nvzfz [803.308843ms]
    Jan 18 22:26:01.151: INFO: Created: latency-svc-8gjgd
    Jan 18 22:26:01.181: INFO: Got endpoints: latency-svc-lwswz [799.240955ms]
    Jan 18 22:26:01.192: INFO: Created: latency-svc-9w6pm
    Jan 18 22:26:01.226: INFO: Got endpoints: latency-svc-59klk [797.252052ms]
    Jan 18 22:26:01.243: INFO: Created: latency-svc-tjfwl
    Jan 18 22:26:01.285: INFO: Got endpoints: latency-svc-jjnbx [805.615963ms]
    Jan 18 22:26:01.298: INFO: Created: latency-svc-6zbf7
    Jan 18 22:26:01.329: INFO: Got endpoints: latency-svc-7pn7v [736.830691ms]
    Jan 18 22:26:01.343: INFO: Created: latency-svc-v8zp6
    Jan 18 22:26:01.386: INFO: Got endpoints: latency-svc-5qxgw [753.659005ms]
    Jan 18 22:26:01.399: INFO: Created: latency-svc-zdn2s
    Jan 18 22:26:01.426: INFO: Got endpoints: latency-svc-5chg2 [747.8267ms]
    Jan 18 22:26:01.445: INFO: Created: latency-svc-nklsq
    Jan 18 22:26:01.480: INFO: Got endpoints: latency-svc-pv9p8 [749.313093ms]
    Jan 18 22:26:01.496: INFO: Created: latency-svc-5vhjh
    Jan 18 22:26:01.530: INFO: Got endpoints: latency-svc-26bnd [752.941398ms]
    Jan 18 22:26:01.542: INFO: Created: latency-svc-k9n6d
    Jan 18 22:26:01.581: INFO: Got endpoints: latency-svc-z6vsg [748.790577ms]
    Jan 18 22:26:01.597: INFO: Created: latency-svc-6k7t6
    Jan 18 22:26:01.630: INFO: Got endpoints: latency-svc-lttgb [752.861936ms]
    Jan 18 22:26:01.658: INFO: Created: latency-svc-xdwdw
    Jan 18 22:26:01.695: INFO: Got endpoints: latency-svc-m69xw [762.832382ms]
    Jan 18 22:26:01.707: INFO: Created: latency-svc-t6hm2
    Jan 18 22:26:01.727: INFO: Got endpoints: latency-svc-j2dqb [743.294122ms]
    Jan 18 22:26:01.747: INFO: Created: latency-svc-w7wbz
    Jan 18 22:26:01.777: INFO: Got endpoints: latency-svc-9tgdp [745.409305ms]
    Jan 18 22:26:01.797: INFO: Created: latency-svc-7pr6f
    Jan 18 22:26:01.835: INFO: Got endpoints: latency-svc-p8flt [746.186652ms]
    Jan 18 22:26:01.847: INFO: Created: latency-svc-fk7p8
    Jan 18 22:26:01.878: INFO: Got endpoints: latency-svc-8gjgd [744.533465ms]
    Jan 18 22:26:01.899: INFO: Created: latency-svc-x22mv
    Jan 18 22:26:01.938: INFO: Got endpoints: latency-svc-9w6pm [757.350753ms]
    Jan 18 22:26:01.969: INFO: Created: latency-svc-l7hrz
    Jan 18 22:26:01.988: INFO: Got endpoints: latency-svc-tjfwl [762.161582ms]
    Jan 18 22:26:02.005: INFO: Created: latency-svc-9x5sw
    Jan 18 22:26:02.029: INFO: Got endpoints: latency-svc-6zbf7 [743.991805ms]
    Jan 18 22:26:02.050: INFO: Created: latency-svc-t2lw5
    Jan 18 22:26:02.084: INFO: Got endpoints: latency-svc-v8zp6 [754.843676ms]
    Jan 18 22:26:02.112: INFO: Created: latency-svc-f6vx5
    Jan 18 22:26:02.131: INFO: Got endpoints: latency-svc-zdn2s [745.423357ms]
    Jan 18 22:26:02.142: INFO: Created: latency-svc-bpzsj
    Jan 18 22:26:02.177: INFO: Got endpoints: latency-svc-nklsq [750.754189ms]
    Jan 18 22:26:02.209: INFO: Created: latency-svc-v76c9
    Jan 18 22:26:02.234: INFO: Got endpoints: latency-svc-5vhjh [753.647066ms]
    Jan 18 22:26:02.262: INFO: Created: latency-svc-9bpsx
    Jan 18 22:26:02.282: INFO: Got endpoints: latency-svc-k9n6d [752.01055ms]
    Jan 18 22:26:02.306: INFO: Created: latency-svc-xl5sf
    Jan 18 22:26:02.334: INFO: Got endpoints: latency-svc-6k7t6 [753.519308ms]
    Jan 18 22:26:02.358: INFO: Created: latency-svc-45f9s
    Jan 18 22:26:02.388: INFO: Got endpoints: latency-svc-xdwdw [757.451712ms]
    Jan 18 22:26:02.463: INFO: Created: latency-svc-v7hrc
    Jan 18 22:26:02.481: INFO: Got endpoints: latency-svc-t6hm2 [786.066889ms]
    Jan 18 22:26:02.509: INFO: Got endpoints: latency-svc-w7wbz [781.903067ms]
    Jan 18 22:26:02.511: INFO: Created: latency-svc-tzg2z
    Jan 18 22:26:02.561: INFO: Got endpoints: latency-svc-7pr6f [783.25144ms]
    Jan 18 22:26:02.568: INFO: Created: latency-svc-hxn9m
    Jan 18 22:26:02.587: INFO: Got endpoints: latency-svc-fk7p8 [751.875618ms]
    Jan 18 22:26:02.612: INFO: Created: latency-svc-rmdsn
    Jan 18 22:26:02.613: INFO: Created: latency-svc-75dlh
    Jan 18 22:26:02.662: INFO: Got endpoints: latency-svc-x22mv [784.588125ms]
    Jan 18 22:26:02.688: INFO: Got endpoints: latency-svc-l7hrz [749.352361ms]
    Jan 18 22:26:02.691: INFO: Created: latency-svc-mrzzz
    Jan 18 22:26:02.707: INFO: Created: latency-svc-wcbwg
    Jan 18 22:26:02.734: INFO: Got endpoints: latency-svc-9x5sw [745.460491ms]
    Jan 18 22:26:02.757: INFO: Created: latency-svc-krhlf
    Jan 18 22:26:02.791: INFO: Got endpoints: latency-svc-t2lw5 [761.477772ms]
    Jan 18 22:26:02.833: INFO: Got endpoints: latency-svc-f6vx5 [748.78448ms]
    Jan 18 22:26:02.834: INFO: Created: latency-svc-2hs5p
    Jan 18 22:26:02.862: INFO: Created: latency-svc-z97fb
    Jan 18 22:26:02.879: INFO: Got endpoints: latency-svc-bpzsj [748.037385ms]
    Jan 18 22:26:02.892: INFO: Created: latency-svc-ddfs6
    Jan 18 22:26:02.929: INFO: Got endpoints: latency-svc-v76c9 [751.14548ms]
    Jan 18 22:26:02.940: INFO: Created: latency-svc-f6zxz
    Jan 18 22:26:02.980: INFO: Got endpoints: latency-svc-9bpsx [745.687056ms]
    Jan 18 22:26:02.999: INFO: Created: latency-svc-j2gh2
    Jan 18 22:26:03.039: INFO: Got endpoints: latency-svc-xl5sf [756.308346ms]
    Jan 18 22:26:03.056: INFO: Created: latency-svc-lbqr5
    Jan 18 22:26:03.094: INFO: Got endpoints: latency-svc-45f9s [759.362279ms]
    Jan 18 22:26:03.111: INFO: Created: latency-svc-84gnk
    Jan 18 22:26:03.131: INFO: Got endpoints: latency-svc-v7hrc [742.84024ms]
    Jan 18 22:26:03.144: INFO: Created: latency-svc-8jbbc
    Jan 18 22:26:03.179: INFO: Got endpoints: latency-svc-tzg2z [698.66994ms]
    Jan 18 22:26:03.196: INFO: Created: latency-svc-28lmq
    Jan 18 22:26:03.226: INFO: Got endpoints: latency-svc-hxn9m [717.002784ms]
    Jan 18 22:26:03.242: INFO: Created: latency-svc-6m527
    Jan 18 22:26:03.281: INFO: Got endpoints: latency-svc-rmdsn [719.793422ms]
    Jan 18 22:26:03.322: INFO: Created: latency-svc-2gscp
    Jan 18 22:26:03.328: INFO: Got endpoints: latency-svc-75dlh [740.828584ms]
    Jan 18 22:26:03.346: INFO: Created: latency-svc-snw6q
    Jan 18 22:26:03.394: INFO: Got endpoints: latency-svc-mrzzz [731.225691ms]
    Jan 18 22:26:03.413: INFO: Created: latency-svc-6k687
    Jan 18 22:26:03.433: INFO: Got endpoints: latency-svc-wcbwg [745.651684ms]
    Jan 18 22:26:03.448: INFO: Created: latency-svc-5jvjl
    Jan 18 22:26:03.486: INFO: Got endpoints: latency-svc-krhlf [752.359983ms]
    Jan 18 22:26:03.513: INFO: Created: latency-svc-cgbbf
    Jan 18 22:26:03.539: INFO: Got endpoints: latency-svc-2hs5p [747.743402ms]
    Jan 18 22:26:03.549: INFO: Created: latency-svc-fvwjm
    Jan 18 22:26:03.579: INFO: Got endpoints: latency-svc-z97fb [744.68197ms]
    Jan 18 22:26:03.606: INFO: Created: latency-svc-qccf2
    Jan 18 22:26:03.630: INFO: Got endpoints: latency-svc-ddfs6 [750.798837ms]
    Jan 18 22:26:03.658: INFO: Created: latency-svc-48fkh
    Jan 18 22:26:03.684: INFO: Got endpoints: latency-svc-f6zxz [755.319256ms]
    Jan 18 22:26:03.698: INFO: Created: latency-svc-bkgzw
    Jan 18 22:26:03.738: INFO: Got endpoints: latency-svc-j2gh2 [758.077789ms]
    Jan 18 22:26:03.754: INFO: Created: latency-svc-7sjvr
    Jan 18 22:26:03.782: INFO: Got endpoints: latency-svc-lbqr5 [743.095646ms]
    Jan 18 22:26:03.804: INFO: Created: latency-svc-vdlx2
    Jan 18 22:26:03.826: INFO: Got endpoints: latency-svc-84gnk [732.455154ms]
    Jan 18 22:26:03.854: INFO: Created: latency-svc-wtm79
    Jan 18 22:26:03.878: INFO: Got endpoints: latency-svc-8jbbc [746.824257ms]
    Jan 18 22:26:03.893: INFO: Created: latency-svc-94jl2
    Jan 18 22:26:03.930: INFO: Got endpoints: latency-svc-28lmq [750.921519ms]
    Jan 18 22:26:03.959: INFO: Created: latency-svc-85nsl
    Jan 18 22:26:03.980: INFO: Got endpoints: latency-svc-6m527 [753.247373ms]
    Jan 18 22:26:03.994: INFO: Created: latency-svc-n9fr7
    Jan 18 22:26:04.028: INFO: Got endpoints: latency-svc-2gscp [746.66715ms]
    Jan 18 22:26:04.047: INFO: Created: latency-svc-p2n2m
    Jan 18 22:26:04.078: INFO: Got endpoints: latency-svc-snw6q [749.883642ms]
    Jan 18 22:26:04.094: INFO: Created: latency-svc-t78b8
    Jan 18 22:26:04.141: INFO: Got endpoints: latency-svc-6k687 [746.978963ms]
    Jan 18 22:26:04.173: INFO: Created: latency-svc-5xf69
    Jan 18 22:26:04.181: INFO: Got endpoints: latency-svc-5jvjl [747.322649ms]
    Jan 18 22:26:04.196: INFO: Created: latency-svc-7snrz
    Jan 18 22:26:04.235: INFO: Got endpoints: latency-svc-cgbbf [748.432956ms]
    Jan 18 22:26:04.250: INFO: Created: latency-svc-tvf6t
    Jan 18 22:26:04.288: INFO: Got endpoints: latency-svc-fvwjm [749.345819ms]
    Jan 18 22:26:04.301: INFO: Created: latency-svc-sjcf9
    Jan 18 22:26:04.327: INFO: Got endpoints: latency-svc-qccf2 [747.051572ms]
    Jan 18 22:26:04.346: INFO: Created: latency-svc-j8hlp
    Jan 18 22:26:04.376: INFO: Got endpoints: latency-svc-48fkh [735.793447ms]
    Jan 18 22:26:04.392: INFO: Created: latency-svc-g4qzx
    Jan 18 22:26:04.430: INFO: Got endpoints: latency-svc-bkgzw [745.867061ms]
    Jan 18 22:26:04.445: INFO: Created: latency-svc-46sd8
    Jan 18 22:26:04.481: INFO: Got endpoints: latency-svc-7sjvr [742.426608ms]
    Jan 18 22:26:04.512: INFO: Created: latency-svc-tnqlc
    Jan 18 22:26:04.531: INFO: Got endpoints: latency-svc-vdlx2 [748.508094ms]
    Jan 18 22:26:04.550: INFO: Created: latency-svc-q26zs
    Jan 18 22:26:04.598: INFO: Got endpoints: latency-svc-wtm79 [772.079649ms]
    Jan 18 22:26:04.623: INFO: Created: latency-svc-mngp8
    Jan 18 22:26:04.629: INFO: Got endpoints: latency-svc-94jl2 [751.119803ms]
    Jan 18 22:26:04.682: INFO: Got endpoints: latency-svc-85nsl [751.380145ms]
    Jan 18 22:26:04.738: INFO: Got endpoints: latency-svc-n9fr7 [757.916067ms]
    Jan 18 22:26:04.779: INFO: Got endpoints: latency-svc-p2n2m [750.845038ms]
    Jan 18 22:26:04.826: INFO: Got endpoints: latency-svc-t78b8 [747.707836ms]
    Jan 18 22:26:04.887: INFO: Got endpoints: latency-svc-5xf69 [745.858631ms]
    Jan 18 22:26:04.936: INFO: Got endpoints: latency-svc-7snrz [755.405643ms]
    Jan 18 22:26:04.979: INFO: Got endpoints: latency-svc-tvf6t [744.631798ms]
    Jan 18 22:26:05.030: INFO: Got endpoints: latency-svc-sjcf9 [741.762059ms]
    Jan 18 22:26:05.078: INFO: Got endpoints: latency-svc-j8hlp [749.128102ms]
    Jan 18 22:26:05.131: INFO: Got endpoints: latency-svc-g4qzx [754.392551ms]
    Jan 18 22:26:05.176: INFO: Got endpoints: latency-svc-46sd8 [745.324125ms]
    Jan 18 22:26:05.235: INFO: Got endpoints: latency-svc-tnqlc [753.857063ms]
    Jan 18 22:26:05.279: INFO: Got endpoints: latency-svc-q26zs [748.036663ms]
    Jan 18 22:26:05.326: INFO: Got endpoints: latency-svc-mngp8 [727.501662ms]
    Jan 18 22:26:05.327: INFO: Latencies: [47.94756ms 73.736731ms 111.214189ms 116.712428ms 116.803682ms 117.405361ms 126.206893ms 141.17134ms 146.119135ms 163.223197ms 189.92768ms 194.704755ms 194.879379ms 217.395787ms 228.58307ms 229.557228ms 237.965044ms 241.698437ms 243.572511ms 257.570637ms 260.185188ms 264.701677ms 270.974785ms 271.460711ms 272.535784ms 279.984138ms 281.955904ms 289.543128ms 289.751088ms 293.94223ms 294.388983ms 299.446605ms 303.840805ms 307.819572ms 313.235915ms 316.101298ms 318.180201ms 326.432228ms 331.564847ms 340.074043ms 342.42446ms 350.303641ms 355.262601ms 356.667467ms 357.079267ms 359.98239ms 361.574686ms 399.841001ms 427.17753ms 461.569939ms 475.104815ms 475.769669ms 477.978135ms 498.793173ms 503.83151ms 504.439625ms 505.072455ms 514.224312ms 553.690205ms 558.734215ms 562.481691ms 595.386516ms 674.021086ms 693.701239ms 698.66994ms 702.21732ms 711.731532ms 717.002784ms 717.804222ms 719.793422ms 719.858297ms 721.013668ms 724.051462ms 724.591443ms 727.501662ms 731.225691ms 732.455154ms 735.749724ms 735.793447ms 736.830691ms 738.853123ms 739.934843ms 740.828584ms 741.532989ms 741.762059ms 741.819924ms 742.426608ms 742.84024ms 743.095646ms 743.294122ms 743.991805ms 744.533465ms 744.631798ms 744.68197ms 745.324125ms 745.409305ms 745.419607ms 745.423357ms 745.460491ms 745.651684ms 745.687056ms 745.858631ms 745.867061ms 746.186652ms 746.430419ms 746.66715ms 746.824257ms 746.906964ms 746.978963ms 747.051572ms 747.076696ms 747.322649ms 747.454698ms 747.578845ms 747.707836ms 747.743402ms 747.8267ms 748.036663ms 748.037385ms 748.282575ms 748.432956ms 748.508094ms 748.78448ms 748.790577ms 748.994275ms 749.128102ms 749.313093ms 749.345819ms 749.352361ms 749.479624ms 749.671837ms 749.883642ms 750.434636ms 750.439316ms 750.611459ms 750.754189ms 750.798837ms 750.845038ms 750.921519ms 751.119803ms 751.14548ms 751.380145ms 751.875618ms 752.01055ms 752.194522ms 752.359983ms 752.363064ms 752.861936ms 752.941398ms 753.177438ms 753.247373ms 753.519308ms 753.647066ms 753.659005ms 753.857063ms 754.392551ms 754.843676ms 755.319256ms 755.405643ms 756.308346ms 757.350753ms 757.451712ms 757.916067ms 758.077789ms 759.362279ms 759.922518ms 760.072884ms 761.477772ms 761.633851ms 761.907431ms 762.161582ms 762.832382ms 763.648887ms 764.690282ms 770.884644ms 772.079649ms 778.360755ms 781.903067ms 783.25144ms 784.588125ms 786.005928ms 786.066889ms 786.566668ms 788.360483ms 788.756121ms 788.799875ms 797.252052ms 797.257398ms 798.695155ms 799.240955ms 801.588742ms 803.308843ms 805.551272ms 805.563857ms 805.615963ms 810.344291ms 811.295668ms 838.147771ms 840.75235ms 902.153823ms]
    Jan 18 22:26:05.327: INFO: 50 %ile: 745.687056ms
    Jan 18 22:26:05.327: INFO: 90 %ile: 786.005928ms
    Jan 18 22:26:05.327: INFO: 99 %ile: 840.75235ms
    Jan 18 22:26:05.327: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Jan 18 22:26:05.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-3779" for this suite. 01/18/23 22:26:05.337
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:26:05.347
Jan 18 22:26:05.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename proxy 01/18/23 22:26:05.349
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:05.37
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:05.379
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jan 18 22:26:05.384: INFO: Creating pod...
Jan 18 22:26:05.399: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2540" to be "running"
Jan 18 22:26:05.427: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 27.708116ms
Jan 18 22:26:07.432: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.03307322s
Jan 18 22:26:07.432: INFO: Pod "agnhost" satisfied condition "running"
Jan 18 22:26:07.432: INFO: Creating service...
Jan 18 22:26:07.442: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/pods/agnhost/proxy/some/path/with/DELETE
Jan 18 22:26:07.469: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 18 22:26:07.469: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/pods/agnhost/proxy/some/path/with/GET
Jan 18 22:26:07.473: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 18 22:26:07.474: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/pods/agnhost/proxy/some/path/with/HEAD
Jan 18 22:26:07.478: INFO: http.Client request:HEAD | StatusCode:200
Jan 18 22:26:07.478: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/pods/agnhost/proxy/some/path/with/OPTIONS
Jan 18 22:26:07.482: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 18 22:26:07.482: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/pods/agnhost/proxy/some/path/with/PATCH
Jan 18 22:26:07.487: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 18 22:26:07.487: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/pods/agnhost/proxy/some/path/with/POST
Jan 18 22:26:07.492: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 18 22:26:07.492: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/pods/agnhost/proxy/some/path/with/PUT
Jan 18 22:26:07.497: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 18 22:26:07.497: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/services/test-service/proxy/some/path/with/DELETE
Jan 18 22:26:07.504: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 18 22:26:07.504: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/services/test-service/proxy/some/path/with/GET
Jan 18 22:26:07.510: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 18 22:26:07.510: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/services/test-service/proxy/some/path/with/HEAD
Jan 18 22:26:07.518: INFO: http.Client request:HEAD | StatusCode:200
Jan 18 22:26:07.518: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/services/test-service/proxy/some/path/with/OPTIONS
Jan 18 22:26:07.558: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 18 22:26:07.558: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/services/test-service/proxy/some/path/with/PATCH
Jan 18 22:26:07.781: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 18 22:26:07.782: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/services/test-service/proxy/some/path/with/POST
Jan 18 22:26:07.789: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 18 22:26:07.789: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/services/test-service/proxy/some/path/with/PUT
Jan 18 22:26:07.795: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan 18 22:26:07.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2540" for this suite. 01/18/23 22:26:07.802
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":212,"skipped":3799,"failed":0}
------------------------------
• [2.464 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:26:05.347
    Jan 18 22:26:05.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename proxy 01/18/23 22:26:05.349
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:05.37
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:05.379
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jan 18 22:26:05.384: INFO: Creating pod...
    Jan 18 22:26:05.399: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2540" to be "running"
    Jan 18 22:26:05.427: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 27.708116ms
    Jan 18 22:26:07.432: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.03307322s
    Jan 18 22:26:07.432: INFO: Pod "agnhost" satisfied condition "running"
    Jan 18 22:26:07.432: INFO: Creating service...
    Jan 18 22:26:07.442: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/pods/agnhost/proxy/some/path/with/DELETE
    Jan 18 22:26:07.469: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 18 22:26:07.469: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/pods/agnhost/proxy/some/path/with/GET
    Jan 18 22:26:07.473: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 18 22:26:07.474: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/pods/agnhost/proxy/some/path/with/HEAD
    Jan 18 22:26:07.478: INFO: http.Client request:HEAD | StatusCode:200
    Jan 18 22:26:07.478: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/pods/agnhost/proxy/some/path/with/OPTIONS
    Jan 18 22:26:07.482: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 18 22:26:07.482: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/pods/agnhost/proxy/some/path/with/PATCH
    Jan 18 22:26:07.487: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 18 22:26:07.487: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/pods/agnhost/proxy/some/path/with/POST
    Jan 18 22:26:07.492: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 18 22:26:07.492: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/pods/agnhost/proxy/some/path/with/PUT
    Jan 18 22:26:07.497: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 18 22:26:07.497: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/services/test-service/proxy/some/path/with/DELETE
    Jan 18 22:26:07.504: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 18 22:26:07.504: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/services/test-service/proxy/some/path/with/GET
    Jan 18 22:26:07.510: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 18 22:26:07.510: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/services/test-service/proxy/some/path/with/HEAD
    Jan 18 22:26:07.518: INFO: http.Client request:HEAD | StatusCode:200
    Jan 18 22:26:07.518: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/services/test-service/proxy/some/path/with/OPTIONS
    Jan 18 22:26:07.558: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 18 22:26:07.558: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/services/test-service/proxy/some/path/with/PATCH
    Jan 18 22:26:07.781: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 18 22:26:07.782: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/services/test-service/proxy/some/path/with/POST
    Jan 18 22:26:07.789: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 18 22:26:07.789: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2540/services/test-service/proxy/some/path/with/PUT
    Jan 18 22:26:07.795: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan 18 22:26:07.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-2540" for this suite. 01/18/23 22:26:07.802
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:26:07.817
Jan 18 22:26:07.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename var-expansion 01/18/23 22:26:07.818
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:07.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:07.843
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 01/18/23 22:26:07.853
Jan 18 22:26:07.862: INFO: Waiting up to 2m0s for pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56" in namespace "var-expansion-1879" to be "running"
Jan 18 22:26:07.867: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 4.980926ms
Jan 18 22:26:09.880: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01723428s
Jan 18 22:26:11.877: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014471905s
Jan 18 22:26:13.878: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015665475s
Jan 18 22:26:15.879: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016111856s
Jan 18 22:26:17.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011650144s
Jan 18 22:26:19.877: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 12.014765356s
Jan 18 22:26:21.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010635526s
Jan 18 22:26:23.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 16.011668496s
Jan 18 22:26:25.880: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 18.017480449s
Jan 18 22:26:27.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 20.011331944s
Jan 18 22:26:29.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 22.011902422s
Jan 18 22:26:31.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 24.010575282s
Jan 18 22:26:33.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 26.012036757s
Jan 18 22:26:35.875: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 28.012890018s
Jan 18 22:26:37.878: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 30.015495696s
Jan 18 22:26:39.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010556173s
Jan 18 22:26:41.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 34.011194772s
Jan 18 22:26:43.876: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 36.013773648s
Jan 18 22:26:45.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 38.011354846s
Jan 18 22:26:47.872: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009955198s
Jan 18 22:26:49.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 42.010824269s
Jan 18 22:26:51.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 44.01060824s
Jan 18 22:26:53.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 46.010873184s
Jan 18 22:26:55.995: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 48.132724952s
Jan 18 22:26:57.875: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 50.012271342s
Jan 18 22:26:59.875: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 52.013028308s
Jan 18 22:27:01.876: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 54.014015286s
Jan 18 22:27:03.878: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 56.015148789s
Jan 18 22:27:05.875: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 58.012621669s
Jan 18 22:27:07.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.01160674s
Jan 18 22:27:09.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.011827147s
Jan 18 22:27:11.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.010739631s
Jan 18 22:27:13.876: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.013452226s
Jan 18 22:27:15.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.011075296s
Jan 18 22:27:17.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.01175782s
Jan 18 22:27:19.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.011007277s
Jan 18 22:27:21.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.011724269s
Jan 18 22:27:23.875: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.013053364s
Jan 18 22:27:25.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.011761957s
Jan 18 22:27:27.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.010401427s
Jan 18 22:27:29.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.011408932s
Jan 18 22:27:31.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.011405568s
Jan 18 22:27:33.875: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.012488827s
Jan 18 22:27:35.877: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.014839294s
Jan 18 22:27:37.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.010163473s
Jan 18 22:27:39.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.011046168s
Jan 18 22:27:41.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.010390284s
Jan 18 22:27:43.881: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.018219299s
Jan 18 22:27:45.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.010820134s
Jan 18 22:27:47.877: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.014999124s
Jan 18 22:27:49.883: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.020701686s
Jan 18 22:27:51.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.010924939s
Jan 18 22:27:53.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.011899599s
Jan 18 22:27:55.875: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.013097665s
Jan 18 22:27:57.877: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.014963116s
Jan 18 22:27:59.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.011623639s
Jan 18 22:28:01.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.011036021s
Jan 18 22:28:03.875: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.012333781s
Jan 18 22:28:05.875: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.012321643s
Jan 18 22:28:07.880: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.017289074s
Jan 18 22:28:07.887: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.024804439s
STEP: updating the pod 01/18/23 22:28:07.888
Jan 18 22:28:08.436: INFO: Successfully updated pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56"
STEP: waiting for pod running 01/18/23 22:28:08.437
Jan 18 22:28:08.441: INFO: Waiting up to 2m0s for pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56" in namespace "var-expansion-1879" to be "running"
Jan 18 22:28:08.447: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 6.187227ms
Jan 18 22:28:10.452: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Running", Reason="", readiness=true. Elapsed: 2.011345762s
Jan 18 22:28:10.452: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56" satisfied condition "running"
STEP: deleting the pod gracefully 01/18/23 22:28:10.453
Jan 18 22:28:10.453: INFO: Deleting pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56" in namespace "var-expansion-1879"
Jan 18 22:28:10.460: INFO: Wait up to 5m0s for pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 18 22:28:42.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1879" for this suite. 01/18/23 22:28:42.496
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":213,"skipped":3834,"failed":0}
------------------------------
• [SLOW TEST] [154.688 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:26:07.817
    Jan 18 22:26:07.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename var-expansion 01/18/23 22:26:07.818
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:07.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:07.843
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 01/18/23 22:26:07.853
    Jan 18 22:26:07.862: INFO: Waiting up to 2m0s for pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56" in namespace "var-expansion-1879" to be "running"
    Jan 18 22:26:07.867: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 4.980926ms
    Jan 18 22:26:09.880: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01723428s
    Jan 18 22:26:11.877: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014471905s
    Jan 18 22:26:13.878: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015665475s
    Jan 18 22:26:15.879: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016111856s
    Jan 18 22:26:17.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011650144s
    Jan 18 22:26:19.877: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 12.014765356s
    Jan 18 22:26:21.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010635526s
    Jan 18 22:26:23.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 16.011668496s
    Jan 18 22:26:25.880: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 18.017480449s
    Jan 18 22:26:27.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 20.011331944s
    Jan 18 22:26:29.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 22.011902422s
    Jan 18 22:26:31.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 24.010575282s
    Jan 18 22:26:33.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 26.012036757s
    Jan 18 22:26:35.875: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 28.012890018s
    Jan 18 22:26:37.878: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 30.015495696s
    Jan 18 22:26:39.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010556173s
    Jan 18 22:26:41.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 34.011194772s
    Jan 18 22:26:43.876: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 36.013773648s
    Jan 18 22:26:45.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 38.011354846s
    Jan 18 22:26:47.872: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009955198s
    Jan 18 22:26:49.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 42.010824269s
    Jan 18 22:26:51.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 44.01060824s
    Jan 18 22:26:53.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 46.010873184s
    Jan 18 22:26:55.995: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 48.132724952s
    Jan 18 22:26:57.875: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 50.012271342s
    Jan 18 22:26:59.875: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 52.013028308s
    Jan 18 22:27:01.876: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 54.014015286s
    Jan 18 22:27:03.878: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 56.015148789s
    Jan 18 22:27:05.875: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 58.012621669s
    Jan 18 22:27:07.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.01160674s
    Jan 18 22:27:09.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.011827147s
    Jan 18 22:27:11.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.010739631s
    Jan 18 22:27:13.876: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.013452226s
    Jan 18 22:27:15.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.011075296s
    Jan 18 22:27:17.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.01175782s
    Jan 18 22:27:19.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.011007277s
    Jan 18 22:27:21.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.011724269s
    Jan 18 22:27:23.875: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.013053364s
    Jan 18 22:27:25.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.011761957s
    Jan 18 22:27:27.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.010401427s
    Jan 18 22:27:29.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.011408932s
    Jan 18 22:27:31.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.011405568s
    Jan 18 22:27:33.875: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.012488827s
    Jan 18 22:27:35.877: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.014839294s
    Jan 18 22:27:37.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.010163473s
    Jan 18 22:27:39.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.011046168s
    Jan 18 22:27:41.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.010390284s
    Jan 18 22:27:43.881: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.018219299s
    Jan 18 22:27:45.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.010820134s
    Jan 18 22:27:47.877: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.014999124s
    Jan 18 22:27:49.883: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.020701686s
    Jan 18 22:27:51.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.010924939s
    Jan 18 22:27:53.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.011899599s
    Jan 18 22:27:55.875: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.013097665s
    Jan 18 22:27:57.877: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.014963116s
    Jan 18 22:27:59.874: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.011623639s
    Jan 18 22:28:01.873: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.011036021s
    Jan 18 22:28:03.875: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.012333781s
    Jan 18 22:28:05.875: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.012321643s
    Jan 18 22:28:07.880: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.017289074s
    Jan 18 22:28:07.887: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.024804439s
    STEP: updating the pod 01/18/23 22:28:07.888
    Jan 18 22:28:08.436: INFO: Successfully updated pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56"
    STEP: waiting for pod running 01/18/23 22:28:08.437
    Jan 18 22:28:08.441: INFO: Waiting up to 2m0s for pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56" in namespace "var-expansion-1879" to be "running"
    Jan 18 22:28:08.447: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Pending", Reason="", readiness=false. Elapsed: 6.187227ms
    Jan 18 22:28:10.452: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56": Phase="Running", Reason="", readiness=true. Elapsed: 2.011345762s
    Jan 18 22:28:10.452: INFO: Pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56" satisfied condition "running"
    STEP: deleting the pod gracefully 01/18/23 22:28:10.453
    Jan 18 22:28:10.453: INFO: Deleting pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56" in namespace "var-expansion-1879"
    Jan 18 22:28:10.460: INFO: Wait up to 5m0s for pod "var-expansion-505608a3-7296-42ec-b0d7-44089fbbfb56" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 18 22:28:42.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1879" for this suite. 01/18/23 22:28:42.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:28:42.52
Jan 18 22:28:42.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename container-runtime 01/18/23 22:28:42.522
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:42.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:42.544
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 01/18/23 22:28:42.548
STEP: wait for the container to reach Succeeded 01/18/23 22:28:42.566
STEP: get the container status 01/18/23 22:28:46.604
STEP: the container should be terminated 01/18/23 22:28:46.61
STEP: the termination message should be set 01/18/23 22:28:46.61
Jan 18 22:28:46.611: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 01/18/23 22:28:46.611
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 18 22:28:46.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3982" for this suite. 01/18/23 22:28:46.636
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":214,"skipped":3900,"failed":0}
------------------------------
• [4.129 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:28:42.52
    Jan 18 22:28:42.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename container-runtime 01/18/23 22:28:42.522
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:42.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:42.544
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 01/18/23 22:28:42.548
    STEP: wait for the container to reach Succeeded 01/18/23 22:28:42.566
    STEP: get the container status 01/18/23 22:28:46.604
    STEP: the container should be terminated 01/18/23 22:28:46.61
    STEP: the termination message should be set 01/18/23 22:28:46.61
    Jan 18 22:28:46.611: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 01/18/23 22:28:46.611
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 18 22:28:46.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-3982" for this suite. 01/18/23 22:28:46.636
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:28:46.651
Jan 18 22:28:46.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename tables 01/18/23 22:28:46.653
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:46.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:46.685
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Jan 18 22:28:46.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-1186" for this suite. 01/18/23 22:28:46.705
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":215,"skipped":3900,"failed":0}
------------------------------
• [0.066 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:28:46.651
    Jan 18 22:28:46.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename tables 01/18/23 22:28:46.653
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:46.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:46.685
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Jan 18 22:28:46.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-1186" for this suite. 01/18/23 22:28:46.705
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:28:46.718
Jan 18 22:28:46.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename crd-watch 01/18/23 22:28:46.721
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:46.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:46.744
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jan 18 22:28:46.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Creating first CR  01/18/23 22:28:49.339
Jan 18 22:28:49.352: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:28:49Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:28:49Z]] name:name1 resourceVersion:160719 uid:90f03b24-0ca8-46a1-93b4-60c74bc450a4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 01/18/23 22:28:59.354
Jan 18 22:28:59.362: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:28:59Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:28:59Z]] name:name2 resourceVersion:160767 uid:52fce6c2-f75e-47b0-a3fb-b84c39571f2f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 01/18/23 22:29:09.362
Jan 18 22:29:09.372: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:28:49Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:29:09Z]] name:name1 resourceVersion:160804 uid:90f03b24-0ca8-46a1-93b4-60c74bc450a4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 01/18/23 22:29:19.373
Jan 18 22:29:19.382: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:28:59Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:29:19Z]] name:name2 resourceVersion:160840 uid:52fce6c2-f75e-47b0-a3fb-b84c39571f2f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 01/18/23 22:29:29.382
Jan 18 22:29:29.393: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:28:49Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:29:09Z]] name:name1 resourceVersion:160877 uid:90f03b24-0ca8-46a1-93b4-60c74bc450a4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 01/18/23 22:29:39.395
Jan 18 22:29:39.407: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:28:59Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:29:19Z]] name:name2 resourceVersion:160914 uid:52fce6c2-f75e-47b0-a3fb-b84c39571f2f] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 22:29:49.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-9477" for this suite. 01/18/23 22:29:49.937
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":216,"skipped":3901,"failed":0}
------------------------------
• [SLOW TEST] [63.231 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:28:46.718
    Jan 18 22:28:46.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename crd-watch 01/18/23 22:28:46.721
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:28:46.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:28:46.744
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jan 18 22:28:46.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Creating first CR  01/18/23 22:28:49.339
    Jan 18 22:28:49.352: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:28:49Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:28:49Z]] name:name1 resourceVersion:160719 uid:90f03b24-0ca8-46a1-93b4-60c74bc450a4] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 01/18/23 22:28:59.354
    Jan 18 22:28:59.362: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:28:59Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:28:59Z]] name:name2 resourceVersion:160767 uid:52fce6c2-f75e-47b0-a3fb-b84c39571f2f] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 01/18/23 22:29:09.362
    Jan 18 22:29:09.372: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:28:49Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:29:09Z]] name:name1 resourceVersion:160804 uid:90f03b24-0ca8-46a1-93b4-60c74bc450a4] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 01/18/23 22:29:19.373
    Jan 18 22:29:19.382: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:28:59Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:29:19Z]] name:name2 resourceVersion:160840 uid:52fce6c2-f75e-47b0-a3fb-b84c39571f2f] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 01/18/23 22:29:29.382
    Jan 18 22:29:29.393: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:28:49Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:29:09Z]] name:name1 resourceVersion:160877 uid:90f03b24-0ca8-46a1-93b4-60c74bc450a4] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 01/18/23 22:29:39.395
    Jan 18 22:29:39.407: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:28:59Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:29:19Z]] name:name2 resourceVersion:160914 uid:52fce6c2-f75e-47b0-a3fb-b84c39571f2f] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 22:29:49.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-9477" for this suite. 01/18/23 22:29:49.937
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:29:49.955
Jan 18 22:29:49.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename kubectl 01/18/23 22:29:49.958
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:29:49.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:29:49.995
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/18/23 22:29:50
Jan 18 22:29:50.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3730 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 18 22:29:50.127: INFO: stderr: ""
Jan 18 22:29:50.127: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 01/18/23 22:29:50.127
STEP: verifying the pod e2e-test-httpd-pod was created 01/18/23 22:29:55.18
Jan 18 22:29:55.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3730 get pod e2e-test-httpd-pod -o json'
Jan 18 22:29:55.297: INFO: stderr: ""
Jan 18 22:29:55.297: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"4ed49b027dcf041188fa97df9ea5129fbfed3237b4c5c070bdb81e90e4319cd6\",\n            \"cni.projectcalico.org/podIP\": \"10.233.68.52/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.233.68.52/32\"\n        },\n        \"creationTimestamp\": \"2023-01-18T22:29:50Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3730\",\n        \"resourceVersion\": \"160980\",\n        \"uid\": \"70aa7231-ddcb-4286-b8c9-0de2f672d2fa\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-zpmsh\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"v1-25-1-18760-w2\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-zpmsh\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T22:29:50Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T22:29:51Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T22:29:51Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T22:29:50Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://78492a9d71f8342ff1ccec977e88bbab5e78c15751eee69b482a89c6aa27726f\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-18T22:29:51Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.101.216\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.68.52\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.68.52\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-18T22:29:50Z\"\n    }\n}\n"
STEP: replace the image in the pod 01/18/23 22:29:55.297
Jan 18 22:29:55.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3730 replace -f -'
Jan 18 22:29:56.937: INFO: stderr: ""
Jan 18 22:29:56.937: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 01/18/23 22:29:56.937
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Jan 18 22:29:56.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3730 delete pods e2e-test-httpd-pod'
Jan 18 22:29:58.874: INFO: stderr: ""
Jan 18 22:29:58.874: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 22:29:58.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3730" for this suite. 01/18/23 22:29:58.882
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":217,"skipped":3918,"failed":0}
------------------------------
• [SLOW TEST] [8.936 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:29:49.955
    Jan 18 22:29:49.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename kubectl 01/18/23 22:29:49.958
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:29:49.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:29:49.995
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/18/23 22:29:50
    Jan 18 22:29:50.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3730 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 18 22:29:50.127: INFO: stderr: ""
    Jan 18 22:29:50.127: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 01/18/23 22:29:50.127
    STEP: verifying the pod e2e-test-httpd-pod was created 01/18/23 22:29:55.18
    Jan 18 22:29:55.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3730 get pod e2e-test-httpd-pod -o json'
    Jan 18 22:29:55.297: INFO: stderr: ""
    Jan 18 22:29:55.297: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"4ed49b027dcf041188fa97df9ea5129fbfed3237b4c5c070bdb81e90e4319cd6\",\n            \"cni.projectcalico.org/podIP\": \"10.233.68.52/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.233.68.52/32\"\n        },\n        \"creationTimestamp\": \"2023-01-18T22:29:50Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3730\",\n        \"resourceVersion\": \"160980\",\n        \"uid\": \"70aa7231-ddcb-4286-b8c9-0de2f672d2fa\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-zpmsh\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"v1-25-1-18760-w2\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-zpmsh\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T22:29:50Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T22:29:51Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T22:29:51Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T22:29:50Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://78492a9d71f8342ff1ccec977e88bbab5e78c15751eee69b482a89c6aa27726f\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-18T22:29:51Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.101.216\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.68.52\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.68.52\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-18T22:29:50Z\"\n    }\n}\n"
    STEP: replace the image in the pod 01/18/23 22:29:55.297
    Jan 18 22:29:55.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3730 replace -f -'
    Jan 18 22:29:56.937: INFO: stderr: ""
    Jan 18 22:29:56.937: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 01/18/23 22:29:56.937
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Jan 18 22:29:56.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3730 delete pods e2e-test-httpd-pod'
    Jan 18 22:29:58.874: INFO: stderr: ""
    Jan 18 22:29:58.874: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 22:29:58.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3730" for this suite. 01/18/23 22:29:58.882
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:29:58.892
Jan 18 22:29:58.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename downward-api 01/18/23 22:29:58.893
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:29:58.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:29:58.921
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 01/18/23 22:29:58.93
Jan 18 22:29:58.941: INFO: Waiting up to 5m0s for pod "downward-api-1213082e-173b-41f6-afb8-a1a0afc19fad" in namespace "downward-api-2077" to be "Succeeded or Failed"
Jan 18 22:29:58.946: INFO: Pod "downward-api-1213082e-173b-41f6-afb8-a1a0afc19fad": Phase="Pending", Reason="", readiness=false. Elapsed: 5.195983ms
Jan 18 22:30:00.951: INFO: Pod "downward-api-1213082e-173b-41f6-afb8-a1a0afc19fad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010679623s
Jan 18 22:30:02.951: INFO: Pod "downward-api-1213082e-173b-41f6-afb8-a1a0afc19fad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010460171s
STEP: Saw pod success 01/18/23 22:30:02.951
Jan 18 22:30:02.952: INFO: Pod "downward-api-1213082e-173b-41f6-afb8-a1a0afc19fad" satisfied condition "Succeeded or Failed"
Jan 18 22:30:02.956: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downward-api-1213082e-173b-41f6-afb8-a1a0afc19fad container dapi-container: <nil>
STEP: delete the pod 01/18/23 22:30:02.982
Jan 18 22:30:02.998: INFO: Waiting for pod downward-api-1213082e-173b-41f6-afb8-a1a0afc19fad to disappear
Jan 18 22:30:03.006: INFO: Pod downward-api-1213082e-173b-41f6-afb8-a1a0afc19fad no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 18 22:30:03.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2077" for this suite. 01/18/23 22:30:03.024
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":218,"skipped":3919,"failed":0}
------------------------------
• [4.139 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:29:58.892
    Jan 18 22:29:58.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename downward-api 01/18/23 22:29:58.893
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:29:58.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:29:58.921
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 01/18/23 22:29:58.93
    Jan 18 22:29:58.941: INFO: Waiting up to 5m0s for pod "downward-api-1213082e-173b-41f6-afb8-a1a0afc19fad" in namespace "downward-api-2077" to be "Succeeded or Failed"
    Jan 18 22:29:58.946: INFO: Pod "downward-api-1213082e-173b-41f6-afb8-a1a0afc19fad": Phase="Pending", Reason="", readiness=false. Elapsed: 5.195983ms
    Jan 18 22:30:00.951: INFO: Pod "downward-api-1213082e-173b-41f6-afb8-a1a0afc19fad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010679623s
    Jan 18 22:30:02.951: INFO: Pod "downward-api-1213082e-173b-41f6-afb8-a1a0afc19fad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010460171s
    STEP: Saw pod success 01/18/23 22:30:02.951
    Jan 18 22:30:02.952: INFO: Pod "downward-api-1213082e-173b-41f6-afb8-a1a0afc19fad" satisfied condition "Succeeded or Failed"
    Jan 18 22:30:02.956: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downward-api-1213082e-173b-41f6-afb8-a1a0afc19fad container dapi-container: <nil>
    STEP: delete the pod 01/18/23 22:30:02.982
    Jan 18 22:30:02.998: INFO: Waiting for pod downward-api-1213082e-173b-41f6-afb8-a1a0afc19fad to disappear
    Jan 18 22:30:03.006: INFO: Pod downward-api-1213082e-173b-41f6-afb8-a1a0afc19fad no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 18 22:30:03.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2077" for this suite. 01/18/23 22:30:03.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:30:03.035
Jan 18 22:30:03.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 22:30:03.037
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:30:03.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:30:03.069
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-366a80e8-b7b9-4dba-96b4-0cd79a4073f8 01/18/23 22:30:03.079
STEP: Creating secret with name s-test-opt-upd-41e0ab19-3ec0-4e91-8acd-b7559e481a96 01/18/23 22:30:03.086
STEP: Creating the pod 01/18/23 22:30:03.092
Jan 18 22:30:03.103: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bb40d9d6-d3c4-441f-94fd-d2ddace6e94a" in namespace "projected-9861" to be "running and ready"
Jan 18 22:30:03.115: INFO: Pod "pod-projected-secrets-bb40d9d6-d3c4-441f-94fd-d2ddace6e94a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.201232ms
Jan 18 22:30:03.115: INFO: The phase of Pod pod-projected-secrets-bb40d9d6-d3c4-441f-94fd-d2ddace6e94a is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:30:05.119: INFO: Pod "pod-projected-secrets-bb40d9d6-d3c4-441f-94fd-d2ddace6e94a": Phase="Running", Reason="", readiness=true. Elapsed: 2.015769123s
Jan 18 22:30:05.120: INFO: The phase of Pod pod-projected-secrets-bb40d9d6-d3c4-441f-94fd-d2ddace6e94a is Running (Ready = true)
Jan 18 22:30:05.120: INFO: Pod "pod-projected-secrets-bb40d9d6-d3c4-441f-94fd-d2ddace6e94a" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-366a80e8-b7b9-4dba-96b4-0cd79a4073f8 01/18/23 22:30:05.148
STEP: Updating secret s-test-opt-upd-41e0ab19-3ec0-4e91-8acd-b7559e481a96 01/18/23 22:30:05.154
STEP: Creating secret with name s-test-opt-create-02cbaa0a-c7a5-4d85-ba6f-a5bf367d6a08 01/18/23 22:30:05.16
STEP: waiting to observe update in volume 01/18/23 22:30:05.165
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 18 22:30:07.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9861" for this suite. 01/18/23 22:30:07.206
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":219,"skipped":3937,"failed":0}
------------------------------
• [4.180 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:30:03.035
    Jan 18 22:30:03.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 22:30:03.037
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:30:03.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:30:03.069
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-366a80e8-b7b9-4dba-96b4-0cd79a4073f8 01/18/23 22:30:03.079
    STEP: Creating secret with name s-test-opt-upd-41e0ab19-3ec0-4e91-8acd-b7559e481a96 01/18/23 22:30:03.086
    STEP: Creating the pod 01/18/23 22:30:03.092
    Jan 18 22:30:03.103: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bb40d9d6-d3c4-441f-94fd-d2ddace6e94a" in namespace "projected-9861" to be "running and ready"
    Jan 18 22:30:03.115: INFO: Pod "pod-projected-secrets-bb40d9d6-d3c4-441f-94fd-d2ddace6e94a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.201232ms
    Jan 18 22:30:03.115: INFO: The phase of Pod pod-projected-secrets-bb40d9d6-d3c4-441f-94fd-d2ddace6e94a is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:30:05.119: INFO: Pod "pod-projected-secrets-bb40d9d6-d3c4-441f-94fd-d2ddace6e94a": Phase="Running", Reason="", readiness=true. Elapsed: 2.015769123s
    Jan 18 22:30:05.120: INFO: The phase of Pod pod-projected-secrets-bb40d9d6-d3c4-441f-94fd-d2ddace6e94a is Running (Ready = true)
    Jan 18 22:30:05.120: INFO: Pod "pod-projected-secrets-bb40d9d6-d3c4-441f-94fd-d2ddace6e94a" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-366a80e8-b7b9-4dba-96b4-0cd79a4073f8 01/18/23 22:30:05.148
    STEP: Updating secret s-test-opt-upd-41e0ab19-3ec0-4e91-8acd-b7559e481a96 01/18/23 22:30:05.154
    STEP: Creating secret with name s-test-opt-create-02cbaa0a-c7a5-4d85-ba6f-a5bf367d6a08 01/18/23 22:30:05.16
    STEP: waiting to observe update in volume 01/18/23 22:30:05.165
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 18 22:30:07.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9861" for this suite. 01/18/23 22:30:07.206
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:30:07.228
Jan 18 22:30:07.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename disruption 01/18/23 22:30:07.229
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:30:07.252
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:30:07.256
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 01/18/23 22:30:07.266
STEP: Waiting for all pods to be running 01/18/23 22:30:09.317
Jan 18 22:30:09.340: INFO: running pods: 0 < 3
Jan 18 22:30:11.347: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 18 22:30:13.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2602" for this suite. 01/18/23 22:30:13.358
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":220,"skipped":3977,"failed":0}
------------------------------
• [SLOW TEST] [6.139 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:30:07.228
    Jan 18 22:30:07.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename disruption 01/18/23 22:30:07.229
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:30:07.252
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:30:07.256
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 01/18/23 22:30:07.266
    STEP: Waiting for all pods to be running 01/18/23 22:30:09.317
    Jan 18 22:30:09.340: INFO: running pods: 0 < 3
    Jan 18 22:30:11.347: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 18 22:30:13.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2602" for this suite. 01/18/23 22:30:13.358
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:30:13.37
Jan 18 22:30:13.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename var-expansion 01/18/23 22:30:13.373
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:30:13.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:30:13.396
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Jan 18 22:30:13.451: INFO: Waiting up to 2m0s for pod "var-expansion-202aff1f-d67f-4a46-b3a6-6ea4d00a0afb" in namespace "var-expansion-1304" to be "container 0 failed with reason CreateContainerConfigError"
Jan 18 22:30:13.460: INFO: Pod "var-expansion-202aff1f-d67f-4a46-b3a6-6ea4d00a0afb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.405897ms
Jan 18 22:30:15.465: INFO: Pod "var-expansion-202aff1f-d67f-4a46-b3a6-6ea4d00a0afb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013385038s
Jan 18 22:30:15.466: INFO: Pod "var-expansion-202aff1f-d67f-4a46-b3a6-6ea4d00a0afb" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 18 22:30:15.466: INFO: Deleting pod "var-expansion-202aff1f-d67f-4a46-b3a6-6ea4d00a0afb" in namespace "var-expansion-1304"
Jan 18 22:30:15.482: INFO: Wait up to 5m0s for pod "var-expansion-202aff1f-d67f-4a46-b3a6-6ea4d00a0afb" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 18 22:30:19.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1304" for this suite. 01/18/23 22:30:19.505
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":221,"skipped":3996,"failed":0}
------------------------------
• [SLOW TEST] [6.148 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:30:13.37
    Jan 18 22:30:13.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename var-expansion 01/18/23 22:30:13.373
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:30:13.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:30:13.396
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Jan 18 22:30:13.451: INFO: Waiting up to 2m0s for pod "var-expansion-202aff1f-d67f-4a46-b3a6-6ea4d00a0afb" in namespace "var-expansion-1304" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 18 22:30:13.460: INFO: Pod "var-expansion-202aff1f-d67f-4a46-b3a6-6ea4d00a0afb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.405897ms
    Jan 18 22:30:15.465: INFO: Pod "var-expansion-202aff1f-d67f-4a46-b3a6-6ea4d00a0afb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013385038s
    Jan 18 22:30:15.466: INFO: Pod "var-expansion-202aff1f-d67f-4a46-b3a6-6ea4d00a0afb" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 18 22:30:15.466: INFO: Deleting pod "var-expansion-202aff1f-d67f-4a46-b3a6-6ea4d00a0afb" in namespace "var-expansion-1304"
    Jan 18 22:30:15.482: INFO: Wait up to 5m0s for pod "var-expansion-202aff1f-d67f-4a46-b3a6-6ea4d00a0afb" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 18 22:30:19.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1304" for this suite. 01/18/23 22:30:19.505
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:30:19.526
Jan 18 22:30:19.527: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename replicaset 01/18/23 22:30:19.528
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:30:19.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:30:19.583
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/18/23 22:30:19.592
Jan 18 22:30:19.609: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-1168" to be "running and ready"
Jan 18 22:30:19.624: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 14.588674ms
Jan 18 22:30:19.624: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:30:21.637: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.028409724s
Jan 18 22:30:21.638: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jan 18 22:30:21.638: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 01/18/23 22:30:21.644
STEP: Then the orphan pod is adopted 01/18/23 22:30:21.653
STEP: When the matched label of one of its pods change 01/18/23 22:30:22.673
Jan 18 22:30:22.680: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 01/18/23 22:30:22.696
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 18 22:30:23.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1168" for this suite. 01/18/23 22:30:23.722
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":222,"skipped":4015,"failed":0}
------------------------------
• [4.207 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:30:19.526
    Jan 18 22:30:19.527: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename replicaset 01/18/23 22:30:19.528
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:30:19.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:30:19.583
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/18/23 22:30:19.592
    Jan 18 22:30:19.609: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-1168" to be "running and ready"
    Jan 18 22:30:19.624: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 14.588674ms
    Jan 18 22:30:19.624: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:30:21.637: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.028409724s
    Jan 18 22:30:21.638: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jan 18 22:30:21.638: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 01/18/23 22:30:21.644
    STEP: Then the orphan pod is adopted 01/18/23 22:30:21.653
    STEP: When the matched label of one of its pods change 01/18/23 22:30:22.673
    Jan 18 22:30:22.680: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/18/23 22:30:22.696
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 18 22:30:23.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-1168" for this suite. 01/18/23 22:30:23.722
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:30:23.747
Jan 18 22:30:23.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename podtemplate 01/18/23 22:30:23.748
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:30:23.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:30:23.807
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 01/18/23 22:30:23.826
Jan 18 22:30:23.837: INFO: created test-podtemplate-1
Jan 18 22:30:23.854: INFO: created test-podtemplate-2
Jan 18 22:30:23.867: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 01/18/23 22:30:23.868
STEP: delete collection of pod templates 01/18/23 22:30:23.874
Jan 18 22:30:23.874: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 01/18/23 22:30:23.912
Jan 18 22:30:23.912: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan 18 22:30:23.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5585" for this suite. 01/18/23 22:30:23.926
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":223,"skipped":4043,"failed":0}
------------------------------
• [0.191 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:30:23.747
    Jan 18 22:30:23.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename podtemplate 01/18/23 22:30:23.748
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:30:23.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:30:23.807
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 01/18/23 22:30:23.826
    Jan 18 22:30:23.837: INFO: created test-podtemplate-1
    Jan 18 22:30:23.854: INFO: created test-podtemplate-2
    Jan 18 22:30:23.867: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 01/18/23 22:30:23.868
    STEP: delete collection of pod templates 01/18/23 22:30:23.874
    Jan 18 22:30:23.874: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 01/18/23 22:30:23.912
    Jan 18 22:30:23.912: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan 18 22:30:23.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-5585" for this suite. 01/18/23 22:30:23.926
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:30:23.941
Jan 18 22:30:23.941: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename proxy 01/18/23 22:30:23.943
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:30:23.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:30:23.981
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jan 18 22:30:23.993: INFO: Creating pod...
Jan 18 22:30:24.003: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7180" to be "running"
Jan 18 22:30:24.029: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 26.352429ms
Jan 18 22:30:26.040: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03637631s
Jan 18 22:30:28.037: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.033659691s
Jan 18 22:30:28.037: INFO: Pod "agnhost" satisfied condition "running"
Jan 18 22:30:28.037: INFO: Creating service...
Jan 18 22:30:28.051: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/pods/agnhost/proxy?method=DELETE
Jan 18 22:30:28.067: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 18 22:30:28.068: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/pods/agnhost/proxy?method=OPTIONS
Jan 18 22:30:28.083: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 18 22:30:28.083: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/pods/agnhost/proxy?method=PATCH
Jan 18 22:30:28.093: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 18 22:30:28.093: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/pods/agnhost/proxy?method=POST
Jan 18 22:30:28.101: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 18 22:30:28.102: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/pods/agnhost/proxy?method=PUT
Jan 18 22:30:28.109: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 18 22:30:28.109: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/services/e2e-proxy-test-service/proxy?method=DELETE
Jan 18 22:30:28.117: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 18 22:30:28.117: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan 18 22:30:28.126: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 18 22:30:28.126: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/services/e2e-proxy-test-service/proxy?method=PATCH
Jan 18 22:30:28.134: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 18 22:30:28.134: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/services/e2e-proxy-test-service/proxy?method=POST
Jan 18 22:30:28.142: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 18 22:30:28.143: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/services/e2e-proxy-test-service/proxy?method=PUT
Jan 18 22:30:28.151: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 18 22:30:28.151: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/pods/agnhost/proxy?method=GET
Jan 18 22:30:28.156: INFO: http.Client request:GET StatusCode:301
Jan 18 22:30:28.156: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/services/e2e-proxy-test-service/proxy?method=GET
Jan 18 22:30:28.161: INFO: http.Client request:GET StatusCode:301
Jan 18 22:30:28.161: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/pods/agnhost/proxy?method=HEAD
Jan 18 22:30:28.165: INFO: http.Client request:HEAD StatusCode:301
Jan 18 22:30:28.165: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/services/e2e-proxy-test-service/proxy?method=HEAD
Jan 18 22:30:28.172: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan 18 22:30:28.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7180" for this suite. 01/18/23 22:30:28.179
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":224,"skipped":4082,"failed":0}
------------------------------
• [4.246 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:30:23.941
    Jan 18 22:30:23.941: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename proxy 01/18/23 22:30:23.943
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:30:23.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:30:23.981
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jan 18 22:30:23.993: INFO: Creating pod...
    Jan 18 22:30:24.003: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7180" to be "running"
    Jan 18 22:30:24.029: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 26.352429ms
    Jan 18 22:30:26.040: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03637631s
    Jan 18 22:30:28.037: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.033659691s
    Jan 18 22:30:28.037: INFO: Pod "agnhost" satisfied condition "running"
    Jan 18 22:30:28.037: INFO: Creating service...
    Jan 18 22:30:28.051: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/pods/agnhost/proxy?method=DELETE
    Jan 18 22:30:28.067: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 18 22:30:28.068: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/pods/agnhost/proxy?method=OPTIONS
    Jan 18 22:30:28.083: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 18 22:30:28.083: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/pods/agnhost/proxy?method=PATCH
    Jan 18 22:30:28.093: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 18 22:30:28.093: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/pods/agnhost/proxy?method=POST
    Jan 18 22:30:28.101: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 18 22:30:28.102: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/pods/agnhost/proxy?method=PUT
    Jan 18 22:30:28.109: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 18 22:30:28.109: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/services/e2e-proxy-test-service/proxy?method=DELETE
    Jan 18 22:30:28.117: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 18 22:30:28.117: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jan 18 22:30:28.126: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 18 22:30:28.126: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/services/e2e-proxy-test-service/proxy?method=PATCH
    Jan 18 22:30:28.134: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 18 22:30:28.134: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/services/e2e-proxy-test-service/proxy?method=POST
    Jan 18 22:30:28.142: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 18 22:30:28.143: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/services/e2e-proxy-test-service/proxy?method=PUT
    Jan 18 22:30:28.151: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 18 22:30:28.151: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/pods/agnhost/proxy?method=GET
    Jan 18 22:30:28.156: INFO: http.Client request:GET StatusCode:301
    Jan 18 22:30:28.156: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/services/e2e-proxy-test-service/proxy?method=GET
    Jan 18 22:30:28.161: INFO: http.Client request:GET StatusCode:301
    Jan 18 22:30:28.161: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/pods/agnhost/proxy?method=HEAD
    Jan 18 22:30:28.165: INFO: http.Client request:HEAD StatusCode:301
    Jan 18 22:30:28.165: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7180/services/e2e-proxy-test-service/proxy?method=HEAD
    Jan 18 22:30:28.172: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan 18 22:30:28.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-7180" for this suite. 01/18/23 22:30:28.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:30:28.201
Jan 18 22:30:28.203: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename secrets 01/18/23 22:30:28.205
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:30:28.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:30:28.251
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-8bd739e7-551a-4138-b6ca-72c43769e691 01/18/23 22:30:28.262
STEP: Creating a pod to test consume secrets 01/18/23 22:30:28.27
Jan 18 22:30:28.307: INFO: Waiting up to 5m0s for pod "pod-secrets-5656d415-9d75-41b2-b7d4-22a558ec5f70" in namespace "secrets-5662" to be "Succeeded or Failed"
Jan 18 22:30:28.349: INFO: Pod "pod-secrets-5656d415-9d75-41b2-b7d4-22a558ec5f70": Phase="Pending", Reason="", readiness=false. Elapsed: 41.601746ms
Jan 18 22:30:30.355: INFO: Pod "pod-secrets-5656d415-9d75-41b2-b7d4-22a558ec5f70": Phase="Running", Reason="", readiness=true. Elapsed: 2.047171829s
Jan 18 22:30:32.355: INFO: Pod "pod-secrets-5656d415-9d75-41b2-b7d4-22a558ec5f70": Phase="Running", Reason="", readiness=false. Elapsed: 4.047790829s
Jan 18 22:30:34.354: INFO: Pod "pod-secrets-5656d415-9d75-41b2-b7d4-22a558ec5f70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046585253s
STEP: Saw pod success 01/18/23 22:30:34.354
Jan 18 22:30:34.354: INFO: Pod "pod-secrets-5656d415-9d75-41b2-b7d4-22a558ec5f70" satisfied condition "Succeeded or Failed"
Jan 18 22:30:34.359: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-5656d415-9d75-41b2-b7d4-22a558ec5f70 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 22:30:34.368
Jan 18 22:30:34.385: INFO: Waiting for pod pod-secrets-5656d415-9d75-41b2-b7d4-22a558ec5f70 to disappear
Jan 18 22:30:34.389: INFO: Pod pod-secrets-5656d415-9d75-41b2-b7d4-22a558ec5f70 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 18 22:30:34.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5662" for this suite. 01/18/23 22:30:34.395
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":225,"skipped":4095,"failed":0}
------------------------------
• [SLOW TEST] [6.200 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:30:28.201
    Jan 18 22:30:28.203: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename secrets 01/18/23 22:30:28.205
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:30:28.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:30:28.251
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-8bd739e7-551a-4138-b6ca-72c43769e691 01/18/23 22:30:28.262
    STEP: Creating a pod to test consume secrets 01/18/23 22:30:28.27
    Jan 18 22:30:28.307: INFO: Waiting up to 5m0s for pod "pod-secrets-5656d415-9d75-41b2-b7d4-22a558ec5f70" in namespace "secrets-5662" to be "Succeeded or Failed"
    Jan 18 22:30:28.349: INFO: Pod "pod-secrets-5656d415-9d75-41b2-b7d4-22a558ec5f70": Phase="Pending", Reason="", readiness=false. Elapsed: 41.601746ms
    Jan 18 22:30:30.355: INFO: Pod "pod-secrets-5656d415-9d75-41b2-b7d4-22a558ec5f70": Phase="Running", Reason="", readiness=true. Elapsed: 2.047171829s
    Jan 18 22:30:32.355: INFO: Pod "pod-secrets-5656d415-9d75-41b2-b7d4-22a558ec5f70": Phase="Running", Reason="", readiness=false. Elapsed: 4.047790829s
    Jan 18 22:30:34.354: INFO: Pod "pod-secrets-5656d415-9d75-41b2-b7d4-22a558ec5f70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046585253s
    STEP: Saw pod success 01/18/23 22:30:34.354
    Jan 18 22:30:34.354: INFO: Pod "pod-secrets-5656d415-9d75-41b2-b7d4-22a558ec5f70" satisfied condition "Succeeded or Failed"
    Jan 18 22:30:34.359: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-5656d415-9d75-41b2-b7d4-22a558ec5f70 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 22:30:34.368
    Jan 18 22:30:34.385: INFO: Waiting for pod pod-secrets-5656d415-9d75-41b2-b7d4-22a558ec5f70 to disappear
    Jan 18 22:30:34.389: INFO: Pod pod-secrets-5656d415-9d75-41b2-b7d4-22a558ec5f70 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 22:30:34.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5662" for this suite. 01/18/23 22:30:34.395
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:30:34.408
Jan 18 22:30:34.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 22:30:34.409
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:30:34.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:30:34.444
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/18/23 22:30:34.45
Jan 18 22:30:34.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/18/23 22:30:51.659
Jan 18 22:30:51.660: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 22:30:56.886: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 22:31:14.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5784" for this suite. 01/18/23 22:31:14.408
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":226,"skipped":4109,"failed":0}
------------------------------
• [SLOW TEST] [40.011 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:30:34.408
    Jan 18 22:30:34.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 22:30:34.409
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:30:34.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:30:34.444
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/18/23 22:30:34.45
    Jan 18 22:30:34.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/18/23 22:30:51.659
    Jan 18 22:30:51.660: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 22:30:56.886: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 22:31:14.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5784" for this suite. 01/18/23 22:31:14.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:31:14.421
Jan 18 22:31:14.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename deployment 01/18/23 22:31:14.423
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:14.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:14.453
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jan 18 22:31:14.457: INFO: Creating simple deployment test-new-deployment
Jan 18 22:31:14.474: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
STEP: getting scale subresource 01/18/23 22:31:16.492
STEP: updating a scale subresource 01/18/23 22:31:16.497
STEP: verifying the deployment Spec.Replicas was modified 01/18/23 22:31:16.509
STEP: Patch a scale subresource 01/18/23 22:31:16.518
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 22:31:16.546: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-1565  85fade9c-8fcf-4cf7-a3a0-adf02303e4df 161731 3 2023-01-18 22:31:14 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-18 22:31:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:31:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00450ea98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 22:31:16 +0000 UTC,LastTransitionTime:2023-01-18 22:31:16 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-01-18 22:31:16 +0000 UTC,LastTransitionTime:2023-01-18 22:31:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 18 22:31:16.564: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-1565  94d19a68-eda4-4324-beda-5103c7540e06 161734 3 2023-01-18 22:31:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 85fade9c-8fcf-4cf7-a3a0-adf02303e4df 0xc0045383a7 0xc0045383a8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:31:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85fade9c-8fcf-4cf7-a3a0-adf02303e4df\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:31:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004538468 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 22:31:16.581: INFO: Pod "test-new-deployment-845c8977d9-v8wkt" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-v8wkt test-new-deployment-845c8977d9- deployment-1565  75bba9d1-09f9-48cf-9f2c-54f8336a5a5a 161738 0 2023-01-18 22:31:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 94d19a68-eda4-4324-beda-5103c7540e06 0xc004538997 0xc004538998}] [] [{kube-controller-manager Update v1 2023-01-18 22:31:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94d19a68-eda4-4324-beda-5103c7540e06\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:31:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6m7gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6m7gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:31:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:31:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:31:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:31:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:,StartTime:2023-01-18 22:31:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 22:31:16.582: INFO: Pod "test-new-deployment-845c8977d9-w4zjz" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-w4zjz test-new-deployment-845c8977d9- deployment-1565  357ebaf8-55d8-449a-923d-e340965a0609 161722 0 2023-01-18 22:31:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:7edbd96656b4fad75ea74bfce0868488d6c4d637ec47c3169eeb315cb432684f cni.projectcalico.org/podIP:10.233.68.25/32 cni.projectcalico.org/podIPs:10.233.68.25/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 94d19a68-eda4-4324-beda-5103c7540e06 0xc004538bc7 0xc004538bc8}] [] [{kube-controller-manager Update v1 2023-01-18 22:31:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94d19a68-eda4-4324-beda-5103c7540e06\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 22:31:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.25\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-25b6r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-25b6r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:31:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:31:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:31:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:31:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.25,StartTime:2023-01-18 22:31:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:31:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://496634885cf9285a4152fde37ab5b184a9519d1feef7fcc7c8e484c62144ba0c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.25,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 18 22:31:16.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1565" for this suite. 01/18/23 22:31:16.6
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":227,"skipped":4114,"failed":0}
------------------------------
• [2.215 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:31:14.421
    Jan 18 22:31:14.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename deployment 01/18/23 22:31:14.423
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:14.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:14.453
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jan 18 22:31:14.457: INFO: Creating simple deployment test-new-deployment
    Jan 18 22:31:14.474: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
    STEP: getting scale subresource 01/18/23 22:31:16.492
    STEP: updating a scale subresource 01/18/23 22:31:16.497
    STEP: verifying the deployment Spec.Replicas was modified 01/18/23 22:31:16.509
    STEP: Patch a scale subresource 01/18/23 22:31:16.518
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 22:31:16.546: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-1565  85fade9c-8fcf-4cf7-a3a0-adf02303e4df 161731 3 2023-01-18 22:31:14 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-18 22:31:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:31:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00450ea98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 22:31:16 +0000 UTC,LastTransitionTime:2023-01-18 22:31:16 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-01-18 22:31:16 +0000 UTC,LastTransitionTime:2023-01-18 22:31:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 18 22:31:16.564: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-1565  94d19a68-eda4-4324-beda-5103c7540e06 161734 3 2023-01-18 22:31:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 85fade9c-8fcf-4cf7-a3a0-adf02303e4df 0xc0045383a7 0xc0045383a8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 22:31:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85fade9c-8fcf-4cf7-a3a0-adf02303e4df\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 22:31:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004538468 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 22:31:16.581: INFO: Pod "test-new-deployment-845c8977d9-v8wkt" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-v8wkt test-new-deployment-845c8977d9- deployment-1565  75bba9d1-09f9-48cf-9f2c-54f8336a5a5a 161738 0 2023-01-18 22:31:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 94d19a68-eda4-4324-beda-5103c7540e06 0xc004538997 0xc004538998}] [] [{kube-controller-manager Update v1 2023-01-18 22:31:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94d19a68-eda4-4324-beda-5103c7540e06\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 22:31:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6m7gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6m7gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:31:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:31:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:31:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:31:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:,StartTime:2023-01-18 22:31:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 22:31:16.582: INFO: Pod "test-new-deployment-845c8977d9-w4zjz" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-w4zjz test-new-deployment-845c8977d9- deployment-1565  357ebaf8-55d8-449a-923d-e340965a0609 161722 0 2023-01-18 22:31:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:7edbd96656b4fad75ea74bfce0868488d6c4d637ec47c3169eeb315cb432684f cni.projectcalico.org/podIP:10.233.68.25/32 cni.projectcalico.org/podIPs:10.233.68.25/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 94d19a68-eda4-4324-beda-5103c7540e06 0xc004538bc7 0xc004538bc8}] [] [{kube-controller-manager Update v1 2023-01-18 22:31:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94d19a68-eda4-4324-beda-5103c7540e06\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 22:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 22:31:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.25\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-25b6r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-25b6r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:31:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:31:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:31:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:31:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.25,StartTime:2023-01-18 22:31:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:31:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://496634885cf9285a4152fde37ab5b184a9519d1feef7fcc7c8e484c62144ba0c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.25,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 18 22:31:16.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1565" for this suite. 01/18/23 22:31:16.6
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:31:16.639
Jan 18 22:31:16.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename configmap 01/18/23 22:31:16.64
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:16.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:16.7
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-4976/configmap-test-8149ecec-dd1b-4d2c-bc4c-1175d9d1dd86 01/18/23 22:31:16.724
STEP: Creating a pod to test consume configMaps 01/18/23 22:31:16.73
Jan 18 22:31:16.742: INFO: Waiting up to 5m0s for pod "pod-configmaps-f26c91b8-1869-4d2d-9c3b-47f328d52e05" in namespace "configmap-4976" to be "Succeeded or Failed"
Jan 18 22:31:16.746: INFO: Pod "pod-configmaps-f26c91b8-1869-4d2d-9c3b-47f328d52e05": Phase="Pending", Reason="", readiness=false. Elapsed: 3.826597ms
Jan 18 22:31:18.751: INFO: Pod "pod-configmaps-f26c91b8-1869-4d2d-9c3b-47f328d52e05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009419429s
Jan 18 22:31:20.752: INFO: Pod "pod-configmaps-f26c91b8-1869-4d2d-9c3b-47f328d52e05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010278117s
STEP: Saw pod success 01/18/23 22:31:20.752
Jan 18 22:31:20.753: INFO: Pod "pod-configmaps-f26c91b8-1869-4d2d-9c3b-47f328d52e05" satisfied condition "Succeeded or Failed"
Jan 18 22:31:20.760: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-f26c91b8-1869-4d2d-9c3b-47f328d52e05 container env-test: <nil>
STEP: delete the pod 01/18/23 22:31:20.775
Jan 18 22:31:20.800: INFO: Waiting for pod pod-configmaps-f26c91b8-1869-4d2d-9c3b-47f328d52e05 to disappear
Jan 18 22:31:20.806: INFO: Pod pod-configmaps-f26c91b8-1869-4d2d-9c3b-47f328d52e05 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 22:31:20.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4976" for this suite. 01/18/23 22:31:20.814
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":228,"skipped":4160,"failed":0}
------------------------------
• [4.196 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:31:16.639
    Jan 18 22:31:16.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename configmap 01/18/23 22:31:16.64
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:16.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:16.7
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-4976/configmap-test-8149ecec-dd1b-4d2c-bc4c-1175d9d1dd86 01/18/23 22:31:16.724
    STEP: Creating a pod to test consume configMaps 01/18/23 22:31:16.73
    Jan 18 22:31:16.742: INFO: Waiting up to 5m0s for pod "pod-configmaps-f26c91b8-1869-4d2d-9c3b-47f328d52e05" in namespace "configmap-4976" to be "Succeeded or Failed"
    Jan 18 22:31:16.746: INFO: Pod "pod-configmaps-f26c91b8-1869-4d2d-9c3b-47f328d52e05": Phase="Pending", Reason="", readiness=false. Elapsed: 3.826597ms
    Jan 18 22:31:18.751: INFO: Pod "pod-configmaps-f26c91b8-1869-4d2d-9c3b-47f328d52e05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009419429s
    Jan 18 22:31:20.752: INFO: Pod "pod-configmaps-f26c91b8-1869-4d2d-9c3b-47f328d52e05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010278117s
    STEP: Saw pod success 01/18/23 22:31:20.752
    Jan 18 22:31:20.753: INFO: Pod "pod-configmaps-f26c91b8-1869-4d2d-9c3b-47f328d52e05" satisfied condition "Succeeded or Failed"
    Jan 18 22:31:20.760: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-f26c91b8-1869-4d2d-9c3b-47f328d52e05 container env-test: <nil>
    STEP: delete the pod 01/18/23 22:31:20.775
    Jan 18 22:31:20.800: INFO: Waiting for pod pod-configmaps-f26c91b8-1869-4d2d-9c3b-47f328d52e05 to disappear
    Jan 18 22:31:20.806: INFO: Pod pod-configmaps-f26c91b8-1869-4d2d-9c3b-47f328d52e05 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 22:31:20.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4976" for this suite. 01/18/23 22:31:20.814
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:31:20.839
Jan 18 22:31:20.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename replication-controller 01/18/23 22:31:20.841
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:20.868
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:20.875
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 01/18/23 22:31:20.882
STEP: When the matched label of one of its pods change 01/18/23 22:31:20.899
Jan 18 22:31:20.906: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 18 22:31:25.912: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 01/18/23 22:31:25.93
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 18 22:31:26.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1274" for this suite. 01/18/23 22:31:26.965
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":229,"skipped":4170,"failed":0}
------------------------------
• [SLOW TEST] [6.140 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:31:20.839
    Jan 18 22:31:20.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename replication-controller 01/18/23 22:31:20.841
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:20.868
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:20.875
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 01/18/23 22:31:20.882
    STEP: When the matched label of one of its pods change 01/18/23 22:31:20.899
    Jan 18 22:31:20.906: INFO: Pod name pod-release: Found 0 pods out of 1
    Jan 18 22:31:25.912: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/18/23 22:31:25.93
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 18 22:31:26.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-1274" for this suite. 01/18/23 22:31:26.965
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:31:26.983
Jan 18 22:31:26.984: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename services 01/18/23 22:31:26.987
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:27.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:27.02
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-694 01/18/23 22:31:27.031
STEP: creating service affinity-nodeport in namespace services-694 01/18/23 22:31:27.031
STEP: creating replication controller affinity-nodeport in namespace services-694 01/18/23 22:31:27.049
I0118 22:31:27.066334      19 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-694, replica count: 3
I0118 22:31:30.117407      19 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 22:31:30.137: INFO: Creating new exec pod
Jan 18 22:31:30.158: INFO: Waiting up to 5m0s for pod "execpod-affinityz8tvb" in namespace "services-694" to be "running"
Jan 18 22:31:30.170: INFO: Pod "execpod-affinityz8tvb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.338292ms
Jan 18 22:31:32.176: INFO: Pod "execpod-affinityz8tvb": Phase="Running", Reason="", readiness=true. Elapsed: 2.017981003s
Jan 18 22:31:32.176: INFO: Pod "execpod-affinityz8tvb" satisfied condition "running"
Jan 18 22:31:33.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-694 exec execpod-affinityz8tvb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jan 18 22:31:33.408: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport 80\n+ echo hostName\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan 18 22:31:33.408: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 22:31:33.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-694 exec execpod-affinityz8tvb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.17.224 80'
Jan 18 22:31:33.663: INFO: stderr: "+ echo+  hostNamenc\n -v -t -w 2 10.233.17.224 80\nConnection to 10.233.17.224 80 port [tcp/http] succeeded!\n"
Jan 18 22:31:33.663: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 22:31:33.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-694 exec execpod-affinityz8tvb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.168 32578'
Jan 18 22:31:33.895: INFO: stderr: "+ nc -v -t -w 2 192.168.101.168 32578\n+ echo hostName\nConnection to 192.168.101.168 32578 port [tcp/*] succeeded!\n"
Jan 18 22:31:33.895: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 22:31:33.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-694 exec execpod-affinityz8tvb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 32578'
Jan 18 22:31:34.136: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.216 32578\nConnection to 192.168.101.216 32578 port [tcp/*] succeeded!\n"
Jan 18 22:31:34.137: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 22:31:34.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-694 exec execpod-affinityz8tvb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.101.168:32578/ ; done'
Jan 18 22:31:34.491: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n"
Jan 18 22:31:34.492: INFO: stdout: "\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc"
Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
Jan 18 22:31:34.492: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-694, will wait for the garbage collector to delete the pods 01/18/23 22:31:34.508
Jan 18 22:31:34.590: INFO: Deleting ReplicationController affinity-nodeport took: 15.596562ms
Jan 18 22:31:34.691: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.606269ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 22:31:37.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-694" for this suite. 01/18/23 22:31:37.136
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":230,"skipped":4199,"failed":0}
------------------------------
• [SLOW TEST] [10.171 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:31:26.983
    Jan 18 22:31:26.984: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename services 01/18/23 22:31:26.987
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:27.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:27.02
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-694 01/18/23 22:31:27.031
    STEP: creating service affinity-nodeport in namespace services-694 01/18/23 22:31:27.031
    STEP: creating replication controller affinity-nodeport in namespace services-694 01/18/23 22:31:27.049
    I0118 22:31:27.066334      19 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-694, replica count: 3
    I0118 22:31:30.117407      19 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 22:31:30.137: INFO: Creating new exec pod
    Jan 18 22:31:30.158: INFO: Waiting up to 5m0s for pod "execpod-affinityz8tvb" in namespace "services-694" to be "running"
    Jan 18 22:31:30.170: INFO: Pod "execpod-affinityz8tvb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.338292ms
    Jan 18 22:31:32.176: INFO: Pod "execpod-affinityz8tvb": Phase="Running", Reason="", readiness=true. Elapsed: 2.017981003s
    Jan 18 22:31:32.176: INFO: Pod "execpod-affinityz8tvb" satisfied condition "running"
    Jan 18 22:31:33.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-694 exec execpod-affinityz8tvb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Jan 18 22:31:33.408: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport 80\n+ echo hostName\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jan 18 22:31:33.408: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 22:31:33.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-694 exec execpod-affinityz8tvb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.17.224 80'
    Jan 18 22:31:33.663: INFO: stderr: "+ echo+  hostNamenc\n -v -t -w 2 10.233.17.224 80\nConnection to 10.233.17.224 80 port [tcp/http] succeeded!\n"
    Jan 18 22:31:33.663: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 22:31:33.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-694 exec execpod-affinityz8tvb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.168 32578'
    Jan 18 22:31:33.895: INFO: stderr: "+ nc -v -t -w 2 192.168.101.168 32578\n+ echo hostName\nConnection to 192.168.101.168 32578 port [tcp/*] succeeded!\n"
    Jan 18 22:31:33.895: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 22:31:33.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-694 exec execpod-affinityz8tvb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 32578'
    Jan 18 22:31:34.136: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.216 32578\nConnection to 192.168.101.216 32578 port [tcp/*] succeeded!\n"
    Jan 18 22:31:34.137: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 22:31:34.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-694 exec execpod-affinityz8tvb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.101.168:32578/ ; done'
    Jan 18 22:31:34.491: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32578/\n"
    Jan 18 22:31:34.492: INFO: stdout: "\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc\naffinity-nodeport-9d6lc"
    Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
    Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
    Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
    Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
    Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
    Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
    Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
    Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
    Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
    Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
    Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
    Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
    Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
    Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
    Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
    Jan 18 22:31:34.492: INFO: Received response from host: affinity-nodeport-9d6lc
    Jan 18 22:31:34.492: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-694, will wait for the garbage collector to delete the pods 01/18/23 22:31:34.508
    Jan 18 22:31:34.590: INFO: Deleting ReplicationController affinity-nodeport took: 15.596562ms
    Jan 18 22:31:34.691: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.606269ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 22:31:37.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-694" for this suite. 01/18/23 22:31:37.136
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:31:37.16
Jan 18 22:31:37.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename emptydir 01/18/23 22:31:37.163
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:37.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:37.196
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/18/23 22:31:37.201
Jan 18 22:31:37.211: INFO: Waiting up to 5m0s for pod "pod-3405ae8d-8a16-4e5a-9613-e7a083b7c719" in namespace "emptydir-8524" to be "Succeeded or Failed"
Jan 18 22:31:37.218: INFO: Pod "pod-3405ae8d-8a16-4e5a-9613-e7a083b7c719": Phase="Pending", Reason="", readiness=false. Elapsed: 6.166312ms
Jan 18 22:31:39.226: INFO: Pod "pod-3405ae8d-8a16-4e5a-9613-e7a083b7c719": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014527495s
Jan 18 22:31:41.223: INFO: Pod "pod-3405ae8d-8a16-4e5a-9613-e7a083b7c719": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010923073s
STEP: Saw pod success 01/18/23 22:31:41.223
Jan 18 22:31:41.223: INFO: Pod "pod-3405ae8d-8a16-4e5a-9613-e7a083b7c719" satisfied condition "Succeeded or Failed"
Jan 18 22:31:41.228: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-3405ae8d-8a16-4e5a-9613-e7a083b7c719 container test-container: <nil>
STEP: delete the pod 01/18/23 22:31:41.237
Jan 18 22:31:41.252: INFO: Waiting for pod pod-3405ae8d-8a16-4e5a-9613-e7a083b7c719 to disappear
Jan 18 22:31:41.261: INFO: Pod pod-3405ae8d-8a16-4e5a-9613-e7a083b7c719 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 22:31:41.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8524" for this suite. 01/18/23 22:31:41.268
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":231,"skipped":4244,"failed":0}
------------------------------
• [4.115 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:31:37.16
    Jan 18 22:31:37.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename emptydir 01/18/23 22:31:37.163
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:37.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:37.196
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/18/23 22:31:37.201
    Jan 18 22:31:37.211: INFO: Waiting up to 5m0s for pod "pod-3405ae8d-8a16-4e5a-9613-e7a083b7c719" in namespace "emptydir-8524" to be "Succeeded or Failed"
    Jan 18 22:31:37.218: INFO: Pod "pod-3405ae8d-8a16-4e5a-9613-e7a083b7c719": Phase="Pending", Reason="", readiness=false. Elapsed: 6.166312ms
    Jan 18 22:31:39.226: INFO: Pod "pod-3405ae8d-8a16-4e5a-9613-e7a083b7c719": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014527495s
    Jan 18 22:31:41.223: INFO: Pod "pod-3405ae8d-8a16-4e5a-9613-e7a083b7c719": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010923073s
    STEP: Saw pod success 01/18/23 22:31:41.223
    Jan 18 22:31:41.223: INFO: Pod "pod-3405ae8d-8a16-4e5a-9613-e7a083b7c719" satisfied condition "Succeeded or Failed"
    Jan 18 22:31:41.228: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-3405ae8d-8a16-4e5a-9613-e7a083b7c719 container test-container: <nil>
    STEP: delete the pod 01/18/23 22:31:41.237
    Jan 18 22:31:41.252: INFO: Waiting for pod pod-3405ae8d-8a16-4e5a-9613-e7a083b7c719 to disappear
    Jan 18 22:31:41.261: INFO: Pod pod-3405ae8d-8a16-4e5a-9613-e7a083b7c719 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 22:31:41.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8524" for this suite. 01/18/23 22:31:41.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:31:41.278
Jan 18 22:31:41.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 22:31:41.281
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:41.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:41.315
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-fb79d39a-86dc-477d-a9d1-fe577c85dd1b 01/18/23 22:31:41.328
STEP: Creating configMap with name cm-test-opt-upd-94d80d54-3965-48f7-8163-fef800d1fa4f 01/18/23 22:31:41.334
STEP: Creating the pod 01/18/23 22:31:41.341
Jan 18 22:31:41.352: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-02234da4-f134-4870-afdb-688f6353eb27" in namespace "projected-4164" to be "running and ready"
Jan 18 22:31:41.358: INFO: Pod "pod-projected-configmaps-02234da4-f134-4870-afdb-688f6353eb27": Phase="Pending", Reason="", readiness=false. Elapsed: 5.531455ms
Jan 18 22:31:41.358: INFO: The phase of Pod pod-projected-configmaps-02234da4-f134-4870-afdb-688f6353eb27 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:31:43.380: INFO: Pod "pod-projected-configmaps-02234da4-f134-4870-afdb-688f6353eb27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027161603s
Jan 18 22:31:43.380: INFO: The phase of Pod pod-projected-configmaps-02234da4-f134-4870-afdb-688f6353eb27 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:31:45.365: INFO: Pod "pod-projected-configmaps-02234da4-f134-4870-afdb-688f6353eb27": Phase="Running", Reason="", readiness=true. Elapsed: 4.011685898s
Jan 18 22:31:45.365: INFO: The phase of Pod pod-projected-configmaps-02234da4-f134-4870-afdb-688f6353eb27 is Running (Ready = true)
Jan 18 22:31:45.365: INFO: Pod "pod-projected-configmaps-02234da4-f134-4870-afdb-688f6353eb27" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-fb79d39a-86dc-477d-a9d1-fe577c85dd1b 01/18/23 22:31:45.4
STEP: Updating configmap cm-test-opt-upd-94d80d54-3965-48f7-8163-fef800d1fa4f 01/18/23 22:31:45.407
STEP: Creating configMap with name cm-test-opt-create-0c4a004b-bc49-4d46-bf9b-80bbd89982ad 01/18/23 22:31:45.415
STEP: waiting to observe update in volume 01/18/23 22:31:45.421
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 18 22:33:01.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4164" for this suite. 01/18/23 22:33:01.889
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":232,"skipped":4265,"failed":0}
------------------------------
• [SLOW TEST] [80.619 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:31:41.278
    Jan 18 22:31:41.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 22:31:41.281
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:41.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:41.315
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-fb79d39a-86dc-477d-a9d1-fe577c85dd1b 01/18/23 22:31:41.328
    STEP: Creating configMap with name cm-test-opt-upd-94d80d54-3965-48f7-8163-fef800d1fa4f 01/18/23 22:31:41.334
    STEP: Creating the pod 01/18/23 22:31:41.341
    Jan 18 22:31:41.352: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-02234da4-f134-4870-afdb-688f6353eb27" in namespace "projected-4164" to be "running and ready"
    Jan 18 22:31:41.358: INFO: Pod "pod-projected-configmaps-02234da4-f134-4870-afdb-688f6353eb27": Phase="Pending", Reason="", readiness=false. Elapsed: 5.531455ms
    Jan 18 22:31:41.358: INFO: The phase of Pod pod-projected-configmaps-02234da4-f134-4870-afdb-688f6353eb27 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:31:43.380: INFO: Pod "pod-projected-configmaps-02234da4-f134-4870-afdb-688f6353eb27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027161603s
    Jan 18 22:31:43.380: INFO: The phase of Pod pod-projected-configmaps-02234da4-f134-4870-afdb-688f6353eb27 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:31:45.365: INFO: Pod "pod-projected-configmaps-02234da4-f134-4870-afdb-688f6353eb27": Phase="Running", Reason="", readiness=true. Elapsed: 4.011685898s
    Jan 18 22:31:45.365: INFO: The phase of Pod pod-projected-configmaps-02234da4-f134-4870-afdb-688f6353eb27 is Running (Ready = true)
    Jan 18 22:31:45.365: INFO: Pod "pod-projected-configmaps-02234da4-f134-4870-afdb-688f6353eb27" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-fb79d39a-86dc-477d-a9d1-fe577c85dd1b 01/18/23 22:31:45.4
    STEP: Updating configmap cm-test-opt-upd-94d80d54-3965-48f7-8163-fef800d1fa4f 01/18/23 22:31:45.407
    STEP: Creating configMap with name cm-test-opt-create-0c4a004b-bc49-4d46-bf9b-80bbd89982ad 01/18/23 22:31:45.415
    STEP: waiting to observe update in volume 01/18/23 22:31:45.421
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 18 22:33:01.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4164" for this suite. 01/18/23 22:33:01.889
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:33:01.9
Jan 18 22:33:01.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename emptydir 01/18/23 22:33:01.903
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:01.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:01.932
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 01/18/23 22:33:01.938
Jan 18 22:33:01.947: INFO: Waiting up to 5m0s for pod "pod-52a6d89e-4478-487f-93d3-caa0068c6435" in namespace "emptydir-3549" to be "Succeeded or Failed"
Jan 18 22:33:01.951: INFO: Pod "pod-52a6d89e-4478-487f-93d3-caa0068c6435": Phase="Pending", Reason="", readiness=false. Elapsed: 3.854275ms
Jan 18 22:33:03.961: INFO: Pod "pod-52a6d89e-4478-487f-93d3-caa0068c6435": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013471993s
Jan 18 22:33:05.956: INFO: Pod "pod-52a6d89e-4478-487f-93d3-caa0068c6435": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008347237s
STEP: Saw pod success 01/18/23 22:33:05.956
Jan 18 22:33:05.957: INFO: Pod "pod-52a6d89e-4478-487f-93d3-caa0068c6435" satisfied condition "Succeeded or Failed"
Jan 18 22:33:05.961: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-52a6d89e-4478-487f-93d3-caa0068c6435 container test-container: <nil>
STEP: delete the pod 01/18/23 22:33:05.973
Jan 18 22:33:05.991: INFO: Waiting for pod pod-52a6d89e-4478-487f-93d3-caa0068c6435 to disappear
Jan 18 22:33:05.998: INFO: Pod pod-52a6d89e-4478-487f-93d3-caa0068c6435 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 22:33:05.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3549" for this suite. 01/18/23 22:33:06.008
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":233,"skipped":4269,"failed":0}
------------------------------
• [4.127 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:33:01.9
    Jan 18 22:33:01.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename emptydir 01/18/23 22:33:01.903
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:01.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:01.932
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 01/18/23 22:33:01.938
    Jan 18 22:33:01.947: INFO: Waiting up to 5m0s for pod "pod-52a6d89e-4478-487f-93d3-caa0068c6435" in namespace "emptydir-3549" to be "Succeeded or Failed"
    Jan 18 22:33:01.951: INFO: Pod "pod-52a6d89e-4478-487f-93d3-caa0068c6435": Phase="Pending", Reason="", readiness=false. Elapsed: 3.854275ms
    Jan 18 22:33:03.961: INFO: Pod "pod-52a6d89e-4478-487f-93d3-caa0068c6435": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013471993s
    Jan 18 22:33:05.956: INFO: Pod "pod-52a6d89e-4478-487f-93d3-caa0068c6435": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008347237s
    STEP: Saw pod success 01/18/23 22:33:05.956
    Jan 18 22:33:05.957: INFO: Pod "pod-52a6d89e-4478-487f-93d3-caa0068c6435" satisfied condition "Succeeded or Failed"
    Jan 18 22:33:05.961: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-52a6d89e-4478-487f-93d3-caa0068c6435 container test-container: <nil>
    STEP: delete the pod 01/18/23 22:33:05.973
    Jan 18 22:33:05.991: INFO: Waiting for pod pod-52a6d89e-4478-487f-93d3-caa0068c6435 to disappear
    Jan 18 22:33:05.998: INFO: Pod pod-52a6d89e-4478-487f-93d3-caa0068c6435 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 22:33:05.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3549" for this suite. 01/18/23 22:33:06.008
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:33:06.03
Jan 18 22:33:06.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename services 01/18/23 22:33:06.034
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:06.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:06.07
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 22:33:06.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6511" for this suite. 01/18/23 22:33:06.088
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":234,"skipped":4272,"failed":0}
------------------------------
• [0.067 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:33:06.03
    Jan 18 22:33:06.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename services 01/18/23 22:33:06.034
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:06.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:06.07
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 22:33:06.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6511" for this suite. 01/18/23 22:33:06.088
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:33:06.098
Jan 18 22:33:06.099: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename var-expansion 01/18/23 22:33:06.101
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:06.118
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:06.122
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 01/18/23 22:33:06.127
Jan 18 22:33:06.138: INFO: Waiting up to 5m0s for pod "var-expansion-74ecd36e-3f42-47b0-8034-e3de523d20c5" in namespace "var-expansion-9263" to be "Succeeded or Failed"
Jan 18 22:33:06.147: INFO: Pod "var-expansion-74ecd36e-3f42-47b0-8034-e3de523d20c5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.405615ms
Jan 18 22:33:08.152: INFO: Pod "var-expansion-74ecd36e-3f42-47b0-8034-e3de523d20c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014039176s
Jan 18 22:33:10.155: INFO: Pod "var-expansion-74ecd36e-3f42-47b0-8034-e3de523d20c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016575309s
STEP: Saw pod success 01/18/23 22:33:10.155
Jan 18 22:33:10.156: INFO: Pod "var-expansion-74ecd36e-3f42-47b0-8034-e3de523d20c5" satisfied condition "Succeeded or Failed"
Jan 18 22:33:10.161: INFO: Trying to get logs from node v1-25-1-18760-w2 pod var-expansion-74ecd36e-3f42-47b0-8034-e3de523d20c5 container dapi-container: <nil>
STEP: delete the pod 01/18/23 22:33:10.17
Jan 18 22:33:10.186: INFO: Waiting for pod var-expansion-74ecd36e-3f42-47b0-8034-e3de523d20c5 to disappear
Jan 18 22:33:10.190: INFO: Pod var-expansion-74ecd36e-3f42-47b0-8034-e3de523d20c5 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 18 22:33:10.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9263" for this suite. 01/18/23 22:33:10.2
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":235,"skipped":4273,"failed":0}
------------------------------
• [4.118 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:33:06.098
    Jan 18 22:33:06.099: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename var-expansion 01/18/23 22:33:06.101
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:06.118
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:06.122
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 01/18/23 22:33:06.127
    Jan 18 22:33:06.138: INFO: Waiting up to 5m0s for pod "var-expansion-74ecd36e-3f42-47b0-8034-e3de523d20c5" in namespace "var-expansion-9263" to be "Succeeded or Failed"
    Jan 18 22:33:06.147: INFO: Pod "var-expansion-74ecd36e-3f42-47b0-8034-e3de523d20c5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.405615ms
    Jan 18 22:33:08.152: INFO: Pod "var-expansion-74ecd36e-3f42-47b0-8034-e3de523d20c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014039176s
    Jan 18 22:33:10.155: INFO: Pod "var-expansion-74ecd36e-3f42-47b0-8034-e3de523d20c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016575309s
    STEP: Saw pod success 01/18/23 22:33:10.155
    Jan 18 22:33:10.156: INFO: Pod "var-expansion-74ecd36e-3f42-47b0-8034-e3de523d20c5" satisfied condition "Succeeded or Failed"
    Jan 18 22:33:10.161: INFO: Trying to get logs from node v1-25-1-18760-w2 pod var-expansion-74ecd36e-3f42-47b0-8034-e3de523d20c5 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 22:33:10.17
    Jan 18 22:33:10.186: INFO: Waiting for pod var-expansion-74ecd36e-3f42-47b0-8034-e3de523d20c5 to disappear
    Jan 18 22:33:10.190: INFO: Pod var-expansion-74ecd36e-3f42-47b0-8034-e3de523d20c5 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 18 22:33:10.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-9263" for this suite. 01/18/23 22:33:10.2
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:33:10.222
Jan 18 22:33:10.222: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename configmap 01/18/23 22:33:10.225
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:10.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:10.271
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-10da4bf1-ec30-4825-afc5-8a179c71f437 01/18/23 22:33:10.278
STEP: Creating a pod to test consume configMaps 01/18/23 22:33:10.287
Jan 18 22:33:10.296: INFO: Waiting up to 5m0s for pod "pod-configmaps-a4da9fb8-a4b3-4eff-b64e-afa6f3d05db0" in namespace "configmap-1094" to be "Succeeded or Failed"
Jan 18 22:33:10.301: INFO: Pod "pod-configmaps-a4da9fb8-a4b3-4eff-b64e-afa6f3d05db0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.471909ms
Jan 18 22:33:12.307: INFO: Pod "pod-configmaps-a4da9fb8-a4b3-4eff-b64e-afa6f3d05db0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010495833s
Jan 18 22:33:14.310: INFO: Pod "pod-configmaps-a4da9fb8-a4b3-4eff-b64e-afa6f3d05db0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012913423s
STEP: Saw pod success 01/18/23 22:33:14.31
Jan 18 22:33:14.310: INFO: Pod "pod-configmaps-a4da9fb8-a4b3-4eff-b64e-afa6f3d05db0" satisfied condition "Succeeded or Failed"
Jan 18 22:33:14.318: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-a4da9fb8-a4b3-4eff-b64e-afa6f3d05db0 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 22:33:14.329
Jan 18 22:33:14.349: INFO: Waiting for pod pod-configmaps-a4da9fb8-a4b3-4eff-b64e-afa6f3d05db0 to disappear
Jan 18 22:33:14.356: INFO: Pod pod-configmaps-a4da9fb8-a4b3-4eff-b64e-afa6f3d05db0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 22:33:14.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1094" for this suite. 01/18/23 22:33:14.365
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":236,"skipped":4274,"failed":0}
------------------------------
• [4.155 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:33:10.222
    Jan 18 22:33:10.222: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename configmap 01/18/23 22:33:10.225
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:10.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:10.271
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-10da4bf1-ec30-4825-afc5-8a179c71f437 01/18/23 22:33:10.278
    STEP: Creating a pod to test consume configMaps 01/18/23 22:33:10.287
    Jan 18 22:33:10.296: INFO: Waiting up to 5m0s for pod "pod-configmaps-a4da9fb8-a4b3-4eff-b64e-afa6f3d05db0" in namespace "configmap-1094" to be "Succeeded or Failed"
    Jan 18 22:33:10.301: INFO: Pod "pod-configmaps-a4da9fb8-a4b3-4eff-b64e-afa6f3d05db0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.471909ms
    Jan 18 22:33:12.307: INFO: Pod "pod-configmaps-a4da9fb8-a4b3-4eff-b64e-afa6f3d05db0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010495833s
    Jan 18 22:33:14.310: INFO: Pod "pod-configmaps-a4da9fb8-a4b3-4eff-b64e-afa6f3d05db0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012913423s
    STEP: Saw pod success 01/18/23 22:33:14.31
    Jan 18 22:33:14.310: INFO: Pod "pod-configmaps-a4da9fb8-a4b3-4eff-b64e-afa6f3d05db0" satisfied condition "Succeeded or Failed"
    Jan 18 22:33:14.318: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-a4da9fb8-a4b3-4eff-b64e-afa6f3d05db0 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 22:33:14.329
    Jan 18 22:33:14.349: INFO: Waiting for pod pod-configmaps-a4da9fb8-a4b3-4eff-b64e-afa6f3d05db0 to disappear
    Jan 18 22:33:14.356: INFO: Pod pod-configmaps-a4da9fb8-a4b3-4eff-b64e-afa6f3d05db0 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 22:33:14.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1094" for this suite. 01/18/23 22:33:14.365
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:33:14.378
Jan 18 22:33:14.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 22:33:14.38
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:14.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:14.422
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/18/23 22:33:14.43
Jan 18 22:33:14.440: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7391" to be "running and ready"
Jan 18 22:33:14.445: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.101453ms
Jan 18 22:33:14.445: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:33:16.451: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010811373s
Jan 18 22:33:16.451: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 18 22:33:16.451: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 01/18/23 22:33:16.457
Jan 18 22:33:16.470: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-7391" to be "running and ready"
Jan 18 22:33:16.480: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.807424ms
Jan 18 22:33:16.480: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:33:18.485: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.015173073s
Jan 18 22:33:18.486: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jan 18 22:33:18.486: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/18/23 22:33:18.489
Jan 18 22:33:18.496: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 18 22:33:18.510: INFO: Pod pod-with-prestop-http-hook still exists
Jan 18 22:33:20.510: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 18 22:33:20.515: INFO: Pod pod-with-prestop-http-hook still exists
Jan 18 22:33:22.510: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 18 22:33:22.516: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 01/18/23 22:33:22.516
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 18 22:33:22.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7391" for this suite. 01/18/23 22:33:22.545
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":237,"skipped":4275,"failed":0}
------------------------------
• [SLOW TEST] [8.174 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:33:14.378
    Jan 18 22:33:14.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 22:33:14.38
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:14.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:14.422
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/18/23 22:33:14.43
    Jan 18 22:33:14.440: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7391" to be "running and ready"
    Jan 18 22:33:14.445: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.101453ms
    Jan 18 22:33:14.445: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:33:16.451: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010811373s
    Jan 18 22:33:16.451: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 18 22:33:16.451: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 01/18/23 22:33:16.457
    Jan 18 22:33:16.470: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-7391" to be "running and ready"
    Jan 18 22:33:16.480: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.807424ms
    Jan 18 22:33:16.480: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:33:18.485: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.015173073s
    Jan 18 22:33:18.486: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jan 18 22:33:18.486: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/18/23 22:33:18.489
    Jan 18 22:33:18.496: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 18 22:33:18.510: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 18 22:33:20.510: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 18 22:33:20.515: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 18 22:33:22.510: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 18 22:33:22.516: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 01/18/23 22:33:22.516
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 18 22:33:22.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-7391" for this suite. 01/18/23 22:33:22.545
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:33:22.558
Jan 18 22:33:22.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename taint-multiple-pods 01/18/23 22:33:22.564
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:22.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:22.596
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Jan 18 22:33:22.601: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 22:34:22.655: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Jan 18 22:34:22.660: INFO: Starting informer...
STEP: Starting pods... 01/18/23 22:34:22.661
Jan 18 22:34:22.891: INFO: Pod1 is running on v1-25-1-18760-w2. Tainting Node
Jan 18 22:34:23.107: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-6745" to be "running"
Jan 18 22:34:23.114: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.514255ms
Jan 18 22:34:25.118: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011392691s
Jan 18 22:34:25.118: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jan 18 22:34:25.118: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-6745" to be "running"
Jan 18 22:34:25.126: INFO: Pod "taint-eviction-b2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.049248ms
Jan 18 22:34:27.131: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.012905864s
Jan 18 22:34:27.131: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jan 18 22:34:27.132: INFO: Pod2 is running on v1-25-1-18760-w2. Tainting Node
STEP: Trying to apply a taint on the Node 01/18/23 22:34:27.132
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 22:34:27.164
STEP: Waiting for Pod1 and Pod2 to be deleted 01/18/23 22:34:27.175
Jan 18 22:34:33.190: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan 18 22:34:53.285: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 22:34:53.307
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Jan 18 22:34:53.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-6745" for this suite. 01/18/23 22:34:53.326
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":238,"skipped":4310,"failed":0}
------------------------------
• [SLOW TEST] [90.776 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:33:22.558
    Jan 18 22:33:22.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename taint-multiple-pods 01/18/23 22:33:22.564
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:22.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:22.596
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Jan 18 22:33:22.601: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 22:34:22.655: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Jan 18 22:34:22.660: INFO: Starting informer...
    STEP: Starting pods... 01/18/23 22:34:22.661
    Jan 18 22:34:22.891: INFO: Pod1 is running on v1-25-1-18760-w2. Tainting Node
    Jan 18 22:34:23.107: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-6745" to be "running"
    Jan 18 22:34:23.114: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.514255ms
    Jan 18 22:34:25.118: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011392691s
    Jan 18 22:34:25.118: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jan 18 22:34:25.118: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-6745" to be "running"
    Jan 18 22:34:25.126: INFO: Pod "taint-eviction-b2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.049248ms
    Jan 18 22:34:27.131: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.012905864s
    Jan 18 22:34:27.131: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jan 18 22:34:27.132: INFO: Pod2 is running on v1-25-1-18760-w2. Tainting Node
    STEP: Trying to apply a taint on the Node 01/18/23 22:34:27.132
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 22:34:27.164
    STEP: Waiting for Pod1 and Pod2 to be deleted 01/18/23 22:34:27.175
    Jan 18 22:34:33.190: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jan 18 22:34:53.285: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 22:34:53.307
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 22:34:53.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-6745" for this suite. 01/18/23 22:34:53.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:34:53.345
Jan 18 22:34:53.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 22:34:53.348
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:34:53.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:34:53.373
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 01/18/23 22:34:53.378
Jan 18 22:34:53.386: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7a49bd7f-094e-46c1-9f21-9d2117fffc99" in namespace "projected-6356" to be "Succeeded or Failed"
Jan 18 22:34:53.401: INFO: Pod "downwardapi-volume-7a49bd7f-094e-46c1-9f21-9d2117fffc99": Phase="Pending", Reason="", readiness=false. Elapsed: 14.57625ms
Jan 18 22:34:55.409: INFO: Pod "downwardapi-volume-7a49bd7f-094e-46c1-9f21-9d2117fffc99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022070826s
Jan 18 22:34:57.416: INFO: Pod "downwardapi-volume-7a49bd7f-094e-46c1-9f21-9d2117fffc99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029986329s
STEP: Saw pod success 01/18/23 22:34:57.422
Jan 18 22:34:57.423: INFO: Pod "downwardapi-volume-7a49bd7f-094e-46c1-9f21-9d2117fffc99" satisfied condition "Succeeded or Failed"
Jan 18 22:34:57.428: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-7a49bd7f-094e-46c1-9f21-9d2117fffc99 container client-container: <nil>
STEP: delete the pod 01/18/23 22:34:57.451
Jan 18 22:34:57.469: INFO: Waiting for pod downwardapi-volume-7a49bd7f-094e-46c1-9f21-9d2117fffc99 to disappear
Jan 18 22:34:57.474: INFO: Pod downwardapi-volume-7a49bd7f-094e-46c1-9f21-9d2117fffc99 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 22:34:57.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6356" for this suite. 01/18/23 22:34:57.487
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":239,"skipped":4340,"failed":0}
------------------------------
• [4.151 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:34:53.345
    Jan 18 22:34:53.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 22:34:53.348
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:34:53.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:34:53.373
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 01/18/23 22:34:53.378
    Jan 18 22:34:53.386: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7a49bd7f-094e-46c1-9f21-9d2117fffc99" in namespace "projected-6356" to be "Succeeded or Failed"
    Jan 18 22:34:53.401: INFO: Pod "downwardapi-volume-7a49bd7f-094e-46c1-9f21-9d2117fffc99": Phase="Pending", Reason="", readiness=false. Elapsed: 14.57625ms
    Jan 18 22:34:55.409: INFO: Pod "downwardapi-volume-7a49bd7f-094e-46c1-9f21-9d2117fffc99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022070826s
    Jan 18 22:34:57.416: INFO: Pod "downwardapi-volume-7a49bd7f-094e-46c1-9f21-9d2117fffc99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029986329s
    STEP: Saw pod success 01/18/23 22:34:57.422
    Jan 18 22:34:57.423: INFO: Pod "downwardapi-volume-7a49bd7f-094e-46c1-9f21-9d2117fffc99" satisfied condition "Succeeded or Failed"
    Jan 18 22:34:57.428: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-7a49bd7f-094e-46c1-9f21-9d2117fffc99 container client-container: <nil>
    STEP: delete the pod 01/18/23 22:34:57.451
    Jan 18 22:34:57.469: INFO: Waiting for pod downwardapi-volume-7a49bd7f-094e-46c1-9f21-9d2117fffc99 to disappear
    Jan 18 22:34:57.474: INFO: Pod downwardapi-volume-7a49bd7f-094e-46c1-9f21-9d2117fffc99 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 22:34:57.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6356" for this suite. 01/18/23 22:34:57.487
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:34:57.503
Jan 18 22:34:57.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename watch 01/18/23 22:34:57.505
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:34:57.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:34:57.534
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 01/18/23 22:34:57.54
STEP: creating a new configmap 01/18/23 22:34:57.543
STEP: modifying the configmap once 01/18/23 22:34:57.549
STEP: changing the label value of the configmap 01/18/23 22:34:57.561
STEP: Expecting to observe a delete notification for the watched object 01/18/23 22:34:57.596
Jan 18 22:34:57.597: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4695  36a84a1f-f565-4fe2-acf0-ba6573cb83b6 163161 0 2023-01-18 22:34:57 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 22:34:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 22:34:57.597: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4695  36a84a1f-f565-4fe2-acf0-ba6573cb83b6 163162 0 2023-01-18 22:34:57 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 22:34:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 22:34:57.598: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4695  36a84a1f-f565-4fe2-acf0-ba6573cb83b6 163164 0 2023-01-18 22:34:57 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 22:34:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 01/18/23 22:34:57.598
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/18/23 22:34:57.61
STEP: changing the label value of the configmap back 01/18/23 22:35:07.615
STEP: modifying the configmap a third time 01/18/23 22:35:07.635
STEP: deleting the configmap 01/18/23 22:35:07.646
STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/18/23 22:35:07.654
Jan 18 22:35:07.655: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4695  36a84a1f-f565-4fe2-acf0-ba6573cb83b6 163231 0 2023-01-18 22:34:57 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 22:35:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 22:35:07.655: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4695  36a84a1f-f565-4fe2-acf0-ba6573cb83b6 163232 0 2023-01-18 22:34:57 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 22:35:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 22:35:07.656: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4695  36a84a1f-f565-4fe2-acf0-ba6573cb83b6 163234 0 2023-01-18 22:34:57 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 22:35:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 18 22:35:07.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4695" for this suite. 01/18/23 22:35:07.668
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":240,"skipped":4374,"failed":0}
------------------------------
• [SLOW TEST] [10.173 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:34:57.503
    Jan 18 22:34:57.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename watch 01/18/23 22:34:57.505
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:34:57.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:34:57.534
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 01/18/23 22:34:57.54
    STEP: creating a new configmap 01/18/23 22:34:57.543
    STEP: modifying the configmap once 01/18/23 22:34:57.549
    STEP: changing the label value of the configmap 01/18/23 22:34:57.561
    STEP: Expecting to observe a delete notification for the watched object 01/18/23 22:34:57.596
    Jan 18 22:34:57.597: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4695  36a84a1f-f565-4fe2-acf0-ba6573cb83b6 163161 0 2023-01-18 22:34:57 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 22:34:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 22:34:57.597: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4695  36a84a1f-f565-4fe2-acf0-ba6573cb83b6 163162 0 2023-01-18 22:34:57 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 22:34:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 22:34:57.598: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4695  36a84a1f-f565-4fe2-acf0-ba6573cb83b6 163164 0 2023-01-18 22:34:57 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 22:34:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 01/18/23 22:34:57.598
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/18/23 22:34:57.61
    STEP: changing the label value of the configmap back 01/18/23 22:35:07.615
    STEP: modifying the configmap a third time 01/18/23 22:35:07.635
    STEP: deleting the configmap 01/18/23 22:35:07.646
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/18/23 22:35:07.654
    Jan 18 22:35:07.655: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4695  36a84a1f-f565-4fe2-acf0-ba6573cb83b6 163231 0 2023-01-18 22:34:57 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 22:35:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 22:35:07.655: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4695  36a84a1f-f565-4fe2-acf0-ba6573cb83b6 163232 0 2023-01-18 22:34:57 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 22:35:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 22:35:07.656: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4695  36a84a1f-f565-4fe2-acf0-ba6573cb83b6 163234 0 2023-01-18 22:34:57 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 22:35:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 18 22:35:07.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-4695" for this suite. 01/18/23 22:35:07.668
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:35:07.679
Jan 18 22:35:07.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename resourcequota 01/18/23 22:35:07.682
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:35:07.706
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:35:07.716
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 01/18/23 22:35:07.724
STEP: Creating a ResourceQuota 01/18/23 22:35:12.729
STEP: Ensuring resource quota status is calculated 01/18/23 22:35:12.738
STEP: Creating a ReplicationController 01/18/23 22:35:14.745
STEP: Ensuring resource quota status captures replication controller creation 01/18/23 22:35:14.768
STEP: Deleting a ReplicationController 01/18/23 22:35:16.775
STEP: Ensuring resource quota status released usage 01/18/23 22:35:16.785
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 22:35:18.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3936" for this suite. 01/18/23 22:35:18.797
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":241,"skipped":4389,"failed":0}
------------------------------
• [SLOW TEST] [11.127 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:35:07.679
    Jan 18 22:35:07.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename resourcequota 01/18/23 22:35:07.682
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:35:07.706
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:35:07.716
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 01/18/23 22:35:07.724
    STEP: Creating a ResourceQuota 01/18/23 22:35:12.729
    STEP: Ensuring resource quota status is calculated 01/18/23 22:35:12.738
    STEP: Creating a ReplicationController 01/18/23 22:35:14.745
    STEP: Ensuring resource quota status captures replication controller creation 01/18/23 22:35:14.768
    STEP: Deleting a ReplicationController 01/18/23 22:35:16.775
    STEP: Ensuring resource quota status released usage 01/18/23 22:35:16.785
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 22:35:18.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3936" for this suite. 01/18/23 22:35:18.797
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:35:18.811
Jan 18 22:35:18.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename container-probe 01/18/23 22:35:18.814
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:35:18.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:35:18.841
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-39596ddb-0158-4efc-ad03-b38a8b7c152e in namespace container-probe-5019 01/18/23 22:35:18.848
Jan 18 22:35:18.865: INFO: Waiting up to 5m0s for pod "busybox-39596ddb-0158-4efc-ad03-b38a8b7c152e" in namespace "container-probe-5019" to be "not pending"
Jan 18 22:35:18.873: INFO: Pod "busybox-39596ddb-0158-4efc-ad03-b38a8b7c152e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.497196ms
Jan 18 22:35:20.879: INFO: Pod "busybox-39596ddb-0158-4efc-ad03-b38a8b7c152e": Phase="Running", Reason="", readiness=true. Elapsed: 2.013775053s
Jan 18 22:35:20.879: INFO: Pod "busybox-39596ddb-0158-4efc-ad03-b38a8b7c152e" satisfied condition "not pending"
Jan 18 22:35:20.879: INFO: Started pod busybox-39596ddb-0158-4efc-ad03-b38a8b7c152e in namespace container-probe-5019
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 22:35:20.879
Jan 18 22:35:20.885: INFO: Initial restart count of pod busybox-39596ddb-0158-4efc-ad03-b38a8b7c152e is 0
Jan 18 22:36:11.085: INFO: Restart count of pod container-probe-5019/busybox-39596ddb-0158-4efc-ad03-b38a8b7c152e is now 1 (50.199920196s elapsed)
STEP: deleting the pod 01/18/23 22:36:11.085
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 18 22:36:11.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5019" for this suite. 01/18/23 22:36:11.112
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":242,"skipped":4403,"failed":0}
------------------------------
• [SLOW TEST] [52.311 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:35:18.811
    Jan 18 22:35:18.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename container-probe 01/18/23 22:35:18.814
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:35:18.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:35:18.841
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-39596ddb-0158-4efc-ad03-b38a8b7c152e in namespace container-probe-5019 01/18/23 22:35:18.848
    Jan 18 22:35:18.865: INFO: Waiting up to 5m0s for pod "busybox-39596ddb-0158-4efc-ad03-b38a8b7c152e" in namespace "container-probe-5019" to be "not pending"
    Jan 18 22:35:18.873: INFO: Pod "busybox-39596ddb-0158-4efc-ad03-b38a8b7c152e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.497196ms
    Jan 18 22:35:20.879: INFO: Pod "busybox-39596ddb-0158-4efc-ad03-b38a8b7c152e": Phase="Running", Reason="", readiness=true. Elapsed: 2.013775053s
    Jan 18 22:35:20.879: INFO: Pod "busybox-39596ddb-0158-4efc-ad03-b38a8b7c152e" satisfied condition "not pending"
    Jan 18 22:35:20.879: INFO: Started pod busybox-39596ddb-0158-4efc-ad03-b38a8b7c152e in namespace container-probe-5019
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 22:35:20.879
    Jan 18 22:35:20.885: INFO: Initial restart count of pod busybox-39596ddb-0158-4efc-ad03-b38a8b7c152e is 0
    Jan 18 22:36:11.085: INFO: Restart count of pod container-probe-5019/busybox-39596ddb-0158-4efc-ad03-b38a8b7c152e is now 1 (50.199920196s elapsed)
    STEP: deleting the pod 01/18/23 22:36:11.085
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 18 22:36:11.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5019" for this suite. 01/18/23 22:36:11.112
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:36:11.132
Jan 18 22:36:11.133: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename downward-api 01/18/23 22:36:11.134
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:36:11.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:36:11.161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 01/18/23 22:36:11.171
Jan 18 22:36:11.202: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2203db51-1f87-401e-b2f5-4a6f4dfb58b8" in namespace "downward-api-9410" to be "Succeeded or Failed"
Jan 18 22:36:11.211: INFO: Pod "downwardapi-volume-2203db51-1f87-401e-b2f5-4a6f4dfb58b8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.070237ms
Jan 18 22:36:13.218: INFO: Pod "downwardapi-volume-2203db51-1f87-401e-b2f5-4a6f4dfb58b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015383931s
Jan 18 22:36:15.217: INFO: Pod "downwardapi-volume-2203db51-1f87-401e-b2f5-4a6f4dfb58b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015025576s
STEP: Saw pod success 01/18/23 22:36:15.217
Jan 18 22:36:15.218: INFO: Pod "downwardapi-volume-2203db51-1f87-401e-b2f5-4a6f4dfb58b8" satisfied condition "Succeeded or Failed"
Jan 18 22:36:15.222: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-2203db51-1f87-401e-b2f5-4a6f4dfb58b8 container client-container: <nil>
STEP: delete the pod 01/18/23 22:36:15.236
Jan 18 22:36:15.263: INFO: Waiting for pod downwardapi-volume-2203db51-1f87-401e-b2f5-4a6f4dfb58b8 to disappear
Jan 18 22:36:15.267: INFO: Pod downwardapi-volume-2203db51-1f87-401e-b2f5-4a6f4dfb58b8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 22:36:15.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9410" for this suite. 01/18/23 22:36:15.275
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":243,"skipped":4418,"failed":0}
------------------------------
• [4.150 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:36:11.132
    Jan 18 22:36:11.133: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename downward-api 01/18/23 22:36:11.134
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:36:11.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:36:11.161
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 01/18/23 22:36:11.171
    Jan 18 22:36:11.202: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2203db51-1f87-401e-b2f5-4a6f4dfb58b8" in namespace "downward-api-9410" to be "Succeeded or Failed"
    Jan 18 22:36:11.211: INFO: Pod "downwardapi-volume-2203db51-1f87-401e-b2f5-4a6f4dfb58b8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.070237ms
    Jan 18 22:36:13.218: INFO: Pod "downwardapi-volume-2203db51-1f87-401e-b2f5-4a6f4dfb58b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015383931s
    Jan 18 22:36:15.217: INFO: Pod "downwardapi-volume-2203db51-1f87-401e-b2f5-4a6f4dfb58b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015025576s
    STEP: Saw pod success 01/18/23 22:36:15.217
    Jan 18 22:36:15.218: INFO: Pod "downwardapi-volume-2203db51-1f87-401e-b2f5-4a6f4dfb58b8" satisfied condition "Succeeded or Failed"
    Jan 18 22:36:15.222: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-2203db51-1f87-401e-b2f5-4a6f4dfb58b8 container client-container: <nil>
    STEP: delete the pod 01/18/23 22:36:15.236
    Jan 18 22:36:15.263: INFO: Waiting for pod downwardapi-volume-2203db51-1f87-401e-b2f5-4a6f4dfb58b8 to disappear
    Jan 18 22:36:15.267: INFO: Pod downwardapi-volume-2203db51-1f87-401e-b2f5-4a6f4dfb58b8 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 22:36:15.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9410" for this suite. 01/18/23 22:36:15.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:36:15.285
Jan 18 22:36:15.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename gc 01/18/23 22:36:15.287
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:36:15.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:36:15.319
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 01/18/23 22:36:15.33
STEP: delete the rc 01/18/23 22:36:20.37
STEP: wait for the rc to be deleted 01/18/23 22:36:20.401
Jan 18 22:36:21.437: INFO: 80 pods remaining
Jan 18 22:36:21.437: INFO: 80 pods has nil DeletionTimestamp
Jan 18 22:36:21.437: INFO: 
Jan 18 22:36:22.432: INFO: 74 pods remaining
Jan 18 22:36:22.432: INFO: 73 pods has nil DeletionTimestamp
Jan 18 22:36:22.432: INFO: 
Jan 18 22:36:23.428: INFO: 60 pods remaining
Jan 18 22:36:23.428: INFO: 60 pods has nil DeletionTimestamp
Jan 18 22:36:23.428: INFO: 
Jan 18 22:36:24.419: INFO: 40 pods remaining
Jan 18 22:36:24.419: INFO: 40 pods has nil DeletionTimestamp
Jan 18 22:36:24.419: INFO: 
Jan 18 22:36:25.456: INFO: 33 pods remaining
Jan 18 22:36:25.456: INFO: 33 pods has nil DeletionTimestamp
Jan 18 22:36:25.456: INFO: 
Jan 18 22:36:26.419: INFO: 20 pods remaining
Jan 18 22:36:26.419: INFO: 20 pods has nil DeletionTimestamp
Jan 18 22:36:26.419: INFO: 
Jan 18 22:36:27.471: INFO: 0 pods remaining
Jan 18 22:36:27.471: INFO: 0 pods has nil DeletionTimestamp
Jan 18 22:36:27.471: INFO: 
STEP: Gathering metrics 01/18/23 22:36:28.448
Jan 18 22:36:28.565: INFO: Waiting up to 5m0s for pod "kube-controller-manager-v1-25-1-18760-m" in namespace "kube-system" to be "running and ready"
Jan 18 22:36:28.578: INFO: Pod "kube-controller-manager-v1-25-1-18760-m": Phase="Running", Reason="", readiness=true. Elapsed: 12.880773ms
Jan 18 22:36:28.578: INFO: The phase of Pod kube-controller-manager-v1-25-1-18760-m is Running (Ready = true)
Jan 18 22:36:28.578: INFO: Pod "kube-controller-manager-v1-25-1-18760-m" satisfied condition "running and ready"
Jan 18 22:36:28.856: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 18 22:36:28.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1530" for this suite. 01/18/23 22:36:28.898
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":244,"skipped":4423,"failed":0}
------------------------------
• [SLOW TEST] [13.637 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:36:15.285
    Jan 18 22:36:15.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename gc 01/18/23 22:36:15.287
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:36:15.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:36:15.319
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 01/18/23 22:36:15.33
    STEP: delete the rc 01/18/23 22:36:20.37
    STEP: wait for the rc to be deleted 01/18/23 22:36:20.401
    Jan 18 22:36:21.437: INFO: 80 pods remaining
    Jan 18 22:36:21.437: INFO: 80 pods has nil DeletionTimestamp
    Jan 18 22:36:21.437: INFO: 
    Jan 18 22:36:22.432: INFO: 74 pods remaining
    Jan 18 22:36:22.432: INFO: 73 pods has nil DeletionTimestamp
    Jan 18 22:36:22.432: INFO: 
    Jan 18 22:36:23.428: INFO: 60 pods remaining
    Jan 18 22:36:23.428: INFO: 60 pods has nil DeletionTimestamp
    Jan 18 22:36:23.428: INFO: 
    Jan 18 22:36:24.419: INFO: 40 pods remaining
    Jan 18 22:36:24.419: INFO: 40 pods has nil DeletionTimestamp
    Jan 18 22:36:24.419: INFO: 
    Jan 18 22:36:25.456: INFO: 33 pods remaining
    Jan 18 22:36:25.456: INFO: 33 pods has nil DeletionTimestamp
    Jan 18 22:36:25.456: INFO: 
    Jan 18 22:36:26.419: INFO: 20 pods remaining
    Jan 18 22:36:26.419: INFO: 20 pods has nil DeletionTimestamp
    Jan 18 22:36:26.419: INFO: 
    Jan 18 22:36:27.471: INFO: 0 pods remaining
    Jan 18 22:36:27.471: INFO: 0 pods has nil DeletionTimestamp
    Jan 18 22:36:27.471: INFO: 
    STEP: Gathering metrics 01/18/23 22:36:28.448
    Jan 18 22:36:28.565: INFO: Waiting up to 5m0s for pod "kube-controller-manager-v1-25-1-18760-m" in namespace "kube-system" to be "running and ready"
    Jan 18 22:36:28.578: INFO: Pod "kube-controller-manager-v1-25-1-18760-m": Phase="Running", Reason="", readiness=true. Elapsed: 12.880773ms
    Jan 18 22:36:28.578: INFO: The phase of Pod kube-controller-manager-v1-25-1-18760-m is Running (Ready = true)
    Jan 18 22:36:28.578: INFO: Pod "kube-controller-manager-v1-25-1-18760-m" satisfied condition "running and ready"
    Jan 18 22:36:28.856: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 18 22:36:28.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1530" for this suite. 01/18/23 22:36:28.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:36:28.934
Jan 18 22:36:28.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 22:36:28.963
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:36:29.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:36:29.029
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/18/23 22:36:29.044
Jan 18 22:36:29.054: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9784" to be "running and ready"
Jan 18 22:36:29.068: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 13.801814ms
Jan 18 22:36:29.068: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:36:31.075: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020106763s
Jan 18 22:36:31.075: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:36:33.075: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020933929s
Jan 18 22:36:33.075: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:36:35.083: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028334673s
Jan 18 22:36:35.083: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:36:37.091: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.036084491s
Jan 18 22:36:37.091: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:36:39.075: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 10.020042422s
Jan 18 22:36:39.075: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:36:41.074: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 12.019962273s
Jan 18 22:36:41.075: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:36:43.074: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 14.019822678s
Jan 18 22:36:43.074: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 18 22:36:43.075: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 01/18/23 22:36:43.081
Jan 18 22:36:43.092: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-9784" to be "running and ready"
Jan 18 22:36:43.100: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 7.731698ms
Jan 18 22:36:43.101: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:36:45.117: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.024265279s
Jan 18 22:36:45.118: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jan 18 22:36:45.118: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/18/23 22:36:45.122
STEP: delete the pod with lifecycle hook 01/18/23 22:36:45.133
Jan 18 22:36:45.143: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 18 22:36:45.149: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 18 22:36:47.149: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 18 22:36:47.154: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 18 22:36:49.150: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 18 22:36:49.160: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 18 22:36:49.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9784" for this suite. 01/18/23 22:36:49.181
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":245,"skipped":4450,"failed":0}
------------------------------
• [SLOW TEST] [20.264 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:36:28.934
    Jan 18 22:36:28.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 22:36:28.963
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:36:29.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:36:29.029
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/18/23 22:36:29.044
    Jan 18 22:36:29.054: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9784" to be "running and ready"
    Jan 18 22:36:29.068: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 13.801814ms
    Jan 18 22:36:29.068: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:36:31.075: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020106763s
    Jan 18 22:36:31.075: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:36:33.075: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020933929s
    Jan 18 22:36:33.075: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:36:35.083: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028334673s
    Jan 18 22:36:35.083: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:36:37.091: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.036084491s
    Jan 18 22:36:37.091: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:36:39.075: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 10.020042422s
    Jan 18 22:36:39.075: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:36:41.074: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 12.019962273s
    Jan 18 22:36:41.075: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:36:43.074: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 14.019822678s
    Jan 18 22:36:43.074: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 18 22:36:43.075: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 01/18/23 22:36:43.081
    Jan 18 22:36:43.092: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-9784" to be "running and ready"
    Jan 18 22:36:43.100: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 7.731698ms
    Jan 18 22:36:43.101: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:36:45.117: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.024265279s
    Jan 18 22:36:45.118: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jan 18 22:36:45.118: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/18/23 22:36:45.122
    STEP: delete the pod with lifecycle hook 01/18/23 22:36:45.133
    Jan 18 22:36:45.143: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 18 22:36:45.149: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 18 22:36:47.149: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 18 22:36:47.154: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 18 22:36:49.150: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 18 22:36:49.160: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 18 22:36:49.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-9784" for this suite. 01/18/23 22:36:49.181
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:36:49.207
Jan 18 22:36:49.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename podtemplate 01/18/23 22:36:49.21
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:36:49.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:36:49.29
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan 18 22:36:49.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9103" for this suite. 01/18/23 22:36:49.348
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":246,"skipped":4456,"failed":0}
------------------------------
• [0.149 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:36:49.207
    Jan 18 22:36:49.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename podtemplate 01/18/23 22:36:49.21
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:36:49.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:36:49.29
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan 18 22:36:49.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-9103" for this suite. 01/18/23 22:36:49.348
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:36:49.359
Jan 18 22:36:49.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 22:36:49.361
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:36:49.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:36:49.392
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-492c8ee8-dd3e-42a6-bd9a-dc66e093ae32 01/18/23 22:36:49.397
STEP: Creating a pod to test consume secrets 01/18/23 22:36:49.404
Jan 18 22:36:49.426: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-28aba649-8ac8-4dbb-a620-908c5d1d1fb6" in namespace "projected-4537" to be "Succeeded or Failed"
Jan 18 22:36:49.435: INFO: Pod "pod-projected-secrets-28aba649-8ac8-4dbb-a620-908c5d1d1fb6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.48721ms
Jan 18 22:36:51.442: INFO: Pod "pod-projected-secrets-28aba649-8ac8-4dbb-a620-908c5d1d1fb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015008434s
Jan 18 22:36:53.441: INFO: Pod "pod-projected-secrets-28aba649-8ac8-4dbb-a620-908c5d1d1fb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013921533s
STEP: Saw pod success 01/18/23 22:36:53.441
Jan 18 22:36:53.441: INFO: Pod "pod-projected-secrets-28aba649-8ac8-4dbb-a620-908c5d1d1fb6" satisfied condition "Succeeded or Failed"
Jan 18 22:36:53.448: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-secrets-28aba649-8ac8-4dbb-a620-908c5d1d1fb6 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 22:36:53.457
Jan 18 22:36:53.474: INFO: Waiting for pod pod-projected-secrets-28aba649-8ac8-4dbb-a620-908c5d1d1fb6 to disappear
Jan 18 22:36:53.478: INFO: Pod pod-projected-secrets-28aba649-8ac8-4dbb-a620-908c5d1d1fb6 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 18 22:36:53.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4537" for this suite. 01/18/23 22:36:53.488
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":247,"skipped":4472,"failed":0}
------------------------------
• [4.137 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:36:49.359
    Jan 18 22:36:49.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 22:36:49.361
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:36:49.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:36:49.392
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-492c8ee8-dd3e-42a6-bd9a-dc66e093ae32 01/18/23 22:36:49.397
    STEP: Creating a pod to test consume secrets 01/18/23 22:36:49.404
    Jan 18 22:36:49.426: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-28aba649-8ac8-4dbb-a620-908c5d1d1fb6" in namespace "projected-4537" to be "Succeeded or Failed"
    Jan 18 22:36:49.435: INFO: Pod "pod-projected-secrets-28aba649-8ac8-4dbb-a620-908c5d1d1fb6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.48721ms
    Jan 18 22:36:51.442: INFO: Pod "pod-projected-secrets-28aba649-8ac8-4dbb-a620-908c5d1d1fb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015008434s
    Jan 18 22:36:53.441: INFO: Pod "pod-projected-secrets-28aba649-8ac8-4dbb-a620-908c5d1d1fb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013921533s
    STEP: Saw pod success 01/18/23 22:36:53.441
    Jan 18 22:36:53.441: INFO: Pod "pod-projected-secrets-28aba649-8ac8-4dbb-a620-908c5d1d1fb6" satisfied condition "Succeeded or Failed"
    Jan 18 22:36:53.448: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-secrets-28aba649-8ac8-4dbb-a620-908c5d1d1fb6 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 22:36:53.457
    Jan 18 22:36:53.474: INFO: Waiting for pod pod-projected-secrets-28aba649-8ac8-4dbb-a620-908c5d1d1fb6 to disappear
    Jan 18 22:36:53.478: INFO: Pod pod-projected-secrets-28aba649-8ac8-4dbb-a620-908c5d1d1fb6 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 18 22:36:53.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4537" for this suite. 01/18/23 22:36:53.488
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:36:53.5
Jan 18 22:36:53.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename subpath 01/18/23 22:36:53.502
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:36:53.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:36:53.532
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/18/23 22:36:53.538
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-p9kp 01/18/23 22:36:53.551
STEP: Creating a pod to test atomic-volume-subpath 01/18/23 22:36:53.551
Jan 18 22:36:53.562: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-p9kp" in namespace "subpath-8628" to be "Succeeded or Failed"
Jan 18 22:36:53.568: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Pending", Reason="", readiness=false. Elapsed: 5.784643ms
Jan 18 22:36:55.574: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011564402s
Jan 18 22:36:57.574: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Running", Reason="", readiness=true. Elapsed: 4.011790858s
Jan 18 22:36:59.576: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Running", Reason="", readiness=true. Elapsed: 6.013174311s
Jan 18 22:37:01.576: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Running", Reason="", readiness=true. Elapsed: 8.013514405s
Jan 18 22:37:03.576: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Running", Reason="", readiness=true. Elapsed: 10.013203432s
Jan 18 22:37:05.574: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Running", Reason="", readiness=true. Elapsed: 12.011943714s
Jan 18 22:37:07.576: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Running", Reason="", readiness=true. Elapsed: 14.013599846s
Jan 18 22:37:09.575: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Running", Reason="", readiness=true. Elapsed: 16.01297665s
Jan 18 22:37:11.576: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Running", Reason="", readiness=true. Elapsed: 18.013516498s
Jan 18 22:37:13.575: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Running", Reason="", readiness=true. Elapsed: 20.012277107s
Jan 18 22:37:15.577: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Running", Reason="", readiness=true. Elapsed: 22.014570883s
Jan 18 22:37:17.574: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Running", Reason="", readiness=false. Elapsed: 24.011369531s
Jan 18 22:37:19.582: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.019924645s
STEP: Saw pod success 01/18/23 22:37:19.583
Jan 18 22:37:19.583: INFO: Pod "pod-subpath-test-configmap-p9kp" satisfied condition "Succeeded or Failed"
Jan 18 22:37:19.592: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-subpath-test-configmap-p9kp container test-container-subpath-configmap-p9kp: <nil>
STEP: delete the pod 01/18/23 22:37:19.602
Jan 18 22:37:19.621: INFO: Waiting for pod pod-subpath-test-configmap-p9kp to disappear
Jan 18 22:37:19.627: INFO: Pod pod-subpath-test-configmap-p9kp no longer exists
STEP: Deleting pod pod-subpath-test-configmap-p9kp 01/18/23 22:37:19.627
Jan 18 22:37:19.627: INFO: Deleting pod "pod-subpath-test-configmap-p9kp" in namespace "subpath-8628"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 18 22:37:19.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8628" for this suite. 01/18/23 22:37:19.637
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":248,"skipped":4489,"failed":0}
------------------------------
• [SLOW TEST] [26.146 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:36:53.5
    Jan 18 22:36:53.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename subpath 01/18/23 22:36:53.502
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:36:53.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:36:53.532
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/18/23 22:36:53.538
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-p9kp 01/18/23 22:36:53.551
    STEP: Creating a pod to test atomic-volume-subpath 01/18/23 22:36:53.551
    Jan 18 22:36:53.562: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-p9kp" in namespace "subpath-8628" to be "Succeeded or Failed"
    Jan 18 22:36:53.568: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Pending", Reason="", readiness=false. Elapsed: 5.784643ms
    Jan 18 22:36:55.574: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011564402s
    Jan 18 22:36:57.574: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Running", Reason="", readiness=true. Elapsed: 4.011790858s
    Jan 18 22:36:59.576: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Running", Reason="", readiness=true. Elapsed: 6.013174311s
    Jan 18 22:37:01.576: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Running", Reason="", readiness=true. Elapsed: 8.013514405s
    Jan 18 22:37:03.576: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Running", Reason="", readiness=true. Elapsed: 10.013203432s
    Jan 18 22:37:05.574: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Running", Reason="", readiness=true. Elapsed: 12.011943714s
    Jan 18 22:37:07.576: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Running", Reason="", readiness=true. Elapsed: 14.013599846s
    Jan 18 22:37:09.575: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Running", Reason="", readiness=true. Elapsed: 16.01297665s
    Jan 18 22:37:11.576: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Running", Reason="", readiness=true. Elapsed: 18.013516498s
    Jan 18 22:37:13.575: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Running", Reason="", readiness=true. Elapsed: 20.012277107s
    Jan 18 22:37:15.577: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Running", Reason="", readiness=true. Elapsed: 22.014570883s
    Jan 18 22:37:17.574: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Running", Reason="", readiness=false. Elapsed: 24.011369531s
    Jan 18 22:37:19.582: INFO: Pod "pod-subpath-test-configmap-p9kp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.019924645s
    STEP: Saw pod success 01/18/23 22:37:19.583
    Jan 18 22:37:19.583: INFO: Pod "pod-subpath-test-configmap-p9kp" satisfied condition "Succeeded or Failed"
    Jan 18 22:37:19.592: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-subpath-test-configmap-p9kp container test-container-subpath-configmap-p9kp: <nil>
    STEP: delete the pod 01/18/23 22:37:19.602
    Jan 18 22:37:19.621: INFO: Waiting for pod pod-subpath-test-configmap-p9kp to disappear
    Jan 18 22:37:19.627: INFO: Pod pod-subpath-test-configmap-p9kp no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-p9kp 01/18/23 22:37:19.627
    Jan 18 22:37:19.627: INFO: Deleting pod "pod-subpath-test-configmap-p9kp" in namespace "subpath-8628"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 18 22:37:19.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-8628" for this suite. 01/18/23 22:37:19.637
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:37:19.654
Jan 18 22:37:19.655: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename configmap 01/18/23 22:37:19.657
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:19.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:19.689
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-a0581bfa-fd01-4b28-86cf-a5fbdc09f2de 01/18/23 22:37:19.703
STEP: Creating the pod 01/18/23 22:37:19.71
Jan 18 22:37:19.722: INFO: Waiting up to 5m0s for pod "pod-configmaps-cd72d6b8-a045-4527-b0ac-1b1e033dbdb5" in namespace "configmap-1377" to be "running"
Jan 18 22:37:19.726: INFO: Pod "pod-configmaps-cd72d6b8-a045-4527-b0ac-1b1e033dbdb5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.413561ms
Jan 18 22:37:21.735: INFO: Pod "pod-configmaps-cd72d6b8-a045-4527-b0ac-1b1e033dbdb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012604715s
Jan 18 22:37:23.733: INFO: Pod "pod-configmaps-cd72d6b8-a045-4527-b0ac-1b1e033dbdb5": Phase="Running", Reason="", readiness=false. Elapsed: 4.010688639s
Jan 18 22:37:23.733: INFO: Pod "pod-configmaps-cd72d6b8-a045-4527-b0ac-1b1e033dbdb5" satisfied condition "running"
STEP: Waiting for pod with text data 01/18/23 22:37:23.733
STEP: Waiting for pod with binary data 01/18/23 22:37:23.742
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 22:37:23.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1377" for this suite. 01/18/23 22:37:23.755
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":249,"skipped":4490,"failed":0}
------------------------------
• [4.116 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:37:19.654
    Jan 18 22:37:19.655: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename configmap 01/18/23 22:37:19.657
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:19.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:19.689
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-a0581bfa-fd01-4b28-86cf-a5fbdc09f2de 01/18/23 22:37:19.703
    STEP: Creating the pod 01/18/23 22:37:19.71
    Jan 18 22:37:19.722: INFO: Waiting up to 5m0s for pod "pod-configmaps-cd72d6b8-a045-4527-b0ac-1b1e033dbdb5" in namespace "configmap-1377" to be "running"
    Jan 18 22:37:19.726: INFO: Pod "pod-configmaps-cd72d6b8-a045-4527-b0ac-1b1e033dbdb5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.413561ms
    Jan 18 22:37:21.735: INFO: Pod "pod-configmaps-cd72d6b8-a045-4527-b0ac-1b1e033dbdb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012604715s
    Jan 18 22:37:23.733: INFO: Pod "pod-configmaps-cd72d6b8-a045-4527-b0ac-1b1e033dbdb5": Phase="Running", Reason="", readiness=false. Elapsed: 4.010688639s
    Jan 18 22:37:23.733: INFO: Pod "pod-configmaps-cd72d6b8-a045-4527-b0ac-1b1e033dbdb5" satisfied condition "running"
    STEP: Waiting for pod with text data 01/18/23 22:37:23.733
    STEP: Waiting for pod with binary data 01/18/23 22:37:23.742
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 22:37:23.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1377" for this suite. 01/18/23 22:37:23.755
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:37:23.779
Jan 18 22:37:23.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename webhook 01/18/23 22:37:23.781
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:23.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:23.804
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 22:37:23.832
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:37:24.761
STEP: Deploying the webhook pod 01/18/23 22:37:24.781
STEP: Wait for the deployment to be ready 01/18/23 22:37:24.816
Jan 18 22:37:24.862: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 18 22:37:26.877: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 37, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 37, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 37, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 37, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/18/23 22:37:28.883
STEP: Verifying the service has paired with the endpoint 01/18/23 22:37:28.899
Jan 18 22:37:29.900: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/18/23 22:37:29.963
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/18/23 22:37:30
STEP: Creating a dummy validating-webhook-configuration object 01/18/23 22:37:30.022
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/18/23 22:37:30.034
STEP: Creating a dummy mutating-webhook-configuration object 01/18/23 22:37:30.041
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/18/23 22:37:30.056
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 22:37:30.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5901" for this suite. 01/18/23 22:37:30.106
STEP: Destroying namespace "webhook-5901-markers" for this suite. 01/18/23 22:37:30.115
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":250,"skipped":4536,"failed":0}
------------------------------
• [SLOW TEST] [6.435 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:37:23.779
    Jan 18 22:37:23.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename webhook 01/18/23 22:37:23.781
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:23.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:23.804
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 22:37:23.832
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:37:24.761
    STEP: Deploying the webhook pod 01/18/23 22:37:24.781
    STEP: Wait for the deployment to be ready 01/18/23 22:37:24.816
    Jan 18 22:37:24.862: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 18 22:37:26.877: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 37, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 37, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 37, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 37, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/18/23 22:37:28.883
    STEP: Verifying the service has paired with the endpoint 01/18/23 22:37:28.899
    Jan 18 22:37:29.900: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/18/23 22:37:29.963
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/18/23 22:37:30
    STEP: Creating a dummy validating-webhook-configuration object 01/18/23 22:37:30.022
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/18/23 22:37:30.034
    STEP: Creating a dummy mutating-webhook-configuration object 01/18/23 22:37:30.041
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/18/23 22:37:30.056
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 22:37:30.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5901" for this suite. 01/18/23 22:37:30.106
    STEP: Destroying namespace "webhook-5901-markers" for this suite. 01/18/23 22:37:30.115
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:37:30.215
Jan 18 22:37:30.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename container-probe 01/18/23 22:37:30.218
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:30.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:30.275
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-bc6b92f5-e6fb-4e06-9048-d48b80917bb2 in namespace container-probe-7444 01/18/23 22:37:30.295
Jan 18 22:37:30.329: INFO: Waiting up to 5m0s for pod "busybox-bc6b92f5-e6fb-4e06-9048-d48b80917bb2" in namespace "container-probe-7444" to be "not pending"
Jan 18 22:37:30.349: INFO: Pod "busybox-bc6b92f5-e6fb-4e06-9048-d48b80917bb2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.092233ms
Jan 18 22:37:32.358: INFO: Pod "busybox-bc6b92f5-e6fb-4e06-9048-d48b80917bb2": Phase="Running", Reason="", readiness=true. Elapsed: 2.028217472s
Jan 18 22:37:32.358: INFO: Pod "busybox-bc6b92f5-e6fb-4e06-9048-d48b80917bb2" satisfied condition "not pending"
Jan 18 22:37:32.358: INFO: Started pod busybox-bc6b92f5-e6fb-4e06-9048-d48b80917bb2 in namespace container-probe-7444
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 22:37:32.358
Jan 18 22:37:32.363: INFO: Initial restart count of pod busybox-bc6b92f5-e6fb-4e06-9048-d48b80917bb2 is 0
STEP: deleting the pod 01/18/23 22:41:33.235
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 18 22:41:33.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7444" for this suite. 01/18/23 22:41:33.277
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":251,"skipped":4547,"failed":0}
------------------------------
• [SLOW TEST] [243.067 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:37:30.215
    Jan 18 22:37:30.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename container-probe 01/18/23 22:37:30.218
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:37:30.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:37:30.275
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-bc6b92f5-e6fb-4e06-9048-d48b80917bb2 in namespace container-probe-7444 01/18/23 22:37:30.295
    Jan 18 22:37:30.329: INFO: Waiting up to 5m0s for pod "busybox-bc6b92f5-e6fb-4e06-9048-d48b80917bb2" in namespace "container-probe-7444" to be "not pending"
    Jan 18 22:37:30.349: INFO: Pod "busybox-bc6b92f5-e6fb-4e06-9048-d48b80917bb2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.092233ms
    Jan 18 22:37:32.358: INFO: Pod "busybox-bc6b92f5-e6fb-4e06-9048-d48b80917bb2": Phase="Running", Reason="", readiness=true. Elapsed: 2.028217472s
    Jan 18 22:37:32.358: INFO: Pod "busybox-bc6b92f5-e6fb-4e06-9048-d48b80917bb2" satisfied condition "not pending"
    Jan 18 22:37:32.358: INFO: Started pod busybox-bc6b92f5-e6fb-4e06-9048-d48b80917bb2 in namespace container-probe-7444
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 22:37:32.358
    Jan 18 22:37:32.363: INFO: Initial restart count of pod busybox-bc6b92f5-e6fb-4e06-9048-d48b80917bb2 is 0
    STEP: deleting the pod 01/18/23 22:41:33.235
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 18 22:41:33.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7444" for this suite. 01/18/23 22:41:33.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:41:33.286
Jan 18 22:41:33.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename kubectl 01/18/23 22:41:33.288
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:41:33.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:41:33.328
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 01/18/23 22:41:33.337
Jan 18 22:41:33.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4228 cluster-info'
Jan 18 22:41:33.456: INFO: stderr: ""
Jan 18 22:41:33.456: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 22:41:33.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4228" for this suite. 01/18/23 22:41:33.462
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":252,"skipped":4553,"failed":0}
------------------------------
• [0.184 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:41:33.286
    Jan 18 22:41:33.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename kubectl 01/18/23 22:41:33.288
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:41:33.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:41:33.328
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 01/18/23 22:41:33.337
    Jan 18 22:41:33.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4228 cluster-info'
    Jan 18 22:41:33.456: INFO: stderr: ""
    Jan 18 22:41:33.456: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 22:41:33.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4228" for this suite. 01/18/23 22:41:33.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:41:33.475
Jan 18 22:41:33.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 22:41:33.478
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:41:33.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:41:33.506
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 01/18/23 22:41:33.516
Jan 18 22:41:33.529: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b351386d-7271-4444-82f4-e1007d5edee3" in namespace "projected-4211" to be "Succeeded or Failed"
Jan 18 22:41:33.715: INFO: Pod "downwardapi-volume-b351386d-7271-4444-82f4-e1007d5edee3": Phase="Pending", Reason="", readiness=false. Elapsed: 185.122042ms
Jan 18 22:41:35.720: INFO: Pod "downwardapi-volume-b351386d-7271-4444-82f4-e1007d5edee3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.190472732s
Jan 18 22:41:37.723: INFO: Pod "downwardapi-volume-b351386d-7271-4444-82f4-e1007d5edee3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.193034388s
Jan 18 22:41:39.721: INFO: Pod "downwardapi-volume-b351386d-7271-4444-82f4-e1007d5edee3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.191337452s
STEP: Saw pod success 01/18/23 22:41:39.721
Jan 18 22:41:39.722: INFO: Pod "downwardapi-volume-b351386d-7271-4444-82f4-e1007d5edee3" satisfied condition "Succeeded or Failed"
Jan 18 22:41:39.727: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-b351386d-7271-4444-82f4-e1007d5edee3 container client-container: <nil>
STEP: delete the pod 01/18/23 22:41:39.767
Jan 18 22:41:39.780: INFO: Waiting for pod downwardapi-volume-b351386d-7271-4444-82f4-e1007d5edee3 to disappear
Jan 18 22:41:39.784: INFO: Pod downwardapi-volume-b351386d-7271-4444-82f4-e1007d5edee3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 22:41:39.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4211" for this suite. 01/18/23 22:41:39.79
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":253,"skipped":4563,"failed":0}
------------------------------
• [SLOW TEST] [6.324 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:41:33.475
    Jan 18 22:41:33.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 22:41:33.478
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:41:33.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:41:33.506
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 01/18/23 22:41:33.516
    Jan 18 22:41:33.529: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b351386d-7271-4444-82f4-e1007d5edee3" in namespace "projected-4211" to be "Succeeded or Failed"
    Jan 18 22:41:33.715: INFO: Pod "downwardapi-volume-b351386d-7271-4444-82f4-e1007d5edee3": Phase="Pending", Reason="", readiness=false. Elapsed: 185.122042ms
    Jan 18 22:41:35.720: INFO: Pod "downwardapi-volume-b351386d-7271-4444-82f4-e1007d5edee3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.190472732s
    Jan 18 22:41:37.723: INFO: Pod "downwardapi-volume-b351386d-7271-4444-82f4-e1007d5edee3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.193034388s
    Jan 18 22:41:39.721: INFO: Pod "downwardapi-volume-b351386d-7271-4444-82f4-e1007d5edee3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.191337452s
    STEP: Saw pod success 01/18/23 22:41:39.721
    Jan 18 22:41:39.722: INFO: Pod "downwardapi-volume-b351386d-7271-4444-82f4-e1007d5edee3" satisfied condition "Succeeded or Failed"
    Jan 18 22:41:39.727: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-b351386d-7271-4444-82f4-e1007d5edee3 container client-container: <nil>
    STEP: delete the pod 01/18/23 22:41:39.767
    Jan 18 22:41:39.780: INFO: Waiting for pod downwardapi-volume-b351386d-7271-4444-82f4-e1007d5edee3 to disappear
    Jan 18 22:41:39.784: INFO: Pod downwardapi-volume-b351386d-7271-4444-82f4-e1007d5edee3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 22:41:39.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4211" for this suite. 01/18/23 22:41:39.79
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:41:39.811
Jan 18 22:41:39.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename runtimeclass 01/18/23 22:41:39.814
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:41:39.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:41:39.85
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 18 22:41:39.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4785" for this suite. 01/18/23 22:41:39.872
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":254,"skipped":4592,"failed":0}
------------------------------
• [0.070 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:41:39.811
    Jan 18 22:41:39.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename runtimeclass 01/18/23 22:41:39.814
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:41:39.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:41:39.85
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 18 22:41:39.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-4785" for this suite. 01/18/23 22:41:39.872
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:41:39.886
Jan 18 22:41:39.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename downward-api 01/18/23 22:41:39.888
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:41:39.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:41:39.917
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 01/18/23 22:41:39.924
Jan 18 22:41:39.938: INFO: Waiting up to 5m0s for pod "downward-api-379b3e76-a441-4cbd-b903-d581923c9c2e" in namespace "downward-api-5025" to be "Succeeded or Failed"
Jan 18 22:41:39.951: INFO: Pod "downward-api-379b3e76-a441-4cbd-b903-d581923c9c2e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.476221ms
Jan 18 22:41:41.957: INFO: Pod "downward-api-379b3e76-a441-4cbd-b903-d581923c9c2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019099464s
Jan 18 22:41:43.959: INFO: Pod "downward-api-379b3e76-a441-4cbd-b903-d581923c9c2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02038914s
STEP: Saw pod success 01/18/23 22:41:43.959
Jan 18 22:41:43.959: INFO: Pod "downward-api-379b3e76-a441-4cbd-b903-d581923c9c2e" satisfied condition "Succeeded or Failed"
Jan 18 22:41:43.970: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downward-api-379b3e76-a441-4cbd-b903-d581923c9c2e container dapi-container: <nil>
STEP: delete the pod 01/18/23 22:41:43.984
Jan 18 22:41:44.006: INFO: Waiting for pod downward-api-379b3e76-a441-4cbd-b903-d581923c9c2e to disappear
Jan 18 22:41:44.010: INFO: Pod downward-api-379b3e76-a441-4cbd-b903-d581923c9c2e no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 18 22:41:44.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5025" for this suite. 01/18/23 22:41:44.017
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":255,"skipped":4602,"failed":0}
------------------------------
• [4.145 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:41:39.886
    Jan 18 22:41:39.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename downward-api 01/18/23 22:41:39.888
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:41:39.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:41:39.917
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 01/18/23 22:41:39.924
    Jan 18 22:41:39.938: INFO: Waiting up to 5m0s for pod "downward-api-379b3e76-a441-4cbd-b903-d581923c9c2e" in namespace "downward-api-5025" to be "Succeeded or Failed"
    Jan 18 22:41:39.951: INFO: Pod "downward-api-379b3e76-a441-4cbd-b903-d581923c9c2e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.476221ms
    Jan 18 22:41:41.957: INFO: Pod "downward-api-379b3e76-a441-4cbd-b903-d581923c9c2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019099464s
    Jan 18 22:41:43.959: INFO: Pod "downward-api-379b3e76-a441-4cbd-b903-d581923c9c2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02038914s
    STEP: Saw pod success 01/18/23 22:41:43.959
    Jan 18 22:41:43.959: INFO: Pod "downward-api-379b3e76-a441-4cbd-b903-d581923c9c2e" satisfied condition "Succeeded or Failed"
    Jan 18 22:41:43.970: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downward-api-379b3e76-a441-4cbd-b903-d581923c9c2e container dapi-container: <nil>
    STEP: delete the pod 01/18/23 22:41:43.984
    Jan 18 22:41:44.006: INFO: Waiting for pod downward-api-379b3e76-a441-4cbd-b903-d581923c9c2e to disappear
    Jan 18 22:41:44.010: INFO: Pod downward-api-379b3e76-a441-4cbd-b903-d581923c9c2e no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 18 22:41:44.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5025" for this suite. 01/18/23 22:41:44.017
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:41:44.035
Jan 18 22:41:44.036: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename dns 01/18/23 22:41:44.038
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:41:44.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:41:44.078
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/18/23 22:41:44.088
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/18/23 22:41:44.088
STEP: creating a pod to probe DNS 01/18/23 22:41:44.088
STEP: submitting the pod to kubernetes 01/18/23 22:41:44.089
Jan 18 22:41:44.103: INFO: Waiting up to 15m0s for pod "dns-test-6885a841-7741-41ff-a601-fa6714f48da0" in namespace "dns-7467" to be "running"
Jan 18 22:41:44.124: INFO: Pod "dns-test-6885a841-7741-41ff-a601-fa6714f48da0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.758424ms
Jan 18 22:41:46.132: INFO: Pod "dns-test-6885a841-7741-41ff-a601-fa6714f48da0": Phase="Running", Reason="", readiness=true. Elapsed: 2.028650403s
Jan 18 22:41:46.132: INFO: Pod "dns-test-6885a841-7741-41ff-a601-fa6714f48da0" satisfied condition "running"
STEP: retrieving the pod 01/18/23 22:41:46.132
STEP: looking for the results for each expected name from probers 01/18/23 22:41:46.138
Jan 18 22:41:46.163: INFO: DNS probes using dns-7467/dns-test-6885a841-7741-41ff-a601-fa6714f48da0 succeeded

STEP: deleting the pod 01/18/23 22:41:46.163
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 18 22:41:46.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7467" for this suite. 01/18/23 22:41:46.197
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":256,"skipped":4604,"failed":0}
------------------------------
• [2.169 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:41:44.035
    Jan 18 22:41:44.036: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename dns 01/18/23 22:41:44.038
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:41:44.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:41:44.078
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/18/23 22:41:44.088
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/18/23 22:41:44.088
    STEP: creating a pod to probe DNS 01/18/23 22:41:44.088
    STEP: submitting the pod to kubernetes 01/18/23 22:41:44.089
    Jan 18 22:41:44.103: INFO: Waiting up to 15m0s for pod "dns-test-6885a841-7741-41ff-a601-fa6714f48da0" in namespace "dns-7467" to be "running"
    Jan 18 22:41:44.124: INFO: Pod "dns-test-6885a841-7741-41ff-a601-fa6714f48da0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.758424ms
    Jan 18 22:41:46.132: INFO: Pod "dns-test-6885a841-7741-41ff-a601-fa6714f48da0": Phase="Running", Reason="", readiness=true. Elapsed: 2.028650403s
    Jan 18 22:41:46.132: INFO: Pod "dns-test-6885a841-7741-41ff-a601-fa6714f48da0" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 22:41:46.132
    STEP: looking for the results for each expected name from probers 01/18/23 22:41:46.138
    Jan 18 22:41:46.163: INFO: DNS probes using dns-7467/dns-test-6885a841-7741-41ff-a601-fa6714f48da0 succeeded

    STEP: deleting the pod 01/18/23 22:41:46.163
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 18 22:41:46.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-7467" for this suite. 01/18/23 22:41:46.197
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:41:46.221
Jan 18 22:41:46.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename dns 01/18/23 22:41:46.222
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:41:46.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:41:46.247
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 01/18/23 22:41:46.252
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8903.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8903.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 01/18/23 22:41:46.26
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8903.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8903.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 01/18/23 22:41:46.261
STEP: creating a pod to probe DNS 01/18/23 22:41:46.261
STEP: submitting the pod to kubernetes 01/18/23 22:41:46.261
Jan 18 22:41:46.272: INFO: Waiting up to 15m0s for pod "dns-test-7c7a8592-a772-49f1-9c81-788688d3e86b" in namespace "dns-8903" to be "running"
Jan 18 22:41:46.279: INFO: Pod "dns-test-7c7a8592-a772-49f1-9c81-788688d3e86b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.019411ms
Jan 18 22:41:48.286: INFO: Pod "dns-test-7c7a8592-a772-49f1-9c81-788688d3e86b": Phase="Running", Reason="", readiness=true. Elapsed: 2.013598014s
Jan 18 22:41:48.286: INFO: Pod "dns-test-7c7a8592-a772-49f1-9c81-788688d3e86b" satisfied condition "running"
STEP: retrieving the pod 01/18/23 22:41:48.286
STEP: looking for the results for each expected name from probers 01/18/23 22:41:48.294
Jan 18 22:41:48.321: INFO: DNS probes using dns-8903/dns-test-7c7a8592-a772-49f1-9c81-788688d3e86b succeeded

STEP: deleting the pod 01/18/23 22:41:48.321
STEP: deleting the test headless service 01/18/23 22:41:48.381
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 18 22:41:48.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8903" for this suite. 01/18/23 22:41:48.434
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":257,"skipped":4647,"failed":0}
------------------------------
• [2.225 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:41:46.221
    Jan 18 22:41:46.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename dns 01/18/23 22:41:46.222
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:41:46.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:41:46.247
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 01/18/23 22:41:46.252
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8903.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8903.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     01/18/23 22:41:46.26
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8903.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8903.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     01/18/23 22:41:46.261
    STEP: creating a pod to probe DNS 01/18/23 22:41:46.261
    STEP: submitting the pod to kubernetes 01/18/23 22:41:46.261
    Jan 18 22:41:46.272: INFO: Waiting up to 15m0s for pod "dns-test-7c7a8592-a772-49f1-9c81-788688d3e86b" in namespace "dns-8903" to be "running"
    Jan 18 22:41:46.279: INFO: Pod "dns-test-7c7a8592-a772-49f1-9c81-788688d3e86b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.019411ms
    Jan 18 22:41:48.286: INFO: Pod "dns-test-7c7a8592-a772-49f1-9c81-788688d3e86b": Phase="Running", Reason="", readiness=true. Elapsed: 2.013598014s
    Jan 18 22:41:48.286: INFO: Pod "dns-test-7c7a8592-a772-49f1-9c81-788688d3e86b" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 22:41:48.286
    STEP: looking for the results for each expected name from probers 01/18/23 22:41:48.294
    Jan 18 22:41:48.321: INFO: DNS probes using dns-8903/dns-test-7c7a8592-a772-49f1-9c81-788688d3e86b succeeded

    STEP: deleting the pod 01/18/23 22:41:48.321
    STEP: deleting the test headless service 01/18/23 22:41:48.381
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 18 22:41:48.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8903" for this suite. 01/18/23 22:41:48.434
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:41:48.448
Jan 18 22:41:48.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename security-context-test 01/18/23 22:41:48.45
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:41:48.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:41:48.488
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Jan 18 22:41:48.616: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-2ed866fd-cd47-4849-8f16-23456df344ad" in namespace "security-context-test-3650" to be "Succeeded or Failed"
Jan 18 22:41:48.623: INFO: Pod "busybox-readonly-false-2ed866fd-cd47-4849-8f16-23456df344ad": Phase="Pending", Reason="", readiness=false. Elapsed: 6.557707ms
Jan 18 22:41:50.634: INFO: Pod "busybox-readonly-false-2ed866fd-cd47-4849-8f16-23456df344ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017319739s
Jan 18 22:41:52.629: INFO: Pod "busybox-readonly-false-2ed866fd-cd47-4849-8f16-23456df344ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012827658s
Jan 18 22:41:54.631: INFO: Pod "busybox-readonly-false-2ed866fd-cd47-4849-8f16-23456df344ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014616192s
Jan 18 22:41:54.631: INFO: Pod "busybox-readonly-false-2ed866fd-cd47-4849-8f16-23456df344ad" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 18 22:41:54.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3650" for this suite. 01/18/23 22:41:54.64
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":258,"skipped":4648,"failed":0}
------------------------------
• [SLOW TEST] [6.200 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:41:48.448
    Jan 18 22:41:48.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename security-context-test 01/18/23 22:41:48.45
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:41:48.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:41:48.488
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Jan 18 22:41:48.616: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-2ed866fd-cd47-4849-8f16-23456df344ad" in namespace "security-context-test-3650" to be "Succeeded or Failed"
    Jan 18 22:41:48.623: INFO: Pod "busybox-readonly-false-2ed866fd-cd47-4849-8f16-23456df344ad": Phase="Pending", Reason="", readiness=false. Elapsed: 6.557707ms
    Jan 18 22:41:50.634: INFO: Pod "busybox-readonly-false-2ed866fd-cd47-4849-8f16-23456df344ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017319739s
    Jan 18 22:41:52.629: INFO: Pod "busybox-readonly-false-2ed866fd-cd47-4849-8f16-23456df344ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012827658s
    Jan 18 22:41:54.631: INFO: Pod "busybox-readonly-false-2ed866fd-cd47-4849-8f16-23456df344ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014616192s
    Jan 18 22:41:54.631: INFO: Pod "busybox-readonly-false-2ed866fd-cd47-4849-8f16-23456df344ad" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 18 22:41:54.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-3650" for this suite. 01/18/23 22:41:54.64
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:41:54.649
Jan 18 22:41:54.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename services 01/18/23 22:41:54.651
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:41:54.675
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:41:54.681
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-8519 01/18/23 22:41:54.687
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8519 to expose endpoints map[] 01/18/23 22:41:54.697
Jan 18 22:41:54.725: INFO: successfully validated that service endpoint-test2 in namespace services-8519 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8519 01/18/23 22:41:54.725
Jan 18 22:41:54.736: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8519" to be "running and ready"
Jan 18 22:41:54.746: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.623992ms
Jan 18 22:41:54.746: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:41:56.754: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.017584516s
Jan 18 22:41:56.755: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 18 22:41:56.755: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8519 to expose endpoints map[pod1:[80]] 01/18/23 22:41:56.76
Jan 18 22:41:56.776: INFO: successfully validated that service endpoint-test2 in namespace services-8519 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 01/18/23 22:41:56.776
Jan 18 22:41:56.777: INFO: Creating new exec pod
Jan 18 22:41:56.788: INFO: Waiting up to 5m0s for pod "execpod8v5f2" in namespace "services-8519" to be "running"
Jan 18 22:41:56.805: INFO: Pod "execpod8v5f2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.704692ms
Jan 18 22:41:58.811: INFO: Pod "execpod8v5f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022574248s
Jan 18 22:42:00.812: INFO: Pod "execpod8v5f2": Phase="Running", Reason="", readiness=true. Elapsed: 4.023476824s
Jan 18 22:42:00.812: INFO: Pod "execpod8v5f2" satisfied condition "running"
Jan 18 22:42:01.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-8519 exec execpod8v5f2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 18 22:42:02.044: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 18 22:42:02.044: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 22:42:02.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-8519 exec execpod8v5f2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.2.201 80'
Jan 18 22:42:02.236: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.2.201 80\nConnection to 10.233.2.201 80 port [tcp/http] succeeded!\n"
Jan 18 22:42:02.236: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-8519 01/18/23 22:42:02.236
Jan 18 22:42:02.404: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8519" to be "running and ready"
Jan 18 22:42:02.438: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 33.503716ms
Jan 18 22:42:02.438: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:42:04.444: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.03989053s
Jan 18 22:42:04.444: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 18 22:42:04.444: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8519 to expose endpoints map[pod1:[80] pod2:[80]] 01/18/23 22:42:04.448
Jan 18 22:42:04.466: INFO: successfully validated that service endpoint-test2 in namespace services-8519 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 01/18/23 22:42:04.466
Jan 18 22:42:05.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-8519 exec execpod8v5f2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 18 22:42:05.673: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 18 22:42:05.673: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 22:42:05.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-8519 exec execpod8v5f2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.2.201 80'
Jan 18 22:42:05.855: INFO: stderr: "+ nc -v -t -w 2 10.233.2.201 80\nConnection to 10.233.2.201 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Jan 18 22:42:05.855: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-8519 01/18/23 22:42:05.855
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8519 to expose endpoints map[pod2:[80]] 01/18/23 22:42:05.878
Jan 18 22:42:05.919: INFO: successfully validated that service endpoint-test2 in namespace services-8519 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 01/18/23 22:42:05.919
Jan 18 22:42:06.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-8519 exec execpod8v5f2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 18 22:42:07.139: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 18 22:42:07.139: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 22:42:07.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-8519 exec execpod8v5f2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.2.201 80'
Jan 18 22:42:07.335: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.2.201 80\nConnection to 10.233.2.201 80 port [tcp/http] succeeded!\n"
Jan 18 22:42:07.335: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-8519 01/18/23 22:42:07.336
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8519 to expose endpoints map[] 01/18/23 22:42:07.352
Jan 18 22:42:08.391: INFO: successfully validated that service endpoint-test2 in namespace services-8519 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 22:42:08.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8519" for this suite. 01/18/23 22:42:08.422
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":259,"skipped":4659,"failed":0}
------------------------------
• [SLOW TEST] [13.783 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:41:54.649
    Jan 18 22:41:54.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename services 01/18/23 22:41:54.651
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:41:54.675
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:41:54.681
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-8519 01/18/23 22:41:54.687
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8519 to expose endpoints map[] 01/18/23 22:41:54.697
    Jan 18 22:41:54.725: INFO: successfully validated that service endpoint-test2 in namespace services-8519 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-8519 01/18/23 22:41:54.725
    Jan 18 22:41:54.736: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8519" to be "running and ready"
    Jan 18 22:41:54.746: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.623992ms
    Jan 18 22:41:54.746: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:41:56.754: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.017584516s
    Jan 18 22:41:56.755: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 18 22:41:56.755: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8519 to expose endpoints map[pod1:[80]] 01/18/23 22:41:56.76
    Jan 18 22:41:56.776: INFO: successfully validated that service endpoint-test2 in namespace services-8519 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 01/18/23 22:41:56.776
    Jan 18 22:41:56.777: INFO: Creating new exec pod
    Jan 18 22:41:56.788: INFO: Waiting up to 5m0s for pod "execpod8v5f2" in namespace "services-8519" to be "running"
    Jan 18 22:41:56.805: INFO: Pod "execpod8v5f2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.704692ms
    Jan 18 22:41:58.811: INFO: Pod "execpod8v5f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022574248s
    Jan 18 22:42:00.812: INFO: Pod "execpod8v5f2": Phase="Running", Reason="", readiness=true. Elapsed: 4.023476824s
    Jan 18 22:42:00.812: INFO: Pod "execpod8v5f2" satisfied condition "running"
    Jan 18 22:42:01.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-8519 exec execpod8v5f2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan 18 22:42:02.044: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 18 22:42:02.044: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 22:42:02.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-8519 exec execpod8v5f2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.2.201 80'
    Jan 18 22:42:02.236: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.2.201 80\nConnection to 10.233.2.201 80 port [tcp/http] succeeded!\n"
    Jan 18 22:42:02.236: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-8519 01/18/23 22:42:02.236
    Jan 18 22:42:02.404: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8519" to be "running and ready"
    Jan 18 22:42:02.438: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 33.503716ms
    Jan 18 22:42:02.438: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:42:04.444: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.03989053s
    Jan 18 22:42:04.444: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 18 22:42:04.444: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8519 to expose endpoints map[pod1:[80] pod2:[80]] 01/18/23 22:42:04.448
    Jan 18 22:42:04.466: INFO: successfully validated that service endpoint-test2 in namespace services-8519 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 01/18/23 22:42:04.466
    Jan 18 22:42:05.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-8519 exec execpod8v5f2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan 18 22:42:05.673: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 18 22:42:05.673: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 22:42:05.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-8519 exec execpod8v5f2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.2.201 80'
    Jan 18 22:42:05.855: INFO: stderr: "+ nc -v -t -w 2 10.233.2.201 80\nConnection to 10.233.2.201 80 port [tcp/http] succeeded!\n+ echo hostName\n"
    Jan 18 22:42:05.855: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-8519 01/18/23 22:42:05.855
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8519 to expose endpoints map[pod2:[80]] 01/18/23 22:42:05.878
    Jan 18 22:42:05.919: INFO: successfully validated that service endpoint-test2 in namespace services-8519 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 01/18/23 22:42:05.919
    Jan 18 22:42:06.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-8519 exec execpod8v5f2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan 18 22:42:07.139: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 18 22:42:07.139: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 22:42:07.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-8519 exec execpod8v5f2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.2.201 80'
    Jan 18 22:42:07.335: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.2.201 80\nConnection to 10.233.2.201 80 port [tcp/http] succeeded!\n"
    Jan 18 22:42:07.335: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-8519 01/18/23 22:42:07.336
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8519 to expose endpoints map[] 01/18/23 22:42:07.352
    Jan 18 22:42:08.391: INFO: successfully validated that service endpoint-test2 in namespace services-8519 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 22:42:08.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8519" for this suite. 01/18/23 22:42:08.422
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:42:08.438
Jan 18 22:42:08.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename webhook 01/18/23 22:42:08.445
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:42:08.478
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:42:08.488
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 22:42:08.516
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:42:09.103
STEP: Deploying the webhook pod 01/18/23 22:42:09.117
STEP: Wait for the deployment to be ready 01/18/23 22:42:09.129
Jan 18 22:42:09.142: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/18/23 22:42:11.172
STEP: Verifying the service has paired with the endpoint 01/18/23 22:42:11.189
Jan 18 22:42:12.190: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 01/18/23 22:42:12.197
STEP: create a pod that should be denied by the webhook 01/18/23 22:42:12.22
STEP: create a pod that causes the webhook to hang 01/18/23 22:42:12.239
STEP: create a configmap that should be denied by the webhook 01/18/23 22:42:22.254
STEP: create a configmap that should be admitted by the webhook 01/18/23 22:42:22.274
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/18/23 22:42:22.291
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/18/23 22:42:22.308
STEP: create a namespace that bypass the webhook 01/18/23 22:42:22.318
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/18/23 22:42:22.328
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 22:42:22.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7172" for this suite. 01/18/23 22:42:22.379
STEP: Destroying namespace "webhook-7172-markers" for this suite. 01/18/23 22:42:22.39
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":260,"skipped":4663,"failed":0}
------------------------------
• [SLOW TEST] [14.040 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:42:08.438
    Jan 18 22:42:08.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename webhook 01/18/23 22:42:08.445
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:42:08.478
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:42:08.488
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 22:42:08.516
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:42:09.103
    STEP: Deploying the webhook pod 01/18/23 22:42:09.117
    STEP: Wait for the deployment to be ready 01/18/23 22:42:09.129
    Jan 18 22:42:09.142: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/18/23 22:42:11.172
    STEP: Verifying the service has paired with the endpoint 01/18/23 22:42:11.189
    Jan 18 22:42:12.190: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 01/18/23 22:42:12.197
    STEP: create a pod that should be denied by the webhook 01/18/23 22:42:12.22
    STEP: create a pod that causes the webhook to hang 01/18/23 22:42:12.239
    STEP: create a configmap that should be denied by the webhook 01/18/23 22:42:22.254
    STEP: create a configmap that should be admitted by the webhook 01/18/23 22:42:22.274
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/18/23 22:42:22.291
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/18/23 22:42:22.308
    STEP: create a namespace that bypass the webhook 01/18/23 22:42:22.318
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/18/23 22:42:22.328
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 22:42:22.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7172" for this suite. 01/18/23 22:42:22.379
    STEP: Destroying namespace "webhook-7172-markers" for this suite. 01/18/23 22:42:22.39
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:42:22.479
Jan 18 22:42:22.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename secrets 01/18/23 22:42:22.481
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:42:22.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:42:22.548
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-5201/secret-test-7af79556-1ab9-4025-a1d8-97561322a3e6 01/18/23 22:42:22.567
STEP: Creating a pod to test consume secrets 01/18/23 22:42:22.578
Jan 18 22:42:22.590: INFO: Waiting up to 5m0s for pod "pod-configmaps-1ac2d5e5-fa44-4396-8132-9afac5405f06" in namespace "secrets-5201" to be "Succeeded or Failed"
Jan 18 22:42:22.608: INFO: Pod "pod-configmaps-1ac2d5e5-fa44-4396-8132-9afac5405f06": Phase="Pending", Reason="", readiness=false. Elapsed: 17.862031ms
Jan 18 22:42:24.618: INFO: Pod "pod-configmaps-1ac2d5e5-fa44-4396-8132-9afac5405f06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028371121s
Jan 18 22:42:26.617: INFO: Pod "pod-configmaps-1ac2d5e5-fa44-4396-8132-9afac5405f06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027200356s
STEP: Saw pod success 01/18/23 22:42:26.617
Jan 18 22:42:26.618: INFO: Pod "pod-configmaps-1ac2d5e5-fa44-4396-8132-9afac5405f06" satisfied condition "Succeeded or Failed"
Jan 18 22:42:26.622: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-1ac2d5e5-fa44-4396-8132-9afac5405f06 container env-test: <nil>
STEP: delete the pod 01/18/23 22:42:26.633
Jan 18 22:42:26.652: INFO: Waiting for pod pod-configmaps-1ac2d5e5-fa44-4396-8132-9afac5405f06 to disappear
Jan 18 22:42:26.657: INFO: Pod pod-configmaps-1ac2d5e5-fa44-4396-8132-9afac5405f06 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 18 22:42:26.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5201" for this suite. 01/18/23 22:42:26.665
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":261,"skipped":4694,"failed":0}
------------------------------
• [4.197 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:42:22.479
    Jan 18 22:42:22.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename secrets 01/18/23 22:42:22.481
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:42:22.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:42:22.548
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-5201/secret-test-7af79556-1ab9-4025-a1d8-97561322a3e6 01/18/23 22:42:22.567
    STEP: Creating a pod to test consume secrets 01/18/23 22:42:22.578
    Jan 18 22:42:22.590: INFO: Waiting up to 5m0s for pod "pod-configmaps-1ac2d5e5-fa44-4396-8132-9afac5405f06" in namespace "secrets-5201" to be "Succeeded or Failed"
    Jan 18 22:42:22.608: INFO: Pod "pod-configmaps-1ac2d5e5-fa44-4396-8132-9afac5405f06": Phase="Pending", Reason="", readiness=false. Elapsed: 17.862031ms
    Jan 18 22:42:24.618: INFO: Pod "pod-configmaps-1ac2d5e5-fa44-4396-8132-9afac5405f06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028371121s
    Jan 18 22:42:26.617: INFO: Pod "pod-configmaps-1ac2d5e5-fa44-4396-8132-9afac5405f06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027200356s
    STEP: Saw pod success 01/18/23 22:42:26.617
    Jan 18 22:42:26.618: INFO: Pod "pod-configmaps-1ac2d5e5-fa44-4396-8132-9afac5405f06" satisfied condition "Succeeded or Failed"
    Jan 18 22:42:26.622: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-1ac2d5e5-fa44-4396-8132-9afac5405f06 container env-test: <nil>
    STEP: delete the pod 01/18/23 22:42:26.633
    Jan 18 22:42:26.652: INFO: Waiting for pod pod-configmaps-1ac2d5e5-fa44-4396-8132-9afac5405f06 to disappear
    Jan 18 22:42:26.657: INFO: Pod pod-configmaps-1ac2d5e5-fa44-4396-8132-9afac5405f06 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 22:42:26.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5201" for this suite. 01/18/23 22:42:26.665
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:42:26.684
Jan 18 22:42:26.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename secrets 01/18/23 22:42:26.686
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:42:26.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:42:26.712
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-a5ae957d-d759-49d1-a8c1-1520a1a5227f 01/18/23 22:42:26.719
STEP: Creating a pod to test consume secrets 01/18/23 22:42:26.723
Jan 18 22:42:26.731: INFO: Waiting up to 5m0s for pod "pod-secrets-8b70649f-af6a-42c4-8c94-7ee9de657818" in namespace "secrets-9412" to be "Succeeded or Failed"
Jan 18 22:42:26.740: INFO: Pod "pod-secrets-8b70649f-af6a-42c4-8c94-7ee9de657818": Phase="Pending", Reason="", readiness=false. Elapsed: 8.569944ms
Jan 18 22:42:28.746: INFO: Pod "pod-secrets-8b70649f-af6a-42c4-8c94-7ee9de657818": Phase="Running", Reason="", readiness=true. Elapsed: 2.014889988s
Jan 18 22:42:30.754: INFO: Pod "pod-secrets-8b70649f-af6a-42c4-8c94-7ee9de657818": Phase="Running", Reason="", readiness=false. Elapsed: 4.022111044s
Jan 18 22:42:32.747: INFO: Pod "pod-secrets-8b70649f-af6a-42c4-8c94-7ee9de657818": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015093664s
STEP: Saw pod success 01/18/23 22:42:32.747
Jan 18 22:42:32.747: INFO: Pod "pod-secrets-8b70649f-af6a-42c4-8c94-7ee9de657818" satisfied condition "Succeeded or Failed"
Jan 18 22:42:32.751: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-8b70649f-af6a-42c4-8c94-7ee9de657818 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 22:42:32.759
Jan 18 22:42:32.774: INFO: Waiting for pod pod-secrets-8b70649f-af6a-42c4-8c94-7ee9de657818 to disappear
Jan 18 22:42:32.778: INFO: Pod pod-secrets-8b70649f-af6a-42c4-8c94-7ee9de657818 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 18 22:42:32.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9412" for this suite. 01/18/23 22:42:32.79
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":262,"skipped":4711,"failed":0}
------------------------------
• [SLOW TEST] [6.116 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:42:26.684
    Jan 18 22:42:26.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename secrets 01/18/23 22:42:26.686
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:42:26.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:42:26.712
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-a5ae957d-d759-49d1-a8c1-1520a1a5227f 01/18/23 22:42:26.719
    STEP: Creating a pod to test consume secrets 01/18/23 22:42:26.723
    Jan 18 22:42:26.731: INFO: Waiting up to 5m0s for pod "pod-secrets-8b70649f-af6a-42c4-8c94-7ee9de657818" in namespace "secrets-9412" to be "Succeeded or Failed"
    Jan 18 22:42:26.740: INFO: Pod "pod-secrets-8b70649f-af6a-42c4-8c94-7ee9de657818": Phase="Pending", Reason="", readiness=false. Elapsed: 8.569944ms
    Jan 18 22:42:28.746: INFO: Pod "pod-secrets-8b70649f-af6a-42c4-8c94-7ee9de657818": Phase="Running", Reason="", readiness=true. Elapsed: 2.014889988s
    Jan 18 22:42:30.754: INFO: Pod "pod-secrets-8b70649f-af6a-42c4-8c94-7ee9de657818": Phase="Running", Reason="", readiness=false. Elapsed: 4.022111044s
    Jan 18 22:42:32.747: INFO: Pod "pod-secrets-8b70649f-af6a-42c4-8c94-7ee9de657818": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015093664s
    STEP: Saw pod success 01/18/23 22:42:32.747
    Jan 18 22:42:32.747: INFO: Pod "pod-secrets-8b70649f-af6a-42c4-8c94-7ee9de657818" satisfied condition "Succeeded or Failed"
    Jan 18 22:42:32.751: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-8b70649f-af6a-42c4-8c94-7ee9de657818 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 22:42:32.759
    Jan 18 22:42:32.774: INFO: Waiting for pod pod-secrets-8b70649f-af6a-42c4-8c94-7ee9de657818 to disappear
    Jan 18 22:42:32.778: INFO: Pod pod-secrets-8b70649f-af6a-42c4-8c94-7ee9de657818 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 22:42:32.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9412" for this suite. 01/18/23 22:42:32.79
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:42:32.803
Jan 18 22:42:32.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename pods 01/18/23 22:42:32.806
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:42:32.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:42:32.833
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 01/18/23 22:42:32.838
STEP: submitting the pod to kubernetes 01/18/23 22:42:32.839
STEP: verifying QOS class is set on the pod 01/18/23 22:42:32.85
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Jan 18 22:42:32.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7101" for this suite. 01/18/23 22:42:32.88
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":263,"skipped":4711,"failed":0}
------------------------------
• [0.086 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:42:32.803
    Jan 18 22:42:32.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename pods 01/18/23 22:42:32.806
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:42:32.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:42:32.833
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 01/18/23 22:42:32.838
    STEP: submitting the pod to kubernetes 01/18/23 22:42:32.839
    STEP: verifying QOS class is set on the pod 01/18/23 22:42:32.85
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Jan 18 22:42:32.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7101" for this suite. 01/18/23 22:42:32.88
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:42:32.891
Jan 18 22:42:32.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename job 01/18/23 22:42:32.894
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:42:33.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:42:33.142
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 01/18/23 22:42:33.147
STEP: Ensuring job reaches completions 01/18/23 22:42:33.154
STEP: Ensuring pods with index for job exist 01/18/23 22:42:43.161
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 18 22:42:43.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7537" for this suite. 01/18/23 22:42:43.176
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":264,"skipped":4712,"failed":0}
------------------------------
• [SLOW TEST] [10.291 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:42:32.891
    Jan 18 22:42:32.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename job 01/18/23 22:42:32.894
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:42:33.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:42:33.142
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 01/18/23 22:42:33.147
    STEP: Ensuring job reaches completions 01/18/23 22:42:33.154
    STEP: Ensuring pods with index for job exist 01/18/23 22:42:43.161
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 18 22:42:43.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7537" for this suite. 01/18/23 22:42:43.176
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:42:43.188
Jan 18 22:42:43.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename security-context-test 01/18/23 22:42:43.192
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:42:43.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:42:43.219
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Jan 18 22:42:43.251: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-5aa387d7-176a-43cb-98ad-cf30ad088a6a" in namespace "security-context-test-2159" to be "Succeeded or Failed"
Jan 18 22:42:43.264: INFO: Pod "alpine-nnp-false-5aa387d7-176a-43cb-98ad-cf30ad088a6a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.893561ms
Jan 18 22:42:45.269: INFO: Pod "alpine-nnp-false-5aa387d7-176a-43cb-98ad-cf30ad088a6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017935171s
Jan 18 22:42:47.272: INFO: Pod "alpine-nnp-false-5aa387d7-176a-43cb-98ad-cf30ad088a6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020622524s
Jan 18 22:42:47.272: INFO: Pod "alpine-nnp-false-5aa387d7-176a-43cb-98ad-cf30ad088a6a" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 18 22:42:47.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2159" for this suite. 01/18/23 22:42:47.287
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":265,"skipped":4713,"failed":0}
------------------------------
• [4.106 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:42:43.188
    Jan 18 22:42:43.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename security-context-test 01/18/23 22:42:43.192
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:42:43.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:42:43.219
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Jan 18 22:42:43.251: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-5aa387d7-176a-43cb-98ad-cf30ad088a6a" in namespace "security-context-test-2159" to be "Succeeded or Failed"
    Jan 18 22:42:43.264: INFO: Pod "alpine-nnp-false-5aa387d7-176a-43cb-98ad-cf30ad088a6a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.893561ms
    Jan 18 22:42:45.269: INFO: Pod "alpine-nnp-false-5aa387d7-176a-43cb-98ad-cf30ad088a6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017935171s
    Jan 18 22:42:47.272: INFO: Pod "alpine-nnp-false-5aa387d7-176a-43cb-98ad-cf30ad088a6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020622524s
    Jan 18 22:42:47.272: INFO: Pod "alpine-nnp-false-5aa387d7-176a-43cb-98ad-cf30ad088a6a" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 18 22:42:47.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-2159" for this suite. 01/18/23 22:42:47.287
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:42:47.305
Jan 18 22:42:47.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename emptydir 01/18/23 22:42:47.307
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:42:47.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:42:47.335
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/18/23 22:42:47.34
Jan 18 22:42:47.356: INFO: Waiting up to 5m0s for pod "pod-e78c5293-f93c-4cf2-bfa2-ac16f9dba4f0" in namespace "emptydir-8548" to be "Succeeded or Failed"
Jan 18 22:42:47.366: INFO: Pod "pod-e78c5293-f93c-4cf2-bfa2-ac16f9dba4f0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.942972ms
Jan 18 22:42:49.372: INFO: Pod "pod-e78c5293-f93c-4cf2-bfa2-ac16f9dba4f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01599255s
Jan 18 22:42:51.373: INFO: Pod "pod-e78c5293-f93c-4cf2-bfa2-ac16f9dba4f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016261408s
STEP: Saw pod success 01/18/23 22:42:51.373
Jan 18 22:42:51.373: INFO: Pod "pod-e78c5293-f93c-4cf2-bfa2-ac16f9dba4f0" satisfied condition "Succeeded or Failed"
Jan 18 22:42:51.383: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-e78c5293-f93c-4cf2-bfa2-ac16f9dba4f0 container test-container: <nil>
STEP: delete the pod 01/18/23 22:42:51.394
Jan 18 22:42:51.407: INFO: Waiting for pod pod-e78c5293-f93c-4cf2-bfa2-ac16f9dba4f0 to disappear
Jan 18 22:42:51.410: INFO: Pod pod-e78c5293-f93c-4cf2-bfa2-ac16f9dba4f0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 22:42:51.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8548" for this suite. 01/18/23 22:42:51.416
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":266,"skipped":4717,"failed":0}
------------------------------
• [4.118 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:42:47.305
    Jan 18 22:42:47.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename emptydir 01/18/23 22:42:47.307
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:42:47.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:42:47.335
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/18/23 22:42:47.34
    Jan 18 22:42:47.356: INFO: Waiting up to 5m0s for pod "pod-e78c5293-f93c-4cf2-bfa2-ac16f9dba4f0" in namespace "emptydir-8548" to be "Succeeded or Failed"
    Jan 18 22:42:47.366: INFO: Pod "pod-e78c5293-f93c-4cf2-bfa2-ac16f9dba4f0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.942972ms
    Jan 18 22:42:49.372: INFO: Pod "pod-e78c5293-f93c-4cf2-bfa2-ac16f9dba4f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01599255s
    Jan 18 22:42:51.373: INFO: Pod "pod-e78c5293-f93c-4cf2-bfa2-ac16f9dba4f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016261408s
    STEP: Saw pod success 01/18/23 22:42:51.373
    Jan 18 22:42:51.373: INFO: Pod "pod-e78c5293-f93c-4cf2-bfa2-ac16f9dba4f0" satisfied condition "Succeeded or Failed"
    Jan 18 22:42:51.383: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-e78c5293-f93c-4cf2-bfa2-ac16f9dba4f0 container test-container: <nil>
    STEP: delete the pod 01/18/23 22:42:51.394
    Jan 18 22:42:51.407: INFO: Waiting for pod pod-e78c5293-f93c-4cf2-bfa2-ac16f9dba4f0 to disappear
    Jan 18 22:42:51.410: INFO: Pod pod-e78c5293-f93c-4cf2-bfa2-ac16f9dba4f0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 22:42:51.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8548" for this suite. 01/18/23 22:42:51.416
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:42:51.431
Jan 18 22:42:51.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename dns 01/18/23 22:42:51.432
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:42:51.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:42:51.456
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 01/18/23 22:42:51.461
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4241 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4241;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4241 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4241;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4241.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4241.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4241.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4241.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4241.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4241.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4241.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4241.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4241.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4241.svc;check="$$(dig +notcp +noall +answer +search 118.59.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.59.118_udp@PTR;check="$$(dig +tcp +noall +answer +search 118.59.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.59.118_tcp@PTR;sleep 1; done
 01/18/23 22:42:51.485
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4241 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4241;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4241 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4241;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4241.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4241.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4241.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4241.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4241.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4241.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4241.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4241.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4241.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4241.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4241.svc;check="$$(dig +notcp +noall +answer +search 118.59.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.59.118_udp@PTR;check="$$(dig +tcp +noall +answer +search 118.59.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.59.118_tcp@PTR;sleep 1; done
 01/18/23 22:42:51.486
STEP: creating a pod to probe DNS 01/18/23 22:42:51.487
STEP: submitting the pod to kubernetes 01/18/23 22:42:51.487
Jan 18 22:42:51.502: INFO: Waiting up to 15m0s for pod "dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d" in namespace "dns-4241" to be "running"
Jan 18 22:42:51.513: INFO: Pod "dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.963109ms
Jan 18 22:42:53.520: INFO: Pod "dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d": Phase="Running", Reason="", readiness=true. Elapsed: 2.018150959s
Jan 18 22:42:53.520: INFO: Pod "dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d" satisfied condition "running"
STEP: retrieving the pod 01/18/23 22:42:53.52
STEP: looking for the results for each expected name from probers 01/18/23 22:42:53.526
Jan 18 22:42:53.532: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:53.537: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:53.547: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:53.553: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:53.559: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:53.565: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:53.570: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:53.577: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:53.605: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:53.611: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:53.616: INFO: Unable to read jessie_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:53.620: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:53.624: INFO: Unable to read jessie_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:53.629: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:53.634: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:53.646: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:53.672: INFO: Lookups using dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4241 wheezy_tcp@dns-test-service.dns-4241 wheezy_udp@dns-test-service.dns-4241.svc wheezy_tcp@dns-test-service.dns-4241.svc wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4241 jessie_tcp@dns-test-service.dns-4241 jessie_udp@dns-test-service.dns-4241.svc jessie_tcp@dns-test-service.dns-4241.svc jessie_udp@_http._tcp.dns-test-service.dns-4241.svc jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc]

Jan 18 22:42:58.680: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:58.695: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:58.716: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:58.727: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:58.733: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:58.740: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:58.747: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:58.754: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:58.808: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:58.820: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:58.827: INFO: Unable to read jessie_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:58.839: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:58.845: INFO: Unable to read jessie_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:58.851: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:58.857: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:58.865: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:42:58.894: INFO: Lookups using dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4241 wheezy_tcp@dns-test-service.dns-4241 wheezy_udp@dns-test-service.dns-4241.svc wheezy_tcp@dns-test-service.dns-4241.svc wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4241 jessie_tcp@dns-test-service.dns-4241 jessie_udp@dns-test-service.dns-4241.svc jessie_tcp@dns-test-service.dns-4241.svc jessie_udp@_http._tcp.dns-test-service.dns-4241.svc jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc]

Jan 18 22:43:03.680: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:03.691: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:03.700: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:03.711: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:03.718: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:03.723: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:03.728: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:03.737: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:03.767: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:03.772: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:03.778: INFO: Unable to read jessie_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:03.792: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:03.800: INFO: Unable to read jessie_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:03.813: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:03.823: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:03.838: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:03.888: INFO: Lookups using dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4241 wheezy_tcp@dns-test-service.dns-4241 wheezy_udp@dns-test-service.dns-4241.svc wheezy_tcp@dns-test-service.dns-4241.svc wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4241 jessie_tcp@dns-test-service.dns-4241 jessie_udp@dns-test-service.dns-4241.svc jessie_tcp@dns-test-service.dns-4241.svc jessie_udp@_http._tcp.dns-test-service.dns-4241.svc jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc]

Jan 18 22:43:08.679: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:08.684: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:08.694: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:08.720: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:08.732: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:08.787: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:08.793: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:08.800: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:08.855: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:08.863: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:08.871: INFO: Unable to read jessie_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:08.880: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:08.885: INFO: Unable to read jessie_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:08.892: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:08.903: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:08.909: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:08.930: INFO: Lookups using dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4241 wheezy_tcp@dns-test-service.dns-4241 wheezy_udp@dns-test-service.dns-4241.svc wheezy_tcp@dns-test-service.dns-4241.svc wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4241 jessie_tcp@dns-test-service.dns-4241 jessie_udp@dns-test-service.dns-4241.svc jessie_tcp@dns-test-service.dns-4241.svc jessie_udp@_http._tcp.dns-test-service.dns-4241.svc jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc]

Jan 18 22:43:13.680: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:13.686: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:13.692: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:13.697: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:13.702: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:13.707: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:13.715: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:13.722: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:13.762: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:13.766: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:13.771: INFO: Unable to read jessie_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:13.777: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:13.783: INFO: Unable to read jessie_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:13.789: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:13.795: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:13.801: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:13.839: INFO: Lookups using dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4241 wheezy_tcp@dns-test-service.dns-4241 wheezy_udp@dns-test-service.dns-4241.svc wheezy_tcp@dns-test-service.dns-4241.svc wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4241 jessie_tcp@dns-test-service.dns-4241 jessie_udp@dns-test-service.dns-4241.svc jessie_tcp@dns-test-service.dns-4241.svc jessie_udp@_http._tcp.dns-test-service.dns-4241.svc jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc]

Jan 18 22:43:18.681: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:18.688: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:18.694: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:18.701: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:18.708: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:18.713: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:18.718: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:18.723: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:18.759: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:18.765: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:18.782: INFO: Unable to read jessie_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:18.793: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:18.800: INFO: Unable to read jessie_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:18.815: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:18.821: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:18.826: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:18.849: INFO: Lookups using dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4241 wheezy_tcp@dns-test-service.dns-4241 wheezy_udp@dns-test-service.dns-4241.svc wheezy_tcp@dns-test-service.dns-4241.svc wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4241 jessie_tcp@dns-test-service.dns-4241 jessie_udp@dns-test-service.dns-4241.svc jessie_tcp@dns-test-service.dns-4241.svc jessie_udp@_http._tcp.dns-test-service.dns-4241.svc jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc]

Jan 18 22:43:23.690: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:23.698: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:23.704: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:23.709: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:23.718: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:23.725: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:23.732: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:23.738: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:23.778: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:23.784: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:23.795: INFO: Unable to read jessie_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:23.800: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:23.809: INFO: Unable to read jessie_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:23.815: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:23.824: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:23.844: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
Jan 18 22:43:23.875: INFO: Lookups using dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4241 wheezy_tcp@dns-test-service.dns-4241 wheezy_udp@dns-test-service.dns-4241.svc wheezy_tcp@dns-test-service.dns-4241.svc wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4241 jessie_tcp@dns-test-service.dns-4241 jessie_udp@dns-test-service.dns-4241.svc jessie_tcp@dns-test-service.dns-4241.svc jessie_udp@_http._tcp.dns-test-service.dns-4241.svc jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc]

Jan 18 22:43:28.844: INFO: DNS probes using dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d succeeded

STEP: deleting the pod 01/18/23 22:43:28.844
STEP: deleting the test service 01/18/23 22:43:28.909
STEP: deleting the test headless service 01/18/23 22:43:28.938
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 18 22:43:28.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4241" for this suite. 01/18/23 22:43:28.984
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":267,"skipped":4739,"failed":0}
------------------------------
• [SLOW TEST] [37.568 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:42:51.431
    Jan 18 22:42:51.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename dns 01/18/23 22:42:51.432
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:42:51.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:42:51.456
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 01/18/23 22:42:51.461
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4241 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4241;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4241 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4241;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4241.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4241.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4241.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4241.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4241.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4241.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4241.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4241.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4241.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4241.svc;check="$$(dig +notcp +noall +answer +search 118.59.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.59.118_udp@PTR;check="$$(dig +tcp +noall +answer +search 118.59.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.59.118_tcp@PTR;sleep 1; done
     01/18/23 22:42:51.485
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4241 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4241;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4241 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4241;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4241.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4241.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4241.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4241.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4241.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4241.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4241.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4241.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4241.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4241.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4241.svc;check="$$(dig +notcp +noall +answer +search 118.59.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.59.118_udp@PTR;check="$$(dig +tcp +noall +answer +search 118.59.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.59.118_tcp@PTR;sleep 1; done
     01/18/23 22:42:51.486
    STEP: creating a pod to probe DNS 01/18/23 22:42:51.487
    STEP: submitting the pod to kubernetes 01/18/23 22:42:51.487
    Jan 18 22:42:51.502: INFO: Waiting up to 15m0s for pod "dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d" in namespace "dns-4241" to be "running"
    Jan 18 22:42:51.513: INFO: Pod "dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.963109ms
    Jan 18 22:42:53.520: INFO: Pod "dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d": Phase="Running", Reason="", readiness=true. Elapsed: 2.018150959s
    Jan 18 22:42:53.520: INFO: Pod "dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 22:42:53.52
    STEP: looking for the results for each expected name from probers 01/18/23 22:42:53.526
    Jan 18 22:42:53.532: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:53.537: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:53.547: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:53.553: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:53.559: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:53.565: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:53.570: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:53.577: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:53.605: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:53.611: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:53.616: INFO: Unable to read jessie_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:53.620: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:53.624: INFO: Unable to read jessie_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:53.629: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:53.634: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:53.646: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:53.672: INFO: Lookups using dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4241 wheezy_tcp@dns-test-service.dns-4241 wheezy_udp@dns-test-service.dns-4241.svc wheezy_tcp@dns-test-service.dns-4241.svc wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4241 jessie_tcp@dns-test-service.dns-4241 jessie_udp@dns-test-service.dns-4241.svc jessie_tcp@dns-test-service.dns-4241.svc jessie_udp@_http._tcp.dns-test-service.dns-4241.svc jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc]

    Jan 18 22:42:58.680: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:58.695: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:58.716: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:58.727: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:58.733: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:58.740: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:58.747: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:58.754: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:58.808: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:58.820: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:58.827: INFO: Unable to read jessie_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:58.839: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:58.845: INFO: Unable to read jessie_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:58.851: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:58.857: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:58.865: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:42:58.894: INFO: Lookups using dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4241 wheezy_tcp@dns-test-service.dns-4241 wheezy_udp@dns-test-service.dns-4241.svc wheezy_tcp@dns-test-service.dns-4241.svc wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4241 jessie_tcp@dns-test-service.dns-4241 jessie_udp@dns-test-service.dns-4241.svc jessie_tcp@dns-test-service.dns-4241.svc jessie_udp@_http._tcp.dns-test-service.dns-4241.svc jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc]

    Jan 18 22:43:03.680: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:03.691: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:03.700: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:03.711: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:03.718: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:03.723: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:03.728: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:03.737: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:03.767: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:03.772: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:03.778: INFO: Unable to read jessie_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:03.792: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:03.800: INFO: Unable to read jessie_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:03.813: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:03.823: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:03.838: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:03.888: INFO: Lookups using dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4241 wheezy_tcp@dns-test-service.dns-4241 wheezy_udp@dns-test-service.dns-4241.svc wheezy_tcp@dns-test-service.dns-4241.svc wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4241 jessie_tcp@dns-test-service.dns-4241 jessie_udp@dns-test-service.dns-4241.svc jessie_tcp@dns-test-service.dns-4241.svc jessie_udp@_http._tcp.dns-test-service.dns-4241.svc jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc]

    Jan 18 22:43:08.679: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:08.684: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:08.694: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:08.720: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:08.732: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:08.787: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:08.793: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:08.800: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:08.855: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:08.863: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:08.871: INFO: Unable to read jessie_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:08.880: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:08.885: INFO: Unable to read jessie_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:08.892: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:08.903: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:08.909: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:08.930: INFO: Lookups using dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4241 wheezy_tcp@dns-test-service.dns-4241 wheezy_udp@dns-test-service.dns-4241.svc wheezy_tcp@dns-test-service.dns-4241.svc wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4241 jessie_tcp@dns-test-service.dns-4241 jessie_udp@dns-test-service.dns-4241.svc jessie_tcp@dns-test-service.dns-4241.svc jessie_udp@_http._tcp.dns-test-service.dns-4241.svc jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc]

    Jan 18 22:43:13.680: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:13.686: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:13.692: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:13.697: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:13.702: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:13.707: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:13.715: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:13.722: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:13.762: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:13.766: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:13.771: INFO: Unable to read jessie_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:13.777: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:13.783: INFO: Unable to read jessie_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:13.789: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:13.795: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:13.801: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:13.839: INFO: Lookups using dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4241 wheezy_tcp@dns-test-service.dns-4241 wheezy_udp@dns-test-service.dns-4241.svc wheezy_tcp@dns-test-service.dns-4241.svc wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4241 jessie_tcp@dns-test-service.dns-4241 jessie_udp@dns-test-service.dns-4241.svc jessie_tcp@dns-test-service.dns-4241.svc jessie_udp@_http._tcp.dns-test-service.dns-4241.svc jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc]

    Jan 18 22:43:18.681: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:18.688: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:18.694: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:18.701: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:18.708: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:18.713: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:18.718: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:18.723: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:18.759: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:18.765: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:18.782: INFO: Unable to read jessie_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:18.793: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:18.800: INFO: Unable to read jessie_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:18.815: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:18.821: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:18.826: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:18.849: INFO: Lookups using dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4241 wheezy_tcp@dns-test-service.dns-4241 wheezy_udp@dns-test-service.dns-4241.svc wheezy_tcp@dns-test-service.dns-4241.svc wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4241 jessie_tcp@dns-test-service.dns-4241 jessie_udp@dns-test-service.dns-4241.svc jessie_tcp@dns-test-service.dns-4241.svc jessie_udp@_http._tcp.dns-test-service.dns-4241.svc jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc]

    Jan 18 22:43:23.690: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:23.698: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:23.704: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:23.709: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:23.718: INFO: Unable to read wheezy_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:23.725: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:23.732: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:23.738: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:23.778: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:23.784: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:23.795: INFO: Unable to read jessie_udp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:23.800: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241 from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:23.809: INFO: Unable to read jessie_udp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:23.815: INFO: Unable to read jessie_tcp@dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:23.824: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:23.844: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc from pod dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d: the server could not find the requested resource (get pods dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d)
    Jan 18 22:43:23.875: INFO: Lookups using dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4241 wheezy_tcp@dns-test-service.dns-4241 wheezy_udp@dns-test-service.dns-4241.svc wheezy_tcp@dns-test-service.dns-4241.svc wheezy_udp@_http._tcp.dns-test-service.dns-4241.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4241.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4241 jessie_tcp@dns-test-service.dns-4241 jessie_udp@dns-test-service.dns-4241.svc jessie_tcp@dns-test-service.dns-4241.svc jessie_udp@_http._tcp.dns-test-service.dns-4241.svc jessie_tcp@_http._tcp.dns-test-service.dns-4241.svc]

    Jan 18 22:43:28.844: INFO: DNS probes using dns-4241/dns-test-b3f7d3f4-300b-461d-aae1-f916d6ffb23d succeeded

    STEP: deleting the pod 01/18/23 22:43:28.844
    STEP: deleting the test service 01/18/23 22:43:28.909
    STEP: deleting the test headless service 01/18/23 22:43:28.938
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 18 22:43:28.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4241" for this suite. 01/18/23 22:43:28.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:43:29.004
Jan 18 22:43:29.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename webhook 01/18/23 22:43:29.007
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:43:29.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:43:29.069
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 22:43:29.108
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:43:30.228
STEP: Deploying the webhook pod 01/18/23 22:43:30.238
STEP: Wait for the deployment to be ready 01/18/23 22:43:30.256
Jan 18 22:43:30.276: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 22:43:32.291
STEP: Verifying the service has paired with the endpoint 01/18/23 22:43:32.305
Jan 18 22:43:33.305: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/18/23 22:43:33.31
STEP: create a namespace for the webhook 01/18/23 22:43:33.333
STEP: create a configmap should be unconditionally rejected by the webhook 01/18/23 22:43:33.344
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 22:43:33.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7067" for this suite. 01/18/23 22:43:33.404
STEP: Destroying namespace "webhook-7067-markers" for this suite. 01/18/23 22:43:33.411
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":268,"skipped":4760,"failed":0}
------------------------------
• [4.466 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:43:29.004
    Jan 18 22:43:29.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename webhook 01/18/23 22:43:29.007
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:43:29.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:43:29.069
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 22:43:29.108
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:43:30.228
    STEP: Deploying the webhook pod 01/18/23 22:43:30.238
    STEP: Wait for the deployment to be ready 01/18/23 22:43:30.256
    Jan 18 22:43:30.276: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 22:43:32.291
    STEP: Verifying the service has paired with the endpoint 01/18/23 22:43:32.305
    Jan 18 22:43:33.305: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/18/23 22:43:33.31
    STEP: create a namespace for the webhook 01/18/23 22:43:33.333
    STEP: create a configmap should be unconditionally rejected by the webhook 01/18/23 22:43:33.344
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 22:43:33.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7067" for this suite. 01/18/23 22:43:33.404
    STEP: Destroying namespace "webhook-7067-markers" for this suite. 01/18/23 22:43:33.411
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:43:33.471
Jan 18 22:43:33.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename secrets 01/18/23 22:43:33.475
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:43:33.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:43:33.534
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-7622c0e9-6835-4e8c-a10b-aaae3744a51f 01/18/23 22:43:33.546
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 18 22:43:33.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2436" for this suite. 01/18/23 22:43:33.554
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":269,"skipped":4767,"failed":0}
------------------------------
• [0.109 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:43:33.471
    Jan 18 22:43:33.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename secrets 01/18/23 22:43:33.475
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:43:33.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:43:33.534
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-7622c0e9-6835-4e8c-a10b-aaae3744a51f 01/18/23 22:43:33.546
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 22:43:33.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2436" for this suite. 01/18/23 22:43:33.554
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:43:33.581
Jan 18 22:43:33.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename services 01/18/23 22:43:33.583
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:43:33.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:43:33.615
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-2214 01/18/23 22:43:33.625
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2214 to expose endpoints map[] 01/18/23 22:43:33.645
Jan 18 22:43:33.651: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jan 18 22:43:34.664: INFO: successfully validated that service multi-endpoint-test in namespace services-2214 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2214 01/18/23 22:43:34.664
Jan 18 22:43:34.679: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2214" to be "running and ready"
Jan 18 22:43:34.693: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.664445ms
Jan 18 22:43:34.693: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:43:36.699: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.020107728s
Jan 18 22:43:36.699: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 18 22:43:36.699: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2214 to expose endpoints map[pod1:[100]] 01/18/23 22:43:36.704
Jan 18 22:43:36.716: INFO: successfully validated that service multi-endpoint-test in namespace services-2214 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-2214 01/18/23 22:43:36.717
Jan 18 22:43:36.722: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2214" to be "running and ready"
Jan 18 22:43:36.731: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.848978ms
Jan 18 22:43:36.731: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:43:38.737: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014941665s
Jan 18 22:43:38.737: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 18 22:43:38.737: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2214 to expose endpoints map[pod1:[100] pod2:[101]] 01/18/23 22:43:38.746
Jan 18 22:43:38.780: INFO: successfully validated that service multi-endpoint-test in namespace services-2214 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 01/18/23 22:43:38.78
Jan 18 22:43:38.781: INFO: Creating new exec pod
Jan 18 22:43:38.841: INFO: Waiting up to 5m0s for pod "execpodk49g2" in namespace "services-2214" to be "running"
Jan 18 22:43:38.859: INFO: Pod "execpodk49g2": Phase="Pending", Reason="", readiness=false. Elapsed: 17.517713ms
Jan 18 22:43:40.865: INFO: Pod "execpodk49g2": Phase="Running", Reason="", readiness=true. Elapsed: 2.023486018s
Jan 18 22:43:40.865: INFO: Pod "execpodk49g2" satisfied condition "running"
Jan 18 22:43:41.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-2214 exec execpodk49g2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jan 18 22:43:42.080: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 80\n+ echo hostName\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan 18 22:43:42.080: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 22:43:42.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-2214 exec execpodk49g2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.20.11 80'
Jan 18 22:43:42.281: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.20.11 80\nConnection to 10.233.20.11 80 port [tcp/http] succeeded!\n"
Jan 18 22:43:42.281: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 22:43:42.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-2214 exec execpodk49g2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jan 18 22:43:42.511: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan 18 22:43:42.511: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 22:43:42.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-2214 exec execpodk49g2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.20.11 81'
Jan 18 22:43:42.702: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.20.11 81\nConnection to 10.233.20.11 81 port [tcp/*] succeeded!\n"
Jan 18 22:43:42.702: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-2214 01/18/23 22:43:42.702
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2214 to expose endpoints map[pod2:[101]] 01/18/23 22:43:42.725
Jan 18 22:43:42.750: INFO: successfully validated that service multi-endpoint-test in namespace services-2214 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-2214 01/18/23 22:43:42.75
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2214 to expose endpoints map[] 01/18/23 22:43:42.785
Jan 18 22:43:42.821: INFO: successfully validated that service multi-endpoint-test in namespace services-2214 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 22:43:42.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2214" for this suite. 01/18/23 22:43:42.86
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":270,"skipped":4779,"failed":0}
------------------------------
• [SLOW TEST] [9.286 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:43:33.581
    Jan 18 22:43:33.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename services 01/18/23 22:43:33.583
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:43:33.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:43:33.615
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-2214 01/18/23 22:43:33.625
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2214 to expose endpoints map[] 01/18/23 22:43:33.645
    Jan 18 22:43:33.651: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Jan 18 22:43:34.664: INFO: successfully validated that service multi-endpoint-test in namespace services-2214 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-2214 01/18/23 22:43:34.664
    Jan 18 22:43:34.679: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2214" to be "running and ready"
    Jan 18 22:43:34.693: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.664445ms
    Jan 18 22:43:34.693: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:43:36.699: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.020107728s
    Jan 18 22:43:36.699: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 18 22:43:36.699: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2214 to expose endpoints map[pod1:[100]] 01/18/23 22:43:36.704
    Jan 18 22:43:36.716: INFO: successfully validated that service multi-endpoint-test in namespace services-2214 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-2214 01/18/23 22:43:36.717
    Jan 18 22:43:36.722: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2214" to be "running and ready"
    Jan 18 22:43:36.731: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.848978ms
    Jan 18 22:43:36.731: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:43:38.737: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014941665s
    Jan 18 22:43:38.737: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 18 22:43:38.737: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2214 to expose endpoints map[pod1:[100] pod2:[101]] 01/18/23 22:43:38.746
    Jan 18 22:43:38.780: INFO: successfully validated that service multi-endpoint-test in namespace services-2214 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 01/18/23 22:43:38.78
    Jan 18 22:43:38.781: INFO: Creating new exec pod
    Jan 18 22:43:38.841: INFO: Waiting up to 5m0s for pod "execpodk49g2" in namespace "services-2214" to be "running"
    Jan 18 22:43:38.859: INFO: Pod "execpodk49g2": Phase="Pending", Reason="", readiness=false. Elapsed: 17.517713ms
    Jan 18 22:43:40.865: INFO: Pod "execpodk49g2": Phase="Running", Reason="", readiness=true. Elapsed: 2.023486018s
    Jan 18 22:43:40.865: INFO: Pod "execpodk49g2" satisfied condition "running"
    Jan 18 22:43:41.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-2214 exec execpodk49g2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Jan 18 22:43:42.080: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 80\n+ echo hostName\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jan 18 22:43:42.080: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 22:43:42.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-2214 exec execpodk49g2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.20.11 80'
    Jan 18 22:43:42.281: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.20.11 80\nConnection to 10.233.20.11 80 port [tcp/http] succeeded!\n"
    Jan 18 22:43:42.281: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 22:43:42.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-2214 exec execpodk49g2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Jan 18 22:43:42.511: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jan 18 22:43:42.511: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 22:43:42.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-2214 exec execpodk49g2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.20.11 81'
    Jan 18 22:43:42.702: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.20.11 81\nConnection to 10.233.20.11 81 port [tcp/*] succeeded!\n"
    Jan 18 22:43:42.702: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-2214 01/18/23 22:43:42.702
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2214 to expose endpoints map[pod2:[101]] 01/18/23 22:43:42.725
    Jan 18 22:43:42.750: INFO: successfully validated that service multi-endpoint-test in namespace services-2214 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-2214 01/18/23 22:43:42.75
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2214 to expose endpoints map[] 01/18/23 22:43:42.785
    Jan 18 22:43:42.821: INFO: successfully validated that service multi-endpoint-test in namespace services-2214 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 22:43:42.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2214" for this suite. 01/18/23 22:43:42.86
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:43:42.875
Jan 18 22:43:42.875: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename discovery 01/18/23 22:43:42.877
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:43:42.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:43:42.905
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 01/18/23 22:43:42.935
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jan 18 22:43:43.850: INFO: Checking APIGroup: apiregistration.k8s.io
Jan 18 22:43:43.853: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan 18 22:43:43.854: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan 18 22:43:43.854: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan 18 22:43:43.854: INFO: Checking APIGroup: apps
Jan 18 22:43:43.857: INFO: PreferredVersion.GroupVersion: apps/v1
Jan 18 22:43:43.857: INFO: Versions found [{apps/v1 v1}]
Jan 18 22:43:43.857: INFO: apps/v1 matches apps/v1
Jan 18 22:43:43.857: INFO: Checking APIGroup: events.k8s.io
Jan 18 22:43:43.860: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan 18 22:43:43.860: INFO: Versions found [{events.k8s.io/v1 v1}]
Jan 18 22:43:43.860: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan 18 22:43:43.860: INFO: Checking APIGroup: authentication.k8s.io
Jan 18 22:43:43.862: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan 18 22:43:43.863: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan 18 22:43:43.863: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan 18 22:43:43.863: INFO: Checking APIGroup: authorization.k8s.io
Jan 18 22:43:43.865: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan 18 22:43:43.865: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan 18 22:43:43.865: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan 18 22:43:43.865: INFO: Checking APIGroup: autoscaling
Jan 18 22:43:43.867: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan 18 22:43:43.867: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Jan 18 22:43:43.867: INFO: autoscaling/v2 matches autoscaling/v2
Jan 18 22:43:43.867: INFO: Checking APIGroup: batch
Jan 18 22:43:43.869: INFO: PreferredVersion.GroupVersion: batch/v1
Jan 18 22:43:43.869: INFO: Versions found [{batch/v1 v1}]
Jan 18 22:43:43.869: INFO: batch/v1 matches batch/v1
Jan 18 22:43:43.869: INFO: Checking APIGroup: certificates.k8s.io
Jan 18 22:43:43.871: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan 18 22:43:43.871: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan 18 22:43:43.871: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan 18 22:43:43.872: INFO: Checking APIGroup: networking.k8s.io
Jan 18 22:43:43.874: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan 18 22:43:43.874: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jan 18 22:43:43.874: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan 18 22:43:43.874: INFO: Checking APIGroup: policy
Jan 18 22:43:43.879: INFO: PreferredVersion.GroupVersion: policy/v1
Jan 18 22:43:43.879: INFO: Versions found [{policy/v1 v1}]
Jan 18 22:43:43.879: INFO: policy/v1 matches policy/v1
Jan 18 22:43:43.879: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan 18 22:43:43.881: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan 18 22:43:43.881: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan 18 22:43:43.881: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan 18 22:43:43.881: INFO: Checking APIGroup: storage.k8s.io
Jan 18 22:43:43.883: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan 18 22:43:43.883: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan 18 22:43:43.883: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan 18 22:43:43.884: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan 18 22:43:43.886: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan 18 22:43:43.886: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan 18 22:43:43.886: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan 18 22:43:43.886: INFO: Checking APIGroup: apiextensions.k8s.io
Jan 18 22:43:43.888: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan 18 22:43:43.888: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan 18 22:43:43.889: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan 18 22:43:43.889: INFO: Checking APIGroup: scheduling.k8s.io
Jan 18 22:43:43.890: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan 18 22:43:43.891: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan 18 22:43:43.891: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan 18 22:43:43.891: INFO: Checking APIGroup: coordination.k8s.io
Jan 18 22:43:43.896: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan 18 22:43:43.897: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan 18 22:43:43.897: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan 18 22:43:43.897: INFO: Checking APIGroup: node.k8s.io
Jan 18 22:43:43.899: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan 18 22:43:43.900: INFO: Versions found [{node.k8s.io/v1 v1}]
Jan 18 22:43:43.900: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan 18 22:43:43.901: INFO: Checking APIGroup: discovery.k8s.io
Jan 18 22:43:43.903: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan 18 22:43:43.904: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jan 18 22:43:43.904: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan 18 22:43:43.904: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan 18 22:43:43.907: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Jan 18 22:43:43.907: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jan 18 22:43:43.907: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Jan 18 22:43:43.907: INFO: Checking APIGroup: crd.projectcalico.org
Jan 18 22:43:43.909: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jan 18 22:43:43.909: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jan 18 22:43:43.909: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jan 18 22:43:43.909: INFO: Checking APIGroup: notification.toolkit.fluxcd.io
Jan 18 22:43:43.911: INFO: PreferredVersion.GroupVersion: notification.toolkit.fluxcd.io/v1beta2
Jan 18 22:43:43.911: INFO: Versions found [{notification.toolkit.fluxcd.io/v1beta2 v1beta2} {notification.toolkit.fluxcd.io/v1beta1 v1beta1}]
Jan 18 22:43:43.911: INFO: notification.toolkit.fluxcd.io/v1beta2 matches notification.toolkit.fluxcd.io/v1beta2
Jan 18 22:43:43.911: INFO: Checking APIGroup: source.toolkit.fluxcd.io
Jan 18 22:43:43.913: INFO: PreferredVersion.GroupVersion: source.toolkit.fluxcd.io/v1beta2
Jan 18 22:43:43.913: INFO: Versions found [{source.toolkit.fluxcd.io/v1beta2 v1beta2} {source.toolkit.fluxcd.io/v1beta1 v1beta1}]
Jan 18 22:43:43.913: INFO: source.toolkit.fluxcd.io/v1beta2 matches source.toolkit.fluxcd.io/v1beta2
Jan 18 22:43:43.913: INFO: Checking APIGroup: helm.toolkit.fluxcd.io
Jan 18 22:43:43.915: INFO: PreferredVersion.GroupVersion: helm.toolkit.fluxcd.io/v2beta1
Jan 18 22:43:43.915: INFO: Versions found [{helm.toolkit.fluxcd.io/v2beta1 v2beta1}]
Jan 18 22:43:43.915: INFO: helm.toolkit.fluxcd.io/v2beta1 matches helm.toolkit.fluxcd.io/v2beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Jan 18 22:43:43.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-3841" for this suite. 01/18/23 22:43:43.921
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":271,"skipped":4824,"failed":0}
------------------------------
• [1.053 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:43:42.875
    Jan 18 22:43:42.875: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename discovery 01/18/23 22:43:42.877
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:43:42.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:43:42.905
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 01/18/23 22:43:42.935
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jan 18 22:43:43.850: INFO: Checking APIGroup: apiregistration.k8s.io
    Jan 18 22:43:43.853: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jan 18 22:43:43.854: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jan 18 22:43:43.854: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jan 18 22:43:43.854: INFO: Checking APIGroup: apps
    Jan 18 22:43:43.857: INFO: PreferredVersion.GroupVersion: apps/v1
    Jan 18 22:43:43.857: INFO: Versions found [{apps/v1 v1}]
    Jan 18 22:43:43.857: INFO: apps/v1 matches apps/v1
    Jan 18 22:43:43.857: INFO: Checking APIGroup: events.k8s.io
    Jan 18 22:43:43.860: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jan 18 22:43:43.860: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jan 18 22:43:43.860: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jan 18 22:43:43.860: INFO: Checking APIGroup: authentication.k8s.io
    Jan 18 22:43:43.862: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jan 18 22:43:43.863: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jan 18 22:43:43.863: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jan 18 22:43:43.863: INFO: Checking APIGroup: authorization.k8s.io
    Jan 18 22:43:43.865: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jan 18 22:43:43.865: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jan 18 22:43:43.865: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jan 18 22:43:43.865: INFO: Checking APIGroup: autoscaling
    Jan 18 22:43:43.867: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jan 18 22:43:43.867: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Jan 18 22:43:43.867: INFO: autoscaling/v2 matches autoscaling/v2
    Jan 18 22:43:43.867: INFO: Checking APIGroup: batch
    Jan 18 22:43:43.869: INFO: PreferredVersion.GroupVersion: batch/v1
    Jan 18 22:43:43.869: INFO: Versions found [{batch/v1 v1}]
    Jan 18 22:43:43.869: INFO: batch/v1 matches batch/v1
    Jan 18 22:43:43.869: INFO: Checking APIGroup: certificates.k8s.io
    Jan 18 22:43:43.871: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jan 18 22:43:43.871: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jan 18 22:43:43.871: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jan 18 22:43:43.872: INFO: Checking APIGroup: networking.k8s.io
    Jan 18 22:43:43.874: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jan 18 22:43:43.874: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jan 18 22:43:43.874: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jan 18 22:43:43.874: INFO: Checking APIGroup: policy
    Jan 18 22:43:43.879: INFO: PreferredVersion.GroupVersion: policy/v1
    Jan 18 22:43:43.879: INFO: Versions found [{policy/v1 v1}]
    Jan 18 22:43:43.879: INFO: policy/v1 matches policy/v1
    Jan 18 22:43:43.879: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jan 18 22:43:43.881: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jan 18 22:43:43.881: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jan 18 22:43:43.881: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jan 18 22:43:43.881: INFO: Checking APIGroup: storage.k8s.io
    Jan 18 22:43:43.883: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jan 18 22:43:43.883: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jan 18 22:43:43.883: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jan 18 22:43:43.884: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jan 18 22:43:43.886: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jan 18 22:43:43.886: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jan 18 22:43:43.886: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jan 18 22:43:43.886: INFO: Checking APIGroup: apiextensions.k8s.io
    Jan 18 22:43:43.888: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jan 18 22:43:43.888: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jan 18 22:43:43.889: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jan 18 22:43:43.889: INFO: Checking APIGroup: scheduling.k8s.io
    Jan 18 22:43:43.890: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jan 18 22:43:43.891: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jan 18 22:43:43.891: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jan 18 22:43:43.891: INFO: Checking APIGroup: coordination.k8s.io
    Jan 18 22:43:43.896: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jan 18 22:43:43.897: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jan 18 22:43:43.897: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jan 18 22:43:43.897: INFO: Checking APIGroup: node.k8s.io
    Jan 18 22:43:43.899: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jan 18 22:43:43.900: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jan 18 22:43:43.900: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jan 18 22:43:43.901: INFO: Checking APIGroup: discovery.k8s.io
    Jan 18 22:43:43.903: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jan 18 22:43:43.904: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jan 18 22:43:43.904: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jan 18 22:43:43.904: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jan 18 22:43:43.907: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Jan 18 22:43:43.907: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Jan 18 22:43:43.907: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Jan 18 22:43:43.907: INFO: Checking APIGroup: crd.projectcalico.org
    Jan 18 22:43:43.909: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Jan 18 22:43:43.909: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Jan 18 22:43:43.909: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Jan 18 22:43:43.909: INFO: Checking APIGroup: notification.toolkit.fluxcd.io
    Jan 18 22:43:43.911: INFO: PreferredVersion.GroupVersion: notification.toolkit.fluxcd.io/v1beta2
    Jan 18 22:43:43.911: INFO: Versions found [{notification.toolkit.fluxcd.io/v1beta2 v1beta2} {notification.toolkit.fluxcd.io/v1beta1 v1beta1}]
    Jan 18 22:43:43.911: INFO: notification.toolkit.fluxcd.io/v1beta2 matches notification.toolkit.fluxcd.io/v1beta2
    Jan 18 22:43:43.911: INFO: Checking APIGroup: source.toolkit.fluxcd.io
    Jan 18 22:43:43.913: INFO: PreferredVersion.GroupVersion: source.toolkit.fluxcd.io/v1beta2
    Jan 18 22:43:43.913: INFO: Versions found [{source.toolkit.fluxcd.io/v1beta2 v1beta2} {source.toolkit.fluxcd.io/v1beta1 v1beta1}]
    Jan 18 22:43:43.913: INFO: source.toolkit.fluxcd.io/v1beta2 matches source.toolkit.fluxcd.io/v1beta2
    Jan 18 22:43:43.913: INFO: Checking APIGroup: helm.toolkit.fluxcd.io
    Jan 18 22:43:43.915: INFO: PreferredVersion.GroupVersion: helm.toolkit.fluxcd.io/v2beta1
    Jan 18 22:43:43.915: INFO: Versions found [{helm.toolkit.fluxcd.io/v2beta1 v2beta1}]
    Jan 18 22:43:43.915: INFO: helm.toolkit.fluxcd.io/v2beta1 matches helm.toolkit.fluxcd.io/v2beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Jan 18 22:43:43.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-3841" for this suite. 01/18/23 22:43:43.921
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:43:43.933
Jan 18 22:43:43.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename sched-preemption 01/18/23 22:43:43.935
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:43:43.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:43:43.957
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 18 22:43:43.978: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 22:44:44.030: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 01/18/23 22:44:44.034
Jan 18 22:44:44.075: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 18 22:44:44.094: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 18 22:44:44.292: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 18 22:44:44.304: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/18/23 22:44:44.304
Jan 18 22:44:44.305: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4639" to be "running"
Jan 18 22:44:44.328: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 23.225452ms
Jan 18 22:44:46.334: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029192672s
Jan 18 22:44:48.335: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030434744s
Jan 18 22:44:50.334: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029522261s
Jan 18 22:44:52.337: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031661559s
Jan 18 22:44:54.335: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.030151765s
Jan 18 22:44:56.344: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.03919931s
Jan 18 22:44:56.344: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 18 22:44:56.344: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4639" to be "running"
Jan 18 22:44:56.350: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.658049ms
Jan 18 22:44:56.350: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 18 22:44:56.350: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4639" to be "running"
Jan 18 22:44:56.354: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.208871ms
Jan 18 22:44:56.355: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 18 22:44:56.355: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4639" to be "running"
Jan 18 22:44:56.359: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.041543ms
Jan 18 22:44:56.359: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/18/23 22:44:56.359
Jan 18 22:44:56.374: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-4639" to be "running"
Jan 18 22:44:56.378: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.825067ms
Jan 18 22:44:58.386: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012621644s
Jan 18 22:45:00.384: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010256823s
Jan 18 22:45:02.385: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.011180419s
Jan 18 22:45:02.385: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 18 22:45:02.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4639" for this suite. 01/18/23 22:45:02.412
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":272,"skipped":4832,"failed":0}
------------------------------
• [SLOW TEST] [78.542 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:43:43.933
    Jan 18 22:43:43.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename sched-preemption 01/18/23 22:43:43.935
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:43:43.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:43:43.957
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 18 22:43:43.978: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 22:44:44.030: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 01/18/23 22:44:44.034
    Jan 18 22:44:44.075: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 18 22:44:44.094: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 18 22:44:44.292: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 18 22:44:44.304: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/18/23 22:44:44.304
    Jan 18 22:44:44.305: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4639" to be "running"
    Jan 18 22:44:44.328: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 23.225452ms
    Jan 18 22:44:46.334: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029192672s
    Jan 18 22:44:48.335: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030434744s
    Jan 18 22:44:50.334: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029522261s
    Jan 18 22:44:52.337: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031661559s
    Jan 18 22:44:54.335: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.030151765s
    Jan 18 22:44:56.344: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.03919931s
    Jan 18 22:44:56.344: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 18 22:44:56.344: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4639" to be "running"
    Jan 18 22:44:56.350: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.658049ms
    Jan 18 22:44:56.350: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 18 22:44:56.350: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4639" to be "running"
    Jan 18 22:44:56.354: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.208871ms
    Jan 18 22:44:56.355: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 18 22:44:56.355: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4639" to be "running"
    Jan 18 22:44:56.359: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.041543ms
    Jan 18 22:44:56.359: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/18/23 22:44:56.359
    Jan 18 22:44:56.374: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-4639" to be "running"
    Jan 18 22:44:56.378: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.825067ms
    Jan 18 22:44:58.386: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012621644s
    Jan 18 22:45:00.384: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010256823s
    Jan 18 22:45:02.385: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.011180419s
    Jan 18 22:45:02.385: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 22:45:02.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-4639" for this suite. 01/18/23 22:45:02.412
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:45:02.486
Jan 18 22:45:02.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/18/23 22:45:02.488
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:02.51
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:02.515
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 01/18/23 22:45:02.521
STEP: Creating hostNetwork=false pod 01/18/23 22:45:02.521
Jan 18 22:45:02.530: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-4215" to be "running and ready"
Jan 18 22:45:02.544: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.534888ms
Jan 18 22:45:02.544: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:45:04.551: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021151011s
Jan 18 22:45:04.551: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:45:06.552: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.022082978s
Jan 18 22:45:06.552: INFO: The phase of Pod test-pod is Running (Ready = true)
Jan 18 22:45:06.552: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 01/18/23 22:45:06.558
Jan 18 22:45:06.567: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-4215" to be "running and ready"
Jan 18 22:45:06.573: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.547046ms
Jan 18 22:45:06.573: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:45:08.600: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.033159562s
Jan 18 22:45:08.600: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jan 18 22:45:08.600: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 01/18/23 22:45:08.613
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/18/23 22:45:08.614
Jan 18 22:45:08.614: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4215 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:45:08.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 22:45:08.616: INFO: ExecWithOptions: Clientset creation
Jan 18 22:45:08.617: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4215/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 18 22:45:08.733: INFO: Exec stderr: ""
Jan 18 22:45:08.733: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4215 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:45:08.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 22:45:08.735: INFO: ExecWithOptions: Clientset creation
Jan 18 22:45:08.735: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4215/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 18 22:45:08.898: INFO: Exec stderr: ""
Jan 18 22:45:08.898: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4215 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:45:08.898: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 22:45:08.899: INFO: ExecWithOptions: Clientset creation
Jan 18 22:45:08.899: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4215/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 18 22:45:09.038: INFO: Exec stderr: ""
Jan 18 22:45:09.038: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4215 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:45:09.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 22:45:09.039: INFO: ExecWithOptions: Clientset creation
Jan 18 22:45:09.039: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4215/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 18 22:45:09.174: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/18/23 22:45:09.174
Jan 18 22:45:09.174: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4215 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:45:09.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 22:45:09.176: INFO: ExecWithOptions: Clientset creation
Jan 18 22:45:09.176: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4215/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 18 22:45:09.295: INFO: Exec stderr: ""
Jan 18 22:45:09.295: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4215 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:45:09.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 22:45:09.296: INFO: ExecWithOptions: Clientset creation
Jan 18 22:45:09.296: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4215/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 18 22:45:09.381: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/18/23 22:45:09.381
Jan 18 22:45:09.381: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4215 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:45:09.381: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 22:45:09.382: INFO: ExecWithOptions: Clientset creation
Jan 18 22:45:09.382: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4215/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 18 22:45:09.577: INFO: Exec stderr: ""
Jan 18 22:45:09.577: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4215 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:45:09.577: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 22:45:09.578: INFO: ExecWithOptions: Clientset creation
Jan 18 22:45:09.578: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4215/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 18 22:45:09.687: INFO: Exec stderr: ""
Jan 18 22:45:09.687: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4215 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:45:09.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 22:45:09.688: INFO: ExecWithOptions: Clientset creation
Jan 18 22:45:09.688: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4215/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 18 22:45:09.821: INFO: Exec stderr: ""
Jan 18 22:45:09.821: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4215 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:45:09.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 22:45:09.822: INFO: ExecWithOptions: Clientset creation
Jan 18 22:45:09.822: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4215/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 18 22:45:09.925: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Jan 18 22:45:09.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4215" for this suite. 01/18/23 22:45:09.943
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":273,"skipped":4865,"failed":0}
------------------------------
• [SLOW TEST] [7.477 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:45:02.486
    Jan 18 22:45:02.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/18/23 22:45:02.488
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:02.51
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:02.515
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 01/18/23 22:45:02.521
    STEP: Creating hostNetwork=false pod 01/18/23 22:45:02.521
    Jan 18 22:45:02.530: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-4215" to be "running and ready"
    Jan 18 22:45:02.544: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.534888ms
    Jan 18 22:45:02.544: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:45:04.551: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021151011s
    Jan 18 22:45:04.551: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:45:06.552: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.022082978s
    Jan 18 22:45:06.552: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jan 18 22:45:06.552: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 01/18/23 22:45:06.558
    Jan 18 22:45:06.567: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-4215" to be "running and ready"
    Jan 18 22:45:06.573: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.547046ms
    Jan 18 22:45:06.573: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:45:08.600: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.033159562s
    Jan 18 22:45:08.600: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jan 18 22:45:08.600: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 01/18/23 22:45:08.613
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/18/23 22:45:08.614
    Jan 18 22:45:08.614: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4215 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:45:08.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 22:45:08.616: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:45:08.617: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4215/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 18 22:45:08.733: INFO: Exec stderr: ""
    Jan 18 22:45:08.733: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4215 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:45:08.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 22:45:08.735: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:45:08.735: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4215/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 18 22:45:08.898: INFO: Exec stderr: ""
    Jan 18 22:45:08.898: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4215 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:45:08.898: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 22:45:08.899: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:45:08.899: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4215/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 18 22:45:09.038: INFO: Exec stderr: ""
    Jan 18 22:45:09.038: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4215 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:45:09.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 22:45:09.039: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:45:09.039: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4215/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 18 22:45:09.174: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/18/23 22:45:09.174
    Jan 18 22:45:09.174: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4215 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:45:09.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 22:45:09.176: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:45:09.176: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4215/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 18 22:45:09.295: INFO: Exec stderr: ""
    Jan 18 22:45:09.295: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4215 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:45:09.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 22:45:09.296: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:45:09.296: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4215/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 18 22:45:09.381: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/18/23 22:45:09.381
    Jan 18 22:45:09.381: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4215 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:45:09.381: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 22:45:09.382: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:45:09.382: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4215/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 18 22:45:09.577: INFO: Exec stderr: ""
    Jan 18 22:45:09.577: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4215 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:45:09.577: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 22:45:09.578: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:45:09.578: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4215/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 18 22:45:09.687: INFO: Exec stderr: ""
    Jan 18 22:45:09.687: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4215 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:45:09.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 22:45:09.688: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:45:09.688: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4215/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 18 22:45:09.821: INFO: Exec stderr: ""
    Jan 18 22:45:09.821: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4215 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:45:09.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 22:45:09.822: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:45:09.822: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4215/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 18 22:45:09.925: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Jan 18 22:45:09.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-4215" for this suite. 01/18/23 22:45:09.943
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:45:09.964
Jan 18 22:45:09.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename events 01/18/23 22:45:09.969
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:10.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:10.021
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 01/18/23 22:45:10.034
STEP: get a list of Events with a label in the current namespace 01/18/23 22:45:10.053
STEP: delete a list of events 01/18/23 22:45:10.06
Jan 18 22:45:10.061: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/18/23 22:45:10.124
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jan 18 22:45:10.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4848" for this suite. 01/18/23 22:45:10.144
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":274,"skipped":4868,"failed":0}
------------------------------
• [0.194 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:45:09.964
    Jan 18 22:45:09.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename events 01/18/23 22:45:09.969
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:10.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:10.021
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 01/18/23 22:45:10.034
    STEP: get a list of Events with a label in the current namespace 01/18/23 22:45:10.053
    STEP: delete a list of events 01/18/23 22:45:10.06
    Jan 18 22:45:10.061: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/18/23 22:45:10.124
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jan 18 22:45:10.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-4848" for this suite. 01/18/23 22:45:10.144
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:45:10.161
Jan 18 22:45:10.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 22:45:10.163
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:10.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:10.225
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/18/23 22:45:10.248
Jan 18 22:45:10.269: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5978" to be "running and ready"
Jan 18 22:45:10.274: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.625183ms
Jan 18 22:45:10.275: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:45:12.280: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010629279s
Jan 18 22:45:12.280: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 18 22:45:12.281: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 01/18/23 22:45:12.286
Jan 18 22:45:12.307: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-5978" to be "running and ready"
Jan 18 22:45:12.313: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.832762ms
Jan 18 22:45:12.313: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:45:14.319: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011687791s
Jan 18 22:45:14.319: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jan 18 22:45:14.319: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/18/23 22:45:14.323
Jan 18 22:45:14.335: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 18 22:45:14.346: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 18 22:45:16.346: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 18 22:45:16.508: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 18 22:45:18.346: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 18 22:45:18.352: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 01/18/23 22:45:18.352
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 18 22:45:18.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5978" for this suite. 01/18/23 22:45:18.392
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":275,"skipped":4877,"failed":0}
------------------------------
• [SLOW TEST] [8.237 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:45:10.161
    Jan 18 22:45:10.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 22:45:10.163
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:10.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:10.225
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/18/23 22:45:10.248
    Jan 18 22:45:10.269: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5978" to be "running and ready"
    Jan 18 22:45:10.274: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.625183ms
    Jan 18 22:45:10.275: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:45:12.280: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010629279s
    Jan 18 22:45:12.280: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 18 22:45:12.281: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 01/18/23 22:45:12.286
    Jan 18 22:45:12.307: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-5978" to be "running and ready"
    Jan 18 22:45:12.313: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.832762ms
    Jan 18 22:45:12.313: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:45:14.319: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011687791s
    Jan 18 22:45:14.319: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jan 18 22:45:14.319: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/18/23 22:45:14.323
    Jan 18 22:45:14.335: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 18 22:45:14.346: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 18 22:45:16.346: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 18 22:45:16.508: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 18 22:45:18.346: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 18 22:45:18.352: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 01/18/23 22:45:18.352
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 18 22:45:18.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-5978" for this suite. 01/18/23 22:45:18.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:45:18.406
Jan 18 22:45:18.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 22:45:18.407
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:18.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:18.489
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-61ad9026-77be-435a-940c-3903605d95fa 01/18/23 22:45:18.493
STEP: Creating a pod to test consume configMaps 01/18/23 22:45:18.498
Jan 18 22:45:18.510: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-abd93394-00a2-4c5f-83e7-c3b605ddc709" in namespace "projected-3083" to be "Succeeded or Failed"
Jan 18 22:45:18.517: INFO: Pod "pod-projected-configmaps-abd93394-00a2-4c5f-83e7-c3b605ddc709": Phase="Pending", Reason="", readiness=false. Elapsed: 6.932017ms
Jan 18 22:45:20.523: INFO: Pod "pod-projected-configmaps-abd93394-00a2-4c5f-83e7-c3b605ddc709": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013653937s
Jan 18 22:45:22.524: INFO: Pod "pod-projected-configmaps-abd93394-00a2-4c5f-83e7-c3b605ddc709": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014160326s
STEP: Saw pod success 01/18/23 22:45:22.524
Jan 18 22:45:22.525: INFO: Pod "pod-projected-configmaps-abd93394-00a2-4c5f-83e7-c3b605ddc709" satisfied condition "Succeeded or Failed"
Jan 18 22:45:22.532: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-abd93394-00a2-4c5f-83e7-c3b605ddc709 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 22:45:22.552
Jan 18 22:45:22.577: INFO: Waiting for pod pod-projected-configmaps-abd93394-00a2-4c5f-83e7-c3b605ddc709 to disappear
Jan 18 22:45:22.583: INFO: Pod pod-projected-configmaps-abd93394-00a2-4c5f-83e7-c3b605ddc709 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 18 22:45:22.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3083" for this suite. 01/18/23 22:45:22.593
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":276,"skipped":4888,"failed":0}
------------------------------
• [4.195 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:45:18.406
    Jan 18 22:45:18.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 22:45:18.407
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:18.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:18.489
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-61ad9026-77be-435a-940c-3903605d95fa 01/18/23 22:45:18.493
    STEP: Creating a pod to test consume configMaps 01/18/23 22:45:18.498
    Jan 18 22:45:18.510: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-abd93394-00a2-4c5f-83e7-c3b605ddc709" in namespace "projected-3083" to be "Succeeded or Failed"
    Jan 18 22:45:18.517: INFO: Pod "pod-projected-configmaps-abd93394-00a2-4c5f-83e7-c3b605ddc709": Phase="Pending", Reason="", readiness=false. Elapsed: 6.932017ms
    Jan 18 22:45:20.523: INFO: Pod "pod-projected-configmaps-abd93394-00a2-4c5f-83e7-c3b605ddc709": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013653937s
    Jan 18 22:45:22.524: INFO: Pod "pod-projected-configmaps-abd93394-00a2-4c5f-83e7-c3b605ddc709": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014160326s
    STEP: Saw pod success 01/18/23 22:45:22.524
    Jan 18 22:45:22.525: INFO: Pod "pod-projected-configmaps-abd93394-00a2-4c5f-83e7-c3b605ddc709" satisfied condition "Succeeded or Failed"
    Jan 18 22:45:22.532: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-abd93394-00a2-4c5f-83e7-c3b605ddc709 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 22:45:22.552
    Jan 18 22:45:22.577: INFO: Waiting for pod pod-projected-configmaps-abd93394-00a2-4c5f-83e7-c3b605ddc709 to disappear
    Jan 18 22:45:22.583: INFO: Pod pod-projected-configmaps-abd93394-00a2-4c5f-83e7-c3b605ddc709 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 18 22:45:22.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3083" for this suite. 01/18/23 22:45:22.593
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:45:22.609
Jan 18 22:45:22.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename statefulset 01/18/23 22:45:22.611
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:22.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:22.638
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6804 01/18/23 22:45:22.643
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-6804 01/18/23 22:45:22.654
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6804 01/18/23 22:45:22.665
Jan 18 22:45:22.671: INFO: Found 0 stateful pods, waiting for 1
Jan 18 22:45:32.677: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/18/23 22:45:32.678
Jan 18 22:45:32.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-6804 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 22:45:32.913: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 22:45:32.913: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 22:45:32.913: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 22:45:32.917: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 18 22:45:42.924: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 22:45:42.925: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 22:45:42.952: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Jan 18 22:45:42.953: INFO: ss-0  v1-25-1-18760-w2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:22 +0000 UTC  }]
Jan 18 22:45:42.953: INFO: 
Jan 18 22:45:42.953: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 18 22:45:43.966: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992879261s
Jan 18 22:45:44.973: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.979746773s
Jan 18 22:45:45.980: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.972212997s
Jan 18 22:45:46.988: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.965273901s
Jan 18 22:45:47.994: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.957683215s
Jan 18 22:45:49.003: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.951109984s
Jan 18 22:45:50.010: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.9425965s
Jan 18 22:45:51.019: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.935052148s
Jan 18 22:45:52.027: INFO: Verifying statefulset ss doesn't scale past 3 for another 926.035907ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6804 01/18/23 22:45:53.028
Jan 18 22:45:53.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-6804 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 22:45:53.259: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 22:45:53.259: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 22:45:53.259: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 22:45:53.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-6804 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 22:45:53.461: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 18 22:45:53.461: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 22:45:53.461: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 22:45:53.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-6804 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 22:45:53.648: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 18 22:45:53.648: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 22:45:53.648: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 22:45:53.654: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jan 18 22:46:03.660: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 22:46:03.660: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 22:46:03.660: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 01/18/23 22:46:03.66
Jan 18 22:46:03.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-6804 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 22:46:04.172: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 22:46:04.172: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 22:46:04.172: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 22:46:04.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-6804 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 22:46:04.422: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 22:46:04.422: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 22:46:04.422: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 22:46:04.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-6804 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 22:46:04.695: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 22:46:04.695: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 22:46:04.695: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 22:46:04.695: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 22:46:04.701: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 18 22:46:14.713: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 22:46:14.713: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 22:46:14.713: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 22:46:14.732: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Jan 18 22:46:14.732: INFO: ss-0  v1-25-1-18760-w2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:22 +0000 UTC  }]
Jan 18 22:46:14.732: INFO: ss-1  v1-25-1-18760-w2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:42 +0000 UTC  }]
Jan 18 22:46:14.732: INFO: ss-2  v1-25-1-18760-w   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:43 +0000 UTC  }]
Jan 18 22:46:14.732: INFO: 
Jan 18 22:46:14.732: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 18 22:46:15.738: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Jan 18 22:46:15.738: INFO: ss-0  v1-25-1-18760-w2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:22 +0000 UTC  }]
Jan 18 22:46:15.738: INFO: ss-1  v1-25-1-18760-w2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:42 +0000 UTC  }]
Jan 18 22:46:15.738: INFO: ss-2  v1-25-1-18760-w   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:43 +0000 UTC  }]
Jan 18 22:46:15.738: INFO: 
Jan 18 22:46:15.738: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 18 22:46:16.743: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.987758428s
Jan 18 22:46:17.748: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.982321962s
Jan 18 22:46:18.757: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.977864487s
Jan 18 22:46:19.761: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.968765739s
Jan 18 22:46:20.767: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.964307363s
Jan 18 22:46:21.772: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.958660832s
Jan 18 22:46:22.777: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.954019714s
Jan 18 22:46:23.783: INFO: Verifying statefulset ss doesn't scale past 0 for another 948.872057ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6804 01/18/23 22:46:24.784
Jan 18 22:46:24.790: INFO: Scaling statefulset ss to 0
Jan 18 22:46:24.806: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 22:46:24.812: INFO: Deleting all statefulset in ns statefulset-6804
Jan 18 22:46:24.816: INFO: Scaling statefulset ss to 0
Jan 18 22:46:24.834: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 22:46:24.838: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 18 22:46:24.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6804" for this suite. 01/18/23 22:46:24.863
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":277,"skipped":4958,"failed":0}
------------------------------
• [SLOW TEST] [62.267 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:45:22.609
    Jan 18 22:45:22.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename statefulset 01/18/23 22:45:22.611
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:45:22.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:45:22.638
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-6804 01/18/23 22:45:22.643
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-6804 01/18/23 22:45:22.654
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6804 01/18/23 22:45:22.665
    Jan 18 22:45:22.671: INFO: Found 0 stateful pods, waiting for 1
    Jan 18 22:45:32.677: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/18/23 22:45:32.678
    Jan 18 22:45:32.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-6804 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 22:45:32.913: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 22:45:32.913: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 22:45:32.913: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 22:45:32.917: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 18 22:45:42.924: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 22:45:42.925: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 22:45:42.952: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
    Jan 18 22:45:42.953: INFO: ss-0  v1-25-1-18760-w2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:22 +0000 UTC  }]
    Jan 18 22:45:42.953: INFO: 
    Jan 18 22:45:42.953: INFO: StatefulSet ss has not reached scale 3, at 1
    Jan 18 22:45:43.966: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992879261s
    Jan 18 22:45:44.973: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.979746773s
    Jan 18 22:45:45.980: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.972212997s
    Jan 18 22:45:46.988: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.965273901s
    Jan 18 22:45:47.994: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.957683215s
    Jan 18 22:45:49.003: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.951109984s
    Jan 18 22:45:50.010: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.9425965s
    Jan 18 22:45:51.019: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.935052148s
    Jan 18 22:45:52.027: INFO: Verifying statefulset ss doesn't scale past 3 for another 926.035907ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6804 01/18/23 22:45:53.028
    Jan 18 22:45:53.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-6804 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 22:45:53.259: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 22:45:53.259: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 22:45:53.259: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 22:45:53.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-6804 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 22:45:53.461: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 18 22:45:53.461: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 22:45:53.461: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 22:45:53.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-6804 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 22:45:53.648: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 18 22:45:53.648: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 22:45:53.648: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 22:45:53.654: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Jan 18 22:46:03.660: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 22:46:03.660: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 22:46:03.660: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 01/18/23 22:46:03.66
    Jan 18 22:46:03.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-6804 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 22:46:04.172: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 22:46:04.172: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 22:46:04.172: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 22:46:04.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-6804 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 22:46:04.422: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 22:46:04.422: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 22:46:04.422: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 22:46:04.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-6804 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 22:46:04.695: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 22:46:04.695: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 22:46:04.695: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 22:46:04.695: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 22:46:04.701: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jan 18 22:46:14.713: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 22:46:14.713: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 22:46:14.713: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 22:46:14.732: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
    Jan 18 22:46:14.732: INFO: ss-0  v1-25-1-18760-w2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:22 +0000 UTC  }]
    Jan 18 22:46:14.732: INFO: ss-1  v1-25-1-18760-w2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:42 +0000 UTC  }]
    Jan 18 22:46:14.732: INFO: ss-2  v1-25-1-18760-w   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:43 +0000 UTC  }]
    Jan 18 22:46:14.732: INFO: 
    Jan 18 22:46:14.732: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan 18 22:46:15.738: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
    Jan 18 22:46:15.738: INFO: ss-0  v1-25-1-18760-w2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:22 +0000 UTC  }]
    Jan 18 22:46:15.738: INFO: ss-1  v1-25-1-18760-w2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:42 +0000 UTC  }]
    Jan 18 22:46:15.738: INFO: ss-2  v1-25-1-18760-w   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:46:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:45:43 +0000 UTC  }]
    Jan 18 22:46:15.738: INFO: 
    Jan 18 22:46:15.738: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan 18 22:46:16.743: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.987758428s
    Jan 18 22:46:17.748: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.982321962s
    Jan 18 22:46:18.757: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.977864487s
    Jan 18 22:46:19.761: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.968765739s
    Jan 18 22:46:20.767: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.964307363s
    Jan 18 22:46:21.772: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.958660832s
    Jan 18 22:46:22.777: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.954019714s
    Jan 18 22:46:23.783: INFO: Verifying statefulset ss doesn't scale past 0 for another 948.872057ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6804 01/18/23 22:46:24.784
    Jan 18 22:46:24.790: INFO: Scaling statefulset ss to 0
    Jan 18 22:46:24.806: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 18 22:46:24.812: INFO: Deleting all statefulset in ns statefulset-6804
    Jan 18 22:46:24.816: INFO: Scaling statefulset ss to 0
    Jan 18 22:46:24.834: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 22:46:24.838: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 18 22:46:24.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-6804" for this suite. 01/18/23 22:46:24.863
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:46:24.887
Jan 18 22:46:24.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename webhook 01/18/23 22:46:24.889
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:24.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:24.915
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 22:46:24.934
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:46:25.568
STEP: Deploying the webhook pod 01/18/23 22:46:25.578
STEP: Wait for the deployment to be ready 01/18/23 22:46:25.59
Jan 18 22:46:25.603: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 22:46:27.619
STEP: Verifying the service has paired with the endpoint 01/18/23 22:46:27.638
Jan 18 22:46:28.640: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 01/18/23 22:46:28.646
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/18/23 22:46:28.649
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/18/23 22:46:28.65
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/18/23 22:46:28.65
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/18/23 22:46:28.653
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/18/23 22:46:28.654
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/18/23 22:46:28.664
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 22:46:28.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-143" for this suite. 01/18/23 22:46:28.671
STEP: Destroying namespace "webhook-143-markers" for this suite. 01/18/23 22:46:28.683
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":278,"skipped":4974,"failed":0}
------------------------------
• [4.117 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:46:24.887
    Jan 18 22:46:24.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename webhook 01/18/23 22:46:24.889
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:24.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:24.915
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 22:46:24.934
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:46:25.568
    STEP: Deploying the webhook pod 01/18/23 22:46:25.578
    STEP: Wait for the deployment to be ready 01/18/23 22:46:25.59
    Jan 18 22:46:25.603: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 22:46:27.619
    STEP: Verifying the service has paired with the endpoint 01/18/23 22:46:27.638
    Jan 18 22:46:28.640: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 01/18/23 22:46:28.646
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/18/23 22:46:28.649
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/18/23 22:46:28.65
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/18/23 22:46:28.65
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/18/23 22:46:28.653
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/18/23 22:46:28.654
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/18/23 22:46:28.664
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 22:46:28.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-143" for this suite. 01/18/23 22:46:28.671
    STEP: Destroying namespace "webhook-143-markers" for this suite. 01/18/23 22:46:28.683
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:46:29.034
Jan 18 22:46:29.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename kubectl 01/18/23 22:46:29.037
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:29.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:29.065
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Jan 18 22:46:29.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4451 version'
Jan 18 22:46:29.225: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan 18 22:46:29.225: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4\", GitCommit:\"872a965c6c6526caa949f0c6ac028ef7aff3fb78\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:36:36Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4\", GitCommit:\"872a965c6c6526caa949f0c6ac028ef7aff3fb78\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:29:58Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 22:46:29.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4451" for this suite. 01/18/23 22:46:29.233
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":279,"skipped":5025,"failed":0}
------------------------------
• [0.207 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:46:29.034
    Jan 18 22:46:29.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename kubectl 01/18/23 22:46:29.037
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:29.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:29.065
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Jan 18 22:46:29.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-4451 version'
    Jan 18 22:46:29.225: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jan 18 22:46:29.225: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4\", GitCommit:\"872a965c6c6526caa949f0c6ac028ef7aff3fb78\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:36:36Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4\", GitCommit:\"872a965c6c6526caa949f0c6ac028ef7aff3fb78\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:29:58Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 22:46:29.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4451" for this suite. 01/18/23 22:46:29.233
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:46:29.243
Jan 18 22:46:29.243: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename lease-test 01/18/23 22:46:29.245
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:29.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:29.28
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Jan 18 22:46:29.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-6689" for this suite. 01/18/23 22:46:29.373
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":280,"skipped":5052,"failed":0}
------------------------------
• [0.138 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:46:29.243
    Jan 18 22:46:29.243: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename lease-test 01/18/23 22:46:29.245
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:29.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:29.28
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Jan 18 22:46:29.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-6689" for this suite. 01/18/23 22:46:29.373
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:46:29.383
Jan 18 22:46:29.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename ingressclass 01/18/23 22:46:29.385
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:29.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:29.416
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 01/18/23 22:46:29.422
STEP: getting /apis/networking.k8s.io 01/18/23 22:46:29.428
STEP: getting /apis/networking.k8s.iov1 01/18/23 22:46:29.431
STEP: creating 01/18/23 22:46:29.433
STEP: getting 01/18/23 22:46:29.447
STEP: listing 01/18/23 22:46:29.451
STEP: watching 01/18/23 22:46:29.456
Jan 18 22:46:29.456: INFO: starting watch
STEP: patching 01/18/23 22:46:29.459
STEP: updating 01/18/23 22:46:29.464
Jan 18 22:46:29.473: INFO: waiting for watch events with expected annotations
Jan 18 22:46:29.473: INFO: saw patched and updated annotations
STEP: deleting 01/18/23 22:46:29.474
STEP: deleting a collection 01/18/23 22:46:29.487
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Jan 18 22:46:29.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-5911" for this suite. 01/18/23 22:46:29.52
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":281,"skipped":5064,"failed":0}
------------------------------
• [0.145 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:46:29.383
    Jan 18 22:46:29.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename ingressclass 01/18/23 22:46:29.385
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:29.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:29.416
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 01/18/23 22:46:29.422
    STEP: getting /apis/networking.k8s.io 01/18/23 22:46:29.428
    STEP: getting /apis/networking.k8s.iov1 01/18/23 22:46:29.431
    STEP: creating 01/18/23 22:46:29.433
    STEP: getting 01/18/23 22:46:29.447
    STEP: listing 01/18/23 22:46:29.451
    STEP: watching 01/18/23 22:46:29.456
    Jan 18 22:46:29.456: INFO: starting watch
    STEP: patching 01/18/23 22:46:29.459
    STEP: updating 01/18/23 22:46:29.464
    Jan 18 22:46:29.473: INFO: waiting for watch events with expected annotations
    Jan 18 22:46:29.473: INFO: saw patched and updated annotations
    STEP: deleting 01/18/23 22:46:29.474
    STEP: deleting a collection 01/18/23 22:46:29.487
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Jan 18 22:46:29.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-5911" for this suite. 01/18/23 22:46:29.52
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:46:29.532
Jan 18 22:46:29.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename services 01/18/23 22:46:29.534
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:29.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:29.567
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9437 01/18/23 22:46:29.573
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/18/23 22:46:29.609
STEP: creating service externalsvc in namespace services-9437 01/18/23 22:46:29.61
STEP: creating replication controller externalsvc in namespace services-9437 01/18/23 22:46:29.637
I0118 22:46:29.650951      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9437, replica count: 2
I0118 22:46:32.704178      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 01/18/23 22:46:32.709
Jan 18 22:46:32.727: INFO: Creating new exec pod
Jan 18 22:46:32.744: INFO: Waiting up to 5m0s for pod "execpodbz5vr" in namespace "services-9437" to be "running"
Jan 18 22:46:32.752: INFO: Pod "execpodbz5vr": Phase="Pending", Reason="", readiness=false. Elapsed: 7.85027ms
Jan 18 22:46:34.765: INFO: Pod "execpodbz5vr": Phase="Running", Reason="", readiness=true. Elapsed: 2.021190014s
Jan 18 22:46:34.766: INFO: Pod "execpodbz5vr" satisfied condition "running"
Jan 18 22:46:34.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-9437 exec execpodbz5vr -- /bin/sh -x -c nslookup nodeport-service.services-9437.svc.cluster.local'
Jan 18 22:46:35.180: INFO: stderr: "+ nslookup nodeport-service.services-9437.svc.cluster.local\n"
Jan 18 22:46:35.180: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nnodeport-service.services-9437.svc.cluster.local\tcanonical name = externalsvc.services-9437.svc.cluster.local.\nName:\texternalsvc.services-9437.svc.cluster.local\nAddress: 10.233.23.32\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9437, will wait for the garbage collector to delete the pods 01/18/23 22:46:35.18
Jan 18 22:46:35.242: INFO: Deleting ReplicationController externalsvc took: 6.816718ms
Jan 18 22:46:35.343: INFO: Terminating ReplicationController externalsvc pods took: 101.656463ms
Jan 18 22:46:37.466: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 22:46:37.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9437" for this suite. 01/18/23 22:46:37.507
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":282,"skipped":5064,"failed":0}
------------------------------
• [SLOW TEST] [7.993 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:46:29.532
    Jan 18 22:46:29.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename services 01/18/23 22:46:29.534
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:29.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:29.567
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-9437 01/18/23 22:46:29.573
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/18/23 22:46:29.609
    STEP: creating service externalsvc in namespace services-9437 01/18/23 22:46:29.61
    STEP: creating replication controller externalsvc in namespace services-9437 01/18/23 22:46:29.637
    I0118 22:46:29.650951      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9437, replica count: 2
    I0118 22:46:32.704178      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 01/18/23 22:46:32.709
    Jan 18 22:46:32.727: INFO: Creating new exec pod
    Jan 18 22:46:32.744: INFO: Waiting up to 5m0s for pod "execpodbz5vr" in namespace "services-9437" to be "running"
    Jan 18 22:46:32.752: INFO: Pod "execpodbz5vr": Phase="Pending", Reason="", readiness=false. Elapsed: 7.85027ms
    Jan 18 22:46:34.765: INFO: Pod "execpodbz5vr": Phase="Running", Reason="", readiness=true. Elapsed: 2.021190014s
    Jan 18 22:46:34.766: INFO: Pod "execpodbz5vr" satisfied condition "running"
    Jan 18 22:46:34.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-9437 exec execpodbz5vr -- /bin/sh -x -c nslookup nodeport-service.services-9437.svc.cluster.local'
    Jan 18 22:46:35.180: INFO: stderr: "+ nslookup nodeport-service.services-9437.svc.cluster.local\n"
    Jan 18 22:46:35.180: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nnodeport-service.services-9437.svc.cluster.local\tcanonical name = externalsvc.services-9437.svc.cluster.local.\nName:\texternalsvc.services-9437.svc.cluster.local\nAddress: 10.233.23.32\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-9437, will wait for the garbage collector to delete the pods 01/18/23 22:46:35.18
    Jan 18 22:46:35.242: INFO: Deleting ReplicationController externalsvc took: 6.816718ms
    Jan 18 22:46:35.343: INFO: Terminating ReplicationController externalsvc pods took: 101.656463ms
    Jan 18 22:46:37.466: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 22:46:37.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9437" for this suite. 01/18/23 22:46:37.507
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:46:37.536
Jan 18 22:46:37.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename kubectl 01/18/23 22:46:37.54
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:37.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:37.574
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 01/18/23 22:46:37.583
Jan 18 22:46:37.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7877 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 18 22:46:37.722: INFO: stderr: ""
Jan 18 22:46:37.722: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 01/18/23 22:46:37.722
Jan 18 22:46:37.723: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 18 22:46:37.723: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7877" to be "running and ready, or succeeded"
Jan 18 22:46:37.754: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 31.371198ms
Jan 18 22:46:37.754: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'v1-25-1-18760-w2' to be 'Running' but was 'Pending'
Jan 18 22:46:39.759: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.035992426s
Jan 18 22:46:39.759: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 18 22:46:39.759: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 01/18/23 22:46:39.759
Jan 18 22:46:39.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7877 logs logs-generator logs-generator'
Jan 18 22:46:39.884: INFO: stderr: ""
Jan 18 22:46:39.884: INFO: stdout: "I0118 22:46:39.040883       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/tmvc 291\nI0118 22:46:39.240966       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/pvw6 472\nI0118 22:46:39.441527       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/6bjc 537\nI0118 22:46:39.641554       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/dwh5 299\nI0118 22:46:39.842421       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/22p 379\n"
STEP: limiting log lines 01/18/23 22:46:39.884
Jan 18 22:46:39.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7877 logs logs-generator logs-generator --tail=1'
Jan 18 22:46:40.012: INFO: stderr: ""
Jan 18 22:46:40.012: INFO: stdout: "I0118 22:46:39.842421       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/22p 379\n"
Jan 18 22:46:40.012: INFO: got output "I0118 22:46:39.842421       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/22p 379\n"
STEP: limiting log bytes 01/18/23 22:46:40.012
Jan 18 22:46:40.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7877 logs logs-generator logs-generator --limit-bytes=1'
Jan 18 22:46:40.161: INFO: stderr: ""
Jan 18 22:46:40.161: INFO: stdout: "I"
Jan 18 22:46:40.161: INFO: got output "I"
STEP: exposing timestamps 01/18/23 22:46:40.161
Jan 18 22:46:40.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7877 logs logs-generator logs-generator --tail=1 --timestamps'
Jan 18 22:46:40.318: INFO: stderr: ""
Jan 18 22:46:40.318: INFO: stdout: "2023-01-18T22:46:40.241122963Z I0118 22:46:40.240995       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/tcst 364\n"
Jan 18 22:46:40.318: INFO: got output "2023-01-18T22:46:40.241122963Z I0118 22:46:40.240995       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/tcst 364\n"
STEP: restricting to a time range 01/18/23 22:46:40.318
Jan 18 22:46:42.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7877 logs logs-generator logs-generator --since=1s'
Jan 18 22:46:43.038: INFO: stderr: ""
Jan 18 22:46:43.038: INFO: stdout: "I0118 22:46:42.041213       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/64b4 566\nI0118 22:46:42.241727       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/zrvr 280\nI0118 22:46:42.441201       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/ht7k 406\nI0118 22:46:42.641773       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/d4p6 448\nI0118 22:46:42.862480       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/7tq 298\n"
Jan 18 22:46:43.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7877 logs logs-generator logs-generator --since=24h'
Jan 18 22:46:43.209: INFO: stderr: ""
Jan 18 22:46:43.209: INFO: stdout: "I0118 22:46:39.040883       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/tmvc 291\nI0118 22:46:39.240966       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/pvw6 472\nI0118 22:46:39.441527       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/6bjc 537\nI0118 22:46:39.641554       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/dwh5 299\nI0118 22:46:39.842421       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/22p 379\nI0118 22:46:40.041745       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/4x7 318\nI0118 22:46:40.240995       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/tcst 364\nI0118 22:46:40.441599       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/7sj 287\nI0118 22:46:40.641290       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/v7d 409\nI0118 22:46:40.841782       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/nd79 266\nI0118 22:46:41.041489       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/qx7m 230\nI0118 22:46:41.241165       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/slj7 221\nI0118 22:46:41.441585       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/ng2z 311\nI0118 22:46:41.641163       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/2x9q 588\nI0118 22:46:41.841727       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/s7qf 419\nI0118 22:46:42.041213       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/64b4 566\nI0118 22:46:42.241727       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/zrvr 280\nI0118 22:46:42.441201       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/ht7k 406\nI0118 22:46:42.641773       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/d4p6 448\nI0118 22:46:42.862480       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/7tq 298\nI0118 22:46:43.040917       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/ksh6 353\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Jan 18 22:46:43.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7877 delete pod logs-generator'
Jan 18 22:46:44.336: INFO: stderr: ""
Jan 18 22:46:44.336: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 22:46:44.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7877" for this suite. 01/18/23 22:46:44.358
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":283,"skipped":5070,"failed":0}
------------------------------
• [SLOW TEST] [6.832 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:46:37.536
    Jan 18 22:46:37.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename kubectl 01/18/23 22:46:37.54
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:37.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:37.574
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 01/18/23 22:46:37.583
    Jan 18 22:46:37.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7877 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jan 18 22:46:37.722: INFO: stderr: ""
    Jan 18 22:46:37.722: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 01/18/23 22:46:37.722
    Jan 18 22:46:37.723: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jan 18 22:46:37.723: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7877" to be "running and ready, or succeeded"
    Jan 18 22:46:37.754: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 31.371198ms
    Jan 18 22:46:37.754: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'v1-25-1-18760-w2' to be 'Running' but was 'Pending'
    Jan 18 22:46:39.759: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.035992426s
    Jan 18 22:46:39.759: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jan 18 22:46:39.759: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 01/18/23 22:46:39.759
    Jan 18 22:46:39.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7877 logs logs-generator logs-generator'
    Jan 18 22:46:39.884: INFO: stderr: ""
    Jan 18 22:46:39.884: INFO: stdout: "I0118 22:46:39.040883       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/tmvc 291\nI0118 22:46:39.240966       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/pvw6 472\nI0118 22:46:39.441527       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/6bjc 537\nI0118 22:46:39.641554       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/dwh5 299\nI0118 22:46:39.842421       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/22p 379\n"
    STEP: limiting log lines 01/18/23 22:46:39.884
    Jan 18 22:46:39.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7877 logs logs-generator logs-generator --tail=1'
    Jan 18 22:46:40.012: INFO: stderr: ""
    Jan 18 22:46:40.012: INFO: stdout: "I0118 22:46:39.842421       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/22p 379\n"
    Jan 18 22:46:40.012: INFO: got output "I0118 22:46:39.842421       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/22p 379\n"
    STEP: limiting log bytes 01/18/23 22:46:40.012
    Jan 18 22:46:40.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7877 logs logs-generator logs-generator --limit-bytes=1'
    Jan 18 22:46:40.161: INFO: stderr: ""
    Jan 18 22:46:40.161: INFO: stdout: "I"
    Jan 18 22:46:40.161: INFO: got output "I"
    STEP: exposing timestamps 01/18/23 22:46:40.161
    Jan 18 22:46:40.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7877 logs logs-generator logs-generator --tail=1 --timestamps'
    Jan 18 22:46:40.318: INFO: stderr: ""
    Jan 18 22:46:40.318: INFO: stdout: "2023-01-18T22:46:40.241122963Z I0118 22:46:40.240995       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/tcst 364\n"
    Jan 18 22:46:40.318: INFO: got output "2023-01-18T22:46:40.241122963Z I0118 22:46:40.240995       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/tcst 364\n"
    STEP: restricting to a time range 01/18/23 22:46:40.318
    Jan 18 22:46:42.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7877 logs logs-generator logs-generator --since=1s'
    Jan 18 22:46:43.038: INFO: stderr: ""
    Jan 18 22:46:43.038: INFO: stdout: "I0118 22:46:42.041213       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/64b4 566\nI0118 22:46:42.241727       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/zrvr 280\nI0118 22:46:42.441201       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/ht7k 406\nI0118 22:46:42.641773       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/d4p6 448\nI0118 22:46:42.862480       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/7tq 298\n"
    Jan 18 22:46:43.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7877 logs logs-generator logs-generator --since=24h'
    Jan 18 22:46:43.209: INFO: stderr: ""
    Jan 18 22:46:43.209: INFO: stdout: "I0118 22:46:39.040883       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/tmvc 291\nI0118 22:46:39.240966       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/pvw6 472\nI0118 22:46:39.441527       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/6bjc 537\nI0118 22:46:39.641554       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/dwh5 299\nI0118 22:46:39.842421       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/22p 379\nI0118 22:46:40.041745       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/4x7 318\nI0118 22:46:40.240995       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/tcst 364\nI0118 22:46:40.441599       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/7sj 287\nI0118 22:46:40.641290       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/v7d 409\nI0118 22:46:40.841782       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/nd79 266\nI0118 22:46:41.041489       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/qx7m 230\nI0118 22:46:41.241165       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/slj7 221\nI0118 22:46:41.441585       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/ng2z 311\nI0118 22:46:41.641163       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/2x9q 588\nI0118 22:46:41.841727       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/s7qf 419\nI0118 22:46:42.041213       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/64b4 566\nI0118 22:46:42.241727       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/zrvr 280\nI0118 22:46:42.441201       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/ht7k 406\nI0118 22:46:42.641773       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/d4p6 448\nI0118 22:46:42.862480       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/7tq 298\nI0118 22:46:43.040917       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/ksh6 353\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Jan 18 22:46:43.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7877 delete pod logs-generator'
    Jan 18 22:46:44.336: INFO: stderr: ""
    Jan 18 22:46:44.336: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 22:46:44.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7877" for this suite. 01/18/23 22:46:44.358
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:46:44.368
Jan 18 22:46:44.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename runtimeclass 01/18/23 22:46:44.37
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:44.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:44.41
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-9076-delete-me 01/18/23 22:46:44.449
STEP: Waiting for the RuntimeClass to disappear 01/18/23 22:46:44.518
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 18 22:46:44.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9076" for this suite. 01/18/23 22:46:44.549
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":284,"skipped":5074,"failed":0}
------------------------------
• [0.188 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:46:44.368
    Jan 18 22:46:44.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename runtimeclass 01/18/23 22:46:44.37
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:44.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:44.41
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-9076-delete-me 01/18/23 22:46:44.449
    STEP: Waiting for the RuntimeClass to disappear 01/18/23 22:46:44.518
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 18 22:46:44.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-9076" for this suite. 01/18/23 22:46:44.549
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:46:44.558
Jan 18 22:46:44.558: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename kubectl 01/18/23 22:46:44.562
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:44.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:44.608
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 01/18/23 22:46:44.614
Jan 18 22:46:44.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-1132 api-versions'
Jan 18 22:46:44.764: INFO: stderr: ""
Jan 18 22:46:44.764: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nhelm.toolkit.fluxcd.io/v2beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnotification.toolkit.fluxcd.io/v1beta1\nnotification.toolkit.fluxcd.io/v1beta2\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsource.toolkit.fluxcd.io/v1beta1\nsource.toolkit.fluxcd.io/v1beta2\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 22:46:44.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1132" for this suite. 01/18/23 22:46:44.771
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":285,"skipped":5079,"failed":0}
------------------------------
• [0.219 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:46:44.558
    Jan 18 22:46:44.558: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename kubectl 01/18/23 22:46:44.562
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:44.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:44.608
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 01/18/23 22:46:44.614
    Jan 18 22:46:44.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-1132 api-versions'
    Jan 18 22:46:44.764: INFO: stderr: ""
    Jan 18 22:46:44.764: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nhelm.toolkit.fluxcd.io/v2beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnotification.toolkit.fluxcd.io/v1beta1\nnotification.toolkit.fluxcd.io/v1beta2\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsource.toolkit.fluxcd.io/v1beta1\nsource.toolkit.fluxcd.io/v1beta2\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 22:46:44.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1132" for this suite. 01/18/23 22:46:44.771
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:46:44.782
Jan 18 22:46:44.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename kubelet-test 01/18/23 22:46:44.786
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:44.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:44.829
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jan 18 22:46:44.842: INFO: Waiting up to 5m0s for pod "busybox-scheduling-77569a97-4133-46b3-8774-6041a481edc8" in namespace "kubelet-test-9924" to be "running and ready"
Jan 18 22:46:44.868: INFO: Pod "busybox-scheduling-77569a97-4133-46b3-8774-6041a481edc8": Phase="Pending", Reason="", readiness=false. Elapsed: 26.908373ms
Jan 18 22:46:44.869: INFO: The phase of Pod busybox-scheduling-77569a97-4133-46b3-8774-6041a481edc8 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:46:46.874: INFO: Pod "busybox-scheduling-77569a97-4133-46b3-8774-6041a481edc8": Phase="Running", Reason="", readiness=true. Elapsed: 2.032856549s
Jan 18 22:46:46.874: INFO: The phase of Pod busybox-scheduling-77569a97-4133-46b3-8774-6041a481edc8 is Running (Ready = true)
Jan 18 22:46:46.875: INFO: Pod "busybox-scheduling-77569a97-4133-46b3-8774-6041a481edc8" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 18 22:46:46.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9924" for this suite. 01/18/23 22:46:46.907
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":286,"skipped":5100,"failed":0}
------------------------------
• [2.135 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:46:44.782
    Jan 18 22:46:44.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename kubelet-test 01/18/23 22:46:44.786
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:44.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:44.829
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jan 18 22:46:44.842: INFO: Waiting up to 5m0s for pod "busybox-scheduling-77569a97-4133-46b3-8774-6041a481edc8" in namespace "kubelet-test-9924" to be "running and ready"
    Jan 18 22:46:44.868: INFO: Pod "busybox-scheduling-77569a97-4133-46b3-8774-6041a481edc8": Phase="Pending", Reason="", readiness=false. Elapsed: 26.908373ms
    Jan 18 22:46:44.869: INFO: The phase of Pod busybox-scheduling-77569a97-4133-46b3-8774-6041a481edc8 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:46:46.874: INFO: Pod "busybox-scheduling-77569a97-4133-46b3-8774-6041a481edc8": Phase="Running", Reason="", readiness=true. Elapsed: 2.032856549s
    Jan 18 22:46:46.874: INFO: The phase of Pod busybox-scheduling-77569a97-4133-46b3-8774-6041a481edc8 is Running (Ready = true)
    Jan 18 22:46:46.875: INFO: Pod "busybox-scheduling-77569a97-4133-46b3-8774-6041a481edc8" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 18 22:46:46.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-9924" for this suite. 01/18/23 22:46:46.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:46:46.92
Jan 18 22:46:46.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename dns 01/18/23 22:46:46.922
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:46.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:46.946
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3739.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3739.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 01/18/23 22:46:46.951
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3739.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3739.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 01/18/23 22:46:46.951
STEP: creating a pod to probe /etc/hosts 01/18/23 22:46:46.951
STEP: submitting the pod to kubernetes 01/18/23 22:46:46.952
Jan 18 22:46:46.964: INFO: Waiting up to 15m0s for pod "dns-test-f90e119e-8ca5-402e-8c14-649d707eaa6d" in namespace "dns-3739" to be "running"
Jan 18 22:46:46.969: INFO: Pod "dns-test-f90e119e-8ca5-402e-8c14-649d707eaa6d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.919714ms
Jan 18 22:46:48.975: INFO: Pod "dns-test-f90e119e-8ca5-402e-8c14-649d707eaa6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011066647s
Jan 18 22:46:50.977: INFO: Pod "dns-test-f90e119e-8ca5-402e-8c14-649d707eaa6d": Phase="Running", Reason="", readiness=true. Elapsed: 4.013410234s
Jan 18 22:46:50.978: INFO: Pod "dns-test-f90e119e-8ca5-402e-8c14-649d707eaa6d" satisfied condition "running"
STEP: retrieving the pod 01/18/23 22:46:50.978
STEP: looking for the results for each expected name from probers 01/18/23 22:46:50.984
Jan 18 22:46:51.006: INFO: DNS probes using dns-3739/dns-test-f90e119e-8ca5-402e-8c14-649d707eaa6d succeeded

STEP: deleting the pod 01/18/23 22:46:51.006
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 18 22:46:51.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3739" for this suite. 01/18/23 22:46:51.034
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":287,"skipped":5110,"failed":0}
------------------------------
• [4.123 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:46:46.92
    Jan 18 22:46:46.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename dns 01/18/23 22:46:46.922
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:46.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:46.946
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3739.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3739.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     01/18/23 22:46:46.951
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3739.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3739.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     01/18/23 22:46:46.951
    STEP: creating a pod to probe /etc/hosts 01/18/23 22:46:46.951
    STEP: submitting the pod to kubernetes 01/18/23 22:46:46.952
    Jan 18 22:46:46.964: INFO: Waiting up to 15m0s for pod "dns-test-f90e119e-8ca5-402e-8c14-649d707eaa6d" in namespace "dns-3739" to be "running"
    Jan 18 22:46:46.969: INFO: Pod "dns-test-f90e119e-8ca5-402e-8c14-649d707eaa6d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.919714ms
    Jan 18 22:46:48.975: INFO: Pod "dns-test-f90e119e-8ca5-402e-8c14-649d707eaa6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011066647s
    Jan 18 22:46:50.977: INFO: Pod "dns-test-f90e119e-8ca5-402e-8c14-649d707eaa6d": Phase="Running", Reason="", readiness=true. Elapsed: 4.013410234s
    Jan 18 22:46:50.978: INFO: Pod "dns-test-f90e119e-8ca5-402e-8c14-649d707eaa6d" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 22:46:50.978
    STEP: looking for the results for each expected name from probers 01/18/23 22:46:50.984
    Jan 18 22:46:51.006: INFO: DNS probes using dns-3739/dns-test-f90e119e-8ca5-402e-8c14-649d707eaa6d succeeded

    STEP: deleting the pod 01/18/23 22:46:51.006
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 18 22:46:51.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3739" for this suite. 01/18/23 22:46:51.034
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:46:51.043
Jan 18 22:46:51.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename configmap 01/18/23 22:46:51.045
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:51.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:51.073
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-9301/configmap-test-bf5f6681-0535-4eb0-b30e-a6d21af43afb 01/18/23 22:46:51.079
STEP: Creating a pod to test consume configMaps 01/18/23 22:46:51.086
Jan 18 22:46:51.096: INFO: Waiting up to 5m0s for pod "pod-configmaps-b19e63b3-1c14-4158-a291-575301121f51" in namespace "configmap-9301" to be "Succeeded or Failed"
Jan 18 22:46:51.103: INFO: Pod "pod-configmaps-b19e63b3-1c14-4158-a291-575301121f51": Phase="Pending", Reason="", readiness=false. Elapsed: 7.063459ms
Jan 18 22:46:53.110: INFO: Pod "pod-configmaps-b19e63b3-1c14-4158-a291-575301121f51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013396313s
Jan 18 22:46:55.109: INFO: Pod "pod-configmaps-b19e63b3-1c14-4158-a291-575301121f51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012657727s
STEP: Saw pod success 01/18/23 22:46:55.109
Jan 18 22:46:55.110: INFO: Pod "pod-configmaps-b19e63b3-1c14-4158-a291-575301121f51" satisfied condition "Succeeded or Failed"
Jan 18 22:46:55.114: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-b19e63b3-1c14-4158-a291-575301121f51 container env-test: <nil>
STEP: delete the pod 01/18/23 22:46:55.122
Jan 18 22:46:55.136: INFO: Waiting for pod pod-configmaps-b19e63b3-1c14-4158-a291-575301121f51 to disappear
Jan 18 22:46:55.142: INFO: Pod pod-configmaps-b19e63b3-1c14-4158-a291-575301121f51 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 22:46:55.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9301" for this suite. 01/18/23 22:46:55.148
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":288,"skipped":5114,"failed":0}
------------------------------
• [4.112 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:46:51.043
    Jan 18 22:46:51.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename configmap 01/18/23 22:46:51.045
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:51.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:51.073
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-9301/configmap-test-bf5f6681-0535-4eb0-b30e-a6d21af43afb 01/18/23 22:46:51.079
    STEP: Creating a pod to test consume configMaps 01/18/23 22:46:51.086
    Jan 18 22:46:51.096: INFO: Waiting up to 5m0s for pod "pod-configmaps-b19e63b3-1c14-4158-a291-575301121f51" in namespace "configmap-9301" to be "Succeeded or Failed"
    Jan 18 22:46:51.103: INFO: Pod "pod-configmaps-b19e63b3-1c14-4158-a291-575301121f51": Phase="Pending", Reason="", readiness=false. Elapsed: 7.063459ms
    Jan 18 22:46:53.110: INFO: Pod "pod-configmaps-b19e63b3-1c14-4158-a291-575301121f51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013396313s
    Jan 18 22:46:55.109: INFO: Pod "pod-configmaps-b19e63b3-1c14-4158-a291-575301121f51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012657727s
    STEP: Saw pod success 01/18/23 22:46:55.109
    Jan 18 22:46:55.110: INFO: Pod "pod-configmaps-b19e63b3-1c14-4158-a291-575301121f51" satisfied condition "Succeeded or Failed"
    Jan 18 22:46:55.114: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-b19e63b3-1c14-4158-a291-575301121f51 container env-test: <nil>
    STEP: delete the pod 01/18/23 22:46:55.122
    Jan 18 22:46:55.136: INFO: Waiting for pod pod-configmaps-b19e63b3-1c14-4158-a291-575301121f51 to disappear
    Jan 18 22:46:55.142: INFO: Pod pod-configmaps-b19e63b3-1c14-4158-a291-575301121f51 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 22:46:55.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9301" for this suite. 01/18/23 22:46:55.148
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:46:55.155
Jan 18 22:46:55.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename endpointslice 01/18/23 22:46:55.158
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:55.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:55.184
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 01/18/23 22:47:00.541
STEP: referencing matching pods with named port 01/18/23 22:47:05.552
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/18/23 22:47:10.564
STEP: recreating EndpointSlices after they've been deleted 01/18/23 22:47:15.577
Jan 18 22:47:15.608: INFO: EndpointSlice for Service endpointslice-3603/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 18 22:47:25.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3603" for this suite. 01/18/23 22:47:25.641
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":289,"skipped":5119,"failed":0}
------------------------------
• [SLOW TEST] [30.493 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:46:55.155
    Jan 18 22:46:55.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename endpointslice 01/18/23 22:46:55.158
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:46:55.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:46:55.184
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 01/18/23 22:47:00.541
    STEP: referencing matching pods with named port 01/18/23 22:47:05.552
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/18/23 22:47:10.564
    STEP: recreating EndpointSlices after they've been deleted 01/18/23 22:47:15.577
    Jan 18 22:47:15.608: INFO: EndpointSlice for Service endpointslice-3603/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 18 22:47:25.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-3603" for this suite. 01/18/23 22:47:25.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:47:25.661
Jan 18 22:47:25.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename services 01/18/23 22:47:25.664
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:25.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:25.715
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 01/18/23 22:47:25.839
Jan 18 22:47:25.839: INFO: Creating e2e-svc-a-k9hcr
Jan 18 22:47:25.855: INFO: Creating e2e-svc-b-wqdfv
Jan 18 22:47:25.865: INFO: Creating e2e-svc-c-72c2r
STEP: deleting service collection 01/18/23 22:47:25.884
Jan 18 22:47:25.925: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 22:47:25.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4844" for this suite. 01/18/23 22:47:25.934
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":290,"skipped":5144,"failed":0}
------------------------------
• [0.288 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:47:25.661
    Jan 18 22:47:25.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename services 01/18/23 22:47:25.664
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:25.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:25.715
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 01/18/23 22:47:25.839
    Jan 18 22:47:25.839: INFO: Creating e2e-svc-a-k9hcr
    Jan 18 22:47:25.855: INFO: Creating e2e-svc-b-wqdfv
    Jan 18 22:47:25.865: INFO: Creating e2e-svc-c-72c2r
    STEP: deleting service collection 01/18/23 22:47:25.884
    Jan 18 22:47:25.925: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 22:47:25.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4844" for this suite. 01/18/23 22:47:25.934
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:47:25.951
Jan 18 22:47:25.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename webhook 01/18/23 22:47:25.954
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:25.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:25.99
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 22:47:26.017
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:47:26.804
STEP: Deploying the webhook pod 01/18/23 22:47:26.815
STEP: Wait for the deployment to be ready 01/18/23 22:47:26.829
Jan 18 22:47:26.843: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 22:47:28.859
STEP: Verifying the service has paired with the endpoint 01/18/23 22:47:28.877
Jan 18 22:47:29.878: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/18/23 22:47:29.883
STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 22:47:29.883
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/18/23 22:47:29.905
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/18/23 22:47:30.973
STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 22:47:30.973
STEP: Having no error when timeout is longer than webhook latency 01/18/23 22:47:32.036
STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 22:47:32.036
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/18/23 22:47:37.085
STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 22:47:37.086
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 22:47:42.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-884" for this suite. 01/18/23 22:47:42.14
STEP: Destroying namespace "webhook-884-markers" for this suite. 01/18/23 22:47:42.148
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":291,"skipped":5154,"failed":0}
------------------------------
• [SLOW TEST] [16.257 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:47:25.951
    Jan 18 22:47:25.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename webhook 01/18/23 22:47:25.954
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:25.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:25.99
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 22:47:26.017
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:47:26.804
    STEP: Deploying the webhook pod 01/18/23 22:47:26.815
    STEP: Wait for the deployment to be ready 01/18/23 22:47:26.829
    Jan 18 22:47:26.843: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 22:47:28.859
    STEP: Verifying the service has paired with the endpoint 01/18/23 22:47:28.877
    Jan 18 22:47:29.878: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/18/23 22:47:29.883
    STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 22:47:29.883
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/18/23 22:47:29.905
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/18/23 22:47:30.973
    STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 22:47:30.973
    STEP: Having no error when timeout is longer than webhook latency 01/18/23 22:47:32.036
    STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 22:47:32.036
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/18/23 22:47:37.085
    STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 22:47:37.086
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 22:47:42.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-884" for this suite. 01/18/23 22:47:42.14
    STEP: Destroying namespace "webhook-884-markers" for this suite. 01/18/23 22:47:42.148
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:47:42.216
Jan 18 22:47:42.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename ingress 01/18/23 22:47:42.222
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:42.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:42.301
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 01/18/23 22:47:42.331
STEP: getting /apis/networking.k8s.io 01/18/23 22:47:42.335
STEP: getting /apis/networking.k8s.iov1 01/18/23 22:47:42.337
STEP: creating 01/18/23 22:47:42.339
STEP: getting 01/18/23 22:47:42.359
STEP: listing 01/18/23 22:47:42.372
STEP: watching 01/18/23 22:47:42.377
Jan 18 22:47:42.377: INFO: starting watch
STEP: cluster-wide listing 01/18/23 22:47:42.379
STEP: cluster-wide watching 01/18/23 22:47:42.386
Jan 18 22:47:42.387: INFO: starting watch
STEP: patching 01/18/23 22:47:42.391
STEP: updating 01/18/23 22:47:42.4
Jan 18 22:47:42.415: INFO: waiting for watch events with expected annotations
Jan 18 22:47:42.415: INFO: saw patched and updated annotations
STEP: patching /status 01/18/23 22:47:42.416
STEP: updating /status 01/18/23 22:47:42.431
STEP: get /status 01/18/23 22:47:42.458
STEP: deleting 01/18/23 22:47:42.463
STEP: deleting a collection 01/18/23 22:47:42.487
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Jan 18 22:47:42.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-9636" for this suite. 01/18/23 22:47:42.52
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":292,"skipped":5180,"failed":0}
------------------------------
• [0.311 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:47:42.216
    Jan 18 22:47:42.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename ingress 01/18/23 22:47:42.222
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:42.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:42.301
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 01/18/23 22:47:42.331
    STEP: getting /apis/networking.k8s.io 01/18/23 22:47:42.335
    STEP: getting /apis/networking.k8s.iov1 01/18/23 22:47:42.337
    STEP: creating 01/18/23 22:47:42.339
    STEP: getting 01/18/23 22:47:42.359
    STEP: listing 01/18/23 22:47:42.372
    STEP: watching 01/18/23 22:47:42.377
    Jan 18 22:47:42.377: INFO: starting watch
    STEP: cluster-wide listing 01/18/23 22:47:42.379
    STEP: cluster-wide watching 01/18/23 22:47:42.386
    Jan 18 22:47:42.387: INFO: starting watch
    STEP: patching 01/18/23 22:47:42.391
    STEP: updating 01/18/23 22:47:42.4
    Jan 18 22:47:42.415: INFO: waiting for watch events with expected annotations
    Jan 18 22:47:42.415: INFO: saw patched and updated annotations
    STEP: patching /status 01/18/23 22:47:42.416
    STEP: updating /status 01/18/23 22:47:42.431
    STEP: get /status 01/18/23 22:47:42.458
    STEP: deleting 01/18/23 22:47:42.463
    STEP: deleting a collection 01/18/23 22:47:42.487
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Jan 18 22:47:42.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-9636" for this suite. 01/18/23 22:47:42.52
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:47:42.529
Jan 18 22:47:42.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename gc 01/18/23 22:47:42.531
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:42.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:42.557
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 01/18/23 22:47:42.564
STEP: Wait for the Deployment to create new ReplicaSet 01/18/23 22:47:42.576
STEP: delete the deployment 01/18/23 22:47:42.585
STEP: wait for all rs to be garbage collected 01/18/23 22:47:42.609
STEP: expected 0 pods, got 2 pods 01/18/23 22:47:42.67
STEP: Gathering metrics 01/18/23 22:47:43.183
Jan 18 22:47:43.255: INFO: Waiting up to 5m0s for pod "kube-controller-manager-v1-25-1-18760-m" in namespace "kube-system" to be "running and ready"
Jan 18 22:47:43.271: INFO: Pod "kube-controller-manager-v1-25-1-18760-m": Phase="Running", Reason="", readiness=true. Elapsed: 16.148755ms
Jan 18 22:47:43.271: INFO: The phase of Pod kube-controller-manager-v1-25-1-18760-m is Running (Ready = true)
Jan 18 22:47:43.271: INFO: Pod "kube-controller-manager-v1-25-1-18760-m" satisfied condition "running and ready"
Jan 18 22:47:43.429: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 18 22:47:43.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5951" for this suite. 01/18/23 22:47:43.436
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":293,"skipped":5188,"failed":0}
------------------------------
• [0.915 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:47:42.529
    Jan 18 22:47:42.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename gc 01/18/23 22:47:42.531
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:42.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:42.557
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 01/18/23 22:47:42.564
    STEP: Wait for the Deployment to create new ReplicaSet 01/18/23 22:47:42.576
    STEP: delete the deployment 01/18/23 22:47:42.585
    STEP: wait for all rs to be garbage collected 01/18/23 22:47:42.609
    STEP: expected 0 pods, got 2 pods 01/18/23 22:47:42.67
    STEP: Gathering metrics 01/18/23 22:47:43.183
    Jan 18 22:47:43.255: INFO: Waiting up to 5m0s for pod "kube-controller-manager-v1-25-1-18760-m" in namespace "kube-system" to be "running and ready"
    Jan 18 22:47:43.271: INFO: Pod "kube-controller-manager-v1-25-1-18760-m": Phase="Running", Reason="", readiness=true. Elapsed: 16.148755ms
    Jan 18 22:47:43.271: INFO: The phase of Pod kube-controller-manager-v1-25-1-18760-m is Running (Ready = true)
    Jan 18 22:47:43.271: INFO: Pod "kube-controller-manager-v1-25-1-18760-m" satisfied condition "running and ready"
    Jan 18 22:47:43.429: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 18 22:47:43.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5951" for this suite. 01/18/23 22:47:43.436
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:47:43.444
Jan 18 22:47:43.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename container-probe 01/18/23 22:47:43.445
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:43.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:43.477
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-a10adb81-9db0-44b2-9b25-835dccc5a6b9 in namespace container-probe-7944 01/18/23 22:47:43.491
Jan 18 22:47:43.512: INFO: Waiting up to 5m0s for pod "liveness-a10adb81-9db0-44b2-9b25-835dccc5a6b9" in namespace "container-probe-7944" to be "not pending"
Jan 18 22:47:43.522: INFO: Pod "liveness-a10adb81-9db0-44b2-9b25-835dccc5a6b9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.249515ms
Jan 18 22:47:45.527: INFO: Pod "liveness-a10adb81-9db0-44b2-9b25-835dccc5a6b9": Phase="Running", Reason="", readiness=true. Elapsed: 2.015373607s
Jan 18 22:47:45.527: INFO: Pod "liveness-a10adb81-9db0-44b2-9b25-835dccc5a6b9" satisfied condition "not pending"
Jan 18 22:47:45.527: INFO: Started pod liveness-a10adb81-9db0-44b2-9b25-835dccc5a6b9 in namespace container-probe-7944
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 22:47:45.527
Jan 18 22:47:45.532: INFO: Initial restart count of pod liveness-a10adb81-9db0-44b2-9b25-835dccc5a6b9 is 0
STEP: deleting the pod 01/18/23 22:51:46.35
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 18 22:51:46.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7944" for this suite. 01/18/23 22:51:46.391
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":294,"skipped":5202,"failed":0}
------------------------------
• [SLOW TEST] [242.957 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:47:43.444
    Jan 18 22:47:43.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename container-probe 01/18/23 22:47:43.445
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:47:43.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:47:43.477
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-a10adb81-9db0-44b2-9b25-835dccc5a6b9 in namespace container-probe-7944 01/18/23 22:47:43.491
    Jan 18 22:47:43.512: INFO: Waiting up to 5m0s for pod "liveness-a10adb81-9db0-44b2-9b25-835dccc5a6b9" in namespace "container-probe-7944" to be "not pending"
    Jan 18 22:47:43.522: INFO: Pod "liveness-a10adb81-9db0-44b2-9b25-835dccc5a6b9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.249515ms
    Jan 18 22:47:45.527: INFO: Pod "liveness-a10adb81-9db0-44b2-9b25-835dccc5a6b9": Phase="Running", Reason="", readiness=true. Elapsed: 2.015373607s
    Jan 18 22:47:45.527: INFO: Pod "liveness-a10adb81-9db0-44b2-9b25-835dccc5a6b9" satisfied condition "not pending"
    Jan 18 22:47:45.527: INFO: Started pod liveness-a10adb81-9db0-44b2-9b25-835dccc5a6b9 in namespace container-probe-7944
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 22:47:45.527
    Jan 18 22:47:45.532: INFO: Initial restart count of pod liveness-a10adb81-9db0-44b2-9b25-835dccc5a6b9 is 0
    STEP: deleting the pod 01/18/23 22:51:46.35
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 18 22:51:46.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7944" for this suite. 01/18/23 22:51:46.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:51:46.443
Jan 18 22:51:46.443: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename runtimeclass 01/18/23 22:51:46.446
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:51:46.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:51:46.503
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jan 18 22:51:46.530: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1017 to be scheduled
Jan 18 22:51:46.533: INFO: 1 pods are not scheduled: [runtimeclass-1017/test-runtimeclass-runtimeclass-1017-preconfigured-handler-brzfg(833e1c0b-416b-4392-9994-9eeb869cdb9c)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 18 22:51:48.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1017" for this suite. 01/18/23 22:51:48.553
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":295,"skipped":5221,"failed":0}
------------------------------
• [2.120 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:51:46.443
    Jan 18 22:51:46.443: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename runtimeclass 01/18/23 22:51:46.446
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:51:46.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:51:46.503
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jan 18 22:51:46.530: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1017 to be scheduled
    Jan 18 22:51:46.533: INFO: 1 pods are not scheduled: [runtimeclass-1017/test-runtimeclass-runtimeclass-1017-preconfigured-handler-brzfg(833e1c0b-416b-4392-9994-9eeb869cdb9c)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 18 22:51:48.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-1017" for this suite. 01/18/23 22:51:48.553
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:51:48.567
Jan 18 22:51:48.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename sched-pred 01/18/23 22:51:48.57
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:51:48.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:51:48.601
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 18 22:51:48.609: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 18 22:51:48.623: INFO: Waiting for terminating namespaces to be deleted...
Jan 18 22:51:48.642: INFO: 
Logging pods the apiserver thinks is on node v1-25-1-18760-w before test
Jan 18 22:51:48.654: INFO: helm-controller-578bd944df-dzr2c from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:51:48.654: INFO: 	Container manager ready: true, restart count 0
Jan 18 22:51:48.654: INFO: notification-controller-6459696d4f-gdmj8 from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:51:48.654: INFO: 	Container manager ready: true, restart count 0
Jan 18 22:51:48.654: INFO: source-controller-84f6bcbfb8-ljjjm from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:51:48.654: INFO: 	Container manager ready: true, restart count 0
Jan 18 22:51:48.654: INFO: calico-node-59tts from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
Jan 18 22:51:48.654: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 22:51:48.654: INFO: coredns-coredns-5f9b955d9-pjqxb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:51:48.654: INFO: 	Container coredns ready: true, restart count 0
Jan 18 22:51:48.654: INFO: coredns-coredns-5f9b955d9-vsgsc from kube-system started at 2023-01-18 22:34:27 +0000 UTC (1 container statuses recorded)
Jan 18 22:51:48.654: INFO: 	Container coredns ready: true, restart count 0
Jan 18 22:51:48.654: INFO: coredns-coredns-autoscaler-7d98f7496d-7zbwb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:51:48.654: INFO: 	Container autoscaler ready: true, restart count 0
Jan 18 22:51:48.654: INFO: kube-proxy-rzflq from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
Jan 18 22:51:48.654: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 22:51:48.654: INFO: metrics-server-58498b9c56-q88cp from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 22:51:48.654: INFO: 	Container metrics-server ready: true, restart count 0
Jan 18 22:51:48.655: INFO: nginx-proxy-v1-25-1-18760-w from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
Jan 18 22:51:48.655: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 18 22:51:48.655: INFO: nodelocaldns-skdxm from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
Jan 18 22:51:48.655: INFO: 	Container node-cache ready: true, restart count 0
Jan 18 22:51:48.655: INFO: openstack-cinder-csi-controllerplugin-55d57dd799-6496v from kube-system started at 2023-01-18 14:39:48 +0000 UTC (6 container statuses recorded)
Jan 18 22:51:48.655: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 18 22:51:48.655: INFO: 	Container csi-attacher ready: true, restart count 0
Jan 18 22:51:48.655: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan 18 22:51:48.655: INFO: 	Container csi-resizer ready: true, restart count 0
Jan 18 22:51:48.655: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan 18 22:51:48.655: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 18 22:51:48.655: INFO: openstack-cinder-csi-nodeplugin-w7sxz from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
Jan 18 22:51:48.655: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 18 22:51:48.655: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 18 22:51:48.655: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 18 22:51:48.655: INFO: sonobuoy-systemd-logs-daemon-set-c355d4758f184dbb-d68zx from sonobuoy started at 2023-01-18 21:29:30 +0000 UTC (2 container statuses recorded)
Jan 18 22:51:48.655: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:51:48.655: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 22:51:48.655: INFO: 
Logging pods the apiserver thinks is on node v1-25-1-18760-w2 before test
Jan 18 22:51:48.671: INFO: calico-node-qx4f9 from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
Jan 18 22:51:48.671: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 22:51:48.671: INFO: kube-proxy-8r2pt from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
Jan 18 22:51:48.671: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 22:51:48.671: INFO: nginx-proxy-v1-25-1-18760-w2 from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
Jan 18 22:51:48.671: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 18 22:51:48.671: INFO: nodelocaldns-dzwbj from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
Jan 18 22:51:48.671: INFO: 	Container node-cache ready: true, restart count 0
Jan 18 22:51:48.671: INFO: openstack-cinder-csi-nodeplugin-qm575 from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
Jan 18 22:51:48.671: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 18 22:51:48.671: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 18 22:51:48.671: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 18 22:51:48.671: INFO: test-runtimeclass-runtimeclass-1017-preconfigured-handler-brzfg from runtimeclass-1017 started at 2023-01-18 22:51:46 +0000 UTC (1 container statuses recorded)
Jan 18 22:51:48.671: INFO: 	Container test ready: false, restart count 0
Jan 18 22:51:48.671: INFO: sonobuoy from sonobuoy started at 2023-01-18 21:29:28 +0000 UTC (1 container statuses recorded)
Jan 18 22:51:48.671: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 18 22:51:48.671: INFO: sonobuoy-e2e-job-2f3390b6578e4b8c from sonobuoy started at 2023-01-18 21:29:29 +0000 UTC (2 container statuses recorded)
Jan 18 22:51:48.671: INFO: 	Container e2e ready: true, restart count 0
Jan 18 22:51:48.671: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:51:48.671: INFO: sonobuoy-systemd-logs-daemon-set-c355d4758f184dbb-svdn4 from sonobuoy started at 2023-01-18 21:29:29 +0000 UTC (2 container statuses recorded)
Jan 18 22:51:48.671: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:51:48.671: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 01/18/23 22:51:48.671
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.173b88fd7702fd5e], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 01/18/23 22:51:48.714
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 18 22:51:49.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-573" for this suite. 01/18/23 22:51:49.717
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":296,"skipped":5253,"failed":0}
------------------------------
• [1.159 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:51:48.567
    Jan 18 22:51:48.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename sched-pred 01/18/23 22:51:48.57
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:51:48.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:51:48.601
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 18 22:51:48.609: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 18 22:51:48.623: INFO: Waiting for terminating namespaces to be deleted...
    Jan 18 22:51:48.642: INFO: 
    Logging pods the apiserver thinks is on node v1-25-1-18760-w before test
    Jan 18 22:51:48.654: INFO: helm-controller-578bd944df-dzr2c from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 22:51:48.654: INFO: 	Container manager ready: true, restart count 0
    Jan 18 22:51:48.654: INFO: notification-controller-6459696d4f-gdmj8 from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 22:51:48.654: INFO: 	Container manager ready: true, restart count 0
    Jan 18 22:51:48.654: INFO: source-controller-84f6bcbfb8-ljjjm from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 22:51:48.654: INFO: 	Container manager ready: true, restart count 0
    Jan 18 22:51:48.654: INFO: calico-node-59tts from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
    Jan 18 22:51:48.654: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 22:51:48.654: INFO: coredns-coredns-5f9b955d9-pjqxb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 22:51:48.654: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 22:51:48.654: INFO: coredns-coredns-5f9b955d9-vsgsc from kube-system started at 2023-01-18 22:34:27 +0000 UTC (1 container statuses recorded)
    Jan 18 22:51:48.654: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 22:51:48.654: INFO: coredns-coredns-autoscaler-7d98f7496d-7zbwb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 22:51:48.654: INFO: 	Container autoscaler ready: true, restart count 0
    Jan 18 22:51:48.654: INFO: kube-proxy-rzflq from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
    Jan 18 22:51:48.654: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 22:51:48.654: INFO: metrics-server-58498b9c56-q88cp from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 22:51:48.654: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 18 22:51:48.655: INFO: nginx-proxy-v1-25-1-18760-w from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
    Jan 18 22:51:48.655: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 18 22:51:48.655: INFO: nodelocaldns-skdxm from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
    Jan 18 22:51:48.655: INFO: 	Container node-cache ready: true, restart count 0
    Jan 18 22:51:48.655: INFO: openstack-cinder-csi-controllerplugin-55d57dd799-6496v from kube-system started at 2023-01-18 14:39:48 +0000 UTC (6 container statuses recorded)
    Jan 18 22:51:48.655: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 18 22:51:48.655: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan 18 22:51:48.655: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan 18 22:51:48.655: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan 18 22:51:48.655: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan 18 22:51:48.655: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 18 22:51:48.655: INFO: openstack-cinder-csi-nodeplugin-w7sxz from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
    Jan 18 22:51:48.655: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 18 22:51:48.655: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 18 22:51:48.655: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 18 22:51:48.655: INFO: sonobuoy-systemd-logs-daemon-set-c355d4758f184dbb-d68zx from sonobuoy started at 2023-01-18 21:29:30 +0000 UTC (2 container statuses recorded)
    Jan 18 22:51:48.655: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 22:51:48.655: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 18 22:51:48.655: INFO: 
    Logging pods the apiserver thinks is on node v1-25-1-18760-w2 before test
    Jan 18 22:51:48.671: INFO: calico-node-qx4f9 from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
    Jan 18 22:51:48.671: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 22:51:48.671: INFO: kube-proxy-8r2pt from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
    Jan 18 22:51:48.671: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 22:51:48.671: INFO: nginx-proxy-v1-25-1-18760-w2 from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
    Jan 18 22:51:48.671: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 18 22:51:48.671: INFO: nodelocaldns-dzwbj from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
    Jan 18 22:51:48.671: INFO: 	Container node-cache ready: true, restart count 0
    Jan 18 22:51:48.671: INFO: openstack-cinder-csi-nodeplugin-qm575 from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
    Jan 18 22:51:48.671: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 18 22:51:48.671: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 18 22:51:48.671: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 18 22:51:48.671: INFO: test-runtimeclass-runtimeclass-1017-preconfigured-handler-brzfg from runtimeclass-1017 started at 2023-01-18 22:51:46 +0000 UTC (1 container statuses recorded)
    Jan 18 22:51:48.671: INFO: 	Container test ready: false, restart count 0
    Jan 18 22:51:48.671: INFO: sonobuoy from sonobuoy started at 2023-01-18 21:29:28 +0000 UTC (1 container statuses recorded)
    Jan 18 22:51:48.671: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 18 22:51:48.671: INFO: sonobuoy-e2e-job-2f3390b6578e4b8c from sonobuoy started at 2023-01-18 21:29:29 +0000 UTC (2 container statuses recorded)
    Jan 18 22:51:48.671: INFO: 	Container e2e ready: true, restart count 0
    Jan 18 22:51:48.671: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 22:51:48.671: INFO: sonobuoy-systemd-logs-daemon-set-c355d4758f184dbb-svdn4 from sonobuoy started at 2023-01-18 21:29:29 +0000 UTC (2 container statuses recorded)
    Jan 18 22:51:48.671: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 22:51:48.671: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 01/18/23 22:51:48.671
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.173b88fd7702fd5e], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 01/18/23 22:51:48.714
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 22:51:49.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-573" for this suite. 01/18/23 22:51:49.717
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:51:49.733
Jan 18 22:51:49.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename emptydir 01/18/23 22:51:49.735
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:51:49.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:51:49.76
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 01/18/23 22:51:49.766
Jan 18 22:51:49.775: INFO: Waiting up to 5m0s for pod "pod-dd3e093b-298a-407f-b148-dce117fe8b7d" in namespace "emptydir-2710" to be "Succeeded or Failed"
Jan 18 22:51:49.780: INFO: Pod "pod-dd3e093b-298a-407f-b148-dce117fe8b7d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.835238ms
Jan 18 22:51:51.788: INFO: Pod "pod-dd3e093b-298a-407f-b148-dce117fe8b7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012574052s
Jan 18 22:51:53.792: INFO: Pod "pod-dd3e093b-298a-407f-b148-dce117fe8b7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016781449s
STEP: Saw pod success 01/18/23 22:51:53.792
Jan 18 22:51:53.793: INFO: Pod "pod-dd3e093b-298a-407f-b148-dce117fe8b7d" satisfied condition "Succeeded or Failed"
Jan 18 22:51:53.804: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-dd3e093b-298a-407f-b148-dce117fe8b7d container test-container: <nil>
STEP: delete the pod 01/18/23 22:51:53.839
Jan 18 22:51:53.863: INFO: Waiting for pod pod-dd3e093b-298a-407f-b148-dce117fe8b7d to disappear
Jan 18 22:51:53.868: INFO: Pod pod-dd3e093b-298a-407f-b148-dce117fe8b7d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 22:51:53.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2710" for this suite. 01/18/23 22:51:53.874
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":297,"skipped":5286,"failed":0}
------------------------------
• [4.156 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:51:49.733
    Jan 18 22:51:49.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename emptydir 01/18/23 22:51:49.735
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:51:49.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:51:49.76
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/18/23 22:51:49.766
    Jan 18 22:51:49.775: INFO: Waiting up to 5m0s for pod "pod-dd3e093b-298a-407f-b148-dce117fe8b7d" in namespace "emptydir-2710" to be "Succeeded or Failed"
    Jan 18 22:51:49.780: INFO: Pod "pod-dd3e093b-298a-407f-b148-dce117fe8b7d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.835238ms
    Jan 18 22:51:51.788: INFO: Pod "pod-dd3e093b-298a-407f-b148-dce117fe8b7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012574052s
    Jan 18 22:51:53.792: INFO: Pod "pod-dd3e093b-298a-407f-b148-dce117fe8b7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016781449s
    STEP: Saw pod success 01/18/23 22:51:53.792
    Jan 18 22:51:53.793: INFO: Pod "pod-dd3e093b-298a-407f-b148-dce117fe8b7d" satisfied condition "Succeeded or Failed"
    Jan 18 22:51:53.804: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-dd3e093b-298a-407f-b148-dce117fe8b7d container test-container: <nil>
    STEP: delete the pod 01/18/23 22:51:53.839
    Jan 18 22:51:53.863: INFO: Waiting for pod pod-dd3e093b-298a-407f-b148-dce117fe8b7d to disappear
    Jan 18 22:51:53.868: INFO: Pod pod-dd3e093b-298a-407f-b148-dce117fe8b7d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 22:51:53.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2710" for this suite. 01/18/23 22:51:53.874
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:51:53.905
Jan 18 22:51:53.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename sched-preemption 01/18/23 22:51:53.907
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:51:53.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:51:53.943
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 18 22:51:53.966: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 22:52:54.053: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:52:54.06
Jan 18 22:52:54.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename sched-preemption-path 01/18/23 22:52:54.062
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:52:54.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:52:54.09
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Jan 18 22:52:54.118: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jan 18 22:52:54.124: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Jan 18 22:52:54.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-6635" for this suite. 01/18/23 22:52:54.167
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 18 22:52:54.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2410" for this suite. 01/18/23 22:52:54.201
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":298,"skipped":5360,"failed":0}
------------------------------
• [SLOW TEST] [60.381 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:51:53.905
    Jan 18 22:51:53.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename sched-preemption 01/18/23 22:51:53.907
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:51:53.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:51:53.943
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 18 22:51:53.966: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 22:52:54.053: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:52:54.06
    Jan 18 22:52:54.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename sched-preemption-path 01/18/23 22:52:54.062
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:52:54.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:52:54.09
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Jan 18 22:52:54.118: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jan 18 22:52:54.124: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Jan 18 22:52:54.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-6635" for this suite. 01/18/23 22:52:54.167
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 22:52:54.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-2410" for this suite. 01/18/23 22:52:54.201
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:52:54.297
Jan 18 22:52:54.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename pods 01/18/23 22:52:54.299
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:52:54.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:52:54.332
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Jan 18 22:52:54.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: creating the pod 01/18/23 22:52:54.341
STEP: submitting the pod to kubernetes 01/18/23 22:52:54.341
Jan 18 22:52:54.353: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-dbffee67-105d-4d7e-be72-2275780277af" in namespace "pods-8610" to be "running and ready"
Jan 18 22:52:54.358: INFO: Pod "pod-exec-websocket-dbffee67-105d-4d7e-be72-2275780277af": Phase="Pending", Reason="", readiness=false. Elapsed: 5.185495ms
Jan 18 22:52:54.359: INFO: The phase of Pod pod-exec-websocket-dbffee67-105d-4d7e-be72-2275780277af is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:52:56.369: INFO: Pod "pod-exec-websocket-dbffee67-105d-4d7e-be72-2275780277af": Phase="Running", Reason="", readiness=true. Elapsed: 2.015481764s
Jan 18 22:52:56.369: INFO: The phase of Pod pod-exec-websocket-dbffee67-105d-4d7e-be72-2275780277af is Running (Ready = true)
Jan 18 22:52:56.369: INFO: Pod "pod-exec-websocket-dbffee67-105d-4d7e-be72-2275780277af" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 22:52:56.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8610" for this suite. 01/18/23 22:52:56.49
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":299,"skipped":5371,"failed":0}
------------------------------
• [2.202 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:52:54.297
    Jan 18 22:52:54.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename pods 01/18/23 22:52:54.299
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:52:54.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:52:54.332
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Jan 18 22:52:54.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: creating the pod 01/18/23 22:52:54.341
    STEP: submitting the pod to kubernetes 01/18/23 22:52:54.341
    Jan 18 22:52:54.353: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-dbffee67-105d-4d7e-be72-2275780277af" in namespace "pods-8610" to be "running and ready"
    Jan 18 22:52:54.358: INFO: Pod "pod-exec-websocket-dbffee67-105d-4d7e-be72-2275780277af": Phase="Pending", Reason="", readiness=false. Elapsed: 5.185495ms
    Jan 18 22:52:54.359: INFO: The phase of Pod pod-exec-websocket-dbffee67-105d-4d7e-be72-2275780277af is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:52:56.369: INFO: Pod "pod-exec-websocket-dbffee67-105d-4d7e-be72-2275780277af": Phase="Running", Reason="", readiness=true. Elapsed: 2.015481764s
    Jan 18 22:52:56.369: INFO: The phase of Pod pod-exec-websocket-dbffee67-105d-4d7e-be72-2275780277af is Running (Ready = true)
    Jan 18 22:52:56.369: INFO: Pod "pod-exec-websocket-dbffee67-105d-4d7e-be72-2275780277af" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 22:52:56.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8610" for this suite. 01/18/23 22:52:56.49
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:52:56.501
Jan 18 22:52:56.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename downward-api 01/18/23 22:52:56.504
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:52:56.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:52:56.535
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 01/18/23 22:52:56.542
Jan 18 22:52:56.563: INFO: Waiting up to 5m0s for pod "downward-api-13347ec0-98f9-43cb-aa4c-70916a56625b" in namespace "downward-api-6720" to be "Succeeded or Failed"
Jan 18 22:52:56.612: INFO: Pod "downward-api-13347ec0-98f9-43cb-aa4c-70916a56625b": Phase="Pending", Reason="", readiness=false. Elapsed: 48.299163ms
Jan 18 22:52:58.617: INFO: Pod "downward-api-13347ec0-98f9-43cb-aa4c-70916a56625b": Phase="Running", Reason="", readiness=true. Elapsed: 2.053936174s
Jan 18 22:53:00.627: INFO: Pod "downward-api-13347ec0-98f9-43cb-aa4c-70916a56625b": Phase="Running", Reason="", readiness=false. Elapsed: 4.063236018s
Jan 18 22:53:02.621: INFO: Pod "downward-api-13347ec0-98f9-43cb-aa4c-70916a56625b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057267585s
STEP: Saw pod success 01/18/23 22:53:02.621
Jan 18 22:53:02.621: INFO: Pod "downward-api-13347ec0-98f9-43cb-aa4c-70916a56625b" satisfied condition "Succeeded or Failed"
Jan 18 22:53:02.626: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downward-api-13347ec0-98f9-43cb-aa4c-70916a56625b container dapi-container: <nil>
STEP: delete the pod 01/18/23 22:53:02.636
Jan 18 22:53:02.653: INFO: Waiting for pod downward-api-13347ec0-98f9-43cb-aa4c-70916a56625b to disappear
Jan 18 22:53:02.657: INFO: Pod downward-api-13347ec0-98f9-43cb-aa4c-70916a56625b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 18 22:53:02.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6720" for this suite. 01/18/23 22:53:02.663
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":300,"skipped":5373,"failed":0}
------------------------------
• [SLOW TEST] [6.169 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:52:56.501
    Jan 18 22:52:56.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename downward-api 01/18/23 22:52:56.504
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:52:56.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:52:56.535
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 01/18/23 22:52:56.542
    Jan 18 22:52:56.563: INFO: Waiting up to 5m0s for pod "downward-api-13347ec0-98f9-43cb-aa4c-70916a56625b" in namespace "downward-api-6720" to be "Succeeded or Failed"
    Jan 18 22:52:56.612: INFO: Pod "downward-api-13347ec0-98f9-43cb-aa4c-70916a56625b": Phase="Pending", Reason="", readiness=false. Elapsed: 48.299163ms
    Jan 18 22:52:58.617: INFO: Pod "downward-api-13347ec0-98f9-43cb-aa4c-70916a56625b": Phase="Running", Reason="", readiness=true. Elapsed: 2.053936174s
    Jan 18 22:53:00.627: INFO: Pod "downward-api-13347ec0-98f9-43cb-aa4c-70916a56625b": Phase="Running", Reason="", readiness=false. Elapsed: 4.063236018s
    Jan 18 22:53:02.621: INFO: Pod "downward-api-13347ec0-98f9-43cb-aa4c-70916a56625b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057267585s
    STEP: Saw pod success 01/18/23 22:53:02.621
    Jan 18 22:53:02.621: INFO: Pod "downward-api-13347ec0-98f9-43cb-aa4c-70916a56625b" satisfied condition "Succeeded or Failed"
    Jan 18 22:53:02.626: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downward-api-13347ec0-98f9-43cb-aa4c-70916a56625b container dapi-container: <nil>
    STEP: delete the pod 01/18/23 22:53:02.636
    Jan 18 22:53:02.653: INFO: Waiting for pod downward-api-13347ec0-98f9-43cb-aa4c-70916a56625b to disappear
    Jan 18 22:53:02.657: INFO: Pod downward-api-13347ec0-98f9-43cb-aa4c-70916a56625b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 18 22:53:02.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6720" for this suite. 01/18/23 22:53:02.663
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:53:02.671
Jan 18 22:53:02.671: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename resourcequota 01/18/23 22:53:02.673
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:02.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:02.706
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 01/18/23 22:53:02.712
STEP: Getting a ResourceQuota 01/18/23 22:53:02.718
STEP: Listing all ResourceQuotas with LabelSelector 01/18/23 22:53:02.731
STEP: Patching the ResourceQuota 01/18/23 22:53:02.737
STEP: Deleting a Collection of ResourceQuotas 01/18/23 22:53:02.743
STEP: Verifying the deleted ResourceQuota 01/18/23 22:53:02.757
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 22:53:02.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4100" for this suite. 01/18/23 22:53:02.767
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":301,"skipped":5375,"failed":0}
------------------------------
• [0.105 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:53:02.671
    Jan 18 22:53:02.671: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename resourcequota 01/18/23 22:53:02.673
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:02.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:02.706
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 01/18/23 22:53:02.712
    STEP: Getting a ResourceQuota 01/18/23 22:53:02.718
    STEP: Listing all ResourceQuotas with LabelSelector 01/18/23 22:53:02.731
    STEP: Patching the ResourceQuota 01/18/23 22:53:02.737
    STEP: Deleting a Collection of ResourceQuotas 01/18/23 22:53:02.743
    STEP: Verifying the deleted ResourceQuota 01/18/23 22:53:02.757
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 22:53:02.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4100" for this suite. 01/18/23 22:53:02.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:53:02.777
Jan 18 22:53:02.777: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename var-expansion 01/18/23 22:53:02.78
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:02.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:02.816
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 01/18/23 22:53:02.822
Jan 18 22:53:02.837: INFO: Waiting up to 5m0s for pod "var-expansion-df90f9c5-eff7-40e3-b7cc-9062c3477179" in namespace "var-expansion-5765" to be "Succeeded or Failed"
Jan 18 22:53:02.843: INFO: Pod "var-expansion-df90f9c5-eff7-40e3-b7cc-9062c3477179": Phase="Pending", Reason="", readiness=false. Elapsed: 6.325907ms
Jan 18 22:53:04.850: INFO: Pod "var-expansion-df90f9c5-eff7-40e3-b7cc-9062c3477179": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012850378s
Jan 18 22:53:06.850: INFO: Pod "var-expansion-df90f9c5-eff7-40e3-b7cc-9062c3477179": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013385283s
STEP: Saw pod success 01/18/23 22:53:06.85
Jan 18 22:53:06.850: INFO: Pod "var-expansion-df90f9c5-eff7-40e3-b7cc-9062c3477179" satisfied condition "Succeeded or Failed"
Jan 18 22:53:06.856: INFO: Trying to get logs from node v1-25-1-18760-w2 pod var-expansion-df90f9c5-eff7-40e3-b7cc-9062c3477179 container dapi-container: <nil>
STEP: delete the pod 01/18/23 22:53:06.864
Jan 18 22:53:06.877: INFO: Waiting for pod var-expansion-df90f9c5-eff7-40e3-b7cc-9062c3477179 to disappear
Jan 18 22:53:06.882: INFO: Pod var-expansion-df90f9c5-eff7-40e3-b7cc-9062c3477179 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 18 22:53:06.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5765" for this suite. 01/18/23 22:53:06.889
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":302,"skipped":5385,"failed":0}
------------------------------
• [4.120 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:53:02.777
    Jan 18 22:53:02.777: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename var-expansion 01/18/23 22:53:02.78
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:02.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:02.816
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 01/18/23 22:53:02.822
    Jan 18 22:53:02.837: INFO: Waiting up to 5m0s for pod "var-expansion-df90f9c5-eff7-40e3-b7cc-9062c3477179" in namespace "var-expansion-5765" to be "Succeeded or Failed"
    Jan 18 22:53:02.843: INFO: Pod "var-expansion-df90f9c5-eff7-40e3-b7cc-9062c3477179": Phase="Pending", Reason="", readiness=false. Elapsed: 6.325907ms
    Jan 18 22:53:04.850: INFO: Pod "var-expansion-df90f9c5-eff7-40e3-b7cc-9062c3477179": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012850378s
    Jan 18 22:53:06.850: INFO: Pod "var-expansion-df90f9c5-eff7-40e3-b7cc-9062c3477179": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013385283s
    STEP: Saw pod success 01/18/23 22:53:06.85
    Jan 18 22:53:06.850: INFO: Pod "var-expansion-df90f9c5-eff7-40e3-b7cc-9062c3477179" satisfied condition "Succeeded or Failed"
    Jan 18 22:53:06.856: INFO: Trying to get logs from node v1-25-1-18760-w2 pod var-expansion-df90f9c5-eff7-40e3-b7cc-9062c3477179 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 22:53:06.864
    Jan 18 22:53:06.877: INFO: Waiting for pod var-expansion-df90f9c5-eff7-40e3-b7cc-9062c3477179 to disappear
    Jan 18 22:53:06.882: INFO: Pod var-expansion-df90f9c5-eff7-40e3-b7cc-9062c3477179 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 18 22:53:06.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5765" for this suite. 01/18/23 22:53:06.889
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:53:06.905
Jan 18 22:53:06.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename kubectl 01/18/23 22:53:06.907
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:06.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:06.93
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 01/18/23 22:53:06.936
Jan 18 22:53:06.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-2449 create -f -'
Jan 18 22:53:08.026: INFO: stderr: ""
Jan 18 22:53:08.026: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/18/23 22:53:08.026
Jan 18 22:53:09.156: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 22:53:09.156: INFO: Found 0 / 1
Jan 18 22:53:10.031: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 22:53:10.032: INFO: Found 1 / 1
Jan 18 22:53:10.032: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 01/18/23 22:53:10.032
Jan 18 22:53:10.037: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 22:53:10.037: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 18 22:53:10.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-2449 patch pod agnhost-primary-kjzg6 -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 18 22:53:10.183: INFO: stderr: ""
Jan 18 22:53:10.183: INFO: stdout: "pod/agnhost-primary-kjzg6 patched\n"
STEP: checking annotations 01/18/23 22:53:10.183
Jan 18 22:53:10.188: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 22:53:10.188: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 22:53:10.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2449" for this suite. 01/18/23 22:53:10.206
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":303,"skipped":5393,"failed":0}
------------------------------
• [3.309 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:53:06.905
    Jan 18 22:53:06.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename kubectl 01/18/23 22:53:06.907
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:06.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:06.93
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 01/18/23 22:53:06.936
    Jan 18 22:53:06.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-2449 create -f -'
    Jan 18 22:53:08.026: INFO: stderr: ""
    Jan 18 22:53:08.026: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/18/23 22:53:08.026
    Jan 18 22:53:09.156: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 22:53:09.156: INFO: Found 0 / 1
    Jan 18 22:53:10.031: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 22:53:10.032: INFO: Found 1 / 1
    Jan 18 22:53:10.032: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 01/18/23 22:53:10.032
    Jan 18 22:53:10.037: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 22:53:10.037: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 18 22:53:10.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-2449 patch pod agnhost-primary-kjzg6 -p {"metadata":{"annotations":{"x":"y"}}}'
    Jan 18 22:53:10.183: INFO: stderr: ""
    Jan 18 22:53:10.183: INFO: stdout: "pod/agnhost-primary-kjzg6 patched\n"
    STEP: checking annotations 01/18/23 22:53:10.183
    Jan 18 22:53:10.188: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 22:53:10.188: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 22:53:10.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2449" for this suite. 01/18/23 22:53:10.206
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:53:10.215
Jan 18 22:53:10.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename replicaset 01/18/23 22:53:10.217
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:10.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:10.245
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jan 18 22:53:10.269: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 18 22:53:15.285: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 22:53:15.285
STEP: Scaling up "test-rs" replicaset  01/18/23 22:53:15.286
Jan 18 22:53:15.304: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 01/18/23 22:53:15.304
W0118 22:53:15.322289      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 18 22:53:15.325: INFO: observed ReplicaSet test-rs in namespace replicaset-9450 with ReadyReplicas 1, AvailableReplicas 1
Jan 18 22:53:15.345: INFO: observed ReplicaSet test-rs in namespace replicaset-9450 with ReadyReplicas 1, AvailableReplicas 1
Jan 18 22:53:15.389: INFO: observed ReplicaSet test-rs in namespace replicaset-9450 with ReadyReplicas 1, AvailableReplicas 1
Jan 18 22:53:15.399: INFO: observed ReplicaSet test-rs in namespace replicaset-9450 with ReadyReplicas 1, AvailableReplicas 1
Jan 18 22:53:16.997: INFO: observed ReplicaSet test-rs in namespace replicaset-9450 with ReadyReplicas 2, AvailableReplicas 2
Jan 18 22:53:17.797: INFO: observed Replicaset test-rs in namespace replicaset-9450 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 18 22:53:17.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9450" for this suite. 01/18/23 22:53:17.807
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":304,"skipped":5405,"failed":0}
------------------------------
• [SLOW TEST] [7.601 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:53:10.215
    Jan 18 22:53:10.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename replicaset 01/18/23 22:53:10.217
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:10.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:10.245
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jan 18 22:53:10.269: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 18 22:53:15.285: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 22:53:15.285
    STEP: Scaling up "test-rs" replicaset  01/18/23 22:53:15.286
    Jan 18 22:53:15.304: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 01/18/23 22:53:15.304
    W0118 22:53:15.322289      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 18 22:53:15.325: INFO: observed ReplicaSet test-rs in namespace replicaset-9450 with ReadyReplicas 1, AvailableReplicas 1
    Jan 18 22:53:15.345: INFO: observed ReplicaSet test-rs in namespace replicaset-9450 with ReadyReplicas 1, AvailableReplicas 1
    Jan 18 22:53:15.389: INFO: observed ReplicaSet test-rs in namespace replicaset-9450 with ReadyReplicas 1, AvailableReplicas 1
    Jan 18 22:53:15.399: INFO: observed ReplicaSet test-rs in namespace replicaset-9450 with ReadyReplicas 1, AvailableReplicas 1
    Jan 18 22:53:16.997: INFO: observed ReplicaSet test-rs in namespace replicaset-9450 with ReadyReplicas 2, AvailableReplicas 2
    Jan 18 22:53:17.797: INFO: observed Replicaset test-rs in namespace replicaset-9450 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 18 22:53:17.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-9450" for this suite. 01/18/23 22:53:17.807
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:53:17.818
Jan 18 22:53:17.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename init-container 01/18/23 22:53:17.819
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:17.882
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:17.889
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 01/18/23 22:53:17.895
Jan 18 22:53:17.896: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 18 22:53:21.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-530" for this suite. 01/18/23 22:53:21.834
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":305,"skipped":5415,"failed":0}
------------------------------
• [4.024 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:53:17.818
    Jan 18 22:53:17.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename init-container 01/18/23 22:53:17.819
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:17.882
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:17.889
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 01/18/23 22:53:17.895
    Jan 18 22:53:17.896: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 18 22:53:21.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-530" for this suite. 01/18/23 22:53:21.834
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:53:21.846
Jan 18 22:53:21.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename emptydir 01/18/23 22:53:21.847
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:21.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:21.872
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 01/18/23 22:53:21.876
Jan 18 22:53:21.885: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-0fa06941-66af-4147-b6b0-5d6255d311a6" in namespace "emptydir-1721" to be "running"
Jan 18 22:53:21.891: INFO: Pod "pod-sharedvolume-0fa06941-66af-4147-b6b0-5d6255d311a6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.376104ms
Jan 18 22:53:23.897: INFO: Pod "pod-sharedvolume-0fa06941-66af-4147-b6b0-5d6255d311a6": Phase="Running", Reason="", readiness=false. Elapsed: 2.011059161s
Jan 18 22:53:23.897: INFO: Pod "pod-sharedvolume-0fa06941-66af-4147-b6b0-5d6255d311a6" satisfied condition "running"
STEP: Reading file content from the nginx-container 01/18/23 22:53:23.897
Jan 18 22:53:23.897: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1721 PodName:pod-sharedvolume-0fa06941-66af-4147-b6b0-5d6255d311a6 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:53:23.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 22:53:23.899: INFO: ExecWithOptions: Clientset creation
Jan 18 22:53:23.899: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-1721/pods/pod-sharedvolume-0fa06941-66af-4147-b6b0-5d6255d311a6/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan 18 22:53:24.006: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 22:53:24.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1721" for this suite. 01/18/23 22:53:24.015
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":306,"skipped":5428,"failed":0}
------------------------------
• [2.396 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:53:21.846
    Jan 18 22:53:21.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename emptydir 01/18/23 22:53:21.847
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:21.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:21.872
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 01/18/23 22:53:21.876
    Jan 18 22:53:21.885: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-0fa06941-66af-4147-b6b0-5d6255d311a6" in namespace "emptydir-1721" to be "running"
    Jan 18 22:53:21.891: INFO: Pod "pod-sharedvolume-0fa06941-66af-4147-b6b0-5d6255d311a6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.376104ms
    Jan 18 22:53:23.897: INFO: Pod "pod-sharedvolume-0fa06941-66af-4147-b6b0-5d6255d311a6": Phase="Running", Reason="", readiness=false. Elapsed: 2.011059161s
    Jan 18 22:53:23.897: INFO: Pod "pod-sharedvolume-0fa06941-66af-4147-b6b0-5d6255d311a6" satisfied condition "running"
    STEP: Reading file content from the nginx-container 01/18/23 22:53:23.897
    Jan 18 22:53:23.897: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1721 PodName:pod-sharedvolume-0fa06941-66af-4147-b6b0-5d6255d311a6 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:53:23.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 22:53:23.899: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:53:23.899: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-1721/pods/pod-sharedvolume-0fa06941-66af-4147-b6b0-5d6255d311a6/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jan 18 22:53:24.006: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 22:53:24.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1721" for this suite. 01/18/23 22:53:24.015
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:53:24.25
Jan 18 22:53:24.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 22:53:24.251
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:24.274
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:24.277
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-b991b027-d346-47cf-be52-ac639121f8e4 01/18/23 22:53:24.281
STEP: Creating a pod to test consume secrets 01/18/23 22:53:24.286
Jan 18 22:53:24.297: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-123edbc2-c91e-4655-8f74-fea113be783a" in namespace "projected-6287" to be "Succeeded or Failed"
Jan 18 22:53:24.313: INFO: Pod "pod-projected-secrets-123edbc2-c91e-4655-8f74-fea113be783a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.896006ms
Jan 18 22:53:26.320: INFO: Pod "pod-projected-secrets-123edbc2-c91e-4655-8f74-fea113be783a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023100556s
Jan 18 22:53:28.323: INFO: Pod "pod-projected-secrets-123edbc2-c91e-4655-8f74-fea113be783a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0265365s
STEP: Saw pod success 01/18/23 22:53:28.324
Jan 18 22:53:28.324: INFO: Pod "pod-projected-secrets-123edbc2-c91e-4655-8f74-fea113be783a" satisfied condition "Succeeded or Failed"
Jan 18 22:53:28.329: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-secrets-123edbc2-c91e-4655-8f74-fea113be783a container projected-secret-volume-test: <nil>
STEP: delete the pod 01/18/23 22:53:28.337
Jan 18 22:53:28.379: INFO: Waiting for pod pod-projected-secrets-123edbc2-c91e-4655-8f74-fea113be783a to disappear
Jan 18 22:53:28.393: INFO: Pod pod-projected-secrets-123edbc2-c91e-4655-8f74-fea113be783a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 18 22:53:28.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6287" for this suite. 01/18/23 22:53:28.398
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":307,"skipped":5477,"failed":0}
------------------------------
• [4.156 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:53:24.25
    Jan 18 22:53:24.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 22:53:24.251
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:24.274
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:24.277
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-b991b027-d346-47cf-be52-ac639121f8e4 01/18/23 22:53:24.281
    STEP: Creating a pod to test consume secrets 01/18/23 22:53:24.286
    Jan 18 22:53:24.297: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-123edbc2-c91e-4655-8f74-fea113be783a" in namespace "projected-6287" to be "Succeeded or Failed"
    Jan 18 22:53:24.313: INFO: Pod "pod-projected-secrets-123edbc2-c91e-4655-8f74-fea113be783a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.896006ms
    Jan 18 22:53:26.320: INFO: Pod "pod-projected-secrets-123edbc2-c91e-4655-8f74-fea113be783a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023100556s
    Jan 18 22:53:28.323: INFO: Pod "pod-projected-secrets-123edbc2-c91e-4655-8f74-fea113be783a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0265365s
    STEP: Saw pod success 01/18/23 22:53:28.324
    Jan 18 22:53:28.324: INFO: Pod "pod-projected-secrets-123edbc2-c91e-4655-8f74-fea113be783a" satisfied condition "Succeeded or Failed"
    Jan 18 22:53:28.329: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-secrets-123edbc2-c91e-4655-8f74-fea113be783a container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 22:53:28.337
    Jan 18 22:53:28.379: INFO: Waiting for pod pod-projected-secrets-123edbc2-c91e-4655-8f74-fea113be783a to disappear
    Jan 18 22:53:28.393: INFO: Pod pod-projected-secrets-123edbc2-c91e-4655-8f74-fea113be783a no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 18 22:53:28.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6287" for this suite. 01/18/23 22:53:28.398
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:53:28.427
Jan 18 22:53:28.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename emptydir 01/18/23 22:53:28.429
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:28.447
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:28.451
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/18/23 22:53:28.459
Jan 18 22:53:28.476: INFO: Waiting up to 5m0s for pod "pod-5cf48d91-8c87-450c-95ab-85a63dacd4df" in namespace "emptydir-974" to be "Succeeded or Failed"
Jan 18 22:53:28.482: INFO: Pod "pod-5cf48d91-8c87-450c-95ab-85a63dacd4df": Phase="Pending", Reason="", readiness=false. Elapsed: 6.149205ms
Jan 18 22:53:30.490: INFO: Pod "pod-5cf48d91-8c87-450c-95ab-85a63dacd4df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013572821s
Jan 18 22:53:32.490: INFO: Pod "pod-5cf48d91-8c87-450c-95ab-85a63dacd4df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013597916s
STEP: Saw pod success 01/18/23 22:53:32.49
Jan 18 22:53:32.490: INFO: Pod "pod-5cf48d91-8c87-450c-95ab-85a63dacd4df" satisfied condition "Succeeded or Failed"
Jan 18 22:53:32.497: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-5cf48d91-8c87-450c-95ab-85a63dacd4df container test-container: <nil>
STEP: delete the pod 01/18/23 22:53:32.511
Jan 18 22:53:32.532: INFO: Waiting for pod pod-5cf48d91-8c87-450c-95ab-85a63dacd4df to disappear
Jan 18 22:53:32.538: INFO: Pod pod-5cf48d91-8c87-450c-95ab-85a63dacd4df no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 22:53:32.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-974" for this suite. 01/18/23 22:53:32.545
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":308,"skipped":5567,"failed":0}
------------------------------
• [4.125 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:53:28.427
    Jan 18 22:53:28.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename emptydir 01/18/23 22:53:28.429
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:28.447
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:28.451
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/18/23 22:53:28.459
    Jan 18 22:53:28.476: INFO: Waiting up to 5m0s for pod "pod-5cf48d91-8c87-450c-95ab-85a63dacd4df" in namespace "emptydir-974" to be "Succeeded or Failed"
    Jan 18 22:53:28.482: INFO: Pod "pod-5cf48d91-8c87-450c-95ab-85a63dacd4df": Phase="Pending", Reason="", readiness=false. Elapsed: 6.149205ms
    Jan 18 22:53:30.490: INFO: Pod "pod-5cf48d91-8c87-450c-95ab-85a63dacd4df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013572821s
    Jan 18 22:53:32.490: INFO: Pod "pod-5cf48d91-8c87-450c-95ab-85a63dacd4df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013597916s
    STEP: Saw pod success 01/18/23 22:53:32.49
    Jan 18 22:53:32.490: INFO: Pod "pod-5cf48d91-8c87-450c-95ab-85a63dacd4df" satisfied condition "Succeeded or Failed"
    Jan 18 22:53:32.497: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-5cf48d91-8c87-450c-95ab-85a63dacd4df container test-container: <nil>
    STEP: delete the pod 01/18/23 22:53:32.511
    Jan 18 22:53:32.532: INFO: Waiting for pod pod-5cf48d91-8c87-450c-95ab-85a63dacd4df to disappear
    Jan 18 22:53:32.538: INFO: Pod pod-5cf48d91-8c87-450c-95ab-85a63dacd4df no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 22:53:32.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-974" for this suite. 01/18/23 22:53:32.545
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:53:32.558
Jan 18 22:53:32.558: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename var-expansion 01/18/23 22:53:32.56
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:32.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:32.603
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 01/18/23 22:53:32.609
STEP: waiting for pod running 01/18/23 22:53:32.626
Jan 18 22:53:32.626: INFO: Waiting up to 2m0s for pod "var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a" in namespace "var-expansion-4144" to be "running"
Jan 18 22:53:32.655: INFO: Pod "var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a": Phase="Pending", Reason="", readiness=false. Elapsed: 28.601683ms
Jan 18 22:53:34.659: INFO: Pod "var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a": Phase="Running", Reason="", readiness=true. Elapsed: 2.033386518s
Jan 18 22:53:34.659: INFO: Pod "var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a" satisfied condition "running"
STEP: creating a file in subpath 01/18/23 22:53:34.659
Jan 18 22:53:34.664: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4144 PodName:var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:53:34.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 22:53:34.666: INFO: ExecWithOptions: Clientset creation
Jan 18 22:53:34.666: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-4144/pods/var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 01/18/23 22:53:34.748
Jan 18 22:53:34.754: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4144 PodName:var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:53:34.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 22:53:34.755: INFO: ExecWithOptions: Clientset creation
Jan 18 22:53:34.755: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-4144/pods/var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 01/18/23 22:53:34.869
Jan 18 22:53:35.387: INFO: Successfully updated pod "var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a"
STEP: waiting for annotated pod running 01/18/23 22:53:35.387
Jan 18 22:53:35.388: INFO: Waiting up to 2m0s for pod "var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a" in namespace "var-expansion-4144" to be "running"
Jan 18 22:53:35.393: INFO: Pod "var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a": Phase="Running", Reason="", readiness=true. Elapsed: 5.165151ms
Jan 18 22:53:35.393: INFO: Pod "var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a" satisfied condition "running"
STEP: deleting the pod gracefully 01/18/23 22:53:35.394
Jan 18 22:53:35.394: INFO: Deleting pod "var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a" in namespace "var-expansion-4144"
Jan 18 22:53:35.406: INFO: Wait up to 5m0s for pod "var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 18 22:54:09.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4144" for this suite. 01/18/23 22:54:09.425
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":309,"skipped":5597,"failed":0}
------------------------------
• [SLOW TEST] [36.876 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:53:32.558
    Jan 18 22:53:32.558: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename var-expansion 01/18/23 22:53:32.56
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:53:32.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:53:32.603
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 01/18/23 22:53:32.609
    STEP: waiting for pod running 01/18/23 22:53:32.626
    Jan 18 22:53:32.626: INFO: Waiting up to 2m0s for pod "var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a" in namespace "var-expansion-4144" to be "running"
    Jan 18 22:53:32.655: INFO: Pod "var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a": Phase="Pending", Reason="", readiness=false. Elapsed: 28.601683ms
    Jan 18 22:53:34.659: INFO: Pod "var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a": Phase="Running", Reason="", readiness=true. Elapsed: 2.033386518s
    Jan 18 22:53:34.659: INFO: Pod "var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a" satisfied condition "running"
    STEP: creating a file in subpath 01/18/23 22:53:34.659
    Jan 18 22:53:34.664: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4144 PodName:var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:53:34.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 22:53:34.666: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:53:34.666: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-4144/pods/var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 01/18/23 22:53:34.748
    Jan 18 22:53:34.754: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4144 PodName:var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:53:34.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 22:53:34.755: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:53:34.755: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-4144/pods/var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 01/18/23 22:53:34.869
    Jan 18 22:53:35.387: INFO: Successfully updated pod "var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a"
    STEP: waiting for annotated pod running 01/18/23 22:53:35.387
    Jan 18 22:53:35.388: INFO: Waiting up to 2m0s for pod "var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a" in namespace "var-expansion-4144" to be "running"
    Jan 18 22:53:35.393: INFO: Pod "var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a": Phase="Running", Reason="", readiness=true. Elapsed: 5.165151ms
    Jan 18 22:53:35.393: INFO: Pod "var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a" satisfied condition "running"
    STEP: deleting the pod gracefully 01/18/23 22:53:35.394
    Jan 18 22:53:35.394: INFO: Deleting pod "var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a" in namespace "var-expansion-4144"
    Jan 18 22:53:35.406: INFO: Wait up to 5m0s for pod "var-expansion-03ed5156-d422-4748-9e32-8ef0a77db01a" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 18 22:54:09.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-4144" for this suite. 01/18/23 22:54:09.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:54:09.436
Jan 18 22:54:09.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename webhook 01/18/23 22:54:09.439
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:09.463
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:09.468
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 22:54:09.486
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:54:10.765
STEP: Deploying the webhook pod 01/18/23 22:54:10.775
STEP: Wait for the deployment to be ready 01/18/23 22:54:10.788
Jan 18 22:54:10.806: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 22:54:12.83
STEP: Verifying the service has paired with the endpoint 01/18/23 22:54:12.854
Jan 18 22:54:13.855: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 01/18/23 22:54:13.861
STEP: create a pod 01/18/23 22:54:13.885
Jan 18 22:54:13.904: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-6100" to be "running"
Jan 18 22:54:13.914: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.938876ms
Jan 18 22:54:15.919: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014728347s
Jan 18 22:54:17.920: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.01645909s
Jan 18 22:54:17.921: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 01/18/23 22:54:17.921
Jan 18 22:54:17.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=webhook-6100 attach --namespace=webhook-6100 to-be-attached-pod -i -c=container1'
Jan 18 22:54:18.061: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 22:54:18.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6100" for this suite. 01/18/23 22:54:18.084
STEP: Destroying namespace "webhook-6100-markers" for this suite. 01/18/23 22:54:18.09
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":310,"skipped":5602,"failed":0}
------------------------------
• [SLOW TEST] [8.741 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:54:09.436
    Jan 18 22:54:09.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename webhook 01/18/23 22:54:09.439
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:09.463
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:09.468
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 22:54:09.486
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:54:10.765
    STEP: Deploying the webhook pod 01/18/23 22:54:10.775
    STEP: Wait for the deployment to be ready 01/18/23 22:54:10.788
    Jan 18 22:54:10.806: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 22:54:12.83
    STEP: Verifying the service has paired with the endpoint 01/18/23 22:54:12.854
    Jan 18 22:54:13.855: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 01/18/23 22:54:13.861
    STEP: create a pod 01/18/23 22:54:13.885
    Jan 18 22:54:13.904: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-6100" to be "running"
    Jan 18 22:54:13.914: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.938876ms
    Jan 18 22:54:15.919: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014728347s
    Jan 18 22:54:17.920: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.01645909s
    Jan 18 22:54:17.921: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 01/18/23 22:54:17.921
    Jan 18 22:54:17.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=webhook-6100 attach --namespace=webhook-6100 to-be-attached-pod -i -c=container1'
    Jan 18 22:54:18.061: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 22:54:18.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6100" for this suite. 01/18/23 22:54:18.084
    STEP: Destroying namespace "webhook-6100-markers" for this suite. 01/18/23 22:54:18.09
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:54:18.177
Jan 18 22:54:18.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename replicaset 01/18/23 22:54:18.179
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:18.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:18.25
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 01/18/23 22:54:18.274
STEP: Verify that the required pods have come up. 01/18/23 22:54:18.292
Jan 18 22:54:18.308: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 22:54:18.308
Jan 18 22:54:18.308: INFO: Waiting up to 5m0s for pod "test-rs-t7chh" in namespace "replicaset-8402" to be "running"
Jan 18 22:54:18.345: INFO: Pod "test-rs-t7chh": Phase="Pending", Reason="", readiness=false. Elapsed: 37.402689ms
Jan 18 22:54:20.350: INFO: Pod "test-rs-t7chh": Phase="Running", Reason="", readiness=true. Elapsed: 2.042117703s
Jan 18 22:54:20.350: INFO: Pod "test-rs-t7chh" satisfied condition "running"
STEP: Getting /status 01/18/23 22:54:20.35
Jan 18 22:54:20.355: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 01/18/23 22:54:20.355
Jan 18 22:54:20.367: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 01/18/23 22:54:20.367
Jan 18 22:54:20.371: INFO: Observed &ReplicaSet event: ADDED
Jan 18 22:54:20.372: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 22:54:20.372: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 22:54:20.373: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 22:54:20.373: INFO: Found replicaset test-rs in namespace replicaset-8402 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 18 22:54:20.373: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 01/18/23 22:54:20.373
Jan 18 22:54:20.374: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 18 22:54:20.383: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 01/18/23 22:54:20.384
Jan 18 22:54:20.387: INFO: Observed &ReplicaSet event: ADDED
Jan 18 22:54:20.388: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 22:54:20.389: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 22:54:20.389: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 22:54:20.390: INFO: Observed replicaset test-rs in namespace replicaset-8402 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 18 22:54:20.390: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 22:54:20.390: INFO: Found replicaset test-rs in namespace replicaset-8402 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan 18 22:54:20.390: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 18 22:54:20.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8402" for this suite. 01/18/23 22:54:20.397
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":311,"skipped":5602,"failed":0}
------------------------------
• [2.226 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:54:18.177
    Jan 18 22:54:18.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename replicaset 01/18/23 22:54:18.179
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:18.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:18.25
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 01/18/23 22:54:18.274
    STEP: Verify that the required pods have come up. 01/18/23 22:54:18.292
    Jan 18 22:54:18.308: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 22:54:18.308
    Jan 18 22:54:18.308: INFO: Waiting up to 5m0s for pod "test-rs-t7chh" in namespace "replicaset-8402" to be "running"
    Jan 18 22:54:18.345: INFO: Pod "test-rs-t7chh": Phase="Pending", Reason="", readiness=false. Elapsed: 37.402689ms
    Jan 18 22:54:20.350: INFO: Pod "test-rs-t7chh": Phase="Running", Reason="", readiness=true. Elapsed: 2.042117703s
    Jan 18 22:54:20.350: INFO: Pod "test-rs-t7chh" satisfied condition "running"
    STEP: Getting /status 01/18/23 22:54:20.35
    Jan 18 22:54:20.355: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 01/18/23 22:54:20.355
    Jan 18 22:54:20.367: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 01/18/23 22:54:20.367
    Jan 18 22:54:20.371: INFO: Observed &ReplicaSet event: ADDED
    Jan 18 22:54:20.372: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 22:54:20.372: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 22:54:20.373: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 22:54:20.373: INFO: Found replicaset test-rs in namespace replicaset-8402 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 18 22:54:20.373: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 01/18/23 22:54:20.373
    Jan 18 22:54:20.374: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 18 22:54:20.383: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 01/18/23 22:54:20.384
    Jan 18 22:54:20.387: INFO: Observed &ReplicaSet event: ADDED
    Jan 18 22:54:20.388: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 22:54:20.389: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 22:54:20.389: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 22:54:20.390: INFO: Observed replicaset test-rs in namespace replicaset-8402 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 18 22:54:20.390: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 22:54:20.390: INFO: Found replicaset test-rs in namespace replicaset-8402 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jan 18 22:54:20.390: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 18 22:54:20.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-8402" for this suite. 01/18/23 22:54:20.397
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:54:20.407
Jan 18 22:54:20.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename watch 01/18/23 22:54:20.409
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:20.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:20.433
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 01/18/23 22:54:20.438
STEP: creating a watch on configmaps with label B 01/18/23 22:54:20.44
STEP: creating a watch on configmaps with label A or B 01/18/23 22:54:20.442
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/18/23 22:54:20.444
Jan 18 22:54:20.450: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9254  309aca2f-7493-4cff-b19f-49b639b4c558 171738 0 2023-01-18 22:54:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 22:54:20.450: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9254  309aca2f-7493-4cff-b19f-49b639b4c558 171738 0 2023-01-18 22:54:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/18/23 22:54:20.451
Jan 18 22:54:20.462: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9254  309aca2f-7493-4cff-b19f-49b639b4c558 171739 0 2023-01-18 22:54:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:54:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 22:54:20.463: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9254  309aca2f-7493-4cff-b19f-49b639b4c558 171739 0 2023-01-18 22:54:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:54:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/18/23 22:54:20.463
Jan 18 22:54:20.474: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9254  309aca2f-7493-4cff-b19f-49b639b4c558 171740 0 2023-01-18 22:54:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:54:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 22:54:20.474: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9254  309aca2f-7493-4cff-b19f-49b639b4c558 171740 0 2023-01-18 22:54:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:54:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/18/23 22:54:20.474
Jan 18 22:54:20.481: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9254  309aca2f-7493-4cff-b19f-49b639b4c558 171741 0 2023-01-18 22:54:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:54:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 22:54:20.481: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9254  309aca2f-7493-4cff-b19f-49b639b4c558 171741 0 2023-01-18 22:54:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:54:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/18/23 22:54:20.481
Jan 18 22:54:20.486: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9254  d2d2c484-2ab2-41e0-b40c-6831ceb47514 171742 0 2023-01-18 22:54:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 22:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 22:54:20.487: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9254  d2d2c484-2ab2-41e0-b40c-6831ceb47514 171742 0 2023-01-18 22:54:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 22:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/18/23 22:54:30.488
Jan 18 22:54:30.495: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9254  d2d2c484-2ab2-41e0-b40c-6831ceb47514 171822 0 2023-01-18 22:54:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 22:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 22:54:30.496: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9254  d2d2c484-2ab2-41e0-b40c-6831ceb47514 171822 0 2023-01-18 22:54:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 22:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 18 22:54:40.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9254" for this suite. 01/18/23 22:54:40.507
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":312,"skipped":5603,"failed":0}
------------------------------
• [SLOW TEST] [20.111 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:54:20.407
    Jan 18 22:54:20.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename watch 01/18/23 22:54:20.409
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:20.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:20.433
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 01/18/23 22:54:20.438
    STEP: creating a watch on configmaps with label B 01/18/23 22:54:20.44
    STEP: creating a watch on configmaps with label A or B 01/18/23 22:54:20.442
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/18/23 22:54:20.444
    Jan 18 22:54:20.450: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9254  309aca2f-7493-4cff-b19f-49b639b4c558 171738 0 2023-01-18 22:54:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 22:54:20.450: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9254  309aca2f-7493-4cff-b19f-49b639b4c558 171738 0 2023-01-18 22:54:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/18/23 22:54:20.451
    Jan 18 22:54:20.462: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9254  309aca2f-7493-4cff-b19f-49b639b4c558 171739 0 2023-01-18 22:54:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:54:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 22:54:20.463: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9254  309aca2f-7493-4cff-b19f-49b639b4c558 171739 0 2023-01-18 22:54:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:54:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/18/23 22:54:20.463
    Jan 18 22:54:20.474: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9254  309aca2f-7493-4cff-b19f-49b639b4c558 171740 0 2023-01-18 22:54:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:54:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 22:54:20.474: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9254  309aca2f-7493-4cff-b19f-49b639b4c558 171740 0 2023-01-18 22:54:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:54:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/18/23 22:54:20.474
    Jan 18 22:54:20.481: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9254  309aca2f-7493-4cff-b19f-49b639b4c558 171741 0 2023-01-18 22:54:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:54:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 22:54:20.481: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9254  309aca2f-7493-4cff-b19f-49b639b4c558 171741 0 2023-01-18 22:54:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:54:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/18/23 22:54:20.481
    Jan 18 22:54:20.486: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9254  d2d2c484-2ab2-41e0-b40c-6831ceb47514 171742 0 2023-01-18 22:54:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 22:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 22:54:20.487: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9254  d2d2c484-2ab2-41e0-b40c-6831ceb47514 171742 0 2023-01-18 22:54:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 22:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/18/23 22:54:30.488
    Jan 18 22:54:30.495: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9254  d2d2c484-2ab2-41e0-b40c-6831ceb47514 171822 0 2023-01-18 22:54:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 22:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 22:54:30.496: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9254  d2d2c484-2ab2-41e0-b40c-6831ceb47514 171822 0 2023-01-18 22:54:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 22:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 18 22:54:40.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-9254" for this suite. 01/18/23 22:54:40.507
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 22:54:40.519
Jan 18 22:54:40.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename cronjob 01/18/23 22:54:40.522
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:40.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:40.542
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 01/18/23 22:54:40.547
STEP: Ensuring a job is scheduled 01/18/23 22:54:40.554
STEP: Ensuring exactly one is scheduled 01/18/23 22:55:00.561
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/18/23 22:55:00.567
STEP: Ensuring no more jobs are scheduled 01/18/23 22:55:00.576
STEP: Removing cronjob 01/18/23 23:00:00.585
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 18 23:00:00.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3905" for this suite. 01/18/23 23:00:00.607
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":313,"skipped":5605,"failed":0}
------------------------------
• [SLOW TEST] [320.103 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 22:54:40.519
    Jan 18 22:54:40.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename cronjob 01/18/23 22:54:40.522
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:54:40.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:54:40.542
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 01/18/23 22:54:40.547
    STEP: Ensuring a job is scheduled 01/18/23 22:54:40.554
    STEP: Ensuring exactly one is scheduled 01/18/23 22:55:00.561
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/18/23 22:55:00.567
    STEP: Ensuring no more jobs are scheduled 01/18/23 22:55:00.576
    STEP: Removing cronjob 01/18/23 23:00:00.585
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 18 23:00:00.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-3905" for this suite. 01/18/23 23:00:00.607
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:00:00.628
Jan 18 23:00:00.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename job 01/18/23 23:00:00.632
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:00:00.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:00:00.664
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 01/18/23 23:00:00.695
STEP: Ensuring active pods == parallelism 01/18/23 23:00:00.706
STEP: Orphaning one of the Job's Pods 01/18/23 23:00:04.714
Jan 18 23:00:05.235: INFO: Successfully updated pod "adopt-release-fk5v5"
STEP: Checking that the Job readopts the Pod 01/18/23 23:00:05.235
Jan 18 23:00:05.235: INFO: Waiting up to 15m0s for pod "adopt-release-fk5v5" in namespace "job-5324" to be "adopted"
Jan 18 23:00:05.243: INFO: Pod "adopt-release-fk5v5": Phase="Running", Reason="", readiness=true. Elapsed: 8.214205ms
Jan 18 23:00:07.252: INFO: Pod "adopt-release-fk5v5": Phase="Running", Reason="", readiness=true. Elapsed: 2.016791871s
Jan 18 23:00:07.252: INFO: Pod "adopt-release-fk5v5" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 01/18/23 23:00:07.252
Jan 18 23:00:07.771: INFO: Successfully updated pod "adopt-release-fk5v5"
STEP: Checking that the Job releases the Pod 01/18/23 23:00:07.771
Jan 18 23:00:07.772: INFO: Waiting up to 15m0s for pod "adopt-release-fk5v5" in namespace "job-5324" to be "released"
Jan 18 23:00:07.778: INFO: Pod "adopt-release-fk5v5": Phase="Running", Reason="", readiness=true. Elapsed: 6.494477ms
Jan 18 23:00:09.805: INFO: Pod "adopt-release-fk5v5": Phase="Running", Reason="", readiness=true. Elapsed: 2.033128917s
Jan 18 23:00:09.805: INFO: Pod "adopt-release-fk5v5" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 18 23:00:09.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5324" for this suite. 01/18/23 23:00:09.814
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":314,"skipped":5605,"failed":0}
------------------------------
• [SLOW TEST] [9.197 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:00:00.628
    Jan 18 23:00:00.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename job 01/18/23 23:00:00.632
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:00:00.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:00:00.664
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 01/18/23 23:00:00.695
    STEP: Ensuring active pods == parallelism 01/18/23 23:00:00.706
    STEP: Orphaning one of the Job's Pods 01/18/23 23:00:04.714
    Jan 18 23:00:05.235: INFO: Successfully updated pod "adopt-release-fk5v5"
    STEP: Checking that the Job readopts the Pod 01/18/23 23:00:05.235
    Jan 18 23:00:05.235: INFO: Waiting up to 15m0s for pod "adopt-release-fk5v5" in namespace "job-5324" to be "adopted"
    Jan 18 23:00:05.243: INFO: Pod "adopt-release-fk5v5": Phase="Running", Reason="", readiness=true. Elapsed: 8.214205ms
    Jan 18 23:00:07.252: INFO: Pod "adopt-release-fk5v5": Phase="Running", Reason="", readiness=true. Elapsed: 2.016791871s
    Jan 18 23:00:07.252: INFO: Pod "adopt-release-fk5v5" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 01/18/23 23:00:07.252
    Jan 18 23:00:07.771: INFO: Successfully updated pod "adopt-release-fk5v5"
    STEP: Checking that the Job releases the Pod 01/18/23 23:00:07.771
    Jan 18 23:00:07.772: INFO: Waiting up to 15m0s for pod "adopt-release-fk5v5" in namespace "job-5324" to be "released"
    Jan 18 23:00:07.778: INFO: Pod "adopt-release-fk5v5": Phase="Running", Reason="", readiness=true. Elapsed: 6.494477ms
    Jan 18 23:00:09.805: INFO: Pod "adopt-release-fk5v5": Phase="Running", Reason="", readiness=true. Elapsed: 2.033128917s
    Jan 18 23:00:09.805: INFO: Pod "adopt-release-fk5v5" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 18 23:00:09.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5324" for this suite. 01/18/23 23:00:09.814
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:00:09.828
Jan 18 23:00:09.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 23:00:09.832
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:00:09.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:00:09.876
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/18/23 23:00:09.883
Jan 18 23:00:09.886: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
Jan 18 23:00:15.376: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 23:00:34.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2367" for this suite. 01/18/23 23:00:34.797
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":315,"skipped":5608,"failed":0}
------------------------------
• [SLOW TEST] [24.978 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:00:09.828
    Jan 18 23:00:09.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 23:00:09.832
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:00:09.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:00:09.876
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/18/23 23:00:09.883
    Jan 18 23:00:09.886: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    Jan 18 23:00:15.376: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 23:00:34.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2367" for this suite. 01/18/23 23:00:34.797
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:00:34.81
Jan 18 23:00:34.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename server-version 01/18/23 23:00:34.812
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:00:34.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:00:34.89
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 01/18/23 23:00:34.899
STEP: Confirm major version 01/18/23 23:00:34.906
Jan 18 23:00:34.906: INFO: Major version: 1
STEP: Confirm minor version 01/18/23 23:00:34.906
Jan 18 23:00:34.910: INFO: cleanMinorVersion: 25
Jan 18 23:00:34.910: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Jan 18 23:00:34.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-1316" for this suite. 01/18/23 23:00:34.919
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":316,"skipped":5669,"failed":0}
------------------------------
• [0.119 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:00:34.81
    Jan 18 23:00:34.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename server-version 01/18/23 23:00:34.812
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:00:34.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:00:34.89
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 01/18/23 23:00:34.899
    STEP: Confirm major version 01/18/23 23:00:34.906
    Jan 18 23:00:34.906: INFO: Major version: 1
    STEP: Confirm minor version 01/18/23 23:00:34.906
    Jan 18 23:00:34.910: INFO: cleanMinorVersion: 25
    Jan 18 23:00:34.910: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Jan 18 23:00:34.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-1316" for this suite. 01/18/23 23:00:34.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:00:34.936
Jan 18 23:00:34.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename init-container 01/18/23 23:00:34.937
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:00:34.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:00:34.964
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 01/18/23 23:00:34.972
Jan 18 23:00:34.973: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 18 23:00:40.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6616" for this suite. 01/18/23 23:00:40.526
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":317,"skipped":5683,"failed":0}
------------------------------
• [SLOW TEST] [5.597 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:00:34.936
    Jan 18 23:00:34.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename init-container 01/18/23 23:00:34.937
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:00:34.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:00:34.964
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 01/18/23 23:00:34.972
    Jan 18 23:00:34.973: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 18 23:00:40.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-6616" for this suite. 01/18/23 23:00:40.526
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:00:40.538
Jan 18 23:00:40.539: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename webhook 01/18/23 23:00:40.54
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:00:40.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:00:40.567
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 23:00:40.595
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 23:00:41.19
STEP: Deploying the webhook pod 01/18/23 23:00:41.199
STEP: Wait for the deployment to be ready 01/18/23 23:00:41.21
Jan 18 23:00:41.224: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 23:00:43.418
STEP: Verifying the service has paired with the endpoint 01/18/23 23:00:43.439
Jan 18 23:00:44.442: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 01/18/23 23:00:44.447
STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/18/23 23:00:44.472
STEP: Creating a configMap that should not be mutated 01/18/23 23:00:44.479
STEP: Patching a mutating webhook configuration's rules to include the create operation 01/18/23 23:00:44.494
STEP: Creating a configMap that should be mutated 01/18/23 23:00:44.504
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 23:00:44.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2494" for this suite. 01/18/23 23:00:44.545
STEP: Destroying namespace "webhook-2494-markers" for this suite. 01/18/23 23:00:44.555
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":318,"skipped":5701,"failed":0}
------------------------------
• [4.093 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:00:40.538
    Jan 18 23:00:40.539: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename webhook 01/18/23 23:00:40.54
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:00:40.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:00:40.567
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 23:00:40.595
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 23:00:41.19
    STEP: Deploying the webhook pod 01/18/23 23:00:41.199
    STEP: Wait for the deployment to be ready 01/18/23 23:00:41.21
    Jan 18 23:00:41.224: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 23:00:43.418
    STEP: Verifying the service has paired with the endpoint 01/18/23 23:00:43.439
    Jan 18 23:00:44.442: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 01/18/23 23:00:44.447
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/18/23 23:00:44.472
    STEP: Creating a configMap that should not be mutated 01/18/23 23:00:44.479
    STEP: Patching a mutating webhook configuration's rules to include the create operation 01/18/23 23:00:44.494
    STEP: Creating a configMap that should be mutated 01/18/23 23:00:44.504
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 23:00:44.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2494" for this suite. 01/18/23 23:00:44.545
    STEP: Destroying namespace "webhook-2494-markers" for this suite. 01/18/23 23:00:44.555
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:00:44.632
Jan 18 23:00:44.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename resourcequota 01/18/23 23:00:44.636
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:00:44.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:00:44.72
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 01/18/23 23:00:44.727
STEP: Creating a ResourceQuota 01/18/23 23:00:49.74
STEP: Ensuring resource quota status is calculated 01/18/23 23:00:49.781
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 23:00:51.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8008" for this suite. 01/18/23 23:00:51.795
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":319,"skipped":5709,"failed":0}
------------------------------
• [SLOW TEST] [7.173 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:00:44.632
    Jan 18 23:00:44.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename resourcequota 01/18/23 23:00:44.636
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:00:44.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:00:44.72
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 01/18/23 23:00:44.727
    STEP: Creating a ResourceQuota 01/18/23 23:00:49.74
    STEP: Ensuring resource quota status is calculated 01/18/23 23:00:49.781
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 23:00:51.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8008" for this suite. 01/18/23 23:00:51.795
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:00:51.808
Jan 18 23:00:51.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename services 01/18/23 23:00:51.812
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:00:51.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:00:51.842
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 01/18/23 23:00:51.85
STEP: waiting for available Endpoint 01/18/23 23:00:51.857
STEP: listing all Endpoints 01/18/23 23:00:51.859
STEP: updating the Endpoint 01/18/23 23:00:51.864
STEP: fetching the Endpoint 01/18/23 23:00:51.873
STEP: patching the Endpoint 01/18/23 23:00:51.879
STEP: fetching the Endpoint 01/18/23 23:00:51.892
STEP: deleting the Endpoint by Collection 01/18/23 23:00:51.896
STEP: waiting for Endpoint deletion 01/18/23 23:00:51.909
STEP: fetching the Endpoint 01/18/23 23:00:51.913
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 23:00:51.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5776" for this suite. 01/18/23 23:00:51.922
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":320,"skipped":5736,"failed":0}
------------------------------
• [0.123 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:00:51.808
    Jan 18 23:00:51.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename services 01/18/23 23:00:51.812
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:00:51.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:00:51.842
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 01/18/23 23:00:51.85
    STEP: waiting for available Endpoint 01/18/23 23:00:51.857
    STEP: listing all Endpoints 01/18/23 23:00:51.859
    STEP: updating the Endpoint 01/18/23 23:00:51.864
    STEP: fetching the Endpoint 01/18/23 23:00:51.873
    STEP: patching the Endpoint 01/18/23 23:00:51.879
    STEP: fetching the Endpoint 01/18/23 23:00:51.892
    STEP: deleting the Endpoint by Collection 01/18/23 23:00:51.896
    STEP: waiting for Endpoint deletion 01/18/23 23:00:51.909
    STEP: fetching the Endpoint 01/18/23 23:00:51.913
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 23:00:51.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5776" for this suite. 01/18/23 23:00:51.922
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:00:51.935
Jan 18 23:00:51.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename namespaces 01/18/23 23:00:51.937
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:00:51.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:00:51.961
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 01/18/23 23:00:51.967
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:00:52
STEP: Creating a service in the namespace 01/18/23 23:00:52.017
STEP: Deleting the namespace 01/18/23 23:00:52.029
STEP: Waiting for the namespace to be removed. 01/18/23 23:00:52.043
STEP: Recreating the namespace 01/18/23 23:00:58.05
STEP: Verifying there is no service in the namespace 01/18/23 23:00:58.076
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 18 23:00:58.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2461" for this suite. 01/18/23 23:00:58.086
STEP: Destroying namespace "nsdeletetest-4845" for this suite. 01/18/23 23:00:58.095
Jan 18 23:00:58.101: INFO: Namespace nsdeletetest-4845 was already deleted
STEP: Destroying namespace "nsdeletetest-3631" for this suite. 01/18/23 23:00:58.101
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":321,"skipped":5746,"failed":0}
------------------------------
• [SLOW TEST] [6.175 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:00:51.935
    Jan 18 23:00:51.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename namespaces 01/18/23 23:00:51.937
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:00:51.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:00:51.961
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 01/18/23 23:00:51.967
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:00:52
    STEP: Creating a service in the namespace 01/18/23 23:00:52.017
    STEP: Deleting the namespace 01/18/23 23:00:52.029
    STEP: Waiting for the namespace to be removed. 01/18/23 23:00:52.043
    STEP: Recreating the namespace 01/18/23 23:00:58.05
    STEP: Verifying there is no service in the namespace 01/18/23 23:00:58.076
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 23:00:58.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-2461" for this suite. 01/18/23 23:00:58.086
    STEP: Destroying namespace "nsdeletetest-4845" for this suite. 01/18/23 23:00:58.095
    Jan 18 23:00:58.101: INFO: Namespace nsdeletetest-4845 was already deleted
    STEP: Destroying namespace "nsdeletetest-3631" for this suite. 01/18/23 23:00:58.101
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:00:58.116
Jan 18 23:00:58.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename pods 01/18/23 23:00:58.118
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:00:58.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:00:58.151
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 01/18/23 23:00:58.155
STEP: submitting the pod to kubernetes 01/18/23 23:00:58.156
Jan 18 23:00:58.165: INFO: Waiting up to 5m0s for pod "pod-update-5e366099-3877-45a9-b97d-33c3dd70e0e2" in namespace "pods-9357" to be "running and ready"
Jan 18 23:00:58.180: INFO: Pod "pod-update-5e366099-3877-45a9-b97d-33c3dd70e0e2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.222232ms
Jan 18 23:00:58.180: INFO: The phase of Pod pod-update-5e366099-3877-45a9-b97d-33c3dd70e0e2 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:01:00.187: INFO: Pod "pod-update-5e366099-3877-45a9-b97d-33c3dd70e0e2": Phase="Running", Reason="", readiness=true. Elapsed: 2.02143626s
Jan 18 23:01:00.187: INFO: The phase of Pod pod-update-5e366099-3877-45a9-b97d-33c3dd70e0e2 is Running (Ready = true)
Jan 18 23:01:00.187: INFO: Pod "pod-update-5e366099-3877-45a9-b97d-33c3dd70e0e2" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/18/23 23:01:00.192
STEP: updating the pod 01/18/23 23:01:00.197
Jan 18 23:01:00.718: INFO: Successfully updated pod "pod-update-5e366099-3877-45a9-b97d-33c3dd70e0e2"
Jan 18 23:01:00.722: INFO: Waiting up to 5m0s for pod "pod-update-5e366099-3877-45a9-b97d-33c3dd70e0e2" in namespace "pods-9357" to be "running"
Jan 18 23:01:00.728: INFO: Pod "pod-update-5e366099-3877-45a9-b97d-33c3dd70e0e2": Phase="Running", Reason="", readiness=true. Elapsed: 6.2036ms
Jan 18 23:01:00.728: INFO: Pod "pod-update-5e366099-3877-45a9-b97d-33c3dd70e0e2" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 01/18/23 23:01:00.728
Jan 18 23:01:00.738: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 23:01:00.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9357" for this suite. 01/18/23 23:01:00.768
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":322,"skipped":5751,"failed":0}
------------------------------
• [2.665 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:00:58.116
    Jan 18 23:00:58.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename pods 01/18/23 23:00:58.118
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:00:58.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:00:58.151
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 01/18/23 23:00:58.155
    STEP: submitting the pod to kubernetes 01/18/23 23:00:58.156
    Jan 18 23:00:58.165: INFO: Waiting up to 5m0s for pod "pod-update-5e366099-3877-45a9-b97d-33c3dd70e0e2" in namespace "pods-9357" to be "running and ready"
    Jan 18 23:00:58.180: INFO: Pod "pod-update-5e366099-3877-45a9-b97d-33c3dd70e0e2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.222232ms
    Jan 18 23:00:58.180: INFO: The phase of Pod pod-update-5e366099-3877-45a9-b97d-33c3dd70e0e2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:01:00.187: INFO: Pod "pod-update-5e366099-3877-45a9-b97d-33c3dd70e0e2": Phase="Running", Reason="", readiness=true. Elapsed: 2.02143626s
    Jan 18 23:01:00.187: INFO: The phase of Pod pod-update-5e366099-3877-45a9-b97d-33c3dd70e0e2 is Running (Ready = true)
    Jan 18 23:01:00.187: INFO: Pod "pod-update-5e366099-3877-45a9-b97d-33c3dd70e0e2" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/18/23 23:01:00.192
    STEP: updating the pod 01/18/23 23:01:00.197
    Jan 18 23:01:00.718: INFO: Successfully updated pod "pod-update-5e366099-3877-45a9-b97d-33c3dd70e0e2"
    Jan 18 23:01:00.722: INFO: Waiting up to 5m0s for pod "pod-update-5e366099-3877-45a9-b97d-33c3dd70e0e2" in namespace "pods-9357" to be "running"
    Jan 18 23:01:00.728: INFO: Pod "pod-update-5e366099-3877-45a9-b97d-33c3dd70e0e2": Phase="Running", Reason="", readiness=true. Elapsed: 6.2036ms
    Jan 18 23:01:00.728: INFO: Pod "pod-update-5e366099-3877-45a9-b97d-33c3dd70e0e2" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 01/18/23 23:01:00.728
    Jan 18 23:01:00.738: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 23:01:00.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9357" for this suite. 01/18/23 23:01:00.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:01:00.784
Jan 18 23:01:00.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename configmap 01/18/23 23:01:00.787
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:00.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:00.824
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-f558ecbc-a4bf-4e11-8fc3-fd26d779c4a8 01/18/23 23:01:00.835
STEP: Creating the pod 01/18/23 23:01:00.844
Jan 18 23:01:00.854: INFO: Waiting up to 5m0s for pod "pod-configmaps-a2cc0564-6931-410b-8a47-ddfc98b958d7" in namespace "configmap-764" to be "running and ready"
Jan 18 23:01:00.890: INFO: Pod "pod-configmaps-a2cc0564-6931-410b-8a47-ddfc98b958d7": Phase="Pending", Reason="", readiness=false. Elapsed: 36.063564ms
Jan 18 23:01:00.890: INFO: The phase of Pod pod-configmaps-a2cc0564-6931-410b-8a47-ddfc98b958d7 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:01:02.896: INFO: Pod "pod-configmaps-a2cc0564-6931-410b-8a47-ddfc98b958d7": Phase="Running", Reason="", readiness=true. Elapsed: 2.042205424s
Jan 18 23:01:02.896: INFO: The phase of Pod pod-configmaps-a2cc0564-6931-410b-8a47-ddfc98b958d7 is Running (Ready = true)
Jan 18 23:01:02.896: INFO: Pod "pod-configmaps-a2cc0564-6931-410b-8a47-ddfc98b958d7" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-f558ecbc-a4bf-4e11-8fc3-fd26d779c4a8 01/18/23 23:01:02.937
STEP: waiting to observe update in volume 01/18/23 23:01:02.946
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 23:01:04.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-764" for this suite. 01/18/23 23:01:04.974
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":323,"skipped":5790,"failed":0}
------------------------------
• [4.198 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:01:00.784
    Jan 18 23:01:00.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename configmap 01/18/23 23:01:00.787
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:00.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:00.824
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-f558ecbc-a4bf-4e11-8fc3-fd26d779c4a8 01/18/23 23:01:00.835
    STEP: Creating the pod 01/18/23 23:01:00.844
    Jan 18 23:01:00.854: INFO: Waiting up to 5m0s for pod "pod-configmaps-a2cc0564-6931-410b-8a47-ddfc98b958d7" in namespace "configmap-764" to be "running and ready"
    Jan 18 23:01:00.890: INFO: Pod "pod-configmaps-a2cc0564-6931-410b-8a47-ddfc98b958d7": Phase="Pending", Reason="", readiness=false. Elapsed: 36.063564ms
    Jan 18 23:01:00.890: INFO: The phase of Pod pod-configmaps-a2cc0564-6931-410b-8a47-ddfc98b958d7 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:01:02.896: INFO: Pod "pod-configmaps-a2cc0564-6931-410b-8a47-ddfc98b958d7": Phase="Running", Reason="", readiness=true. Elapsed: 2.042205424s
    Jan 18 23:01:02.896: INFO: The phase of Pod pod-configmaps-a2cc0564-6931-410b-8a47-ddfc98b958d7 is Running (Ready = true)
    Jan 18 23:01:02.896: INFO: Pod "pod-configmaps-a2cc0564-6931-410b-8a47-ddfc98b958d7" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-f558ecbc-a4bf-4e11-8fc3-fd26d779c4a8 01/18/23 23:01:02.937
    STEP: waiting to observe update in volume 01/18/23 23:01:02.946
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 23:01:04.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-764" for this suite. 01/18/23 23:01:04.974
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:01:04.985
Jan 18 23:01:04.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename var-expansion 01/18/23 23:01:04.987
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:05.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:05.065
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Jan 18 23:01:05.082: INFO: Waiting up to 2m0s for pod "var-expansion-c7283d0d-a879-4ef4-8208-1becb0c755a0" in namespace "var-expansion-723" to be "container 0 failed with reason CreateContainerConfigError"
Jan 18 23:01:05.087: INFO: Pod "var-expansion-c7283d0d-a879-4ef4-8208-1becb0c755a0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.24768ms
Jan 18 23:01:07.093: INFO: Pod "var-expansion-c7283d0d-a879-4ef4-8208-1becb0c755a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010810771s
Jan 18 23:01:07.093: INFO: Pod "var-expansion-c7283d0d-a879-4ef4-8208-1becb0c755a0" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 18 23:01:07.094: INFO: Deleting pod "var-expansion-c7283d0d-a879-4ef4-8208-1becb0c755a0" in namespace "var-expansion-723"
Jan 18 23:01:07.117: INFO: Wait up to 5m0s for pod "var-expansion-c7283d0d-a879-4ef4-8208-1becb0c755a0" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 18 23:01:11.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-723" for this suite. 01/18/23 23:01:11.174
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":324,"skipped":5790,"failed":0}
------------------------------
• [SLOW TEST] [6.197 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:01:04.985
    Jan 18 23:01:04.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename var-expansion 01/18/23 23:01:04.987
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:05.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:05.065
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Jan 18 23:01:05.082: INFO: Waiting up to 2m0s for pod "var-expansion-c7283d0d-a879-4ef4-8208-1becb0c755a0" in namespace "var-expansion-723" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 18 23:01:05.087: INFO: Pod "var-expansion-c7283d0d-a879-4ef4-8208-1becb0c755a0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.24768ms
    Jan 18 23:01:07.093: INFO: Pod "var-expansion-c7283d0d-a879-4ef4-8208-1becb0c755a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010810771s
    Jan 18 23:01:07.093: INFO: Pod "var-expansion-c7283d0d-a879-4ef4-8208-1becb0c755a0" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 18 23:01:07.094: INFO: Deleting pod "var-expansion-c7283d0d-a879-4ef4-8208-1becb0c755a0" in namespace "var-expansion-723"
    Jan 18 23:01:07.117: INFO: Wait up to 5m0s for pod "var-expansion-c7283d0d-a879-4ef4-8208-1becb0c755a0" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 18 23:01:11.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-723" for this suite. 01/18/23 23:01:11.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:01:11.185
Jan 18 23:01:11.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename services 01/18/23 23:01:11.187
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:11.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:11.214
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1961 01/18/23 23:01:11.232
STEP: changing the ExternalName service to type=ClusterIP 01/18/23 23:01:11.242
STEP: creating replication controller externalname-service in namespace services-1961 01/18/23 23:01:11.282
I0118 23:01:11.288000      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1961, replica count: 2
I0118 23:01:14.339643      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 23:01:14.339: INFO: Creating new exec pod
Jan 18 23:01:14.346: INFO: Waiting up to 5m0s for pod "execpodk5sm5" in namespace "services-1961" to be "running"
Jan 18 23:01:14.352: INFO: Pod "execpodk5sm5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.666013ms
Jan 18 23:01:16.360: INFO: Pod "execpodk5sm5": Phase="Running", Reason="", readiness=true. Elapsed: 2.013770669s
Jan 18 23:01:16.360: INFO: Pod "execpodk5sm5" satisfied condition "running"
Jan 18 23:01:17.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-1961 exec execpodk5sm5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 18 23:01:17.625: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 18 23:01:17.625: INFO: stdout: ""
Jan 18 23:01:18.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-1961 exec execpodk5sm5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 18 23:01:18.828: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 18 23:01:18.828: INFO: stdout: ""
Jan 18 23:01:19.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-1961 exec execpodk5sm5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 18 23:01:19.837: INFO: stderr: "+ + nc -v -t -w 2 externalname-service 80\necho hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 18 23:01:19.837: INFO: stdout: ""
Jan 18 23:01:20.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-1961 exec execpodk5sm5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 18 23:01:20.849: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 18 23:01:20.849: INFO: stdout: "externalname-service-svnf6"
Jan 18 23:01:20.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-1961 exec execpodk5sm5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.43.103 80'
Jan 18 23:01:21.033: INFO: stderr: "+ + nc -v -t -w 2 10.233.43.103 80\necho hostName\nConnection to 10.233.43.103 80 port [tcp/http] succeeded!\n"
Jan 18 23:01:21.033: INFO: stdout: ""
Jan 18 23:01:22.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-1961 exec execpodk5sm5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.43.103 80'
Jan 18 23:01:22.234: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.43.103 80\nConnection to 10.233.43.103 80 port [tcp/http] succeeded!\n"
Jan 18 23:01:22.234: INFO: stdout: "externalname-service-svnf6"
Jan 18 23:01:22.234: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 23:01:22.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1961" for this suite. 01/18/23 23:01:22.27
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":325,"skipped":5818,"failed":0}
------------------------------
• [SLOW TEST] [11.096 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:01:11.185
    Jan 18 23:01:11.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename services 01/18/23 23:01:11.187
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:11.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:11.214
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-1961 01/18/23 23:01:11.232
    STEP: changing the ExternalName service to type=ClusterIP 01/18/23 23:01:11.242
    STEP: creating replication controller externalname-service in namespace services-1961 01/18/23 23:01:11.282
    I0118 23:01:11.288000      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1961, replica count: 2
    I0118 23:01:14.339643      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 23:01:14.339: INFO: Creating new exec pod
    Jan 18 23:01:14.346: INFO: Waiting up to 5m0s for pod "execpodk5sm5" in namespace "services-1961" to be "running"
    Jan 18 23:01:14.352: INFO: Pod "execpodk5sm5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.666013ms
    Jan 18 23:01:16.360: INFO: Pod "execpodk5sm5": Phase="Running", Reason="", readiness=true. Elapsed: 2.013770669s
    Jan 18 23:01:16.360: INFO: Pod "execpodk5sm5" satisfied condition "running"
    Jan 18 23:01:17.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-1961 exec execpodk5sm5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 18 23:01:17.625: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 18 23:01:17.625: INFO: stdout: ""
    Jan 18 23:01:18.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-1961 exec execpodk5sm5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 18 23:01:18.828: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 18 23:01:18.828: INFO: stdout: ""
    Jan 18 23:01:19.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-1961 exec execpodk5sm5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 18 23:01:19.837: INFO: stderr: "+ + nc -v -t -w 2 externalname-service 80\necho hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 18 23:01:19.837: INFO: stdout: ""
    Jan 18 23:01:20.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-1961 exec execpodk5sm5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 18 23:01:20.849: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 18 23:01:20.849: INFO: stdout: "externalname-service-svnf6"
    Jan 18 23:01:20.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-1961 exec execpodk5sm5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.43.103 80'
    Jan 18 23:01:21.033: INFO: stderr: "+ + nc -v -t -w 2 10.233.43.103 80\necho hostName\nConnection to 10.233.43.103 80 port [tcp/http] succeeded!\n"
    Jan 18 23:01:21.033: INFO: stdout: ""
    Jan 18 23:01:22.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-1961 exec execpodk5sm5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.43.103 80'
    Jan 18 23:01:22.234: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.43.103 80\nConnection to 10.233.43.103 80 port [tcp/http] succeeded!\n"
    Jan 18 23:01:22.234: INFO: stdout: "externalname-service-svnf6"
    Jan 18 23:01:22.234: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 23:01:22.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1961" for this suite. 01/18/23 23:01:22.27
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:01:22.282
Jan 18 23:01:22.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename pods 01/18/23 23:01:22.283
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:22.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:22.323
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 01/18/23 23:01:22.329
STEP: setting up watch 01/18/23 23:01:22.329
STEP: submitting the pod to kubernetes 01/18/23 23:01:22.435
STEP: verifying the pod is in kubernetes 01/18/23 23:01:22.447
STEP: verifying pod creation was observed 01/18/23 23:01:22.46
Jan 18 23:01:22.460: INFO: Waiting up to 5m0s for pod "pod-submit-remove-ba1a4e94-a85e-4c0b-8b41-526422c9f223" in namespace "pods-371" to be "running"
Jan 18 23:01:22.466: INFO: Pod "pod-submit-remove-ba1a4e94-a85e-4c0b-8b41-526422c9f223": Phase="Pending", Reason="", readiness=false. Elapsed: 5.529238ms
Jan 18 23:01:24.470: INFO: Pod "pod-submit-remove-ba1a4e94-a85e-4c0b-8b41-526422c9f223": Phase="Running", Reason="", readiness=true. Elapsed: 2.009745359s
Jan 18 23:01:24.470: INFO: Pod "pod-submit-remove-ba1a4e94-a85e-4c0b-8b41-526422c9f223" satisfied condition "running"
STEP: deleting the pod gracefully 01/18/23 23:01:24.474
STEP: verifying pod deletion was observed 01/18/23 23:01:24.485
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 23:01:26.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-371" for this suite. 01/18/23 23:01:26.815
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":326,"skipped":5820,"failed":0}
------------------------------
• [4.544 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:01:22.282
    Jan 18 23:01:22.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename pods 01/18/23 23:01:22.283
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:22.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:22.323
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 01/18/23 23:01:22.329
    STEP: setting up watch 01/18/23 23:01:22.329
    STEP: submitting the pod to kubernetes 01/18/23 23:01:22.435
    STEP: verifying the pod is in kubernetes 01/18/23 23:01:22.447
    STEP: verifying pod creation was observed 01/18/23 23:01:22.46
    Jan 18 23:01:22.460: INFO: Waiting up to 5m0s for pod "pod-submit-remove-ba1a4e94-a85e-4c0b-8b41-526422c9f223" in namespace "pods-371" to be "running"
    Jan 18 23:01:22.466: INFO: Pod "pod-submit-remove-ba1a4e94-a85e-4c0b-8b41-526422c9f223": Phase="Pending", Reason="", readiness=false. Elapsed: 5.529238ms
    Jan 18 23:01:24.470: INFO: Pod "pod-submit-remove-ba1a4e94-a85e-4c0b-8b41-526422c9f223": Phase="Running", Reason="", readiness=true. Elapsed: 2.009745359s
    Jan 18 23:01:24.470: INFO: Pod "pod-submit-remove-ba1a4e94-a85e-4c0b-8b41-526422c9f223" satisfied condition "running"
    STEP: deleting the pod gracefully 01/18/23 23:01:24.474
    STEP: verifying pod deletion was observed 01/18/23 23:01:24.485
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 23:01:26.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-371" for this suite. 01/18/23 23:01:26.815
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:01:26.836
Jan 18 23:01:26.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename configmap 01/18/23 23:01:26.838
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:27.084
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:27.09
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-ff6a52bd-6c0a-48f6-8cf6-67ede6d40e8f 01/18/23 23:01:27.095
STEP: Creating a pod to test consume configMaps 01/18/23 23:01:27.101
Jan 18 23:01:27.110: INFO: Waiting up to 5m0s for pod "pod-configmaps-24c71c75-3b3c-465e-adc8-ea868b75c5f5" in namespace "configmap-2143" to be "Succeeded or Failed"
Jan 18 23:01:27.114: INFO: Pod "pod-configmaps-24c71c75-3b3c-465e-adc8-ea868b75c5f5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.94626ms
Jan 18 23:01:29.131: INFO: Pod "pod-configmaps-24c71c75-3b3c-465e-adc8-ea868b75c5f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021407443s
Jan 18 23:01:31.138: INFO: Pod "pod-configmaps-24c71c75-3b3c-465e-adc8-ea868b75c5f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028066383s
STEP: Saw pod success 01/18/23 23:01:31.138
Jan 18 23:01:31.138: INFO: Pod "pod-configmaps-24c71c75-3b3c-465e-adc8-ea868b75c5f5" satisfied condition "Succeeded or Failed"
Jan 18 23:01:31.143: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-24c71c75-3b3c-465e-adc8-ea868b75c5f5 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 23:01:31.154
Jan 18 23:01:31.169: INFO: Waiting for pod pod-configmaps-24c71c75-3b3c-465e-adc8-ea868b75c5f5 to disappear
Jan 18 23:01:31.173: INFO: Pod pod-configmaps-24c71c75-3b3c-465e-adc8-ea868b75c5f5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 23:01:31.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2143" for this suite. 01/18/23 23:01:31.181
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":327,"skipped":5840,"failed":0}
------------------------------
• [4.351 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:01:26.836
    Jan 18 23:01:26.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename configmap 01/18/23 23:01:26.838
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:27.084
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:27.09
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-ff6a52bd-6c0a-48f6-8cf6-67ede6d40e8f 01/18/23 23:01:27.095
    STEP: Creating a pod to test consume configMaps 01/18/23 23:01:27.101
    Jan 18 23:01:27.110: INFO: Waiting up to 5m0s for pod "pod-configmaps-24c71c75-3b3c-465e-adc8-ea868b75c5f5" in namespace "configmap-2143" to be "Succeeded or Failed"
    Jan 18 23:01:27.114: INFO: Pod "pod-configmaps-24c71c75-3b3c-465e-adc8-ea868b75c5f5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.94626ms
    Jan 18 23:01:29.131: INFO: Pod "pod-configmaps-24c71c75-3b3c-465e-adc8-ea868b75c5f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021407443s
    Jan 18 23:01:31.138: INFO: Pod "pod-configmaps-24c71c75-3b3c-465e-adc8-ea868b75c5f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028066383s
    STEP: Saw pod success 01/18/23 23:01:31.138
    Jan 18 23:01:31.138: INFO: Pod "pod-configmaps-24c71c75-3b3c-465e-adc8-ea868b75c5f5" satisfied condition "Succeeded or Failed"
    Jan 18 23:01:31.143: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-24c71c75-3b3c-465e-adc8-ea868b75c5f5 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 23:01:31.154
    Jan 18 23:01:31.169: INFO: Waiting for pod pod-configmaps-24c71c75-3b3c-465e-adc8-ea868b75c5f5 to disappear
    Jan 18 23:01:31.173: INFO: Pod pod-configmaps-24c71c75-3b3c-465e-adc8-ea868b75c5f5 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 23:01:31.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2143" for this suite. 01/18/23 23:01:31.181
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:01:31.193
Jan 18 23:01:31.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename secrets 01/18/23 23:01:31.195
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:31.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:31.225
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-33d77e6b-0fd7-42a0-8ad6-26f98a181821 01/18/23 23:01:31.274
STEP: Creating a pod to test consume secrets 01/18/23 23:01:31.285
Jan 18 23:01:31.304: INFO: Waiting up to 5m0s for pod "pod-secrets-1d71ee22-1cf5-473e-9f79-afde42c667d2" in namespace "secrets-4422" to be "Succeeded or Failed"
Jan 18 23:01:31.316: INFO: Pod "pod-secrets-1d71ee22-1cf5-473e-9f79-afde42c667d2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.484516ms
Jan 18 23:01:33.321: INFO: Pod "pod-secrets-1d71ee22-1cf5-473e-9f79-afde42c667d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016513772s
Jan 18 23:01:35.492: INFO: Pod "pod-secrets-1d71ee22-1cf5-473e-9f79-afde42c667d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.187340237s
STEP: Saw pod success 01/18/23 23:01:35.492
Jan 18 23:01:35.492: INFO: Pod "pod-secrets-1d71ee22-1cf5-473e-9f79-afde42c667d2" satisfied condition "Succeeded or Failed"
Jan 18 23:01:35.498: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-1d71ee22-1cf5-473e-9f79-afde42c667d2 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 23:01:35.508
Jan 18 23:01:35.524: INFO: Waiting for pod pod-secrets-1d71ee22-1cf5-473e-9f79-afde42c667d2 to disappear
Jan 18 23:01:35.528: INFO: Pod pod-secrets-1d71ee22-1cf5-473e-9f79-afde42c667d2 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 18 23:01:35.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4422" for this suite. 01/18/23 23:01:35.534
STEP: Destroying namespace "secret-namespace-9008" for this suite. 01/18/23 23:01:35.542
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":328,"skipped":5847,"failed":0}
------------------------------
• [4.356 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:01:31.193
    Jan 18 23:01:31.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename secrets 01/18/23 23:01:31.195
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:31.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:31.225
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-33d77e6b-0fd7-42a0-8ad6-26f98a181821 01/18/23 23:01:31.274
    STEP: Creating a pod to test consume secrets 01/18/23 23:01:31.285
    Jan 18 23:01:31.304: INFO: Waiting up to 5m0s for pod "pod-secrets-1d71ee22-1cf5-473e-9f79-afde42c667d2" in namespace "secrets-4422" to be "Succeeded or Failed"
    Jan 18 23:01:31.316: INFO: Pod "pod-secrets-1d71ee22-1cf5-473e-9f79-afde42c667d2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.484516ms
    Jan 18 23:01:33.321: INFO: Pod "pod-secrets-1d71ee22-1cf5-473e-9f79-afde42c667d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016513772s
    Jan 18 23:01:35.492: INFO: Pod "pod-secrets-1d71ee22-1cf5-473e-9f79-afde42c667d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.187340237s
    STEP: Saw pod success 01/18/23 23:01:35.492
    Jan 18 23:01:35.492: INFO: Pod "pod-secrets-1d71ee22-1cf5-473e-9f79-afde42c667d2" satisfied condition "Succeeded or Failed"
    Jan 18 23:01:35.498: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-1d71ee22-1cf5-473e-9f79-afde42c667d2 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 23:01:35.508
    Jan 18 23:01:35.524: INFO: Waiting for pod pod-secrets-1d71ee22-1cf5-473e-9f79-afde42c667d2 to disappear
    Jan 18 23:01:35.528: INFO: Pod pod-secrets-1d71ee22-1cf5-473e-9f79-afde42c667d2 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 23:01:35.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4422" for this suite. 01/18/23 23:01:35.534
    STEP: Destroying namespace "secret-namespace-9008" for this suite. 01/18/23 23:01:35.542
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:01:35.558
Jan 18 23:01:35.558: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename services 01/18/23 23:01:35.56
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:35.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:35.595
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-6428 01/18/23 23:01:35.6
STEP: creating service affinity-nodeport-transition in namespace services-6428 01/18/23 23:01:35.601
STEP: creating replication controller affinity-nodeport-transition in namespace services-6428 01/18/23 23:01:35.623
I0118 23:01:35.634163      19 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6428, replica count: 3
I0118 23:01:38.686552      19 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 23:01:38.702: INFO: Creating new exec pod
Jan 18 23:01:38.709: INFO: Waiting up to 5m0s for pod "execpod-affinity75q7h" in namespace "services-6428" to be "running"
Jan 18 23:01:38.715: INFO: Pod "execpod-affinity75q7h": Phase="Pending", Reason="", readiness=false. Elapsed: 5.874519ms
Jan 18 23:01:40.720: INFO: Pod "execpod-affinity75q7h": Phase="Running", Reason="", readiness=true. Elapsed: 2.011010513s
Jan 18 23:01:40.720: INFO: Pod "execpod-affinity75q7h" satisfied condition "running"
Jan 18 23:01:41.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-6428 exec execpod-affinity75q7h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jan 18 23:01:41.940: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan 18 23:01:41.940: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 23:01:41.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-6428 exec execpod-affinity75q7h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.10.0 80'
Jan 18 23:01:42.129: INFO: stderr: "+ + echo hostNamenc\n -v -t -w 2 10.233.10.0 80\nConnection to 10.233.10.0 80 port [tcp/http] succeeded!\n"
Jan 18 23:01:42.129: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 23:01:42.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-6428 exec execpod-affinity75q7h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.168 30331'
Jan 18 23:01:42.344: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.168 30331\nConnection to 192.168.101.168 30331 port [tcp/*] succeeded!\n"
Jan 18 23:01:42.344: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 23:01:42.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-6428 exec execpod-affinity75q7h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 30331'
Jan 18 23:01:42.615: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.216 30331\nConnection to 192.168.101.216 30331 port [tcp/*] succeeded!\n"
Jan 18 23:01:42.615: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 23:01:42.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-6428 exec execpod-affinity75q7h -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.101.168:30331/ ; done'
Jan 18 23:01:43.074: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n"
Jan 18 23:01:43.074: INFO: stdout: "\naffinity-nodeport-transition-89wr7\naffinity-nodeport-transition-pvn4h\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-89wr7\naffinity-nodeport-transition-pvn4h\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-89wr7\naffinity-nodeport-transition-pvn4h\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-89wr7\naffinity-nodeport-transition-pvn4h\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-89wr7\naffinity-nodeport-transition-pvn4h\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-89wr7"
Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-89wr7
Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-pvn4h
Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-r2p88
Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-89wr7
Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-pvn4h
Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-r2p88
Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-89wr7
Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-pvn4h
Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-r2p88
Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-89wr7
Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-pvn4h
Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-r2p88
Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-89wr7
Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-pvn4h
Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-r2p88
Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-89wr7
Jan 18 23:01:43.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-6428 exec execpod-affinity75q7h -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.101.168:30331/ ; done'
Jan 18 23:01:43.481: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n"
Jan 18 23:01:43.481: INFO: stdout: "\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88"
Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
Jan 18 23:01:43.481: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6428, will wait for the garbage collector to delete the pods 01/18/23 23:01:43.498
Jan 18 23:01:43.571: INFO: Deleting ReplicationController affinity-nodeport-transition took: 9.54168ms
Jan 18 23:01:43.672: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.801633ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 23:01:46.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6428" for this suite. 01/18/23 23:01:46.595
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":329,"skipped":5876,"failed":0}
------------------------------
• [SLOW TEST] [11.045 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:01:35.558
    Jan 18 23:01:35.558: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename services 01/18/23 23:01:35.56
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:35.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:35.595
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-6428 01/18/23 23:01:35.6
    STEP: creating service affinity-nodeport-transition in namespace services-6428 01/18/23 23:01:35.601
    STEP: creating replication controller affinity-nodeport-transition in namespace services-6428 01/18/23 23:01:35.623
    I0118 23:01:35.634163      19 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6428, replica count: 3
    I0118 23:01:38.686552      19 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 23:01:38.702: INFO: Creating new exec pod
    Jan 18 23:01:38.709: INFO: Waiting up to 5m0s for pod "execpod-affinity75q7h" in namespace "services-6428" to be "running"
    Jan 18 23:01:38.715: INFO: Pod "execpod-affinity75q7h": Phase="Pending", Reason="", readiness=false. Elapsed: 5.874519ms
    Jan 18 23:01:40.720: INFO: Pod "execpod-affinity75q7h": Phase="Running", Reason="", readiness=true. Elapsed: 2.011010513s
    Jan 18 23:01:40.720: INFO: Pod "execpod-affinity75q7h" satisfied condition "running"
    Jan 18 23:01:41.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-6428 exec execpod-affinity75q7h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Jan 18 23:01:41.940: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jan 18 23:01:41.940: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 23:01:41.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-6428 exec execpod-affinity75q7h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.10.0 80'
    Jan 18 23:01:42.129: INFO: stderr: "+ + echo hostNamenc\n -v -t -w 2 10.233.10.0 80\nConnection to 10.233.10.0 80 port [tcp/http] succeeded!\n"
    Jan 18 23:01:42.129: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 23:01:42.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-6428 exec execpod-affinity75q7h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.168 30331'
    Jan 18 23:01:42.344: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.168 30331\nConnection to 192.168.101.168 30331 port [tcp/*] succeeded!\n"
    Jan 18 23:01:42.344: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 23:01:42.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-6428 exec execpod-affinity75q7h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 30331'
    Jan 18 23:01:42.615: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.216 30331\nConnection to 192.168.101.216 30331 port [tcp/*] succeeded!\n"
    Jan 18 23:01:42.615: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 23:01:42.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-6428 exec execpod-affinity75q7h -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.101.168:30331/ ; done'
    Jan 18 23:01:43.074: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n"
    Jan 18 23:01:43.074: INFO: stdout: "\naffinity-nodeport-transition-89wr7\naffinity-nodeport-transition-pvn4h\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-89wr7\naffinity-nodeport-transition-pvn4h\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-89wr7\naffinity-nodeport-transition-pvn4h\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-89wr7\naffinity-nodeport-transition-pvn4h\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-89wr7\naffinity-nodeport-transition-pvn4h\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-89wr7"
    Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-89wr7
    Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-pvn4h
    Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-r2p88
    Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-89wr7
    Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-pvn4h
    Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-r2p88
    Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-89wr7
    Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-pvn4h
    Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-r2p88
    Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-89wr7
    Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-pvn4h
    Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-r2p88
    Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-89wr7
    Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-pvn4h
    Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-r2p88
    Jan 18 23:01:43.074: INFO: Received response from host: affinity-nodeport-transition-89wr7
    Jan 18 23:01:43.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-6428 exec execpod-affinity75q7h -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.101.168:30331/ ; done'
    Jan 18 23:01:43.481: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30331/\n"
    Jan 18 23:01:43.481: INFO: stdout: "\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88\naffinity-nodeport-transition-r2p88"
    Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
    Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
    Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
    Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
    Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
    Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
    Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
    Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
    Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
    Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
    Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
    Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
    Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
    Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
    Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
    Jan 18 23:01:43.481: INFO: Received response from host: affinity-nodeport-transition-r2p88
    Jan 18 23:01:43.481: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6428, will wait for the garbage collector to delete the pods 01/18/23 23:01:43.498
    Jan 18 23:01:43.571: INFO: Deleting ReplicationController affinity-nodeport-transition took: 9.54168ms
    Jan 18 23:01:43.672: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.801633ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 23:01:46.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6428" for this suite. 01/18/23 23:01:46.595
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:01:46.658
Jan 18 23:01:46.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename services 01/18/23 23:01:46.66
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:46.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:46.693
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 01/18/23 23:01:46.699
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 23:01:46.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4519" for this suite. 01/18/23 23:01:46.716
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":330,"skipped":6053,"failed":0}
------------------------------
• [0.067 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:01:46.658
    Jan 18 23:01:46.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename services 01/18/23 23:01:46.66
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:46.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:46.693
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 01/18/23 23:01:46.699
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 23:01:46.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4519" for this suite. 01/18/23 23:01:46.716
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:01:46.727
Jan 18 23:01:46.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename statefulset 01/18/23 23:01:46.73
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:46.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:46.76
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6214 01/18/23 23:01:46.766
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 01/18/23 23:01:46.775
Jan 18 23:01:46.796: INFO: Found 0 stateful pods, waiting for 3
Jan 18 23:01:56.804: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 23:01:56.805: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 23:01:56.805: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/18/23 23:01:56.821
Jan 18 23:01:56.846: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/18/23 23:01:56.846
STEP: Not applying an update when the partition is greater than the number of replicas 01/18/23 23:02:06.889
STEP: Performing a canary update 01/18/23 23:02:06.89
Jan 18 23:02:06.916: INFO: Updating stateful set ss2
Jan 18 23:02:06.938: INFO: Waiting for Pod statefulset-6214/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 01/18/23 23:02:16.949
Jan 18 23:02:17.059: INFO: Found 2 stateful pods, waiting for 3
Jan 18 23:02:27.067: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 23:02:27.067: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 23:02:27.067: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 01/18/23 23:02:27.098
Jan 18 23:02:27.127: INFO: Updating stateful set ss2
Jan 18 23:02:27.136: INFO: Waiting for Pod statefulset-6214/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Jan 18 23:02:37.174: INFO: Updating stateful set ss2
Jan 18 23:02:37.213: INFO: Waiting for StatefulSet statefulset-6214/ss2 to complete update
Jan 18 23:02:37.213: INFO: Waiting for Pod statefulset-6214/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 23:02:47.225: INFO: Deleting all statefulset in ns statefulset-6214
Jan 18 23:02:47.230: INFO: Scaling statefulset ss2 to 0
Jan 18 23:02:57.339: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 23:02:57.345: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 18 23:02:57.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6214" for this suite. 01/18/23 23:02:57.371
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":331,"skipped":6067,"failed":0}
------------------------------
• [SLOW TEST] [70.652 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:01:46.727
    Jan 18 23:01:46.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename statefulset 01/18/23 23:01:46.73
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:01:46.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:01:46.76
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-6214 01/18/23 23:01:46.766
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 01/18/23 23:01:46.775
    Jan 18 23:01:46.796: INFO: Found 0 stateful pods, waiting for 3
    Jan 18 23:01:56.804: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 23:01:56.805: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 23:01:56.805: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/18/23 23:01:56.821
    Jan 18 23:01:56.846: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/18/23 23:01:56.846
    STEP: Not applying an update when the partition is greater than the number of replicas 01/18/23 23:02:06.889
    STEP: Performing a canary update 01/18/23 23:02:06.89
    Jan 18 23:02:06.916: INFO: Updating stateful set ss2
    Jan 18 23:02:06.938: INFO: Waiting for Pod statefulset-6214/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 01/18/23 23:02:16.949
    Jan 18 23:02:17.059: INFO: Found 2 stateful pods, waiting for 3
    Jan 18 23:02:27.067: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 23:02:27.067: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 23:02:27.067: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 01/18/23 23:02:27.098
    Jan 18 23:02:27.127: INFO: Updating stateful set ss2
    Jan 18 23:02:27.136: INFO: Waiting for Pod statefulset-6214/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Jan 18 23:02:37.174: INFO: Updating stateful set ss2
    Jan 18 23:02:37.213: INFO: Waiting for StatefulSet statefulset-6214/ss2 to complete update
    Jan 18 23:02:37.213: INFO: Waiting for Pod statefulset-6214/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 18 23:02:47.225: INFO: Deleting all statefulset in ns statefulset-6214
    Jan 18 23:02:47.230: INFO: Scaling statefulset ss2 to 0
    Jan 18 23:02:57.339: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 23:02:57.345: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 18 23:02:57.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-6214" for this suite. 01/18/23 23:02:57.371
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:02:57.388
Jan 18 23:02:57.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename downward-api 01/18/23 23:02:57.391
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:02:57.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:02:57.439
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 01/18/23 23:02:57.444
Jan 18 23:02:57.469: INFO: Waiting up to 5m0s for pod "downward-api-5266cdcf-c2e6-4179-9657-936640a982d5" in namespace "downward-api-5442" to be "Succeeded or Failed"
Jan 18 23:02:57.478: INFO: Pod "downward-api-5266cdcf-c2e6-4179-9657-936640a982d5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.75266ms
Jan 18 23:02:59.484: INFO: Pod "downward-api-5266cdcf-c2e6-4179-9657-936640a982d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013995792s
Jan 18 23:03:01.485: INFO: Pod "downward-api-5266cdcf-c2e6-4179-9657-936640a982d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015396449s
STEP: Saw pod success 01/18/23 23:03:01.485
Jan 18 23:03:01.485: INFO: Pod "downward-api-5266cdcf-c2e6-4179-9657-936640a982d5" satisfied condition "Succeeded or Failed"
Jan 18 23:03:01.490: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downward-api-5266cdcf-c2e6-4179-9657-936640a982d5 container dapi-container: <nil>
STEP: delete the pod 01/18/23 23:03:01.499
Jan 18 23:03:01.516: INFO: Waiting for pod downward-api-5266cdcf-c2e6-4179-9657-936640a982d5 to disappear
Jan 18 23:03:01.521: INFO: Pod downward-api-5266cdcf-c2e6-4179-9657-936640a982d5 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 18 23:03:01.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5442" for this suite. 01/18/23 23:03:01.527
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":332,"skipped":6115,"failed":0}
------------------------------
• [4.149 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:02:57.388
    Jan 18 23:02:57.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename downward-api 01/18/23 23:02:57.391
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:02:57.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:02:57.439
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 01/18/23 23:02:57.444
    Jan 18 23:02:57.469: INFO: Waiting up to 5m0s for pod "downward-api-5266cdcf-c2e6-4179-9657-936640a982d5" in namespace "downward-api-5442" to be "Succeeded or Failed"
    Jan 18 23:02:57.478: INFO: Pod "downward-api-5266cdcf-c2e6-4179-9657-936640a982d5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.75266ms
    Jan 18 23:02:59.484: INFO: Pod "downward-api-5266cdcf-c2e6-4179-9657-936640a982d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013995792s
    Jan 18 23:03:01.485: INFO: Pod "downward-api-5266cdcf-c2e6-4179-9657-936640a982d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015396449s
    STEP: Saw pod success 01/18/23 23:03:01.485
    Jan 18 23:03:01.485: INFO: Pod "downward-api-5266cdcf-c2e6-4179-9657-936640a982d5" satisfied condition "Succeeded or Failed"
    Jan 18 23:03:01.490: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downward-api-5266cdcf-c2e6-4179-9657-936640a982d5 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 23:03:01.499
    Jan 18 23:03:01.516: INFO: Waiting for pod downward-api-5266cdcf-c2e6-4179-9657-936640a982d5 to disappear
    Jan 18 23:03:01.521: INFO: Pod downward-api-5266cdcf-c2e6-4179-9657-936640a982d5 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 18 23:03:01.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5442" for this suite. 01/18/23 23:03:01.527
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:03:01.541
Jan 18 23:03:01.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename kubectl 01/18/23 23:03:01.542
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:03:01.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:03:01.569
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 01/18/23 23:03:01.573
Jan 18 23:03:01.574: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan 18 23:03:01.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3276 create -f -'
Jan 18 23:03:02.605: INFO: stderr: ""
Jan 18 23:03:02.605: INFO: stdout: "service/agnhost-replica created\n"
Jan 18 23:03:02.605: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan 18 23:03:02.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3276 create -f -'
Jan 18 23:03:03.017: INFO: stderr: ""
Jan 18 23:03:03.018: INFO: stdout: "service/agnhost-primary created\n"
Jan 18 23:03:03.018: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 18 23:03:03.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3276 create -f -'
Jan 18 23:03:03.430: INFO: stderr: ""
Jan 18 23:03:03.430: INFO: stdout: "service/frontend created\n"
Jan 18 23:03:03.431: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 18 23:03:03.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3276 create -f -'
Jan 18 23:03:03.874: INFO: stderr: ""
Jan 18 23:03:03.874: INFO: stdout: "deployment.apps/frontend created\n"
Jan 18 23:03:03.875: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 18 23:03:03.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3276 create -f -'
Jan 18 23:03:04.272: INFO: stderr: ""
Jan 18 23:03:04.272: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan 18 23:03:04.272: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 18 23:03:04.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3276 create -f -'
Jan 18 23:03:04.918: INFO: stderr: ""
Jan 18 23:03:04.918: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 01/18/23 23:03:04.918
Jan 18 23:03:04.930: INFO: Waiting for all frontend pods to be Running.
Jan 18 23:03:09.983: INFO: Waiting for frontend to serve content.
Jan 18 23:03:10.011: INFO: Trying to add a new entry to the guestbook.
Jan 18 23:03:10.028: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 01/18/23 23:03:10.046
Jan 18 23:03:10.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3276 delete --grace-period=0 --force -f -'
Jan 18 23:03:10.184: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 23:03:10.184: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 01/18/23 23:03:10.185
Jan 18 23:03:10.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3276 delete --grace-period=0 --force -f -'
Jan 18 23:03:10.344: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 23:03:10.345: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/18/23 23:03:10.345
Jan 18 23:03:10.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3276 delete --grace-period=0 --force -f -'
Jan 18 23:03:10.512: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 23:03:10.512: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/18/23 23:03:10.512
Jan 18 23:03:10.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3276 delete --grace-period=0 --force -f -'
Jan 18 23:03:10.637: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 23:03:10.637: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/18/23 23:03:10.637
Jan 18 23:03:10.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3276 delete --grace-period=0 --force -f -'
Jan 18 23:03:10.868: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 23:03:10.868: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/18/23 23:03:10.868
Jan 18 23:03:10.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3276 delete --grace-period=0 --force -f -'
Jan 18 23:03:11.072: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 23:03:11.072: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 23:03:11.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3276" for this suite. 01/18/23 23:03:11.088
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":333,"skipped":6144,"failed":0}
------------------------------
• [SLOW TEST] [9.585 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:03:01.541
    Jan 18 23:03:01.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename kubectl 01/18/23 23:03:01.542
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:03:01.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:03:01.569
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 01/18/23 23:03:01.573
    Jan 18 23:03:01.574: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jan 18 23:03:01.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3276 create -f -'
    Jan 18 23:03:02.605: INFO: stderr: ""
    Jan 18 23:03:02.605: INFO: stdout: "service/agnhost-replica created\n"
    Jan 18 23:03:02.605: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jan 18 23:03:02.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3276 create -f -'
    Jan 18 23:03:03.017: INFO: stderr: ""
    Jan 18 23:03:03.018: INFO: stdout: "service/agnhost-primary created\n"
    Jan 18 23:03:03.018: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jan 18 23:03:03.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3276 create -f -'
    Jan 18 23:03:03.430: INFO: stderr: ""
    Jan 18 23:03:03.430: INFO: stdout: "service/frontend created\n"
    Jan 18 23:03:03.431: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jan 18 23:03:03.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3276 create -f -'
    Jan 18 23:03:03.874: INFO: stderr: ""
    Jan 18 23:03:03.874: INFO: stdout: "deployment.apps/frontend created\n"
    Jan 18 23:03:03.875: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 18 23:03:03.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3276 create -f -'
    Jan 18 23:03:04.272: INFO: stderr: ""
    Jan 18 23:03:04.272: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jan 18 23:03:04.272: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 18 23:03:04.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3276 create -f -'
    Jan 18 23:03:04.918: INFO: stderr: ""
    Jan 18 23:03:04.918: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 01/18/23 23:03:04.918
    Jan 18 23:03:04.930: INFO: Waiting for all frontend pods to be Running.
    Jan 18 23:03:09.983: INFO: Waiting for frontend to serve content.
    Jan 18 23:03:10.011: INFO: Trying to add a new entry to the guestbook.
    Jan 18 23:03:10.028: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 01/18/23 23:03:10.046
    Jan 18 23:03:10.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3276 delete --grace-period=0 --force -f -'
    Jan 18 23:03:10.184: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 23:03:10.184: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 01/18/23 23:03:10.185
    Jan 18 23:03:10.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3276 delete --grace-period=0 --force -f -'
    Jan 18 23:03:10.344: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 23:03:10.345: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/18/23 23:03:10.345
    Jan 18 23:03:10.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3276 delete --grace-period=0 --force -f -'
    Jan 18 23:03:10.512: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 23:03:10.512: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/18/23 23:03:10.512
    Jan 18 23:03:10.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3276 delete --grace-period=0 --force -f -'
    Jan 18 23:03:10.637: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 23:03:10.637: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/18/23 23:03:10.637
    Jan 18 23:03:10.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3276 delete --grace-period=0 --force -f -'
    Jan 18 23:03:10.868: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 23:03:10.868: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/18/23 23:03:10.868
    Jan 18 23:03:10.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3276 delete --grace-period=0 --force -f -'
    Jan 18 23:03:11.072: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 23:03:11.072: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 23:03:11.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3276" for this suite. 01/18/23 23:03:11.088
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:03:11.136
Jan 18 23:03:11.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 23:03:11.138
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:03:11.174
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:03:11.179
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-9a974ec5-3240-4fd4-aa2d-90f97b45b9d3 01/18/23 23:03:11.185
STEP: Creating a pod to test consume configMaps 01/18/23 23:03:11.199
Jan 18 23:03:11.216: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d9d3c504-fb6a-4f56-be3c-a0ef37b108a0" in namespace "projected-3572" to be "Succeeded or Failed"
Jan 18 23:03:11.243: INFO: Pod "pod-projected-configmaps-d9d3c504-fb6a-4f56-be3c-a0ef37b108a0": Phase="Pending", Reason="", readiness=false. Elapsed: 27.495111ms
Jan 18 23:03:13.248: INFO: Pod "pod-projected-configmaps-d9d3c504-fb6a-4f56-be3c-a0ef37b108a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031904809s
Jan 18 23:03:15.249: INFO: Pod "pod-projected-configmaps-d9d3c504-fb6a-4f56-be3c-a0ef37b108a0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033274606s
Jan 18 23:03:17.249: INFO: Pod "pod-projected-configmaps-d9d3c504-fb6a-4f56-be3c-a0ef37b108a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03320292s
STEP: Saw pod success 01/18/23 23:03:17.249
Jan 18 23:03:17.250: INFO: Pod "pod-projected-configmaps-d9d3c504-fb6a-4f56-be3c-a0ef37b108a0" satisfied condition "Succeeded or Failed"
Jan 18 23:03:17.254: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-d9d3c504-fb6a-4f56-be3c-a0ef37b108a0 container projected-configmap-volume-test: <nil>
STEP: delete the pod 01/18/23 23:03:17.261
Jan 18 23:03:17.274: INFO: Waiting for pod pod-projected-configmaps-d9d3c504-fb6a-4f56-be3c-a0ef37b108a0 to disappear
Jan 18 23:03:17.278: INFO: Pod pod-projected-configmaps-d9d3c504-fb6a-4f56-be3c-a0ef37b108a0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 18 23:03:17.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3572" for this suite. 01/18/23 23:03:17.283
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":334,"skipped":6208,"failed":0}
------------------------------
• [SLOW TEST] [6.155 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:03:11.136
    Jan 18 23:03:11.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 23:03:11.138
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:03:11.174
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:03:11.179
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-9a974ec5-3240-4fd4-aa2d-90f97b45b9d3 01/18/23 23:03:11.185
    STEP: Creating a pod to test consume configMaps 01/18/23 23:03:11.199
    Jan 18 23:03:11.216: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d9d3c504-fb6a-4f56-be3c-a0ef37b108a0" in namespace "projected-3572" to be "Succeeded or Failed"
    Jan 18 23:03:11.243: INFO: Pod "pod-projected-configmaps-d9d3c504-fb6a-4f56-be3c-a0ef37b108a0": Phase="Pending", Reason="", readiness=false. Elapsed: 27.495111ms
    Jan 18 23:03:13.248: INFO: Pod "pod-projected-configmaps-d9d3c504-fb6a-4f56-be3c-a0ef37b108a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031904809s
    Jan 18 23:03:15.249: INFO: Pod "pod-projected-configmaps-d9d3c504-fb6a-4f56-be3c-a0ef37b108a0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033274606s
    Jan 18 23:03:17.249: INFO: Pod "pod-projected-configmaps-d9d3c504-fb6a-4f56-be3c-a0ef37b108a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03320292s
    STEP: Saw pod success 01/18/23 23:03:17.249
    Jan 18 23:03:17.250: INFO: Pod "pod-projected-configmaps-d9d3c504-fb6a-4f56-be3c-a0ef37b108a0" satisfied condition "Succeeded or Failed"
    Jan 18 23:03:17.254: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-d9d3c504-fb6a-4f56-be3c-a0ef37b108a0 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 01/18/23 23:03:17.261
    Jan 18 23:03:17.274: INFO: Waiting for pod pod-projected-configmaps-d9d3c504-fb6a-4f56-be3c-a0ef37b108a0 to disappear
    Jan 18 23:03:17.278: INFO: Pod pod-projected-configmaps-d9d3c504-fb6a-4f56-be3c-a0ef37b108a0 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 18 23:03:17.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3572" for this suite. 01/18/23 23:03:17.283
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:03:17.294
Jan 18 23:03:17.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename endpointslice 01/18/23 23:03:17.296
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:03:17.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:03:17.323
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Jan 18 23:03:17.339: INFO: Endpoints addresses: [192.168.101.240] , ports: [6443]
Jan 18 23:03:17.339: INFO: EndpointSlices addresses: [192.168.101.240] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 18 23:03:17.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2335" for this suite. 01/18/23 23:03:17.346
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":335,"skipped":6221,"failed":0}
------------------------------
• [0.062 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:03:17.294
    Jan 18 23:03:17.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename endpointslice 01/18/23 23:03:17.296
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:03:17.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:03:17.323
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Jan 18 23:03:17.339: INFO: Endpoints addresses: [192.168.101.240] , ports: [6443]
    Jan 18 23:03:17.339: INFO: EndpointSlices addresses: [192.168.101.240] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 18 23:03:17.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-2335" for this suite. 01/18/23 23:03:17.346
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:03:17.357
Jan 18 23:03:17.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename taint-single-pod 01/18/23 23:03:17.359
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:03:17.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:03:17.401
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Jan 18 23:03:17.407: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 23:04:17.456: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Jan 18 23:04:17.461: INFO: Starting informer...
STEP: Starting pod... 01/18/23 23:04:17.461
Jan 18 23:04:17.700: INFO: Pod is running on v1-25-1-18760-w2. Tainting Node
STEP: Trying to apply a taint on the Node 01/18/23 23:04:17.7
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 23:04:17.722
STEP: Waiting short time to make sure Pod is queued for deletion 01/18/23 23:04:17.752
Jan 18 23:04:17.752: INFO: Pod wasn't evicted. Proceeding
Jan 18 23:04:17.752: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 23:04:17.785
STEP: Waiting some time to make sure that toleration time passed. 01/18/23 23:04:17.801
Jan 18 23:05:32.803: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Jan 18 23:05:32.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-3591" for this suite. 01/18/23 23:05:32.814
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":336,"skipped":6240,"failed":0}
------------------------------
• [SLOW TEST] [135.467 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:03:17.357
    Jan 18 23:03:17.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename taint-single-pod 01/18/23 23:03:17.359
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:03:17.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:03:17.401
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Jan 18 23:03:17.407: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 23:04:17.456: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Jan 18 23:04:17.461: INFO: Starting informer...
    STEP: Starting pod... 01/18/23 23:04:17.461
    Jan 18 23:04:17.700: INFO: Pod is running on v1-25-1-18760-w2. Tainting Node
    STEP: Trying to apply a taint on the Node 01/18/23 23:04:17.7
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 23:04:17.722
    STEP: Waiting short time to make sure Pod is queued for deletion 01/18/23 23:04:17.752
    Jan 18 23:04:17.752: INFO: Pod wasn't evicted. Proceeding
    Jan 18 23:04:17.752: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 23:04:17.785
    STEP: Waiting some time to make sure that toleration time passed. 01/18/23 23:04:17.801
    Jan 18 23:05:32.803: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 23:05:32.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-3591" for this suite. 01/18/23 23:05:32.814
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:05:32.825
Jan 18 23:05:32.827: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename gc 01/18/23 23:05:32.831
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:05:32.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:05:32.881
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 01/18/23 23:05:32.895
STEP: create the rc2 01/18/23 23:05:32.903
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/18/23 23:05:37.926
STEP: delete the rc simpletest-rc-to-be-deleted 01/18/23 23:05:39.278
STEP: wait for the rc to be deleted 01/18/23 23:05:39.29
Jan 18 23:05:44.334: INFO: 68 pods remaining
Jan 18 23:05:44.334: INFO: 68 pods has nil DeletionTimestamp
Jan 18 23:05:44.334: INFO: 
STEP: Gathering metrics 01/18/23 23:05:49.314
Jan 18 23:05:49.370: INFO: Waiting up to 5m0s for pod "kube-controller-manager-v1-25-1-18760-m" in namespace "kube-system" to be "running and ready"
Jan 18 23:05:49.380: INFO: Pod "kube-controller-manager-v1-25-1-18760-m": Phase="Running", Reason="", readiness=true. Elapsed: 9.017429ms
Jan 18 23:05:49.402: INFO: The phase of Pod kube-controller-manager-v1-25-1-18760-m is Running (Ready = true)
Jan 18 23:05:49.413: INFO: Pod "kube-controller-manager-v1-25-1-18760-m" satisfied condition "running and ready"
Jan 18 23:05:49.772: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 18 23:05:49.772: INFO: Deleting pod "simpletest-rc-to-be-deleted-2785q" in namespace "gc-3881"
Jan 18 23:05:49.826: INFO: Deleting pod "simpletest-rc-to-be-deleted-28bx7" in namespace "gc-3881"
Jan 18 23:05:49.850: INFO: Deleting pod "simpletest-rc-to-be-deleted-2rz76" in namespace "gc-3881"
Jan 18 23:05:50.173: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xwbw" in namespace "gc-3881"
Jan 18 23:05:50.768: INFO: Deleting pod "simpletest-rc-to-be-deleted-46p27" in namespace "gc-3881"
Jan 18 23:05:50.787: INFO: Deleting pod "simpletest-rc-to-be-deleted-4ftd9" in namespace "gc-3881"
Jan 18 23:05:50.820: INFO: Deleting pod "simpletest-rc-to-be-deleted-4hmjt" in namespace "gc-3881"
Jan 18 23:05:50.834: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vf9t" in namespace "gc-3881"
Jan 18 23:05:50.847: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hqcd" in namespace "gc-3881"
Jan 18 23:05:50.862: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xwc4" in namespace "gc-3881"
Jan 18 23:05:50.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-6cnk7" in namespace "gc-3881"
Jan 18 23:05:50.894: INFO: Deleting pod "simpletest-rc-to-be-deleted-6qr7m" in namespace "gc-3881"
Jan 18 23:05:50.911: INFO: Deleting pod "simpletest-rc-to-be-deleted-6rn6k" in namespace "gc-3881"
Jan 18 23:05:50.950: INFO: Deleting pod "simpletest-rc-to-be-deleted-744jx" in namespace "gc-3881"
Jan 18 23:05:50.970: INFO: Deleting pod "simpletest-rc-to-be-deleted-755gs" in namespace "gc-3881"
Jan 18 23:05:50.990: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bkll" in namespace "gc-3881"
Jan 18 23:05:51.022: INFO: Deleting pod "simpletest-rc-to-be-deleted-7fzq4" in namespace "gc-3881"
Jan 18 23:05:51.048: INFO: Deleting pod "simpletest-rc-to-be-deleted-7kcmr" in namespace "gc-3881"
Jan 18 23:05:51.061: INFO: Deleting pod "simpletest-rc-to-be-deleted-7qbkb" in namespace "gc-3881"
Jan 18 23:05:51.078: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ztr9" in namespace "gc-3881"
Jan 18 23:05:51.113: INFO: Deleting pod "simpletest-rc-to-be-deleted-845x7" in namespace "gc-3881"
Jan 18 23:05:51.132: INFO: Deleting pod "simpletest-rc-to-be-deleted-8bzhd" in namespace "gc-3881"
Jan 18 23:05:51.156: INFO: Deleting pod "simpletest-rc-to-be-deleted-8nhp9" in namespace "gc-3881"
Jan 18 23:05:51.175: INFO: Deleting pod "simpletest-rc-to-be-deleted-98tts" in namespace "gc-3881"
Jan 18 23:05:51.189: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jjc7" in namespace "gc-3881"
Jan 18 23:05:51.202: INFO: Deleting pod "simpletest-rc-to-be-deleted-9kjtr" in namespace "gc-3881"
Jan 18 23:05:51.222: INFO: Deleting pod "simpletest-rc-to-be-deleted-9zds9" in namespace "gc-3881"
Jan 18 23:05:51.256: INFO: Deleting pod "simpletest-rc-to-be-deleted-bl6fg" in namespace "gc-3881"
Jan 18 23:05:51.279: INFO: Deleting pod "simpletest-rc-to-be-deleted-bt8cr" in namespace "gc-3881"
Jan 18 23:05:51.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9dd7" in namespace "gc-3881"
Jan 18 23:05:51.306: INFO: Deleting pod "simpletest-rc-to-be-deleted-cdq4z" in namespace "gc-3881"
Jan 18 23:05:51.320: INFO: Deleting pod "simpletest-rc-to-be-deleted-cj748" in namespace "gc-3881"
Jan 18 23:05:51.333: INFO: Deleting pod "simpletest-rc-to-be-deleted-czdr7" in namespace "gc-3881"
Jan 18 23:05:51.364: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7zzv" in namespace "gc-3881"
Jan 18 23:05:51.377: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlsmt" in namespace "gc-3881"
Jan 18 23:05:51.391: INFO: Deleting pod "simpletest-rc-to-be-deleted-dm7gx" in namespace "gc-3881"
Jan 18 23:05:51.427: INFO: Deleting pod "simpletest-rc-to-be-deleted-dtzbj" in namespace "gc-3881"
Jan 18 23:05:51.443: INFO: Deleting pod "simpletest-rc-to-be-deleted-f6h8n" in namespace "gc-3881"
Jan 18 23:05:51.457: INFO: Deleting pod "simpletest-rc-to-be-deleted-ffgvl" in namespace "gc-3881"
Jan 18 23:05:51.484: INFO: Deleting pod "simpletest-rc-to-be-deleted-ffjcg" in namespace "gc-3881"
Jan 18 23:05:51.512: INFO: Deleting pod "simpletest-rc-to-be-deleted-fl29r" in namespace "gc-3881"
Jan 18 23:05:51.527: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsbn4" in namespace "gc-3881"
Jan 18 23:05:51.557: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftskm" in namespace "gc-3881"
Jan 18 23:05:51.631: INFO: Deleting pod "simpletest-rc-to-be-deleted-glw2z" in namespace "gc-3881"
Jan 18 23:05:51.654: INFO: Deleting pod "simpletest-rc-to-be-deleted-gqj7j" in namespace "gc-3881"
Jan 18 23:05:51.668: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtl54" in namespace "gc-3881"
Jan 18 23:05:51.693: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtqnc" in namespace "gc-3881"
Jan 18 23:05:51.729: INFO: Deleting pod "simpletest-rc-to-be-deleted-h4zhr" in namespace "gc-3881"
Jan 18 23:05:51.757: INFO: Deleting pod "simpletest-rc-to-be-deleted-hbwxq" in namespace "gc-3881"
Jan 18 23:05:51.796: INFO: Deleting pod "simpletest-rc-to-be-deleted-hv7lt" in namespace "gc-3881"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 18 23:05:51.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3881" for this suite. 01/18/23 23:05:51.83
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":337,"skipped":6240,"failed":0}
------------------------------
• [SLOW TEST] [19.020 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:05:32.825
    Jan 18 23:05:32.827: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename gc 01/18/23 23:05:32.831
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:05:32.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:05:32.881
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 01/18/23 23:05:32.895
    STEP: create the rc2 01/18/23 23:05:32.903
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/18/23 23:05:37.926
    STEP: delete the rc simpletest-rc-to-be-deleted 01/18/23 23:05:39.278
    STEP: wait for the rc to be deleted 01/18/23 23:05:39.29
    Jan 18 23:05:44.334: INFO: 68 pods remaining
    Jan 18 23:05:44.334: INFO: 68 pods has nil DeletionTimestamp
    Jan 18 23:05:44.334: INFO: 
    STEP: Gathering metrics 01/18/23 23:05:49.314
    Jan 18 23:05:49.370: INFO: Waiting up to 5m0s for pod "kube-controller-manager-v1-25-1-18760-m" in namespace "kube-system" to be "running and ready"
    Jan 18 23:05:49.380: INFO: Pod "kube-controller-manager-v1-25-1-18760-m": Phase="Running", Reason="", readiness=true. Elapsed: 9.017429ms
    Jan 18 23:05:49.402: INFO: The phase of Pod kube-controller-manager-v1-25-1-18760-m is Running (Ready = true)
    Jan 18 23:05:49.413: INFO: Pod "kube-controller-manager-v1-25-1-18760-m" satisfied condition "running and ready"
    Jan 18 23:05:49.772: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 18 23:05:49.772: INFO: Deleting pod "simpletest-rc-to-be-deleted-2785q" in namespace "gc-3881"
    Jan 18 23:05:49.826: INFO: Deleting pod "simpletest-rc-to-be-deleted-28bx7" in namespace "gc-3881"
    Jan 18 23:05:49.850: INFO: Deleting pod "simpletest-rc-to-be-deleted-2rz76" in namespace "gc-3881"
    Jan 18 23:05:50.173: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xwbw" in namespace "gc-3881"
    Jan 18 23:05:50.768: INFO: Deleting pod "simpletest-rc-to-be-deleted-46p27" in namespace "gc-3881"
    Jan 18 23:05:50.787: INFO: Deleting pod "simpletest-rc-to-be-deleted-4ftd9" in namespace "gc-3881"
    Jan 18 23:05:50.820: INFO: Deleting pod "simpletest-rc-to-be-deleted-4hmjt" in namespace "gc-3881"
    Jan 18 23:05:50.834: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vf9t" in namespace "gc-3881"
    Jan 18 23:05:50.847: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hqcd" in namespace "gc-3881"
    Jan 18 23:05:50.862: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xwc4" in namespace "gc-3881"
    Jan 18 23:05:50.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-6cnk7" in namespace "gc-3881"
    Jan 18 23:05:50.894: INFO: Deleting pod "simpletest-rc-to-be-deleted-6qr7m" in namespace "gc-3881"
    Jan 18 23:05:50.911: INFO: Deleting pod "simpletest-rc-to-be-deleted-6rn6k" in namespace "gc-3881"
    Jan 18 23:05:50.950: INFO: Deleting pod "simpletest-rc-to-be-deleted-744jx" in namespace "gc-3881"
    Jan 18 23:05:50.970: INFO: Deleting pod "simpletest-rc-to-be-deleted-755gs" in namespace "gc-3881"
    Jan 18 23:05:50.990: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bkll" in namespace "gc-3881"
    Jan 18 23:05:51.022: INFO: Deleting pod "simpletest-rc-to-be-deleted-7fzq4" in namespace "gc-3881"
    Jan 18 23:05:51.048: INFO: Deleting pod "simpletest-rc-to-be-deleted-7kcmr" in namespace "gc-3881"
    Jan 18 23:05:51.061: INFO: Deleting pod "simpletest-rc-to-be-deleted-7qbkb" in namespace "gc-3881"
    Jan 18 23:05:51.078: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ztr9" in namespace "gc-3881"
    Jan 18 23:05:51.113: INFO: Deleting pod "simpletest-rc-to-be-deleted-845x7" in namespace "gc-3881"
    Jan 18 23:05:51.132: INFO: Deleting pod "simpletest-rc-to-be-deleted-8bzhd" in namespace "gc-3881"
    Jan 18 23:05:51.156: INFO: Deleting pod "simpletest-rc-to-be-deleted-8nhp9" in namespace "gc-3881"
    Jan 18 23:05:51.175: INFO: Deleting pod "simpletest-rc-to-be-deleted-98tts" in namespace "gc-3881"
    Jan 18 23:05:51.189: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jjc7" in namespace "gc-3881"
    Jan 18 23:05:51.202: INFO: Deleting pod "simpletest-rc-to-be-deleted-9kjtr" in namespace "gc-3881"
    Jan 18 23:05:51.222: INFO: Deleting pod "simpletest-rc-to-be-deleted-9zds9" in namespace "gc-3881"
    Jan 18 23:05:51.256: INFO: Deleting pod "simpletest-rc-to-be-deleted-bl6fg" in namespace "gc-3881"
    Jan 18 23:05:51.279: INFO: Deleting pod "simpletest-rc-to-be-deleted-bt8cr" in namespace "gc-3881"
    Jan 18 23:05:51.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9dd7" in namespace "gc-3881"
    Jan 18 23:05:51.306: INFO: Deleting pod "simpletest-rc-to-be-deleted-cdq4z" in namespace "gc-3881"
    Jan 18 23:05:51.320: INFO: Deleting pod "simpletest-rc-to-be-deleted-cj748" in namespace "gc-3881"
    Jan 18 23:05:51.333: INFO: Deleting pod "simpletest-rc-to-be-deleted-czdr7" in namespace "gc-3881"
    Jan 18 23:05:51.364: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7zzv" in namespace "gc-3881"
    Jan 18 23:05:51.377: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlsmt" in namespace "gc-3881"
    Jan 18 23:05:51.391: INFO: Deleting pod "simpletest-rc-to-be-deleted-dm7gx" in namespace "gc-3881"
    Jan 18 23:05:51.427: INFO: Deleting pod "simpletest-rc-to-be-deleted-dtzbj" in namespace "gc-3881"
    Jan 18 23:05:51.443: INFO: Deleting pod "simpletest-rc-to-be-deleted-f6h8n" in namespace "gc-3881"
    Jan 18 23:05:51.457: INFO: Deleting pod "simpletest-rc-to-be-deleted-ffgvl" in namespace "gc-3881"
    Jan 18 23:05:51.484: INFO: Deleting pod "simpletest-rc-to-be-deleted-ffjcg" in namespace "gc-3881"
    Jan 18 23:05:51.512: INFO: Deleting pod "simpletest-rc-to-be-deleted-fl29r" in namespace "gc-3881"
    Jan 18 23:05:51.527: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsbn4" in namespace "gc-3881"
    Jan 18 23:05:51.557: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftskm" in namespace "gc-3881"
    Jan 18 23:05:51.631: INFO: Deleting pod "simpletest-rc-to-be-deleted-glw2z" in namespace "gc-3881"
    Jan 18 23:05:51.654: INFO: Deleting pod "simpletest-rc-to-be-deleted-gqj7j" in namespace "gc-3881"
    Jan 18 23:05:51.668: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtl54" in namespace "gc-3881"
    Jan 18 23:05:51.693: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtqnc" in namespace "gc-3881"
    Jan 18 23:05:51.729: INFO: Deleting pod "simpletest-rc-to-be-deleted-h4zhr" in namespace "gc-3881"
    Jan 18 23:05:51.757: INFO: Deleting pod "simpletest-rc-to-be-deleted-hbwxq" in namespace "gc-3881"
    Jan 18 23:05:51.796: INFO: Deleting pod "simpletest-rc-to-be-deleted-hv7lt" in namespace "gc-3881"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 18 23:05:51.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-3881" for this suite. 01/18/23 23:05:51.83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:05:51.85
Jan 18 23:05:51.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename configmap 01/18/23 23:05:51.852
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:05:51.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:05:51.952
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-a1e3f360-ffd6-4ffb-82ee-73668e7cf11f 01/18/23 23:05:51.959
STEP: Creating a pod to test consume configMaps 01/18/23 23:05:51.967
Jan 18 23:05:51.976: INFO: Waiting up to 5m0s for pod "pod-configmaps-9818814d-4e5a-406d-9794-e143a00aaece" in namespace "configmap-5020" to be "Succeeded or Failed"
Jan 18 23:05:51.987: INFO: Pod "pod-configmaps-9818814d-4e5a-406d-9794-e143a00aaece": Phase="Pending", Reason="", readiness=false. Elapsed: 11.021635ms
Jan 18 23:05:53.992: INFO: Pod "pod-configmaps-9818814d-4e5a-406d-9794-e143a00aaece": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016011354s
Jan 18 23:05:56.007: INFO: Pod "pod-configmaps-9818814d-4e5a-406d-9794-e143a00aaece": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030455641s
Jan 18 23:05:58.039: INFO: Pod "pod-configmaps-9818814d-4e5a-406d-9794-e143a00aaece": Phase="Pending", Reason="", readiness=false. Elapsed: 6.062599526s
Jan 18 23:06:00.006: INFO: Pod "pod-configmaps-9818814d-4e5a-406d-9794-e143a00aaece": Phase="Pending", Reason="", readiness=false. Elapsed: 8.029799656s
Jan 18 23:06:02.008: INFO: Pod "pod-configmaps-9818814d-4e5a-406d-9794-e143a00aaece": Phase="Pending", Reason="", readiness=false. Elapsed: 10.031624375s
Jan 18 23:06:03.992: INFO: Pod "pod-configmaps-9818814d-4e5a-406d-9794-e143a00aaece": Phase="Pending", Reason="", readiness=false. Elapsed: 12.015929762s
Jan 18 23:06:05.993: INFO: Pod "pod-configmaps-9818814d-4e5a-406d-9794-e143a00aaece": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.016476329s
STEP: Saw pod success 01/18/23 23:06:05.993
Jan 18 23:06:05.994: INFO: Pod "pod-configmaps-9818814d-4e5a-406d-9794-e143a00aaece" satisfied condition "Succeeded or Failed"
Jan 18 23:06:05.999: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-9818814d-4e5a-406d-9794-e143a00aaece container agnhost-container: <nil>
STEP: delete the pod 01/18/23 23:06:06.351
Jan 18 23:06:06.366: INFO: Waiting for pod pod-configmaps-9818814d-4e5a-406d-9794-e143a00aaece to disappear
Jan 18 23:06:06.375: INFO: Pod pod-configmaps-9818814d-4e5a-406d-9794-e143a00aaece no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 23:06:06.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5020" for this suite. 01/18/23 23:06:06.382
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":338,"skipped":6260,"failed":0}
------------------------------
• [SLOW TEST] [14.545 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:05:51.85
    Jan 18 23:05:51.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename configmap 01/18/23 23:05:51.852
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:05:51.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:05:51.952
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-a1e3f360-ffd6-4ffb-82ee-73668e7cf11f 01/18/23 23:05:51.959
    STEP: Creating a pod to test consume configMaps 01/18/23 23:05:51.967
    Jan 18 23:05:51.976: INFO: Waiting up to 5m0s for pod "pod-configmaps-9818814d-4e5a-406d-9794-e143a00aaece" in namespace "configmap-5020" to be "Succeeded or Failed"
    Jan 18 23:05:51.987: INFO: Pod "pod-configmaps-9818814d-4e5a-406d-9794-e143a00aaece": Phase="Pending", Reason="", readiness=false. Elapsed: 11.021635ms
    Jan 18 23:05:53.992: INFO: Pod "pod-configmaps-9818814d-4e5a-406d-9794-e143a00aaece": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016011354s
    Jan 18 23:05:56.007: INFO: Pod "pod-configmaps-9818814d-4e5a-406d-9794-e143a00aaece": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030455641s
    Jan 18 23:05:58.039: INFO: Pod "pod-configmaps-9818814d-4e5a-406d-9794-e143a00aaece": Phase="Pending", Reason="", readiness=false. Elapsed: 6.062599526s
    Jan 18 23:06:00.006: INFO: Pod "pod-configmaps-9818814d-4e5a-406d-9794-e143a00aaece": Phase="Pending", Reason="", readiness=false. Elapsed: 8.029799656s
    Jan 18 23:06:02.008: INFO: Pod "pod-configmaps-9818814d-4e5a-406d-9794-e143a00aaece": Phase="Pending", Reason="", readiness=false. Elapsed: 10.031624375s
    Jan 18 23:06:03.992: INFO: Pod "pod-configmaps-9818814d-4e5a-406d-9794-e143a00aaece": Phase="Pending", Reason="", readiness=false. Elapsed: 12.015929762s
    Jan 18 23:06:05.993: INFO: Pod "pod-configmaps-9818814d-4e5a-406d-9794-e143a00aaece": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.016476329s
    STEP: Saw pod success 01/18/23 23:06:05.993
    Jan 18 23:06:05.994: INFO: Pod "pod-configmaps-9818814d-4e5a-406d-9794-e143a00aaece" satisfied condition "Succeeded or Failed"
    Jan 18 23:06:05.999: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-9818814d-4e5a-406d-9794-e143a00aaece container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 23:06:06.351
    Jan 18 23:06:06.366: INFO: Waiting for pod pod-configmaps-9818814d-4e5a-406d-9794-e143a00aaece to disappear
    Jan 18 23:06:06.375: INFO: Pod pod-configmaps-9818814d-4e5a-406d-9794-e143a00aaece no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 23:06:06.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5020" for this suite. 01/18/23 23:06:06.382
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:06:06.401
Jan 18 23:06:06.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename configmap 01/18/23 23:06:06.403
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:06:06.425
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:06:06.437
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-b59d8abe-9fe4-410c-94dc-18d00d4e6617 01/18/23 23:06:06.445
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 23:06:06.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8610" for this suite. 01/18/23 23:06:06.455
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":339,"skipped":6291,"failed":0}
------------------------------
• [0.061 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:06:06.401
    Jan 18 23:06:06.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename configmap 01/18/23 23:06:06.403
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:06:06.425
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:06:06.437
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-b59d8abe-9fe4-410c-94dc-18d00d4e6617 01/18/23 23:06:06.445
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 23:06:06.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8610" for this suite. 01/18/23 23:06:06.455
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:06:06.462
Jan 18 23:06:06.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename kubelet-test 01/18/23 23:06:06.463
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:06:06.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:06:06.489
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jan 18 23:06:06.504: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsba6583d8-83b8-4511-8366-a4a6dd38eb07" in namespace "kubelet-test-4564" to be "running and ready"
Jan 18 23:06:06.516: INFO: Pod "busybox-readonly-fsba6583d8-83b8-4511-8366-a4a6dd38eb07": Phase="Pending", Reason="", readiness=false. Elapsed: 12.415605ms
Jan 18 23:06:06.520: INFO: The phase of Pod busybox-readonly-fsba6583d8-83b8-4511-8366-a4a6dd38eb07 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:06:08.526: INFO: Pod "busybox-readonly-fsba6583d8-83b8-4511-8366-a4a6dd38eb07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022492686s
Jan 18 23:06:08.527: INFO: The phase of Pod busybox-readonly-fsba6583d8-83b8-4511-8366-a4a6dd38eb07 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:06:10.525: INFO: Pod "busybox-readonly-fsba6583d8-83b8-4511-8366-a4a6dd38eb07": Phase="Running", Reason="", readiness=true. Elapsed: 4.021416486s
Jan 18 23:06:10.526: INFO: The phase of Pod busybox-readonly-fsba6583d8-83b8-4511-8366-a4a6dd38eb07 is Running (Ready = true)
Jan 18 23:06:10.526: INFO: Pod "busybox-readonly-fsba6583d8-83b8-4511-8366-a4a6dd38eb07" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 18 23:06:10.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4564" for this suite. 01/18/23 23:06:10.555
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":340,"skipped":6294,"failed":0}
------------------------------
• [4.104 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:06:06.462
    Jan 18 23:06:06.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename kubelet-test 01/18/23 23:06:06.463
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:06:06.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:06:06.489
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jan 18 23:06:06.504: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsba6583d8-83b8-4511-8366-a4a6dd38eb07" in namespace "kubelet-test-4564" to be "running and ready"
    Jan 18 23:06:06.516: INFO: Pod "busybox-readonly-fsba6583d8-83b8-4511-8366-a4a6dd38eb07": Phase="Pending", Reason="", readiness=false. Elapsed: 12.415605ms
    Jan 18 23:06:06.520: INFO: The phase of Pod busybox-readonly-fsba6583d8-83b8-4511-8366-a4a6dd38eb07 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:06:08.526: INFO: Pod "busybox-readonly-fsba6583d8-83b8-4511-8366-a4a6dd38eb07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022492686s
    Jan 18 23:06:08.527: INFO: The phase of Pod busybox-readonly-fsba6583d8-83b8-4511-8366-a4a6dd38eb07 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:06:10.525: INFO: Pod "busybox-readonly-fsba6583d8-83b8-4511-8366-a4a6dd38eb07": Phase="Running", Reason="", readiness=true. Elapsed: 4.021416486s
    Jan 18 23:06:10.526: INFO: The phase of Pod busybox-readonly-fsba6583d8-83b8-4511-8366-a4a6dd38eb07 is Running (Ready = true)
    Jan 18 23:06:10.526: INFO: Pod "busybox-readonly-fsba6583d8-83b8-4511-8366-a4a6dd38eb07" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 18 23:06:10.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-4564" for this suite. 01/18/23 23:06:10.555
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:06:10.568
Jan 18 23:06:10.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename pods 01/18/23 23:06:10.57
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:06:10.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:06:10.599
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 01/18/23 23:06:10.605
STEP: submitting the pod to kubernetes 01/18/23 23:06:10.606
Jan 18 23:06:10.617: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-4bfae7ba-ee24-450d-a203-720a7ae19f3e" in namespace "pods-4955" to be "running and ready"
Jan 18 23:06:10.627: INFO: Pod "pod-update-activedeadlineseconds-4bfae7ba-ee24-450d-a203-720a7ae19f3e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.366078ms
Jan 18 23:06:10.627: INFO: The phase of Pod pod-update-activedeadlineseconds-4bfae7ba-ee24-450d-a203-720a7ae19f3e is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:06:12.633: INFO: Pod "pod-update-activedeadlineseconds-4bfae7ba-ee24-450d-a203-720a7ae19f3e": Phase="Running", Reason="", readiness=true. Elapsed: 2.015513022s
Jan 18 23:06:12.633: INFO: The phase of Pod pod-update-activedeadlineseconds-4bfae7ba-ee24-450d-a203-720a7ae19f3e is Running (Ready = true)
Jan 18 23:06:12.633: INFO: Pod "pod-update-activedeadlineseconds-4bfae7ba-ee24-450d-a203-720a7ae19f3e" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/18/23 23:06:12.638
STEP: updating the pod 01/18/23 23:06:12.643
Jan 18 23:06:13.159: INFO: Successfully updated pod "pod-update-activedeadlineseconds-4bfae7ba-ee24-450d-a203-720a7ae19f3e"
Jan 18 23:06:13.159: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-4bfae7ba-ee24-450d-a203-720a7ae19f3e" in namespace "pods-4955" to be "terminated with reason DeadlineExceeded"
Jan 18 23:06:13.167: INFO: Pod "pod-update-activedeadlineseconds-4bfae7ba-ee24-450d-a203-720a7ae19f3e": Phase="Running", Reason="", readiness=true. Elapsed: 7.894262ms
Jan 18 23:06:15.173: INFO: Pod "pod-update-activedeadlineseconds-4bfae7ba-ee24-450d-a203-720a7ae19f3e": Phase="Running", Reason="", readiness=true. Elapsed: 2.014145604s
Jan 18 23:06:17.171: INFO: Pod "pod-update-activedeadlineseconds-4bfae7ba-ee24-450d-a203-720a7ae19f3e": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.01255522s
Jan 18 23:06:17.172: INFO: Pod "pod-update-activedeadlineseconds-4bfae7ba-ee24-450d-a203-720a7ae19f3e" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 23:06:17.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4955" for this suite. 01/18/23 23:06:17.178
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":341,"skipped":6306,"failed":0}
------------------------------
• [SLOW TEST] [6.616 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:06:10.568
    Jan 18 23:06:10.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename pods 01/18/23 23:06:10.57
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:06:10.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:06:10.599
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 01/18/23 23:06:10.605
    STEP: submitting the pod to kubernetes 01/18/23 23:06:10.606
    Jan 18 23:06:10.617: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-4bfae7ba-ee24-450d-a203-720a7ae19f3e" in namespace "pods-4955" to be "running and ready"
    Jan 18 23:06:10.627: INFO: Pod "pod-update-activedeadlineseconds-4bfae7ba-ee24-450d-a203-720a7ae19f3e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.366078ms
    Jan 18 23:06:10.627: INFO: The phase of Pod pod-update-activedeadlineseconds-4bfae7ba-ee24-450d-a203-720a7ae19f3e is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:06:12.633: INFO: Pod "pod-update-activedeadlineseconds-4bfae7ba-ee24-450d-a203-720a7ae19f3e": Phase="Running", Reason="", readiness=true. Elapsed: 2.015513022s
    Jan 18 23:06:12.633: INFO: The phase of Pod pod-update-activedeadlineseconds-4bfae7ba-ee24-450d-a203-720a7ae19f3e is Running (Ready = true)
    Jan 18 23:06:12.633: INFO: Pod "pod-update-activedeadlineseconds-4bfae7ba-ee24-450d-a203-720a7ae19f3e" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/18/23 23:06:12.638
    STEP: updating the pod 01/18/23 23:06:12.643
    Jan 18 23:06:13.159: INFO: Successfully updated pod "pod-update-activedeadlineseconds-4bfae7ba-ee24-450d-a203-720a7ae19f3e"
    Jan 18 23:06:13.159: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-4bfae7ba-ee24-450d-a203-720a7ae19f3e" in namespace "pods-4955" to be "terminated with reason DeadlineExceeded"
    Jan 18 23:06:13.167: INFO: Pod "pod-update-activedeadlineseconds-4bfae7ba-ee24-450d-a203-720a7ae19f3e": Phase="Running", Reason="", readiness=true. Elapsed: 7.894262ms
    Jan 18 23:06:15.173: INFO: Pod "pod-update-activedeadlineseconds-4bfae7ba-ee24-450d-a203-720a7ae19f3e": Phase="Running", Reason="", readiness=true. Elapsed: 2.014145604s
    Jan 18 23:06:17.171: INFO: Pod "pod-update-activedeadlineseconds-4bfae7ba-ee24-450d-a203-720a7ae19f3e": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.01255522s
    Jan 18 23:06:17.172: INFO: Pod "pod-update-activedeadlineseconds-4bfae7ba-ee24-450d-a203-720a7ae19f3e" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 23:06:17.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4955" for this suite. 01/18/23 23:06:17.178
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:06:17.192
Jan 18 23:06:17.192: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename gc 01/18/23 23:06:17.197
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:06:17.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:06:17.224
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 01/18/23 23:06:17.234
STEP: delete the rc 01/18/23 23:06:22.257
STEP: wait for the rc to be deleted 01/18/23 23:06:22.264
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/18/23 23:06:27.277
STEP: Gathering metrics 01/18/23 23:06:57.309
Jan 18 23:06:57.341: INFO: Waiting up to 5m0s for pod "kube-controller-manager-v1-25-1-18760-m" in namespace "kube-system" to be "running and ready"
Jan 18 23:06:57.346: INFO: Pod "kube-controller-manager-v1-25-1-18760-m": Phase="Running", Reason="", readiness=true. Elapsed: 4.129214ms
Jan 18 23:06:57.346: INFO: The phase of Pod kube-controller-manager-v1-25-1-18760-m is Running (Ready = true)
Jan 18 23:06:57.346: INFO: Pod "kube-controller-manager-v1-25-1-18760-m" satisfied condition "running and ready"
Jan 18 23:06:57.500: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 18 23:06:57.502: INFO: Deleting pod "simpletest.rc-2czmn" in namespace "gc-3105"
Jan 18 23:06:57.534: INFO: Deleting pod "simpletest.rc-2dls6" in namespace "gc-3105"
Jan 18 23:06:57.564: INFO: Deleting pod "simpletest.rc-2qn2j" in namespace "gc-3105"
Jan 18 23:06:57.650: INFO: Deleting pod "simpletest.rc-4f968" in namespace "gc-3105"
Jan 18 23:06:57.675: INFO: Deleting pod "simpletest.rc-4j86n" in namespace "gc-3105"
Jan 18 23:06:57.736: INFO: Deleting pod "simpletest.rc-4knvc" in namespace "gc-3105"
Jan 18 23:06:57.771: INFO: Deleting pod "simpletest.rc-4ln5l" in namespace "gc-3105"
Jan 18 23:06:57.802: INFO: Deleting pod "simpletest.rc-4lskd" in namespace "gc-3105"
Jan 18 23:06:57.873: INFO: Deleting pod "simpletest.rc-5xt64" in namespace "gc-3105"
Jan 18 23:06:57.977: INFO: Deleting pod "simpletest.rc-65lrl" in namespace "gc-3105"
Jan 18 23:06:57.996: INFO: Deleting pod "simpletest.rc-6bn6c" in namespace "gc-3105"
Jan 18 23:06:58.244: INFO: Deleting pod "simpletest.rc-6gkd4" in namespace "gc-3105"
Jan 18 23:06:58.262: INFO: Deleting pod "simpletest.rc-6s7xs" in namespace "gc-3105"
Jan 18 23:06:58.329: INFO: Deleting pod "simpletest.rc-6s9xb" in namespace "gc-3105"
Jan 18 23:06:58.349: INFO: Deleting pod "simpletest.rc-6x75p" in namespace "gc-3105"
Jan 18 23:06:58.397: INFO: Deleting pod "simpletest.rc-72klh" in namespace "gc-3105"
Jan 18 23:06:58.427: INFO: Deleting pod "simpletest.rc-75rmq" in namespace "gc-3105"
Jan 18 23:06:58.455: INFO: Deleting pod "simpletest.rc-7b8d2" in namespace "gc-3105"
Jan 18 23:06:58.481: INFO: Deleting pod "simpletest.rc-7np8j" in namespace "gc-3105"
Jan 18 23:06:58.540: INFO: Deleting pod "simpletest.rc-7rt2w" in namespace "gc-3105"
Jan 18 23:06:58.692: INFO: Deleting pod "simpletest.rc-7wrmb" in namespace "gc-3105"
Jan 18 23:06:58.843: INFO: Deleting pod "simpletest.rc-8ksmj" in namespace "gc-3105"
Jan 18 23:06:58.867: INFO: Deleting pod "simpletest.rc-926bx" in namespace "gc-3105"
Jan 18 23:06:58.922: INFO: Deleting pod "simpletest.rc-9fh6j" in namespace "gc-3105"
Jan 18 23:06:58.940: INFO: Deleting pod "simpletest.rc-9jc8g" in namespace "gc-3105"
Jan 18 23:06:58.971: INFO: Deleting pod "simpletest.rc-b2fdb" in namespace "gc-3105"
Jan 18 23:06:59.014: INFO: Deleting pod "simpletest.rc-b4b2t" in namespace "gc-3105"
Jan 18 23:06:59.033: INFO: Deleting pod "simpletest.rc-bsm62" in namespace "gc-3105"
Jan 18 23:06:59.057: INFO: Deleting pod "simpletest.rc-bxv7k" in namespace "gc-3105"
Jan 18 23:06:59.080: INFO: Deleting pod "simpletest.rc-cfj2f" in namespace "gc-3105"
Jan 18 23:06:59.107: INFO: Deleting pod "simpletest.rc-cm6hn" in namespace "gc-3105"
Jan 18 23:06:59.141: INFO: Deleting pod "simpletest.rc-d5nhq" in namespace "gc-3105"
Jan 18 23:06:59.169: INFO: Deleting pod "simpletest.rc-dhk54" in namespace "gc-3105"
Jan 18 23:06:59.179: INFO: Deleting pod "simpletest.rc-dph86" in namespace "gc-3105"
Jan 18 23:06:59.201: INFO: Deleting pod "simpletest.rc-dvx28" in namespace "gc-3105"
Jan 18 23:06:59.220: INFO: Deleting pod "simpletest.rc-f6n5b" in namespace "gc-3105"
Jan 18 23:06:59.238: INFO: Deleting pod "simpletest.rc-f9t4b" in namespace "gc-3105"
Jan 18 23:06:59.251: INFO: Deleting pod "simpletest.rc-fsjk7" in namespace "gc-3105"
Jan 18 23:06:59.277: INFO: Deleting pod "simpletest.rc-g6ffl" in namespace "gc-3105"
Jan 18 23:06:59.307: INFO: Deleting pod "simpletest.rc-h4dmd" in namespace "gc-3105"
Jan 18 23:06:59.359: INFO: Deleting pod "simpletest.rc-h9h8v" in namespace "gc-3105"
Jan 18 23:06:59.381: INFO: Deleting pod "simpletest.rc-hfs5v" in namespace "gc-3105"
Jan 18 23:06:59.434: INFO: Deleting pod "simpletest.rc-hk7k2" in namespace "gc-3105"
Jan 18 23:06:59.679: INFO: Deleting pod "simpletest.rc-hl4lq" in namespace "gc-3105"
Jan 18 23:06:59.720: INFO: Deleting pod "simpletest.rc-hnq8w" in namespace "gc-3105"
Jan 18 23:06:59.743: INFO: Deleting pod "simpletest.rc-htm22" in namespace "gc-3105"
Jan 18 23:06:59.835: INFO: Deleting pod "simpletest.rc-j57ql" in namespace "gc-3105"
Jan 18 23:06:59.853: INFO: Deleting pod "simpletest.rc-jdknx" in namespace "gc-3105"
Jan 18 23:06:59.871: INFO: Deleting pod "simpletest.rc-jjk9h" in namespace "gc-3105"
Jan 18 23:06:59.894: INFO: Deleting pod "simpletest.rc-jjmpq" in namespace "gc-3105"
Jan 18 23:06:59.913: INFO: Deleting pod "simpletest.rc-jnbzg" in namespace "gc-3105"
Jan 18 23:06:59.946: INFO: Deleting pod "simpletest.rc-k4hjp" in namespace "gc-3105"
Jan 18 23:06:59.968: INFO: Deleting pod "simpletest.rc-k4zkm" in namespace "gc-3105"
Jan 18 23:06:59.992: INFO: Deleting pod "simpletest.rc-k5lj7" in namespace "gc-3105"
Jan 18 23:07:00.021: INFO: Deleting pod "simpletest.rc-klpc5" in namespace "gc-3105"
Jan 18 23:07:00.047: INFO: Deleting pod "simpletest.rc-kqs2v" in namespace "gc-3105"
Jan 18 23:07:00.064: INFO: Deleting pod "simpletest.rc-l82k7" in namespace "gc-3105"
Jan 18 23:07:00.079: INFO: Deleting pod "simpletest.rc-llnwd" in namespace "gc-3105"
Jan 18 23:07:00.090: INFO: Deleting pod "simpletest.rc-ln4mj" in namespace "gc-3105"
Jan 18 23:07:00.107: INFO: Deleting pod "simpletest.rc-lv28s" in namespace "gc-3105"
Jan 18 23:07:00.122: INFO: Deleting pod "simpletest.rc-m85w9" in namespace "gc-3105"
Jan 18 23:07:00.144: INFO: Deleting pod "simpletest.rc-mmdrr" in namespace "gc-3105"
Jan 18 23:07:00.167: INFO: Deleting pod "simpletest.rc-mp9f6" in namespace "gc-3105"
Jan 18 23:07:00.182: INFO: Deleting pod "simpletest.rc-n5bt5" in namespace "gc-3105"
Jan 18 23:07:00.223: INFO: Deleting pod "simpletest.rc-n9cvj" in namespace "gc-3105"
Jan 18 23:07:00.263: INFO: Deleting pod "simpletest.rc-ntrzc" in namespace "gc-3105"
Jan 18 23:07:00.284: INFO: Deleting pod "simpletest.rc-p5bxp" in namespace "gc-3105"
Jan 18 23:07:00.325: INFO: Deleting pod "simpletest.rc-p6hgw" in namespace "gc-3105"
Jan 18 23:07:00.357: INFO: Deleting pod "simpletest.rc-pbg4g" in namespace "gc-3105"
Jan 18 23:07:00.382: INFO: Deleting pod "simpletest.rc-pxj5b" in namespace "gc-3105"
Jan 18 23:07:00.396: INFO: Deleting pod "simpletest.rc-qmvk9" in namespace "gc-3105"
Jan 18 23:07:00.421: INFO: Deleting pod "simpletest.rc-qt8rm" in namespace "gc-3105"
Jan 18 23:07:00.538: INFO: Deleting pod "simpletest.rc-qxc9p" in namespace "gc-3105"
Jan 18 23:07:00.637: INFO: Deleting pod "simpletest.rc-rdlhs" in namespace "gc-3105"
Jan 18 23:07:00.663: INFO: Deleting pod "simpletest.rc-rppxm" in namespace "gc-3105"
Jan 18 23:07:00.694: INFO: Deleting pod "simpletest.rc-rq5cj" in namespace "gc-3105"
Jan 18 23:07:00.714: INFO: Deleting pod "simpletest.rc-s4trs" in namespace "gc-3105"
Jan 18 23:07:00.765: INFO: Deleting pod "simpletest.rc-scsvf" in namespace "gc-3105"
Jan 18 23:07:00.780: INFO: Deleting pod "simpletest.rc-sg282" in namespace "gc-3105"
Jan 18 23:07:00.823: INFO: Deleting pod "simpletest.rc-snm4v" in namespace "gc-3105"
Jan 18 23:07:00.859: INFO: Deleting pod "simpletest.rc-swfnm" in namespace "gc-3105"
Jan 18 23:07:00.925: INFO: Deleting pod "simpletest.rc-sx6gr" in namespace "gc-3105"
Jan 18 23:07:01.003: INFO: Deleting pod "simpletest.rc-t4t67" in namespace "gc-3105"
Jan 18 23:07:01.034: INFO: Deleting pod "simpletest.rc-t9xx7" in namespace "gc-3105"
Jan 18 23:07:01.090: INFO: Deleting pod "simpletest.rc-tl2h9" in namespace "gc-3105"
Jan 18 23:07:01.171: INFO: Deleting pod "simpletest.rc-tpkzf" in namespace "gc-3105"
Jan 18 23:07:01.222: INFO: Deleting pod "simpletest.rc-txwq8" in namespace "gc-3105"
Jan 18 23:07:01.239: INFO: Deleting pod "simpletest.rc-tzx4m" in namespace "gc-3105"
Jan 18 23:07:01.254: INFO: Deleting pod "simpletest.rc-v4rtq" in namespace "gc-3105"
Jan 18 23:07:01.269: INFO: Deleting pod "simpletest.rc-vn2m6" in namespace "gc-3105"
Jan 18 23:07:01.283: INFO: Deleting pod "simpletest.rc-w8jhw" in namespace "gc-3105"
Jan 18 23:07:01.295: INFO: Deleting pod "simpletest.rc-wqfd6" in namespace "gc-3105"
Jan 18 23:07:01.336: INFO: Deleting pod "simpletest.rc-wr79c" in namespace "gc-3105"
Jan 18 23:07:01.375: INFO: Deleting pod "simpletest.rc-wzvhm" in namespace "gc-3105"
Jan 18 23:07:01.391: INFO: Deleting pod "simpletest.rc-x7znc" in namespace "gc-3105"
Jan 18 23:07:01.439: INFO: Deleting pod "simpletest.rc-xjtxk" in namespace "gc-3105"
Jan 18 23:07:01.457: INFO: Deleting pod "simpletest.rc-z8h89" in namespace "gc-3105"
Jan 18 23:07:01.475: INFO: Deleting pod "simpletest.rc-zcbvz" in namespace "gc-3105"
Jan 18 23:07:01.493: INFO: Deleting pod "simpletest.rc-zjkqr" in namespace "gc-3105"
Jan 18 23:07:01.513: INFO: Deleting pod "simpletest.rc-zm2hz" in namespace "gc-3105"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 18 23:07:01.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3105" for this suite. 01/18/23 23:07:01.536
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":342,"skipped":6307,"failed":0}
------------------------------
• [SLOW TEST] [44.351 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:06:17.192
    Jan 18 23:06:17.192: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename gc 01/18/23 23:06:17.197
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:06:17.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:06:17.224
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 01/18/23 23:06:17.234
    STEP: delete the rc 01/18/23 23:06:22.257
    STEP: wait for the rc to be deleted 01/18/23 23:06:22.264
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/18/23 23:06:27.277
    STEP: Gathering metrics 01/18/23 23:06:57.309
    Jan 18 23:06:57.341: INFO: Waiting up to 5m0s for pod "kube-controller-manager-v1-25-1-18760-m" in namespace "kube-system" to be "running and ready"
    Jan 18 23:06:57.346: INFO: Pod "kube-controller-manager-v1-25-1-18760-m": Phase="Running", Reason="", readiness=true. Elapsed: 4.129214ms
    Jan 18 23:06:57.346: INFO: The phase of Pod kube-controller-manager-v1-25-1-18760-m is Running (Ready = true)
    Jan 18 23:06:57.346: INFO: Pod "kube-controller-manager-v1-25-1-18760-m" satisfied condition "running and ready"
    Jan 18 23:06:57.500: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 18 23:06:57.502: INFO: Deleting pod "simpletest.rc-2czmn" in namespace "gc-3105"
    Jan 18 23:06:57.534: INFO: Deleting pod "simpletest.rc-2dls6" in namespace "gc-3105"
    Jan 18 23:06:57.564: INFO: Deleting pod "simpletest.rc-2qn2j" in namespace "gc-3105"
    Jan 18 23:06:57.650: INFO: Deleting pod "simpletest.rc-4f968" in namespace "gc-3105"
    Jan 18 23:06:57.675: INFO: Deleting pod "simpletest.rc-4j86n" in namespace "gc-3105"
    Jan 18 23:06:57.736: INFO: Deleting pod "simpletest.rc-4knvc" in namespace "gc-3105"
    Jan 18 23:06:57.771: INFO: Deleting pod "simpletest.rc-4ln5l" in namespace "gc-3105"
    Jan 18 23:06:57.802: INFO: Deleting pod "simpletest.rc-4lskd" in namespace "gc-3105"
    Jan 18 23:06:57.873: INFO: Deleting pod "simpletest.rc-5xt64" in namespace "gc-3105"
    Jan 18 23:06:57.977: INFO: Deleting pod "simpletest.rc-65lrl" in namespace "gc-3105"
    Jan 18 23:06:57.996: INFO: Deleting pod "simpletest.rc-6bn6c" in namespace "gc-3105"
    Jan 18 23:06:58.244: INFO: Deleting pod "simpletest.rc-6gkd4" in namespace "gc-3105"
    Jan 18 23:06:58.262: INFO: Deleting pod "simpletest.rc-6s7xs" in namespace "gc-3105"
    Jan 18 23:06:58.329: INFO: Deleting pod "simpletest.rc-6s9xb" in namespace "gc-3105"
    Jan 18 23:06:58.349: INFO: Deleting pod "simpletest.rc-6x75p" in namespace "gc-3105"
    Jan 18 23:06:58.397: INFO: Deleting pod "simpletest.rc-72klh" in namespace "gc-3105"
    Jan 18 23:06:58.427: INFO: Deleting pod "simpletest.rc-75rmq" in namespace "gc-3105"
    Jan 18 23:06:58.455: INFO: Deleting pod "simpletest.rc-7b8d2" in namespace "gc-3105"
    Jan 18 23:06:58.481: INFO: Deleting pod "simpletest.rc-7np8j" in namespace "gc-3105"
    Jan 18 23:06:58.540: INFO: Deleting pod "simpletest.rc-7rt2w" in namespace "gc-3105"
    Jan 18 23:06:58.692: INFO: Deleting pod "simpletest.rc-7wrmb" in namespace "gc-3105"
    Jan 18 23:06:58.843: INFO: Deleting pod "simpletest.rc-8ksmj" in namespace "gc-3105"
    Jan 18 23:06:58.867: INFO: Deleting pod "simpletest.rc-926bx" in namespace "gc-3105"
    Jan 18 23:06:58.922: INFO: Deleting pod "simpletest.rc-9fh6j" in namespace "gc-3105"
    Jan 18 23:06:58.940: INFO: Deleting pod "simpletest.rc-9jc8g" in namespace "gc-3105"
    Jan 18 23:06:58.971: INFO: Deleting pod "simpletest.rc-b2fdb" in namespace "gc-3105"
    Jan 18 23:06:59.014: INFO: Deleting pod "simpletest.rc-b4b2t" in namespace "gc-3105"
    Jan 18 23:06:59.033: INFO: Deleting pod "simpletest.rc-bsm62" in namespace "gc-3105"
    Jan 18 23:06:59.057: INFO: Deleting pod "simpletest.rc-bxv7k" in namespace "gc-3105"
    Jan 18 23:06:59.080: INFO: Deleting pod "simpletest.rc-cfj2f" in namespace "gc-3105"
    Jan 18 23:06:59.107: INFO: Deleting pod "simpletest.rc-cm6hn" in namespace "gc-3105"
    Jan 18 23:06:59.141: INFO: Deleting pod "simpletest.rc-d5nhq" in namespace "gc-3105"
    Jan 18 23:06:59.169: INFO: Deleting pod "simpletest.rc-dhk54" in namespace "gc-3105"
    Jan 18 23:06:59.179: INFO: Deleting pod "simpletest.rc-dph86" in namespace "gc-3105"
    Jan 18 23:06:59.201: INFO: Deleting pod "simpletest.rc-dvx28" in namespace "gc-3105"
    Jan 18 23:06:59.220: INFO: Deleting pod "simpletest.rc-f6n5b" in namespace "gc-3105"
    Jan 18 23:06:59.238: INFO: Deleting pod "simpletest.rc-f9t4b" in namespace "gc-3105"
    Jan 18 23:06:59.251: INFO: Deleting pod "simpletest.rc-fsjk7" in namespace "gc-3105"
    Jan 18 23:06:59.277: INFO: Deleting pod "simpletest.rc-g6ffl" in namespace "gc-3105"
    Jan 18 23:06:59.307: INFO: Deleting pod "simpletest.rc-h4dmd" in namespace "gc-3105"
    Jan 18 23:06:59.359: INFO: Deleting pod "simpletest.rc-h9h8v" in namespace "gc-3105"
    Jan 18 23:06:59.381: INFO: Deleting pod "simpletest.rc-hfs5v" in namespace "gc-3105"
    Jan 18 23:06:59.434: INFO: Deleting pod "simpletest.rc-hk7k2" in namespace "gc-3105"
    Jan 18 23:06:59.679: INFO: Deleting pod "simpletest.rc-hl4lq" in namespace "gc-3105"
    Jan 18 23:06:59.720: INFO: Deleting pod "simpletest.rc-hnq8w" in namespace "gc-3105"
    Jan 18 23:06:59.743: INFO: Deleting pod "simpletest.rc-htm22" in namespace "gc-3105"
    Jan 18 23:06:59.835: INFO: Deleting pod "simpletest.rc-j57ql" in namespace "gc-3105"
    Jan 18 23:06:59.853: INFO: Deleting pod "simpletest.rc-jdknx" in namespace "gc-3105"
    Jan 18 23:06:59.871: INFO: Deleting pod "simpletest.rc-jjk9h" in namespace "gc-3105"
    Jan 18 23:06:59.894: INFO: Deleting pod "simpletest.rc-jjmpq" in namespace "gc-3105"
    Jan 18 23:06:59.913: INFO: Deleting pod "simpletest.rc-jnbzg" in namespace "gc-3105"
    Jan 18 23:06:59.946: INFO: Deleting pod "simpletest.rc-k4hjp" in namespace "gc-3105"
    Jan 18 23:06:59.968: INFO: Deleting pod "simpletest.rc-k4zkm" in namespace "gc-3105"
    Jan 18 23:06:59.992: INFO: Deleting pod "simpletest.rc-k5lj7" in namespace "gc-3105"
    Jan 18 23:07:00.021: INFO: Deleting pod "simpletest.rc-klpc5" in namespace "gc-3105"
    Jan 18 23:07:00.047: INFO: Deleting pod "simpletest.rc-kqs2v" in namespace "gc-3105"
    Jan 18 23:07:00.064: INFO: Deleting pod "simpletest.rc-l82k7" in namespace "gc-3105"
    Jan 18 23:07:00.079: INFO: Deleting pod "simpletest.rc-llnwd" in namespace "gc-3105"
    Jan 18 23:07:00.090: INFO: Deleting pod "simpletest.rc-ln4mj" in namespace "gc-3105"
    Jan 18 23:07:00.107: INFO: Deleting pod "simpletest.rc-lv28s" in namespace "gc-3105"
    Jan 18 23:07:00.122: INFO: Deleting pod "simpletest.rc-m85w9" in namespace "gc-3105"
    Jan 18 23:07:00.144: INFO: Deleting pod "simpletest.rc-mmdrr" in namespace "gc-3105"
    Jan 18 23:07:00.167: INFO: Deleting pod "simpletest.rc-mp9f6" in namespace "gc-3105"
    Jan 18 23:07:00.182: INFO: Deleting pod "simpletest.rc-n5bt5" in namespace "gc-3105"
    Jan 18 23:07:00.223: INFO: Deleting pod "simpletest.rc-n9cvj" in namespace "gc-3105"
    Jan 18 23:07:00.263: INFO: Deleting pod "simpletest.rc-ntrzc" in namespace "gc-3105"
    Jan 18 23:07:00.284: INFO: Deleting pod "simpletest.rc-p5bxp" in namespace "gc-3105"
    Jan 18 23:07:00.325: INFO: Deleting pod "simpletest.rc-p6hgw" in namespace "gc-3105"
    Jan 18 23:07:00.357: INFO: Deleting pod "simpletest.rc-pbg4g" in namespace "gc-3105"
    Jan 18 23:07:00.382: INFO: Deleting pod "simpletest.rc-pxj5b" in namespace "gc-3105"
    Jan 18 23:07:00.396: INFO: Deleting pod "simpletest.rc-qmvk9" in namespace "gc-3105"
    Jan 18 23:07:00.421: INFO: Deleting pod "simpletest.rc-qt8rm" in namespace "gc-3105"
    Jan 18 23:07:00.538: INFO: Deleting pod "simpletest.rc-qxc9p" in namespace "gc-3105"
    Jan 18 23:07:00.637: INFO: Deleting pod "simpletest.rc-rdlhs" in namespace "gc-3105"
    Jan 18 23:07:00.663: INFO: Deleting pod "simpletest.rc-rppxm" in namespace "gc-3105"
    Jan 18 23:07:00.694: INFO: Deleting pod "simpletest.rc-rq5cj" in namespace "gc-3105"
    Jan 18 23:07:00.714: INFO: Deleting pod "simpletest.rc-s4trs" in namespace "gc-3105"
    Jan 18 23:07:00.765: INFO: Deleting pod "simpletest.rc-scsvf" in namespace "gc-3105"
    Jan 18 23:07:00.780: INFO: Deleting pod "simpletest.rc-sg282" in namespace "gc-3105"
    Jan 18 23:07:00.823: INFO: Deleting pod "simpletest.rc-snm4v" in namespace "gc-3105"
    Jan 18 23:07:00.859: INFO: Deleting pod "simpletest.rc-swfnm" in namespace "gc-3105"
    Jan 18 23:07:00.925: INFO: Deleting pod "simpletest.rc-sx6gr" in namespace "gc-3105"
    Jan 18 23:07:01.003: INFO: Deleting pod "simpletest.rc-t4t67" in namespace "gc-3105"
    Jan 18 23:07:01.034: INFO: Deleting pod "simpletest.rc-t9xx7" in namespace "gc-3105"
    Jan 18 23:07:01.090: INFO: Deleting pod "simpletest.rc-tl2h9" in namespace "gc-3105"
    Jan 18 23:07:01.171: INFO: Deleting pod "simpletest.rc-tpkzf" in namespace "gc-3105"
    Jan 18 23:07:01.222: INFO: Deleting pod "simpletest.rc-txwq8" in namespace "gc-3105"
    Jan 18 23:07:01.239: INFO: Deleting pod "simpletest.rc-tzx4m" in namespace "gc-3105"
    Jan 18 23:07:01.254: INFO: Deleting pod "simpletest.rc-v4rtq" in namespace "gc-3105"
    Jan 18 23:07:01.269: INFO: Deleting pod "simpletest.rc-vn2m6" in namespace "gc-3105"
    Jan 18 23:07:01.283: INFO: Deleting pod "simpletest.rc-w8jhw" in namespace "gc-3105"
    Jan 18 23:07:01.295: INFO: Deleting pod "simpletest.rc-wqfd6" in namespace "gc-3105"
    Jan 18 23:07:01.336: INFO: Deleting pod "simpletest.rc-wr79c" in namespace "gc-3105"
    Jan 18 23:07:01.375: INFO: Deleting pod "simpletest.rc-wzvhm" in namespace "gc-3105"
    Jan 18 23:07:01.391: INFO: Deleting pod "simpletest.rc-x7znc" in namespace "gc-3105"
    Jan 18 23:07:01.439: INFO: Deleting pod "simpletest.rc-xjtxk" in namespace "gc-3105"
    Jan 18 23:07:01.457: INFO: Deleting pod "simpletest.rc-z8h89" in namespace "gc-3105"
    Jan 18 23:07:01.475: INFO: Deleting pod "simpletest.rc-zcbvz" in namespace "gc-3105"
    Jan 18 23:07:01.493: INFO: Deleting pod "simpletest.rc-zjkqr" in namespace "gc-3105"
    Jan 18 23:07:01.513: INFO: Deleting pod "simpletest.rc-zm2hz" in namespace "gc-3105"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 18 23:07:01.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-3105" for this suite. 01/18/23 23:07:01.536
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:07:01.553
Jan 18 23:07:01.553: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename events 01/18/23 23:07:01.556
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:07:01.629
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:07:01.642
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 01/18/23 23:07:01.65
Jan 18 23:07:01.663: INFO: created test-event-1
Jan 18 23:07:01.673: INFO: created test-event-2
Jan 18 23:07:01.682: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 01/18/23 23:07:01.683
STEP: delete collection of events 01/18/23 23:07:01.688
Jan 18 23:07:01.689: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/18/23 23:07:01.749
Jan 18 23:07:01.749: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jan 18 23:07:01.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3137" for this suite. 01/18/23 23:07:01.765
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":343,"skipped":6325,"failed":0}
------------------------------
• [0.219 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:07:01.553
    Jan 18 23:07:01.553: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename events 01/18/23 23:07:01.556
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:07:01.629
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:07:01.642
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 01/18/23 23:07:01.65
    Jan 18 23:07:01.663: INFO: created test-event-1
    Jan 18 23:07:01.673: INFO: created test-event-2
    Jan 18 23:07:01.682: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 01/18/23 23:07:01.683
    STEP: delete collection of events 01/18/23 23:07:01.688
    Jan 18 23:07:01.689: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/18/23 23:07:01.749
    Jan 18 23:07:01.749: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jan 18 23:07:01.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-3137" for this suite. 01/18/23 23:07:01.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:07:01.773
Jan 18 23:07:01.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename webhook 01/18/23 23:07:01.775
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:07:01.806
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:07:01.837
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 23:07:01.861
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 23:07:02.513
STEP: Deploying the webhook pod 01/18/23 23:07:02.525
STEP: Wait for the deployment to be ready 01/18/23 23:07:02.555
Jan 18 23:07:02.592: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 18 23:07:04.623: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 23:07:06.670: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 23:07:08.638: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 23:07:10.644: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/18/23 23:07:12.756
STEP: Verifying the service has paired with the endpoint 01/18/23 23:07:12.779
Jan 18 23:07:13.780: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Jan 18 23:07:13.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1265-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 23:07:14.403
Jan 18 23:07:14.523: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource while v1 is storage version 01/18/23 23:07:14.671
STEP: Patching Custom Resource Definition to set v2 as storage 01/18/23 23:07:16.788
STEP: Patching the custom resource while v2 is storage version 01/18/23 23:07:16.801
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 23:07:17.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1384" for this suite. 01/18/23 23:07:17.363
STEP: Destroying namespace "webhook-1384-markers" for this suite. 01/18/23 23:07:17.377
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":344,"skipped":6331,"failed":0}
------------------------------
• [SLOW TEST] [15.748 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:07:01.773
    Jan 18 23:07:01.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename webhook 01/18/23 23:07:01.775
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:07:01.806
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:07:01.837
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 23:07:01.861
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 23:07:02.513
    STEP: Deploying the webhook pod 01/18/23 23:07:02.525
    STEP: Wait for the deployment to be ready 01/18/23 23:07:02.555
    Jan 18 23:07:02.592: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 18 23:07:04.623: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 23:07:06.670: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 23:07:08.638: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 23:07:10.644: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 7, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/18/23 23:07:12.756
    STEP: Verifying the service has paired with the endpoint 01/18/23 23:07:12.779
    Jan 18 23:07:13.780: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Jan 18 23:07:13.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1265-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 23:07:14.403
    Jan 18 23:07:14.523: INFO: Waiting for webhook configuration to be ready...
    STEP: Creating a custom resource while v1 is storage version 01/18/23 23:07:14.671
    STEP: Patching Custom Resource Definition to set v2 as storage 01/18/23 23:07:16.788
    STEP: Patching the custom resource while v2 is storage version 01/18/23 23:07:16.801
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 23:07:17.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1384" for this suite. 01/18/23 23:07:17.363
    STEP: Destroying namespace "webhook-1384-markers" for this suite. 01/18/23 23:07:17.377
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:07:17.524
Jan 18 23:07:17.524: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename statefulset 01/18/23 23:07:17.525
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:07:17.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:07:17.686
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-865 01/18/23 23:07:17.713
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 01/18/23 23:07:17.725
STEP: Creating stateful set ss in namespace statefulset-865 01/18/23 23:07:17.752
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-865 01/18/23 23:07:17.773
Jan 18 23:07:17.844: INFO: Found 0 stateful pods, waiting for 1
Jan 18 23:07:27.855: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/18/23 23:07:27.855
Jan 18 23:07:27.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-865 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 23:07:28.124: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 23:07:28.124: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 23:07:28.124: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 23:07:28.129: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 18 23:07:38.135: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 23:07:38.136: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 23:07:38.163: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999025s
Jan 18 23:07:39.177: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.98681912s
Jan 18 23:07:40.182: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.975396743s
Jan 18 23:07:41.188: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.970194929s
Jan 18 23:07:42.194: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.964225168s
Jan 18 23:07:43.199: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.958711884s
Jan 18 23:07:44.205: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.95395841s
Jan 18 23:07:45.210: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.947887444s
Jan 18 23:07:46.215: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.942902787s
Jan 18 23:07:47.222: INFO: Verifying statefulset ss doesn't scale past 1 for another 937.762397ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-865 01/18/23 23:07:48.223
Jan 18 23:07:48.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-865 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 23:07:48.447: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 23:07:48.447: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 23:07:48.447: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 23:07:48.451: INFO: Found 1 stateful pods, waiting for 3
Jan 18 23:07:58.459: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 23:07:58.460: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 23:07:58.460: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 01/18/23 23:07:58.46
STEP: Scale down will halt with unhealthy stateful pod 01/18/23 23:07:58.46
Jan 18 23:07:58.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-865 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 23:07:58.724: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 23:07:58.724: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 23:07:58.724: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 23:07:58.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-865 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 23:07:58.954: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 23:07:58.954: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 23:07:58.954: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 23:07:58.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-865 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 23:07:59.188: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 23:07:59.188: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 23:07:59.188: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 23:07:59.188: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 23:07:59.218: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 18 23:08:09.231: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 23:08:09.231: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 23:08:09.231: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 23:08:09.257: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999459s
Jan 18 23:08:10.278: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.984025604s
Jan 18 23:08:11.284: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.963487557s
Jan 18 23:08:12.291: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.95704538s
Jan 18 23:08:13.297: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.95003545s
Jan 18 23:08:14.303: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.944086489s
Jan 18 23:08:15.310: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.938117686s
Jan 18 23:08:16.322: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.930808553s
Jan 18 23:08:17.329: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.919128726s
Jan 18 23:08:18.335: INFO: Verifying statefulset ss doesn't scale past 3 for another 912.295444ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-865 01/18/23 23:08:19.336
Jan 18 23:08:19.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-865 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 23:08:19.606: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 23:08:19.606: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 23:08:19.606: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 23:08:19.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-865 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 23:08:19.823: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 23:08:19.823: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 23:08:19.823: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 23:08:19.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-865 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 23:08:20.053: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 23:08:20.053: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 23:08:20.053: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 23:08:20.053: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 01/18/23 23:08:30.073
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 23:08:30.074: INFO: Deleting all statefulset in ns statefulset-865
Jan 18 23:08:30.078: INFO: Scaling statefulset ss to 0
Jan 18 23:08:30.093: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 23:08:30.097: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 18 23:08:30.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-865" for this suite. 01/18/23 23:08:30.121
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":345,"skipped":6342,"failed":0}
------------------------------
• [SLOW TEST] [72.610 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:07:17.524
    Jan 18 23:07:17.524: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename statefulset 01/18/23 23:07:17.525
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:07:17.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:07:17.686
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-865 01/18/23 23:07:17.713
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 01/18/23 23:07:17.725
    STEP: Creating stateful set ss in namespace statefulset-865 01/18/23 23:07:17.752
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-865 01/18/23 23:07:17.773
    Jan 18 23:07:17.844: INFO: Found 0 stateful pods, waiting for 1
    Jan 18 23:07:27.855: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/18/23 23:07:27.855
    Jan 18 23:07:27.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-865 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 23:07:28.124: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 23:07:28.124: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 23:07:28.124: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 23:07:28.129: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 18 23:07:38.135: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 23:07:38.136: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 23:07:38.163: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999025s
    Jan 18 23:07:39.177: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.98681912s
    Jan 18 23:07:40.182: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.975396743s
    Jan 18 23:07:41.188: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.970194929s
    Jan 18 23:07:42.194: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.964225168s
    Jan 18 23:07:43.199: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.958711884s
    Jan 18 23:07:44.205: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.95395841s
    Jan 18 23:07:45.210: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.947887444s
    Jan 18 23:07:46.215: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.942902787s
    Jan 18 23:07:47.222: INFO: Verifying statefulset ss doesn't scale past 1 for another 937.762397ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-865 01/18/23 23:07:48.223
    Jan 18 23:07:48.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-865 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 23:07:48.447: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 23:07:48.447: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 23:07:48.447: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 23:07:48.451: INFO: Found 1 stateful pods, waiting for 3
    Jan 18 23:07:58.459: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 23:07:58.460: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 23:07:58.460: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 01/18/23 23:07:58.46
    STEP: Scale down will halt with unhealthy stateful pod 01/18/23 23:07:58.46
    Jan 18 23:07:58.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-865 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 23:07:58.724: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 23:07:58.724: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 23:07:58.724: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 23:07:58.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-865 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 23:07:58.954: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 23:07:58.954: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 23:07:58.954: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 23:07:58.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-865 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 23:07:59.188: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 23:07:59.188: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 23:07:59.188: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 23:07:59.188: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 23:07:59.218: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jan 18 23:08:09.231: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 23:08:09.231: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 23:08:09.231: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 23:08:09.257: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999459s
    Jan 18 23:08:10.278: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.984025604s
    Jan 18 23:08:11.284: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.963487557s
    Jan 18 23:08:12.291: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.95704538s
    Jan 18 23:08:13.297: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.95003545s
    Jan 18 23:08:14.303: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.944086489s
    Jan 18 23:08:15.310: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.938117686s
    Jan 18 23:08:16.322: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.930808553s
    Jan 18 23:08:17.329: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.919128726s
    Jan 18 23:08:18.335: INFO: Verifying statefulset ss doesn't scale past 3 for another 912.295444ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-865 01/18/23 23:08:19.336
    Jan 18 23:08:19.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-865 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 23:08:19.606: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 23:08:19.606: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 23:08:19.606: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 23:08:19.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-865 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 23:08:19.823: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 23:08:19.823: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 23:08:19.823: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 23:08:19.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=statefulset-865 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 23:08:20.053: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 23:08:20.053: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 23:08:20.053: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 23:08:20.053: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 01/18/23 23:08:30.073
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 18 23:08:30.074: INFO: Deleting all statefulset in ns statefulset-865
    Jan 18 23:08:30.078: INFO: Scaling statefulset ss to 0
    Jan 18 23:08:30.093: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 23:08:30.097: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 18 23:08:30.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-865" for this suite. 01/18/23 23:08:30.121
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:08:30.148
Jan 18 23:08:30.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename deployment 01/18/23 23:08:30.151
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:08:30.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:08:30.192
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jan 18 23:08:30.205: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 18 23:08:35.219: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 23:08:35.219
Jan 18 23:08:35.219: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 18 23:08:37.228: INFO: Creating deployment "test-rollover-deployment"
Jan 18 23:08:37.240: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 18 23:08:39.250: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 18 23:08:39.267: INFO: Ensure that both replica sets have 1 created replica
Jan 18 23:08:39.276: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 18 23:08:39.295: INFO: Updating deployment test-rollover-deployment
Jan 18 23:08:39.295: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 18 23:08:41.319: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 18 23:08:41.330: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 18 23:08:41.341: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 23:08:41.341: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 8, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 23:08:43.353: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 23:08:43.353: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 8, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 23:08:45.354: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 23:08:45.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 8, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 23:08:47.351: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 23:08:47.351: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 8, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 23:08:49.354: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 23:08:49.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 8, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 23:08:51.350: INFO: 
Jan 18 23:08:51.350: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 23:08:51.362: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-4902  259d7c7b-812e-4414-9de4-fee6192eb8fc 180915 2 2023-01-18 23:08:37 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 23:08:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:08:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d9bca8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 23:08:37 +0000 UTC,LastTransitionTime:2023-01-18 23:08:37 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-01-18 23:08:51 +0000 UTC,LastTransitionTime:2023-01-18 23:08:37 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 18 23:08:51.366: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-4902  f86f8d62-c663-4b20-8f61-a58e7d84abf5 180905 2 2023-01-18 23:08:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 259d7c7b-812e-4414-9de4-fee6192eb8fc 0xc003f4cca7 0xc003f4cca8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 23:08:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"259d7c7b-812e-4414-9de4-fee6192eb8fc\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:08:51 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f4cd58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 23:08:51.366: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 18 23:08:51.366: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4902  351155fe-b176-4fe4-9f4d-471a217ec49f 180914 2 2023-01-18 23:08:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 259d7c7b-812e-4414-9de4-fee6192eb8fc 0xc003f4ca57 0xc003f4ca58}] [] [{e2e.test Update apps/v1 2023-01-18 23:08:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:08:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"259d7c7b-812e-4414-9de4-fee6192eb8fc\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:08:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003f4cb18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 23:08:51.367: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-4902  aea4af69-fd4c-4f43-9c14-38a59182d74f 180840 2 2023-01-18 23:08:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 259d7c7b-812e-4414-9de4-fee6192eb8fc 0xc003f4cb87 0xc003f4cb88}] [] [{kube-controller-manager Update apps/v1 2023-01-18 23:08:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"259d7c7b-812e-4414-9de4-fee6192eb8fc\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:08:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f4cc38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 23:08:51.371: INFO: Pod "test-rollover-deployment-6d45fd857b-pl6j8" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-pl6j8 test-rollover-deployment-6d45fd857b- deployment-4902  c609edea-17f4-419b-9757-9afe447581ee 180863 0 2023-01-18 23:08:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:2a47cae7c03e303a75b8ba50f3aa39cd658af91ac51913128403993acfcbe42e cni.projectcalico.org/podIP:10.233.68.42/32 cni.projectcalico.org/podIPs:10.233.68.42/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b f86f8d62-c663-4b20-8f61-a58e7d84abf5 0xc003f4d317 0xc003f4d318}] [] [{kube-controller-manager Update v1 2023-01-18 23:08:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f86f8d62-c663-4b20-8f61-a58e7d84abf5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 23:08:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 23:08:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.42\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-59nxf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-59nxf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:08:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:08:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:08:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:08:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.42,StartTime:2023-01-18 23:08:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:08:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://40e3fc7151325c3ea971e9692e5f1df3fba8cd073672d1e4600d9329e029bde7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.42,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 18 23:08:51.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4902" for this suite. 01/18/23 23:08:51.378
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":346,"skipped":6388,"failed":0}
------------------------------
• [SLOW TEST] [21.240 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:08:30.148
    Jan 18 23:08:30.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename deployment 01/18/23 23:08:30.151
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:08:30.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:08:30.192
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jan 18 23:08:30.205: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jan 18 23:08:35.219: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 23:08:35.219
    Jan 18 23:08:35.219: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jan 18 23:08:37.228: INFO: Creating deployment "test-rollover-deployment"
    Jan 18 23:08:37.240: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jan 18 23:08:39.250: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jan 18 23:08:39.267: INFO: Ensure that both replica sets have 1 created replica
    Jan 18 23:08:39.276: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jan 18 23:08:39.295: INFO: Updating deployment test-rollover-deployment
    Jan 18 23:08:39.295: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jan 18 23:08:41.319: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jan 18 23:08:41.330: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jan 18 23:08:41.341: INFO: all replica sets need to contain the pod-template-hash label
    Jan 18 23:08:41.341: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 8, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 23:08:43.353: INFO: all replica sets need to contain the pod-template-hash label
    Jan 18 23:08:43.353: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 8, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 23:08:45.354: INFO: all replica sets need to contain the pod-template-hash label
    Jan 18 23:08:45.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 8, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 23:08:47.351: INFO: all replica sets need to contain the pod-template-hash label
    Jan 18 23:08:47.351: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 8, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 23:08:49.354: INFO: all replica sets need to contain the pod-template-hash label
    Jan 18 23:08:49.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 23, 8, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 23, 8, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 23:08:51.350: INFO: 
    Jan 18 23:08:51.350: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 23:08:51.362: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-4902  259d7c7b-812e-4414-9de4-fee6192eb8fc 180915 2 2023-01-18 23:08:37 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 23:08:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:08:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d9bca8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 23:08:37 +0000 UTC,LastTransitionTime:2023-01-18 23:08:37 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-01-18 23:08:51 +0000 UTC,LastTransitionTime:2023-01-18 23:08:37 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 18 23:08:51.366: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-4902  f86f8d62-c663-4b20-8f61-a58e7d84abf5 180905 2 2023-01-18 23:08:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 259d7c7b-812e-4414-9de4-fee6192eb8fc 0xc003f4cca7 0xc003f4cca8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 23:08:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"259d7c7b-812e-4414-9de4-fee6192eb8fc\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:08:51 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f4cd58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 23:08:51.366: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jan 18 23:08:51.366: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4902  351155fe-b176-4fe4-9f4d-471a217ec49f 180914 2 2023-01-18 23:08:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 259d7c7b-812e-4414-9de4-fee6192eb8fc 0xc003f4ca57 0xc003f4ca58}] [] [{e2e.test Update apps/v1 2023-01-18 23:08:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:08:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"259d7c7b-812e-4414-9de4-fee6192eb8fc\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:08:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003f4cb18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 23:08:51.367: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-4902  aea4af69-fd4c-4f43-9c14-38a59182d74f 180840 2 2023-01-18 23:08:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 259d7c7b-812e-4414-9de4-fee6192eb8fc 0xc003f4cb87 0xc003f4cb88}] [] [{kube-controller-manager Update apps/v1 2023-01-18 23:08:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"259d7c7b-812e-4414-9de4-fee6192eb8fc\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 23:08:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f4cc38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 23:08:51.371: INFO: Pod "test-rollover-deployment-6d45fd857b-pl6j8" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-pl6j8 test-rollover-deployment-6d45fd857b- deployment-4902  c609edea-17f4-419b-9757-9afe447581ee 180863 0 2023-01-18 23:08:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:2a47cae7c03e303a75b8ba50f3aa39cd658af91ac51913128403993acfcbe42e cni.projectcalico.org/podIP:10.233.68.42/32 cni.projectcalico.org/podIPs:10.233.68.42/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b f86f8d62-c663-4b20-8f61-a58e7d84abf5 0xc003f4d317 0xc003f4d318}] [] [{kube-controller-manager Update v1 2023-01-18 23:08:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f86f8d62-c663-4b20-8f61-a58e7d84abf5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 23:08:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 23:08:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.42\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-59nxf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-59nxf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:08:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:08:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:08:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 23:08:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.42,StartTime:2023-01-18 23:08:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 23:08:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://40e3fc7151325c3ea971e9692e5f1df3fba8cd073672d1e4600d9329e029bde7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.42,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 18 23:08:51.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4902" for this suite. 01/18/23 23:08:51.378
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:08:51.39
Jan 18 23:08:51.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename kubectl 01/18/23 23:08:51.392
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:08:51.423
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:08:51.429
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 01/18/23 23:08:51.433
Jan 18 23:08:51.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3664 create -f -'
Jan 18 23:08:53.233: INFO: stderr: ""
Jan 18 23:08:53.233: INFO: stdout: "pod/pause created\n"
Jan 18 23:08:53.233: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 18 23:08:53.233: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3664" to be "running and ready"
Jan 18 23:08:53.260: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 27.373095ms
Jan 18 23:08:53.261: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'v1-25-1-18760-w2' to be 'Running' but was 'Pending'
Jan 18 23:08:55.267: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.033983449s
Jan 18 23:08:55.267: INFO: Pod "pause" satisfied condition "running and ready"
Jan 18 23:08:55.267: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 01/18/23 23:08:55.274
Jan 18 23:08:55.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3664 label pods pause testing-label=testing-label-value'
Jan 18 23:08:55.413: INFO: stderr: ""
Jan 18 23:08:55.414: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 01/18/23 23:08:55.414
Jan 18 23:08:55.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3664 get pod pause -L testing-label'
Jan 18 23:08:55.521: INFO: stderr: ""
Jan 18 23:08:55.521: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 01/18/23 23:08:55.521
Jan 18 23:08:55.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3664 label pods pause testing-label-'
Jan 18 23:08:55.669: INFO: stderr: ""
Jan 18 23:08:55.669: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 01/18/23 23:08:55.669
Jan 18 23:08:55.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3664 get pod pause -L testing-label'
Jan 18 23:08:55.791: INFO: stderr: ""
Jan 18 23:08:55.791: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 01/18/23 23:08:55.791
Jan 18 23:08:55.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3664 delete --grace-period=0 --force -f -'
Jan 18 23:08:55.922: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 23:08:55.922: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 18 23:08:55.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3664 get rc,svc -l name=pause --no-headers'
Jan 18 23:08:56.062: INFO: stderr: "No resources found in kubectl-3664 namespace.\n"
Jan 18 23:08:56.062: INFO: stdout: ""
Jan 18 23:08:56.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3664 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 18 23:08:56.158: INFO: stderr: ""
Jan 18 23:08:56.158: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 23:08:56.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3664" for this suite. 01/18/23 23:08:56.164
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":347,"skipped":6392,"failed":0}
------------------------------
• [4.783 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:08:51.39
    Jan 18 23:08:51.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename kubectl 01/18/23 23:08:51.392
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:08:51.423
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:08:51.429
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 01/18/23 23:08:51.433
    Jan 18 23:08:51.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3664 create -f -'
    Jan 18 23:08:53.233: INFO: stderr: ""
    Jan 18 23:08:53.233: INFO: stdout: "pod/pause created\n"
    Jan 18 23:08:53.233: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jan 18 23:08:53.233: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3664" to be "running and ready"
    Jan 18 23:08:53.260: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 27.373095ms
    Jan 18 23:08:53.261: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'v1-25-1-18760-w2' to be 'Running' but was 'Pending'
    Jan 18 23:08:55.267: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.033983449s
    Jan 18 23:08:55.267: INFO: Pod "pause" satisfied condition "running and ready"
    Jan 18 23:08:55.267: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 01/18/23 23:08:55.274
    Jan 18 23:08:55.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3664 label pods pause testing-label=testing-label-value'
    Jan 18 23:08:55.413: INFO: stderr: ""
    Jan 18 23:08:55.414: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 01/18/23 23:08:55.414
    Jan 18 23:08:55.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3664 get pod pause -L testing-label'
    Jan 18 23:08:55.521: INFO: stderr: ""
    Jan 18 23:08:55.521: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 01/18/23 23:08:55.521
    Jan 18 23:08:55.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3664 label pods pause testing-label-'
    Jan 18 23:08:55.669: INFO: stderr: ""
    Jan 18 23:08:55.669: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 01/18/23 23:08:55.669
    Jan 18 23:08:55.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3664 get pod pause -L testing-label'
    Jan 18 23:08:55.791: INFO: stderr: ""
    Jan 18 23:08:55.791: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 01/18/23 23:08:55.791
    Jan 18 23:08:55.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3664 delete --grace-period=0 --force -f -'
    Jan 18 23:08:55.922: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 23:08:55.922: INFO: stdout: "pod \"pause\" force deleted\n"
    Jan 18 23:08:55.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3664 get rc,svc -l name=pause --no-headers'
    Jan 18 23:08:56.062: INFO: stderr: "No resources found in kubectl-3664 namespace.\n"
    Jan 18 23:08:56.062: INFO: stdout: ""
    Jan 18 23:08:56.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-3664 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 18 23:08:56.158: INFO: stderr: ""
    Jan 18 23:08:56.158: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 23:08:56.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3664" for this suite. 01/18/23 23:08:56.164
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:08:56.173
Jan 18 23:08:56.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename downward-api 01/18/23 23:08:56.176
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:08:56.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:08:56.201
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 01/18/23 23:08:56.208
Jan 18 23:08:56.218: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dd12d661-d68c-49f6-9516-f22f3678901a" in namespace "downward-api-4177" to be "Succeeded or Failed"
Jan 18 23:08:56.223: INFO: Pod "downwardapi-volume-dd12d661-d68c-49f6-9516-f22f3678901a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.557369ms
Jan 18 23:08:58.243: INFO: Pod "downwardapi-volume-dd12d661-d68c-49f6-9516-f22f3678901a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024627617s
Jan 18 23:09:00.228: INFO: Pod "downwardapi-volume-dd12d661-d68c-49f6-9516-f22f3678901a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010035487s
Jan 18 23:09:02.230: INFO: Pod "downwardapi-volume-dd12d661-d68c-49f6-9516-f22f3678901a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01121753s
STEP: Saw pod success 01/18/23 23:09:02.23
Jan 18 23:09:02.231: INFO: Pod "downwardapi-volume-dd12d661-d68c-49f6-9516-f22f3678901a" satisfied condition "Succeeded or Failed"
Jan 18 23:09:02.236: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-dd12d661-d68c-49f6-9516-f22f3678901a container client-container: <nil>
STEP: delete the pod 01/18/23 23:09:02.256
Jan 18 23:09:02.271: INFO: Waiting for pod downwardapi-volume-dd12d661-d68c-49f6-9516-f22f3678901a to disappear
Jan 18 23:09:02.277: INFO: Pod downwardapi-volume-dd12d661-d68c-49f6-9516-f22f3678901a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 23:09:02.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4177" for this suite. 01/18/23 23:09:02.284
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":348,"skipped":6396,"failed":0}
------------------------------
• [SLOW TEST] [6.120 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:08:56.173
    Jan 18 23:08:56.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename downward-api 01/18/23 23:08:56.176
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:08:56.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:08:56.201
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 01/18/23 23:08:56.208
    Jan 18 23:08:56.218: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dd12d661-d68c-49f6-9516-f22f3678901a" in namespace "downward-api-4177" to be "Succeeded or Failed"
    Jan 18 23:08:56.223: INFO: Pod "downwardapi-volume-dd12d661-d68c-49f6-9516-f22f3678901a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.557369ms
    Jan 18 23:08:58.243: INFO: Pod "downwardapi-volume-dd12d661-d68c-49f6-9516-f22f3678901a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024627617s
    Jan 18 23:09:00.228: INFO: Pod "downwardapi-volume-dd12d661-d68c-49f6-9516-f22f3678901a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010035487s
    Jan 18 23:09:02.230: INFO: Pod "downwardapi-volume-dd12d661-d68c-49f6-9516-f22f3678901a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01121753s
    STEP: Saw pod success 01/18/23 23:09:02.23
    Jan 18 23:09:02.231: INFO: Pod "downwardapi-volume-dd12d661-d68c-49f6-9516-f22f3678901a" satisfied condition "Succeeded or Failed"
    Jan 18 23:09:02.236: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-dd12d661-d68c-49f6-9516-f22f3678901a container client-container: <nil>
    STEP: delete the pod 01/18/23 23:09:02.256
    Jan 18 23:09:02.271: INFO: Waiting for pod downwardapi-volume-dd12d661-d68c-49f6-9516-f22f3678901a to disappear
    Jan 18 23:09:02.277: INFO: Pod downwardapi-volume-dd12d661-d68c-49f6-9516-f22f3678901a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 23:09:02.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4177" for this suite. 01/18/23 23:09:02.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:09:02.297
Jan 18 23:09:02.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename namespaces 01/18/23 23:09:02.299
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:09:02.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:09:02.339
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 01/18/23 23:09:02.344
STEP: patching the Namespace 01/18/23 23:09:02.369
STEP: get the Namespace and ensuring it has the label 01/18/23 23:09:02.379
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 18 23:09:02.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9935" for this suite. 01/18/23 23:09:02.39
STEP: Destroying namespace "nspatchtest-a75c7b7b-6210-454b-9892-c96b947f6686-8843" for this suite. 01/18/23 23:09:02.396
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":349,"skipped":6404,"failed":0}
------------------------------
• [0.112 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:09:02.297
    Jan 18 23:09:02.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename namespaces 01/18/23 23:09:02.299
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:09:02.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:09:02.339
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 01/18/23 23:09:02.344
    STEP: patching the Namespace 01/18/23 23:09:02.369
    STEP: get the Namespace and ensuring it has the label 01/18/23 23:09:02.379
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 23:09:02.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-9935" for this suite. 01/18/23 23:09:02.39
    STEP: Destroying namespace "nspatchtest-a75c7b7b-6210-454b-9892-c96b947f6686-8843" for this suite. 01/18/23 23:09:02.396
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:09:02.418
Jan 18 23:09:02.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename downward-api 01/18/23 23:09:02.42
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:09:02.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:09:02.446
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 01/18/23 23:09:02.454
Jan 18 23:09:02.481: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bf6c7cc4-d2bf-4d56-a9e6-4ee0893cd0a8" in namespace "downward-api-6267" to be "Succeeded or Failed"
Jan 18 23:09:02.497: INFO: Pod "downwardapi-volume-bf6c7cc4-d2bf-4d56-a9e6-4ee0893cd0a8": Phase="Pending", Reason="", readiness=false. Elapsed: 15.43081ms
Jan 18 23:09:04.504: INFO: Pod "downwardapi-volume-bf6c7cc4-d2bf-4d56-a9e6-4ee0893cd0a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022171748s
Jan 18 23:09:06.504: INFO: Pod "downwardapi-volume-bf6c7cc4-d2bf-4d56-a9e6-4ee0893cd0a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021980564s
STEP: Saw pod success 01/18/23 23:09:06.505
Jan 18 23:09:06.505: INFO: Pod "downwardapi-volume-bf6c7cc4-d2bf-4d56-a9e6-4ee0893cd0a8" satisfied condition "Succeeded or Failed"
Jan 18 23:09:06.511: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-bf6c7cc4-d2bf-4d56-a9e6-4ee0893cd0a8 container client-container: <nil>
STEP: delete the pod 01/18/23 23:09:06.52
Jan 18 23:09:06.538: INFO: Waiting for pod downwardapi-volume-bf6c7cc4-d2bf-4d56-a9e6-4ee0893cd0a8 to disappear
Jan 18 23:09:06.549: INFO: Pod downwardapi-volume-bf6c7cc4-d2bf-4d56-a9e6-4ee0893cd0a8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 23:09:06.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6267" for this suite. 01/18/23 23:09:06.557
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":350,"skipped":6462,"failed":0}
------------------------------
• [4.147 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:09:02.418
    Jan 18 23:09:02.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename downward-api 01/18/23 23:09:02.42
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:09:02.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:09:02.446
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 01/18/23 23:09:02.454
    Jan 18 23:09:02.481: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bf6c7cc4-d2bf-4d56-a9e6-4ee0893cd0a8" in namespace "downward-api-6267" to be "Succeeded or Failed"
    Jan 18 23:09:02.497: INFO: Pod "downwardapi-volume-bf6c7cc4-d2bf-4d56-a9e6-4ee0893cd0a8": Phase="Pending", Reason="", readiness=false. Elapsed: 15.43081ms
    Jan 18 23:09:04.504: INFO: Pod "downwardapi-volume-bf6c7cc4-d2bf-4d56-a9e6-4ee0893cd0a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022171748s
    Jan 18 23:09:06.504: INFO: Pod "downwardapi-volume-bf6c7cc4-d2bf-4d56-a9e6-4ee0893cd0a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021980564s
    STEP: Saw pod success 01/18/23 23:09:06.505
    Jan 18 23:09:06.505: INFO: Pod "downwardapi-volume-bf6c7cc4-d2bf-4d56-a9e6-4ee0893cd0a8" satisfied condition "Succeeded or Failed"
    Jan 18 23:09:06.511: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-bf6c7cc4-d2bf-4d56-a9e6-4ee0893cd0a8 container client-container: <nil>
    STEP: delete the pod 01/18/23 23:09:06.52
    Jan 18 23:09:06.538: INFO: Waiting for pod downwardapi-volume-bf6c7cc4-d2bf-4d56-a9e6-4ee0893cd0a8 to disappear
    Jan 18 23:09:06.549: INFO: Pod downwardapi-volume-bf6c7cc4-d2bf-4d56-a9e6-4ee0893cd0a8 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 23:09:06.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6267" for this suite. 01/18/23 23:09:06.557
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:09:06.567
Jan 18 23:09:06.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename services 01/18/23 23:09:06.575
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:09:06.592
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:09:06.597
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-4188 01/18/23 23:09:06.602
STEP: creating service affinity-clusterip-transition in namespace services-4188 01/18/23 23:09:06.602
STEP: creating replication controller affinity-clusterip-transition in namespace services-4188 01/18/23 23:09:06.649
I0118 23:09:06.662926      19 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-4188, replica count: 3
I0118 23:09:09.715421      19 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 23:09:09.724: INFO: Creating new exec pod
Jan 18 23:09:09.732: INFO: Waiting up to 5m0s for pod "execpod-affinityd7rgg" in namespace "services-4188" to be "running"
Jan 18 23:09:09.744: INFO: Pod "execpod-affinityd7rgg": Phase="Pending", Reason="", readiness=false. Elapsed: 11.676805ms
Jan 18 23:09:11.752: INFO: Pod "execpod-affinityd7rgg": Phase="Running", Reason="", readiness=true. Elapsed: 2.019884773s
Jan 18 23:09:11.753: INFO: Pod "execpod-affinityd7rgg" satisfied condition "running"
Jan 18 23:09:12.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-4188 exec execpod-affinityd7rgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jan 18 23:09:12.978: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan 18 23:09:12.978: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 23:09:12.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-4188 exec execpod-affinityd7rgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.39.17 80'
Jan 18 23:09:13.194: INFO: stderr: "+ + nc -v -t -w 2 10.233.39.17 80\necho hostName\nConnection to 10.233.39.17 80 port [tcp/http] succeeded!\n"
Jan 18 23:09:13.194: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 23:09:13.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-4188 exec execpod-affinityd7rgg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.39.17:80/ ; done'
Jan 18 23:09:13.587: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n"
Jan 18 23:09:13.587: INFO: stdout: "\naffinity-clusterip-transition-5ggx9\naffinity-clusterip-transition-cdj5f\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-5ggx9\naffinity-clusterip-transition-cdj5f\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-5ggx9\naffinity-clusterip-transition-cdj5f\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-5ggx9\naffinity-clusterip-transition-cdj5f\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-5ggx9\naffinity-clusterip-transition-cdj5f\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-5ggx9"
Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-5ggx9
Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-cdj5f
Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-4crz7
Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-5ggx9
Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-cdj5f
Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-4crz7
Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-5ggx9
Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-cdj5f
Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-4crz7
Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-5ggx9
Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-cdj5f
Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-4crz7
Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-5ggx9
Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-cdj5f
Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-4crz7
Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-5ggx9
Jan 18 23:09:13.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-4188 exec execpod-affinityd7rgg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.39.17:80/ ; done'
Jan 18 23:09:13.969: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n"
Jan 18 23:09:13.969: INFO: stdout: "\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7"
Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
Jan 18 23:09:13.969: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4188, will wait for the garbage collector to delete the pods 01/18/23 23:09:13.984
Jan 18 23:09:14.064: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.763912ms
Jan 18 23:09:14.165: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.757764ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 23:09:16.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4188" for this suite. 01/18/23 23:09:16.852
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":351,"skipped":6475,"failed":0}
------------------------------
• [SLOW TEST] [10.298 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:09:06.567
    Jan 18 23:09:06.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename services 01/18/23 23:09:06.575
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:09:06.592
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:09:06.597
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-4188 01/18/23 23:09:06.602
    STEP: creating service affinity-clusterip-transition in namespace services-4188 01/18/23 23:09:06.602
    STEP: creating replication controller affinity-clusterip-transition in namespace services-4188 01/18/23 23:09:06.649
    I0118 23:09:06.662926      19 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-4188, replica count: 3
    I0118 23:09:09.715421      19 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 23:09:09.724: INFO: Creating new exec pod
    Jan 18 23:09:09.732: INFO: Waiting up to 5m0s for pod "execpod-affinityd7rgg" in namespace "services-4188" to be "running"
    Jan 18 23:09:09.744: INFO: Pod "execpod-affinityd7rgg": Phase="Pending", Reason="", readiness=false. Elapsed: 11.676805ms
    Jan 18 23:09:11.752: INFO: Pod "execpod-affinityd7rgg": Phase="Running", Reason="", readiness=true. Elapsed: 2.019884773s
    Jan 18 23:09:11.753: INFO: Pod "execpod-affinityd7rgg" satisfied condition "running"
    Jan 18 23:09:12.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-4188 exec execpod-affinityd7rgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Jan 18 23:09:12.978: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jan 18 23:09:12.978: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 23:09:12.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-4188 exec execpod-affinityd7rgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.39.17 80'
    Jan 18 23:09:13.194: INFO: stderr: "+ + nc -v -t -w 2 10.233.39.17 80\necho hostName\nConnection to 10.233.39.17 80 port [tcp/http] succeeded!\n"
    Jan 18 23:09:13.194: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 23:09:13.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-4188 exec execpod-affinityd7rgg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.39.17:80/ ; done'
    Jan 18 23:09:13.587: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n"
    Jan 18 23:09:13.587: INFO: stdout: "\naffinity-clusterip-transition-5ggx9\naffinity-clusterip-transition-cdj5f\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-5ggx9\naffinity-clusterip-transition-cdj5f\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-5ggx9\naffinity-clusterip-transition-cdj5f\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-5ggx9\naffinity-clusterip-transition-cdj5f\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-5ggx9\naffinity-clusterip-transition-cdj5f\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-5ggx9"
    Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-5ggx9
    Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-cdj5f
    Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-4crz7
    Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-5ggx9
    Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-cdj5f
    Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-4crz7
    Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-5ggx9
    Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-cdj5f
    Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-4crz7
    Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-5ggx9
    Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-cdj5f
    Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-4crz7
    Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-5ggx9
    Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-cdj5f
    Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-4crz7
    Jan 18 23:09:13.587: INFO: Received response from host: affinity-clusterip-transition-5ggx9
    Jan 18 23:09:13.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=services-4188 exec execpod-affinityd7rgg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.39.17:80/ ; done'
    Jan 18 23:09:13.969: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.39.17:80/\n"
    Jan 18 23:09:13.969: INFO: stdout: "\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7\naffinity-clusterip-transition-4crz7"
    Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
    Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
    Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
    Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
    Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
    Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
    Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
    Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
    Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
    Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
    Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
    Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
    Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
    Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
    Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
    Jan 18 23:09:13.969: INFO: Received response from host: affinity-clusterip-transition-4crz7
    Jan 18 23:09:13.969: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4188, will wait for the garbage collector to delete the pods 01/18/23 23:09:13.984
    Jan 18 23:09:14.064: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.763912ms
    Jan 18 23:09:14.165: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.757764ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 23:09:16.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4188" for this suite. 01/18/23 23:09:16.852
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:09:16.878
Jan 18 23:09:16.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 23:09:16.88
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:09:16.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:09:16.908
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-1f09c4d4-cdfd-4b9d-97f1-041273622988 01/18/23 23:09:16.913
STEP: Creating a pod to test consume configMaps 01/18/23 23:09:16.919
Jan 18 23:09:16.932: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5aed6d21-54ce-405d-86fe-9757d02f6a1b" in namespace "projected-4176" to be "Succeeded or Failed"
Jan 18 23:09:16.951: INFO: Pod "pod-projected-configmaps-5aed6d21-54ce-405d-86fe-9757d02f6a1b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.545092ms
Jan 18 23:09:18.957: INFO: Pod "pod-projected-configmaps-5aed6d21-54ce-405d-86fe-9757d02f6a1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024671651s
Jan 18 23:09:20.958: INFO: Pod "pod-projected-configmaps-5aed6d21-54ce-405d-86fe-9757d02f6a1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025878927s
STEP: Saw pod success 01/18/23 23:09:20.958
Jan 18 23:09:20.959: INFO: Pod "pod-projected-configmaps-5aed6d21-54ce-405d-86fe-9757d02f6a1b" satisfied condition "Succeeded or Failed"
Jan 18 23:09:20.964: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-5aed6d21-54ce-405d-86fe-9757d02f6a1b container agnhost-container: <nil>
STEP: delete the pod 01/18/23 23:09:20.975
Jan 18 23:09:20.996: INFO: Waiting for pod pod-projected-configmaps-5aed6d21-54ce-405d-86fe-9757d02f6a1b to disappear
Jan 18 23:09:21.004: INFO: Pod pod-projected-configmaps-5aed6d21-54ce-405d-86fe-9757d02f6a1b no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 18 23:09:21.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4176" for this suite. 01/18/23 23:09:21.013
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":352,"skipped":6562,"failed":0}
------------------------------
• [4.145 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:09:16.878
    Jan 18 23:09:16.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 23:09:16.88
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:09:16.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:09:16.908
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-1f09c4d4-cdfd-4b9d-97f1-041273622988 01/18/23 23:09:16.913
    STEP: Creating a pod to test consume configMaps 01/18/23 23:09:16.919
    Jan 18 23:09:16.932: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5aed6d21-54ce-405d-86fe-9757d02f6a1b" in namespace "projected-4176" to be "Succeeded or Failed"
    Jan 18 23:09:16.951: INFO: Pod "pod-projected-configmaps-5aed6d21-54ce-405d-86fe-9757d02f6a1b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.545092ms
    Jan 18 23:09:18.957: INFO: Pod "pod-projected-configmaps-5aed6d21-54ce-405d-86fe-9757d02f6a1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024671651s
    Jan 18 23:09:20.958: INFO: Pod "pod-projected-configmaps-5aed6d21-54ce-405d-86fe-9757d02f6a1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025878927s
    STEP: Saw pod success 01/18/23 23:09:20.958
    Jan 18 23:09:20.959: INFO: Pod "pod-projected-configmaps-5aed6d21-54ce-405d-86fe-9757d02f6a1b" satisfied condition "Succeeded or Failed"
    Jan 18 23:09:20.964: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-5aed6d21-54ce-405d-86fe-9757d02f6a1b container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 23:09:20.975
    Jan 18 23:09:20.996: INFO: Waiting for pod pod-projected-configmaps-5aed6d21-54ce-405d-86fe-9757d02f6a1b to disappear
    Jan 18 23:09:21.004: INFO: Pod pod-projected-configmaps-5aed6d21-54ce-405d-86fe-9757d02f6a1b no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 18 23:09:21.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4176" for this suite. 01/18/23 23:09:21.013
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:09:21.03
Jan 18 23:09:21.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename emptydir 01/18/23 23:09:21.032
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:09:21.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:09:21.061
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/18/23 23:09:21.067
Jan 18 23:09:21.079: INFO: Waiting up to 5m0s for pod "pod-fe22068b-5f2a-402f-a31f-813b7d48b69f" in namespace "emptydir-1136" to be "Succeeded or Failed"
Jan 18 23:09:21.086: INFO: Pod "pod-fe22068b-5f2a-402f-a31f-813b7d48b69f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.15276ms
Jan 18 23:09:23.209: INFO: Pod "pod-fe22068b-5f2a-402f-a31f-813b7d48b69f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.129582709s
Jan 18 23:09:25.092: INFO: Pod "pod-fe22068b-5f2a-402f-a31f-813b7d48b69f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012739197s
STEP: Saw pod success 01/18/23 23:09:25.093
Jan 18 23:09:25.093: INFO: Pod "pod-fe22068b-5f2a-402f-a31f-813b7d48b69f" satisfied condition "Succeeded or Failed"
Jan 18 23:09:25.098: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-fe22068b-5f2a-402f-a31f-813b7d48b69f container test-container: <nil>
STEP: delete the pod 01/18/23 23:09:25.105
Jan 18 23:09:25.119: INFO: Waiting for pod pod-fe22068b-5f2a-402f-a31f-813b7d48b69f to disappear
Jan 18 23:09:25.125: INFO: Pod pod-fe22068b-5f2a-402f-a31f-813b7d48b69f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 23:09:25.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1136" for this suite. 01/18/23 23:09:25.133
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":353,"skipped":6567,"failed":0}
------------------------------
• [4.114 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:09:21.03
    Jan 18 23:09:21.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename emptydir 01/18/23 23:09:21.032
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:09:21.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:09:21.061
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/18/23 23:09:21.067
    Jan 18 23:09:21.079: INFO: Waiting up to 5m0s for pod "pod-fe22068b-5f2a-402f-a31f-813b7d48b69f" in namespace "emptydir-1136" to be "Succeeded or Failed"
    Jan 18 23:09:21.086: INFO: Pod "pod-fe22068b-5f2a-402f-a31f-813b7d48b69f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.15276ms
    Jan 18 23:09:23.209: INFO: Pod "pod-fe22068b-5f2a-402f-a31f-813b7d48b69f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.129582709s
    Jan 18 23:09:25.092: INFO: Pod "pod-fe22068b-5f2a-402f-a31f-813b7d48b69f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012739197s
    STEP: Saw pod success 01/18/23 23:09:25.093
    Jan 18 23:09:25.093: INFO: Pod "pod-fe22068b-5f2a-402f-a31f-813b7d48b69f" satisfied condition "Succeeded or Failed"
    Jan 18 23:09:25.098: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-fe22068b-5f2a-402f-a31f-813b7d48b69f container test-container: <nil>
    STEP: delete the pod 01/18/23 23:09:25.105
    Jan 18 23:09:25.119: INFO: Waiting for pod pod-fe22068b-5f2a-402f-a31f-813b7d48b69f to disappear
    Jan 18 23:09:25.125: INFO: Pod pod-fe22068b-5f2a-402f-a31f-813b7d48b69f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 23:09:25.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1136" for this suite. 01/18/23 23:09:25.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:09:25.155
Jan 18 23:09:25.155: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename kubectl 01/18/23 23:09:25.157
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:09:25.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:09:25.188
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 01/18/23 23:09:25.197
Jan 18 23:09:25.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 create -f -'
Jan 18 23:09:25.530: INFO: stderr: ""
Jan 18 23:09:25.530: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 23:09:25.53
Jan 18 23:09:25.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 23:09:25.674: INFO: stderr: ""
Jan 18 23:09:25.674: INFO: stdout: "update-demo-nautilus-jjfzb update-demo-nautilus-rn52p "
Jan 18 23:09:25.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods update-demo-nautilus-jjfzb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 23:09:25.780: INFO: stderr: ""
Jan 18 23:09:25.780: INFO: stdout: ""
Jan 18 23:09:25.780: INFO: update-demo-nautilus-jjfzb is created but not running
Jan 18 23:09:30.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 23:09:30.907: INFO: stderr: ""
Jan 18 23:09:30.907: INFO: stdout: "update-demo-nautilus-jjfzb update-demo-nautilus-rn52p "
Jan 18 23:09:30.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods update-demo-nautilus-jjfzb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 23:09:31.011: INFO: stderr: ""
Jan 18 23:09:31.011: INFO: stdout: "true"
Jan 18 23:09:31.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods update-demo-nautilus-jjfzb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 23:09:31.136: INFO: stderr: ""
Jan 18 23:09:31.136: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 18 23:09:31.136: INFO: validating pod update-demo-nautilus-jjfzb
Jan 18 23:09:31.144: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 23:09:31.145: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 23:09:31.145: INFO: update-demo-nautilus-jjfzb is verified up and running
Jan 18 23:09:31.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods update-demo-nautilus-rn52p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 23:09:31.271: INFO: stderr: ""
Jan 18 23:09:31.271: INFO: stdout: "true"
Jan 18 23:09:31.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods update-demo-nautilus-rn52p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 23:09:31.381: INFO: stderr: ""
Jan 18 23:09:31.381: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 18 23:09:31.381: INFO: validating pod update-demo-nautilus-rn52p
Jan 18 23:09:31.387: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 23:09:31.387: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 23:09:31.387: INFO: update-demo-nautilus-rn52p is verified up and running
STEP: scaling down the replication controller 01/18/23 23:09:31.388
Jan 18 23:09:31.390: INFO: scanned /root for discovery docs: <nil>
Jan 18 23:09:31.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan 18 23:09:32.532: INFO: stderr: ""
Jan 18 23:09:32.532: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 23:09:32.532
Jan 18 23:09:32.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 23:09:32.644: INFO: stderr: ""
Jan 18 23:09:32.645: INFO: stdout: "update-demo-nautilus-rn52p "
Jan 18 23:09:32.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods update-demo-nautilus-rn52p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 23:09:32.754: INFO: stderr: ""
Jan 18 23:09:32.754: INFO: stdout: "true"
Jan 18 23:09:32.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods update-demo-nautilus-rn52p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 23:09:32.865: INFO: stderr: ""
Jan 18 23:09:32.865: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 18 23:09:32.865: INFO: validating pod update-demo-nautilus-rn52p
Jan 18 23:09:32.870: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 23:09:32.870: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 23:09:32.870: INFO: update-demo-nautilus-rn52p is verified up and running
STEP: scaling up the replication controller 01/18/23 23:09:32.87
Jan 18 23:09:32.872: INFO: scanned /root for discovery docs: <nil>
Jan 18 23:09:32.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan 18 23:09:34.012: INFO: stderr: ""
Jan 18 23:09:34.012: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 23:09:34.012
Jan 18 23:09:34.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 23:09:34.188: INFO: stderr: ""
Jan 18 23:09:34.188: INFO: stdout: "update-demo-nautilus-9n5jd update-demo-nautilus-rn52p "
Jan 18 23:09:34.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods update-demo-nautilus-9n5jd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 23:09:34.293: INFO: stderr: ""
Jan 18 23:09:34.293: INFO: stdout: ""
Jan 18 23:09:34.293: INFO: update-demo-nautilus-9n5jd is created but not running
Jan 18 23:09:39.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 23:09:39.423: INFO: stderr: ""
Jan 18 23:09:39.423: INFO: stdout: "update-demo-nautilus-9n5jd update-demo-nautilus-rn52p "
Jan 18 23:09:39.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods update-demo-nautilus-9n5jd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 23:09:39.529: INFO: stderr: ""
Jan 18 23:09:39.529: INFO: stdout: "true"
Jan 18 23:09:39.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods update-demo-nautilus-9n5jd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 23:09:39.626: INFO: stderr: ""
Jan 18 23:09:39.626: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 18 23:09:39.626: INFO: validating pod update-demo-nautilus-9n5jd
Jan 18 23:09:39.633: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 23:09:39.633: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 23:09:39.633: INFO: update-demo-nautilus-9n5jd is verified up and running
Jan 18 23:09:39.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods update-demo-nautilus-rn52p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 23:09:39.748: INFO: stderr: ""
Jan 18 23:09:39.748: INFO: stdout: "true"
Jan 18 23:09:39.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods update-demo-nautilus-rn52p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 23:09:39.851: INFO: stderr: ""
Jan 18 23:09:39.851: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 18 23:09:39.851: INFO: validating pod update-demo-nautilus-rn52p
Jan 18 23:09:39.856: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 23:09:39.856: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 23:09:39.856: INFO: update-demo-nautilus-rn52p is verified up and running
STEP: using delete to clean up resources 01/18/23 23:09:39.856
Jan 18 23:09:39.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 delete --grace-period=0 --force -f -'
Jan 18 23:09:39.979: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 23:09:39.979: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 18 23:09:39.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get rc,svc -l name=update-demo --no-headers'
Jan 18 23:09:40.207: INFO: stderr: "No resources found in kubectl-7362 namespace.\n"
Jan 18 23:09:40.207: INFO: stdout: ""
Jan 18 23:09:40.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 18 23:09:40.374: INFO: stderr: ""
Jan 18 23:09:40.374: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 23:09:40.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7362" for this suite. 01/18/23 23:09:40.381
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":354,"skipped":6586,"failed":0}
------------------------------
• [SLOW TEST] [15.234 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:09:25.155
    Jan 18 23:09:25.155: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename kubectl 01/18/23 23:09:25.157
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:09:25.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:09:25.188
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 01/18/23 23:09:25.197
    Jan 18 23:09:25.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 create -f -'
    Jan 18 23:09:25.530: INFO: stderr: ""
    Jan 18 23:09:25.530: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 23:09:25.53
    Jan 18 23:09:25.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 23:09:25.674: INFO: stderr: ""
    Jan 18 23:09:25.674: INFO: stdout: "update-demo-nautilus-jjfzb update-demo-nautilus-rn52p "
    Jan 18 23:09:25.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods update-demo-nautilus-jjfzb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 23:09:25.780: INFO: stderr: ""
    Jan 18 23:09:25.780: INFO: stdout: ""
    Jan 18 23:09:25.780: INFO: update-demo-nautilus-jjfzb is created but not running
    Jan 18 23:09:30.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 23:09:30.907: INFO: stderr: ""
    Jan 18 23:09:30.907: INFO: stdout: "update-demo-nautilus-jjfzb update-demo-nautilus-rn52p "
    Jan 18 23:09:30.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods update-demo-nautilus-jjfzb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 23:09:31.011: INFO: stderr: ""
    Jan 18 23:09:31.011: INFO: stdout: "true"
    Jan 18 23:09:31.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods update-demo-nautilus-jjfzb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 23:09:31.136: INFO: stderr: ""
    Jan 18 23:09:31.136: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 18 23:09:31.136: INFO: validating pod update-demo-nautilus-jjfzb
    Jan 18 23:09:31.144: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 23:09:31.145: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 23:09:31.145: INFO: update-demo-nautilus-jjfzb is verified up and running
    Jan 18 23:09:31.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods update-demo-nautilus-rn52p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 23:09:31.271: INFO: stderr: ""
    Jan 18 23:09:31.271: INFO: stdout: "true"
    Jan 18 23:09:31.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods update-demo-nautilus-rn52p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 23:09:31.381: INFO: stderr: ""
    Jan 18 23:09:31.381: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 18 23:09:31.381: INFO: validating pod update-demo-nautilus-rn52p
    Jan 18 23:09:31.387: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 23:09:31.387: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 23:09:31.387: INFO: update-demo-nautilus-rn52p is verified up and running
    STEP: scaling down the replication controller 01/18/23 23:09:31.388
    Jan 18 23:09:31.390: INFO: scanned /root for discovery docs: <nil>
    Jan 18 23:09:31.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jan 18 23:09:32.532: INFO: stderr: ""
    Jan 18 23:09:32.532: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 23:09:32.532
    Jan 18 23:09:32.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 23:09:32.644: INFO: stderr: ""
    Jan 18 23:09:32.645: INFO: stdout: "update-demo-nautilus-rn52p "
    Jan 18 23:09:32.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods update-demo-nautilus-rn52p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 23:09:32.754: INFO: stderr: ""
    Jan 18 23:09:32.754: INFO: stdout: "true"
    Jan 18 23:09:32.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods update-demo-nautilus-rn52p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 23:09:32.865: INFO: stderr: ""
    Jan 18 23:09:32.865: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 18 23:09:32.865: INFO: validating pod update-demo-nautilus-rn52p
    Jan 18 23:09:32.870: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 23:09:32.870: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 23:09:32.870: INFO: update-demo-nautilus-rn52p is verified up and running
    STEP: scaling up the replication controller 01/18/23 23:09:32.87
    Jan 18 23:09:32.872: INFO: scanned /root for discovery docs: <nil>
    Jan 18 23:09:32.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jan 18 23:09:34.012: INFO: stderr: ""
    Jan 18 23:09:34.012: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 23:09:34.012
    Jan 18 23:09:34.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 23:09:34.188: INFO: stderr: ""
    Jan 18 23:09:34.188: INFO: stdout: "update-demo-nautilus-9n5jd update-demo-nautilus-rn52p "
    Jan 18 23:09:34.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods update-demo-nautilus-9n5jd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 23:09:34.293: INFO: stderr: ""
    Jan 18 23:09:34.293: INFO: stdout: ""
    Jan 18 23:09:34.293: INFO: update-demo-nautilus-9n5jd is created but not running
    Jan 18 23:09:39.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 23:09:39.423: INFO: stderr: ""
    Jan 18 23:09:39.423: INFO: stdout: "update-demo-nautilus-9n5jd update-demo-nautilus-rn52p "
    Jan 18 23:09:39.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods update-demo-nautilus-9n5jd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 23:09:39.529: INFO: stderr: ""
    Jan 18 23:09:39.529: INFO: stdout: "true"
    Jan 18 23:09:39.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods update-demo-nautilus-9n5jd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 23:09:39.626: INFO: stderr: ""
    Jan 18 23:09:39.626: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 18 23:09:39.626: INFO: validating pod update-demo-nautilus-9n5jd
    Jan 18 23:09:39.633: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 23:09:39.633: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 23:09:39.633: INFO: update-demo-nautilus-9n5jd is verified up and running
    Jan 18 23:09:39.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods update-demo-nautilus-rn52p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 23:09:39.748: INFO: stderr: ""
    Jan 18 23:09:39.748: INFO: stdout: "true"
    Jan 18 23:09:39.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods update-demo-nautilus-rn52p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 23:09:39.851: INFO: stderr: ""
    Jan 18 23:09:39.851: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 18 23:09:39.851: INFO: validating pod update-demo-nautilus-rn52p
    Jan 18 23:09:39.856: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 23:09:39.856: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 23:09:39.856: INFO: update-demo-nautilus-rn52p is verified up and running
    STEP: using delete to clean up resources 01/18/23 23:09:39.856
    Jan 18 23:09:39.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 delete --grace-period=0 --force -f -'
    Jan 18 23:09:39.979: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 23:09:39.979: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 18 23:09:39.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get rc,svc -l name=update-demo --no-headers'
    Jan 18 23:09:40.207: INFO: stderr: "No resources found in kubectl-7362 namespace.\n"
    Jan 18 23:09:40.207: INFO: stdout: ""
    Jan 18 23:09:40.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-7362 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 18 23:09:40.374: INFO: stderr: ""
    Jan 18 23:09:40.374: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 23:09:40.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7362" for this suite. 01/18/23 23:09:40.381
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:09:40.39
Jan 18 23:09:40.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename kubectl 01/18/23 23:09:40.392
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:09:40.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:09:40.428
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/18/23 23:09:40.437
Jan 18 23:09:40.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-2430 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Jan 18 23:09:40.678: INFO: stderr: ""
Jan 18 23:09:40.678: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 01/18/23 23:09:40.678
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Jan 18 23:09:40.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-2430 delete pods e2e-test-httpd-pod'
Jan 18 23:09:43.557: INFO: stderr: ""
Jan 18 23:09:43.557: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 23:09:43.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2430" for this suite. 01/18/23 23:09:43.563
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":355,"skipped":6597,"failed":0}
------------------------------
• [3.182 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:09:40.39
    Jan 18 23:09:40.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename kubectl 01/18/23 23:09:40.392
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:09:40.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:09:40.428
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/18/23 23:09:40.437
    Jan 18 23:09:40.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-2430 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Jan 18 23:09:40.678: INFO: stderr: ""
    Jan 18 23:09:40.678: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 01/18/23 23:09:40.678
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Jan 18 23:09:40.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=kubectl-2430 delete pods e2e-test-httpd-pod'
    Jan 18 23:09:43.557: INFO: stderr: ""
    Jan 18 23:09:43.557: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 23:09:43.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2430" for this suite. 01/18/23 23:09:43.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:09:43.575
Jan 18 23:09:43.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename resourcequota 01/18/23 23:09:43.577
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:09:43.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:09:43.595
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 01/18/23 23:09:43.6
STEP: Ensuring ResourceQuota status is calculated 01/18/23 23:09:43.605
STEP: Creating a ResourceQuota with not best effort scope 01/18/23 23:09:45.612
STEP: Ensuring ResourceQuota status is calculated 01/18/23 23:09:45.618
STEP: Creating a best-effort pod 01/18/23 23:09:47.623
STEP: Ensuring resource quota with best effort scope captures the pod usage 01/18/23 23:09:47.636
STEP: Ensuring resource quota with not best effort ignored the pod usage 01/18/23 23:09:49.641
STEP: Deleting the pod 01/18/23 23:09:51.646
STEP: Ensuring resource quota status released the pod usage 01/18/23 23:09:51.659
STEP: Creating a not best-effort pod 01/18/23 23:09:53.664
STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/18/23 23:09:53.677
STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/18/23 23:09:55.683
STEP: Deleting the pod 01/18/23 23:09:57.689
STEP: Ensuring resource quota status released the pod usage 01/18/23 23:09:57.731
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 23:09:59.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6981" for this suite. 01/18/23 23:09:59.744
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":356,"skipped":6611,"failed":0}
------------------------------
• [SLOW TEST] [16.181 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:09:43.575
    Jan 18 23:09:43.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename resourcequota 01/18/23 23:09:43.577
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:09:43.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:09:43.595
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 01/18/23 23:09:43.6
    STEP: Ensuring ResourceQuota status is calculated 01/18/23 23:09:43.605
    STEP: Creating a ResourceQuota with not best effort scope 01/18/23 23:09:45.612
    STEP: Ensuring ResourceQuota status is calculated 01/18/23 23:09:45.618
    STEP: Creating a best-effort pod 01/18/23 23:09:47.623
    STEP: Ensuring resource quota with best effort scope captures the pod usage 01/18/23 23:09:47.636
    STEP: Ensuring resource quota with not best effort ignored the pod usage 01/18/23 23:09:49.641
    STEP: Deleting the pod 01/18/23 23:09:51.646
    STEP: Ensuring resource quota status released the pod usage 01/18/23 23:09:51.659
    STEP: Creating a not best-effort pod 01/18/23 23:09:53.664
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/18/23 23:09:53.677
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/18/23 23:09:55.683
    STEP: Deleting the pod 01/18/23 23:09:57.689
    STEP: Ensuring resource quota status released the pod usage 01/18/23 23:09:57.731
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 23:09:59.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6981" for this suite. 01/18/23 23:09:59.744
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:09:59.756
Jan 18 23:09:59.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename subpath 01/18/23 23:09:59.758
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:09:59.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:09:59.79
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/18/23 23:09:59.795
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-chqj 01/18/23 23:09:59.813
STEP: Creating a pod to test atomic-volume-subpath 01/18/23 23:09:59.814
Jan 18 23:09:59.830: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-chqj" in namespace "subpath-7127" to be "Succeeded or Failed"
Jan 18 23:09:59.848: INFO: Pod "pod-subpath-test-configmap-chqj": Phase="Pending", Reason="", readiness=false. Elapsed: 18.730978ms
Jan 18 23:10:01.853: INFO: Pod "pod-subpath-test-configmap-chqj": Phase="Running", Reason="", readiness=true. Elapsed: 2.02365719s
Jan 18 23:10:03.856: INFO: Pod "pod-subpath-test-configmap-chqj": Phase="Running", Reason="", readiness=true. Elapsed: 4.025798305s
Jan 18 23:10:05.854: INFO: Pod "pod-subpath-test-configmap-chqj": Phase="Running", Reason="", readiness=true. Elapsed: 6.023906929s
Jan 18 23:10:07.855: INFO: Pod "pod-subpath-test-configmap-chqj": Phase="Running", Reason="", readiness=true. Elapsed: 8.025253642s
Jan 18 23:10:09.857: INFO: Pod "pod-subpath-test-configmap-chqj": Phase="Running", Reason="", readiness=true. Elapsed: 10.027272089s
Jan 18 23:10:11.854: INFO: Pod "pod-subpath-test-configmap-chqj": Phase="Running", Reason="", readiness=true. Elapsed: 12.023903876s
Jan 18 23:10:13.853: INFO: Pod "pod-subpath-test-configmap-chqj": Phase="Running", Reason="", readiness=true. Elapsed: 14.023111516s
Jan 18 23:10:15.854: INFO: Pod "pod-subpath-test-configmap-chqj": Phase="Running", Reason="", readiness=true. Elapsed: 16.024267921s
Jan 18 23:10:17.853: INFO: Pod "pod-subpath-test-configmap-chqj": Phase="Running", Reason="", readiness=true. Elapsed: 18.023463345s
Jan 18 23:10:19.855: INFO: Pod "pod-subpath-test-configmap-chqj": Phase="Running", Reason="", readiness=true. Elapsed: 20.025510718s
Jan 18 23:10:21.885: INFO: Pod "pod-subpath-test-configmap-chqj": Phase="Running", Reason="", readiness=false. Elapsed: 22.055250738s
Jan 18 23:10:23.854: INFO: Pod "pod-subpath-test-configmap-chqj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.024336924s
STEP: Saw pod success 01/18/23 23:10:23.854
Jan 18 23:10:23.855: INFO: Pod "pod-subpath-test-configmap-chqj" satisfied condition "Succeeded or Failed"
Jan 18 23:10:23.861: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-subpath-test-configmap-chqj container test-container-subpath-configmap-chqj: <nil>
STEP: delete the pod 01/18/23 23:10:23.87
Jan 18 23:10:23.883: INFO: Waiting for pod pod-subpath-test-configmap-chqj to disappear
Jan 18 23:10:23.888: INFO: Pod pod-subpath-test-configmap-chqj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-chqj 01/18/23 23:10:23.889
Jan 18 23:10:23.889: INFO: Deleting pod "pod-subpath-test-configmap-chqj" in namespace "subpath-7127"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 18 23:10:23.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7127" for this suite. 01/18/23 23:10:23.898
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":357,"skipped":6614,"failed":0}
------------------------------
• [SLOW TEST] [24.149 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:09:59.756
    Jan 18 23:09:59.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename subpath 01/18/23 23:09:59.758
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:09:59.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:09:59.79
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/18/23 23:09:59.795
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-chqj 01/18/23 23:09:59.813
    STEP: Creating a pod to test atomic-volume-subpath 01/18/23 23:09:59.814
    Jan 18 23:09:59.830: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-chqj" in namespace "subpath-7127" to be "Succeeded or Failed"
    Jan 18 23:09:59.848: INFO: Pod "pod-subpath-test-configmap-chqj": Phase="Pending", Reason="", readiness=false. Elapsed: 18.730978ms
    Jan 18 23:10:01.853: INFO: Pod "pod-subpath-test-configmap-chqj": Phase="Running", Reason="", readiness=true. Elapsed: 2.02365719s
    Jan 18 23:10:03.856: INFO: Pod "pod-subpath-test-configmap-chqj": Phase="Running", Reason="", readiness=true. Elapsed: 4.025798305s
    Jan 18 23:10:05.854: INFO: Pod "pod-subpath-test-configmap-chqj": Phase="Running", Reason="", readiness=true. Elapsed: 6.023906929s
    Jan 18 23:10:07.855: INFO: Pod "pod-subpath-test-configmap-chqj": Phase="Running", Reason="", readiness=true. Elapsed: 8.025253642s
    Jan 18 23:10:09.857: INFO: Pod "pod-subpath-test-configmap-chqj": Phase="Running", Reason="", readiness=true. Elapsed: 10.027272089s
    Jan 18 23:10:11.854: INFO: Pod "pod-subpath-test-configmap-chqj": Phase="Running", Reason="", readiness=true. Elapsed: 12.023903876s
    Jan 18 23:10:13.853: INFO: Pod "pod-subpath-test-configmap-chqj": Phase="Running", Reason="", readiness=true. Elapsed: 14.023111516s
    Jan 18 23:10:15.854: INFO: Pod "pod-subpath-test-configmap-chqj": Phase="Running", Reason="", readiness=true. Elapsed: 16.024267921s
    Jan 18 23:10:17.853: INFO: Pod "pod-subpath-test-configmap-chqj": Phase="Running", Reason="", readiness=true. Elapsed: 18.023463345s
    Jan 18 23:10:19.855: INFO: Pod "pod-subpath-test-configmap-chqj": Phase="Running", Reason="", readiness=true. Elapsed: 20.025510718s
    Jan 18 23:10:21.885: INFO: Pod "pod-subpath-test-configmap-chqj": Phase="Running", Reason="", readiness=false. Elapsed: 22.055250738s
    Jan 18 23:10:23.854: INFO: Pod "pod-subpath-test-configmap-chqj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.024336924s
    STEP: Saw pod success 01/18/23 23:10:23.854
    Jan 18 23:10:23.855: INFO: Pod "pod-subpath-test-configmap-chqj" satisfied condition "Succeeded or Failed"
    Jan 18 23:10:23.861: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-subpath-test-configmap-chqj container test-container-subpath-configmap-chqj: <nil>
    STEP: delete the pod 01/18/23 23:10:23.87
    Jan 18 23:10:23.883: INFO: Waiting for pod pod-subpath-test-configmap-chqj to disappear
    Jan 18 23:10:23.888: INFO: Pod pod-subpath-test-configmap-chqj no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-chqj 01/18/23 23:10:23.889
    Jan 18 23:10:23.889: INFO: Deleting pod "pod-subpath-test-configmap-chqj" in namespace "subpath-7127"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 18 23:10:23.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7127" for this suite. 01/18/23 23:10:23.898
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:10:23.913
Jan 18 23:10:23.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 23:10:23.916
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:10:23.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:10:23.953
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Jan 18 23:10:23.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 23:10:29.003
Jan 18 23:10:29.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-4017 --namespace=crd-publish-openapi-4017 create -f -'
Jan 18 23:10:30.338: INFO: stderr: ""
Jan 18 23:10:30.339: INFO: stdout: "e2e-test-crd-publish-openapi-1303-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 18 23:10:30.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-4017 --namespace=crd-publish-openapi-4017 delete e2e-test-crd-publish-openapi-1303-crds test-cr'
Jan 18 23:10:30.455: INFO: stderr: ""
Jan 18 23:10:30.455: INFO: stdout: "e2e-test-crd-publish-openapi-1303-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 18 23:10:30.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-4017 --namespace=crd-publish-openapi-4017 apply -f -'
Jan 18 23:10:30.828: INFO: stderr: ""
Jan 18 23:10:30.828: INFO: stdout: "e2e-test-crd-publish-openapi-1303-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 18 23:10:30.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-4017 --namespace=crd-publish-openapi-4017 delete e2e-test-crd-publish-openapi-1303-crds test-cr'
Jan 18 23:10:31.089: INFO: stderr: ""
Jan 18 23:10:31.090: INFO: stdout: "e2e-test-crd-publish-openapi-1303-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/18/23 23:10:31.09
Jan 18 23:10:31.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-4017 explain e2e-test-crd-publish-openapi-1303-crds'
Jan 18 23:10:31.430: INFO: stderr: ""
Jan 18 23:10:31.430: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1303-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 23:10:36.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4017" for this suite. 01/18/23 23:10:36.544
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":358,"skipped":6616,"failed":0}
------------------------------
• [SLOW TEST] [12.641 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:10:23.913
    Jan 18 23:10:23.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 23:10:23.916
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:10:23.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:10:23.953
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Jan 18 23:10:23.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 23:10:29.003
    Jan 18 23:10:29.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-4017 --namespace=crd-publish-openapi-4017 create -f -'
    Jan 18 23:10:30.338: INFO: stderr: ""
    Jan 18 23:10:30.339: INFO: stdout: "e2e-test-crd-publish-openapi-1303-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 18 23:10:30.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-4017 --namespace=crd-publish-openapi-4017 delete e2e-test-crd-publish-openapi-1303-crds test-cr'
    Jan 18 23:10:30.455: INFO: stderr: ""
    Jan 18 23:10:30.455: INFO: stdout: "e2e-test-crd-publish-openapi-1303-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jan 18 23:10:30.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-4017 --namespace=crd-publish-openapi-4017 apply -f -'
    Jan 18 23:10:30.828: INFO: stderr: ""
    Jan 18 23:10:30.828: INFO: stdout: "e2e-test-crd-publish-openapi-1303-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 18 23:10:30.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-4017 --namespace=crd-publish-openapi-4017 delete e2e-test-crd-publish-openapi-1303-crds test-cr'
    Jan 18 23:10:31.089: INFO: stderr: ""
    Jan 18 23:10:31.090: INFO: stdout: "e2e-test-crd-publish-openapi-1303-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/18/23 23:10:31.09
    Jan 18 23:10:31.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3033012221 --namespace=crd-publish-openapi-4017 explain e2e-test-crd-publish-openapi-1303-crds'
    Jan 18 23:10:31.430: INFO: stderr: ""
    Jan 18 23:10:31.430: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1303-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 23:10:36.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4017" for this suite. 01/18/23 23:10:36.544
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:10:36.556
Jan 18 23:10:36.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename container-probe 01/18/23 23:10:36.558
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:10:36.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:10:36.587
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 18 23:11:36.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2701" for this suite. 01/18/23 23:11:36.621
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":359,"skipped":6628,"failed":0}
------------------------------
• [SLOW TEST] [60.075 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:10:36.556
    Jan 18 23:10:36.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename container-probe 01/18/23 23:10:36.558
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:10:36.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:10:36.587
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 18 23:11:36.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-2701" for this suite. 01/18/23 23:11:36.621
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:11:36.634
Jan 18 23:11:36.634: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename subpath 01/18/23 23:11:36.637
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:11:36.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:11:36.692
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/18/23 23:11:36.699
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-hgzf 01/18/23 23:11:36.725
STEP: Creating a pod to test atomic-volume-subpath 01/18/23 23:11:36.725
Jan 18 23:11:36.738: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-hgzf" in namespace "subpath-7949" to be "Succeeded or Failed"
Jan 18 23:11:36.743: INFO: Pod "pod-subpath-test-secret-hgzf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.555972ms
Jan 18 23:11:38.749: INFO: Pod "pod-subpath-test-secret-hgzf": Phase="Running", Reason="", readiness=true. Elapsed: 2.010725262s
Jan 18 23:11:40.748: INFO: Pod "pod-subpath-test-secret-hgzf": Phase="Running", Reason="", readiness=true. Elapsed: 4.010441694s
Jan 18 23:11:42.749: INFO: Pod "pod-subpath-test-secret-hgzf": Phase="Running", Reason="", readiness=true. Elapsed: 6.010764707s
Jan 18 23:11:44.748: INFO: Pod "pod-subpath-test-secret-hgzf": Phase="Running", Reason="", readiness=true. Elapsed: 8.009971142s
Jan 18 23:11:46.748: INFO: Pod "pod-subpath-test-secret-hgzf": Phase="Running", Reason="", readiness=true. Elapsed: 10.010451073s
Jan 18 23:11:48.749: INFO: Pod "pod-subpath-test-secret-hgzf": Phase="Running", Reason="", readiness=true. Elapsed: 12.011473769s
Jan 18 23:11:50.747: INFO: Pod "pod-subpath-test-secret-hgzf": Phase="Running", Reason="", readiness=true. Elapsed: 14.009313811s
Jan 18 23:11:52.749: INFO: Pod "pod-subpath-test-secret-hgzf": Phase="Running", Reason="", readiness=true. Elapsed: 16.011288319s
Jan 18 23:11:54.756: INFO: Pod "pod-subpath-test-secret-hgzf": Phase="Running", Reason="", readiness=true. Elapsed: 18.017853986s
Jan 18 23:11:56.749: INFO: Pod "pod-subpath-test-secret-hgzf": Phase="Running", Reason="", readiness=true. Elapsed: 20.011202074s
Jan 18 23:11:58.753: INFO: Pod "pod-subpath-test-secret-hgzf": Phase="Running", Reason="", readiness=false. Elapsed: 22.014539558s
Jan 18 23:12:00.752: INFO: Pod "pod-subpath-test-secret-hgzf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.014148996s
STEP: Saw pod success 01/18/23 23:12:00.752
Jan 18 23:12:00.753: INFO: Pod "pod-subpath-test-secret-hgzf" satisfied condition "Succeeded or Failed"
Jan 18 23:12:00.759: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-subpath-test-secret-hgzf container test-container-subpath-secret-hgzf: <nil>
STEP: delete the pod 01/18/23 23:12:00.782
Jan 18 23:12:00.819: INFO: Waiting for pod pod-subpath-test-secret-hgzf to disappear
Jan 18 23:12:00.824: INFO: Pod pod-subpath-test-secret-hgzf no longer exists
STEP: Deleting pod pod-subpath-test-secret-hgzf 01/18/23 23:12:00.825
Jan 18 23:12:00.825: INFO: Deleting pod "pod-subpath-test-secret-hgzf" in namespace "subpath-7949"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 18 23:12:00.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7949" for this suite. 01/18/23 23:12:00.838
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":360,"skipped":6628,"failed":0}
------------------------------
• [SLOW TEST] [24.220 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:11:36.634
    Jan 18 23:11:36.634: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename subpath 01/18/23 23:11:36.637
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:11:36.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:11:36.692
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/18/23 23:11:36.699
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-hgzf 01/18/23 23:11:36.725
    STEP: Creating a pod to test atomic-volume-subpath 01/18/23 23:11:36.725
    Jan 18 23:11:36.738: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-hgzf" in namespace "subpath-7949" to be "Succeeded or Failed"
    Jan 18 23:11:36.743: INFO: Pod "pod-subpath-test-secret-hgzf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.555972ms
    Jan 18 23:11:38.749: INFO: Pod "pod-subpath-test-secret-hgzf": Phase="Running", Reason="", readiness=true. Elapsed: 2.010725262s
    Jan 18 23:11:40.748: INFO: Pod "pod-subpath-test-secret-hgzf": Phase="Running", Reason="", readiness=true. Elapsed: 4.010441694s
    Jan 18 23:11:42.749: INFO: Pod "pod-subpath-test-secret-hgzf": Phase="Running", Reason="", readiness=true. Elapsed: 6.010764707s
    Jan 18 23:11:44.748: INFO: Pod "pod-subpath-test-secret-hgzf": Phase="Running", Reason="", readiness=true. Elapsed: 8.009971142s
    Jan 18 23:11:46.748: INFO: Pod "pod-subpath-test-secret-hgzf": Phase="Running", Reason="", readiness=true. Elapsed: 10.010451073s
    Jan 18 23:11:48.749: INFO: Pod "pod-subpath-test-secret-hgzf": Phase="Running", Reason="", readiness=true. Elapsed: 12.011473769s
    Jan 18 23:11:50.747: INFO: Pod "pod-subpath-test-secret-hgzf": Phase="Running", Reason="", readiness=true. Elapsed: 14.009313811s
    Jan 18 23:11:52.749: INFO: Pod "pod-subpath-test-secret-hgzf": Phase="Running", Reason="", readiness=true. Elapsed: 16.011288319s
    Jan 18 23:11:54.756: INFO: Pod "pod-subpath-test-secret-hgzf": Phase="Running", Reason="", readiness=true. Elapsed: 18.017853986s
    Jan 18 23:11:56.749: INFO: Pod "pod-subpath-test-secret-hgzf": Phase="Running", Reason="", readiness=true. Elapsed: 20.011202074s
    Jan 18 23:11:58.753: INFO: Pod "pod-subpath-test-secret-hgzf": Phase="Running", Reason="", readiness=false. Elapsed: 22.014539558s
    Jan 18 23:12:00.752: INFO: Pod "pod-subpath-test-secret-hgzf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.014148996s
    STEP: Saw pod success 01/18/23 23:12:00.752
    Jan 18 23:12:00.753: INFO: Pod "pod-subpath-test-secret-hgzf" satisfied condition "Succeeded or Failed"
    Jan 18 23:12:00.759: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-subpath-test-secret-hgzf container test-container-subpath-secret-hgzf: <nil>
    STEP: delete the pod 01/18/23 23:12:00.782
    Jan 18 23:12:00.819: INFO: Waiting for pod pod-subpath-test-secret-hgzf to disappear
    Jan 18 23:12:00.824: INFO: Pod pod-subpath-test-secret-hgzf no longer exists
    STEP: Deleting pod pod-subpath-test-secret-hgzf 01/18/23 23:12:00.825
    Jan 18 23:12:00.825: INFO: Deleting pod "pod-subpath-test-secret-hgzf" in namespace "subpath-7949"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 18 23:12:00.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7949" for this suite. 01/18/23 23:12:00.838
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:12:00.86
Jan 18 23:12:00.861: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename projected 01/18/23 23:12:00.862
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:12:00.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:12:00.891
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 01/18/23 23:12:00.897
Jan 18 23:12:00.907: INFO: Waiting up to 5m0s for pod "labelsupdate3093cd5e-157c-433f-945d-ef1d30cd3ba5" in namespace "projected-6454" to be "running and ready"
Jan 18 23:12:00.915: INFO: Pod "labelsupdate3093cd5e-157c-433f-945d-ef1d30cd3ba5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.451197ms
Jan 18 23:12:00.915: INFO: The phase of Pod labelsupdate3093cd5e-157c-433f-945d-ef1d30cd3ba5 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:12:02.919: INFO: Pod "labelsupdate3093cd5e-157c-433f-945d-ef1d30cd3ba5": Phase="Running", Reason="", readiness=true. Elapsed: 2.012376457s
Jan 18 23:12:02.919: INFO: The phase of Pod labelsupdate3093cd5e-157c-433f-945d-ef1d30cd3ba5 is Running (Ready = true)
Jan 18 23:12:02.920: INFO: Pod "labelsupdate3093cd5e-157c-433f-945d-ef1d30cd3ba5" satisfied condition "running and ready"
Jan 18 23:12:03.447: INFO: Successfully updated pod "labelsupdate3093cd5e-157c-433f-945d-ef1d30cd3ba5"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 23:12:07.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6454" for this suite. 01/18/23 23:12:07.485
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":361,"skipped":6665,"failed":0}
------------------------------
• [SLOW TEST] [6.633 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:12:00.86
    Jan 18 23:12:00.861: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename projected 01/18/23 23:12:00.862
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:12:00.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:12:00.891
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 01/18/23 23:12:00.897
    Jan 18 23:12:00.907: INFO: Waiting up to 5m0s for pod "labelsupdate3093cd5e-157c-433f-945d-ef1d30cd3ba5" in namespace "projected-6454" to be "running and ready"
    Jan 18 23:12:00.915: INFO: Pod "labelsupdate3093cd5e-157c-433f-945d-ef1d30cd3ba5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.451197ms
    Jan 18 23:12:00.915: INFO: The phase of Pod labelsupdate3093cd5e-157c-433f-945d-ef1d30cd3ba5 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:12:02.919: INFO: Pod "labelsupdate3093cd5e-157c-433f-945d-ef1d30cd3ba5": Phase="Running", Reason="", readiness=true. Elapsed: 2.012376457s
    Jan 18 23:12:02.919: INFO: The phase of Pod labelsupdate3093cd5e-157c-433f-945d-ef1d30cd3ba5 is Running (Ready = true)
    Jan 18 23:12:02.920: INFO: Pod "labelsupdate3093cd5e-157c-433f-945d-ef1d30cd3ba5" satisfied condition "running and ready"
    Jan 18 23:12:03.447: INFO: Successfully updated pod "labelsupdate3093cd5e-157c-433f-945d-ef1d30cd3ba5"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 23:12:07.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6454" for this suite. 01/18/23 23:12:07.485
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 23:12:07.496
Jan 18 23:12:07.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
STEP: Building a namespace api object, basename pods 01/18/23 23:12:07.498
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:12:07.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:12:07.527
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 01/18/23 23:12:07.532
Jan 18 23:12:07.541: INFO: Waiting up to 5m0s for pod "pod-hostip-4cc926f6-d5fb-44f7-9cb2-e382f8a8b96d" in namespace "pods-39" to be "running and ready"
Jan 18 23:12:07.547: INFO: Pod "pod-hostip-4cc926f6-d5fb-44f7-9cb2-e382f8a8b96d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.471513ms
Jan 18 23:12:07.547: INFO: The phase of Pod pod-hostip-4cc926f6-d5fb-44f7-9cb2-e382f8a8b96d is Pending, waiting for it to be Running (with Ready = true)
Jan 18 23:12:09.554: INFO: Pod "pod-hostip-4cc926f6-d5fb-44f7-9cb2-e382f8a8b96d": Phase="Running", Reason="", readiness=true. Elapsed: 2.012506591s
Jan 18 23:12:09.554: INFO: The phase of Pod pod-hostip-4cc926f6-d5fb-44f7-9cb2-e382f8a8b96d is Running (Ready = true)
Jan 18 23:12:09.554: INFO: Pod "pod-hostip-4cc926f6-d5fb-44f7-9cb2-e382f8a8b96d" satisfied condition "running and ready"
Jan 18 23:12:09.561: INFO: Pod pod-hostip-4cc926f6-d5fb-44f7-9cb2-e382f8a8b96d has hostIP: 192.168.101.216
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 23:12:09.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-39" for this suite. 01/18/23 23:12:09.572
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":362,"skipped":6680,"failed":0}
------------------------------
• [2.084 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 23:12:07.496
    Jan 18 23:12:07.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3033012221
    STEP: Building a namespace api object, basename pods 01/18/23 23:12:07.498
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 23:12:07.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 23:12:07.527
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 01/18/23 23:12:07.532
    Jan 18 23:12:07.541: INFO: Waiting up to 5m0s for pod "pod-hostip-4cc926f6-d5fb-44f7-9cb2-e382f8a8b96d" in namespace "pods-39" to be "running and ready"
    Jan 18 23:12:07.547: INFO: Pod "pod-hostip-4cc926f6-d5fb-44f7-9cb2-e382f8a8b96d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.471513ms
    Jan 18 23:12:07.547: INFO: The phase of Pod pod-hostip-4cc926f6-d5fb-44f7-9cb2-e382f8a8b96d is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 23:12:09.554: INFO: Pod "pod-hostip-4cc926f6-d5fb-44f7-9cb2-e382f8a8b96d": Phase="Running", Reason="", readiness=true. Elapsed: 2.012506591s
    Jan 18 23:12:09.554: INFO: The phase of Pod pod-hostip-4cc926f6-d5fb-44f7-9cb2-e382f8a8b96d is Running (Ready = true)
    Jan 18 23:12:09.554: INFO: Pod "pod-hostip-4cc926f6-d5fb-44f7-9cb2-e382f8a8b96d" satisfied condition "running and ready"
    Jan 18 23:12:09.561: INFO: Pod pod-hostip-4cc926f6-d5fb-44f7-9cb2-e382f8a8b96d has hostIP: 192.168.101.216
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 23:12:09.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-39" for this suite. 01/18/23 23:12:09.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6705,"failed":0}
Jan 18 23:12:09.600: INFO: Running AfterSuite actions on all nodes
Jan 18 23:12:09.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Jan 18 23:12:09.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Jan 18 23:12:09.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Jan 18 23:12:09.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jan 18 23:12:09.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jan 18 23:12:09.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jan 18 23:12:09.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Jan 18 23:12:09.600: INFO: Running AfterSuite actions on node 1
Jan 18 23:12:09.600: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.001 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jan 18 23:12:09.600: INFO: Running AfterSuite actions on all nodes
    Jan 18 23:12:09.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Jan 18 23:12:09.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Jan 18 23:12:09.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Jan 18 23:12:09.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Jan 18 23:12:09.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Jan 18 23:12:09.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Jan 18 23:12:09.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jan 18 23:12:09.600: INFO: Running AfterSuite actions on node 1
    Jan 18 23:12:09.600: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.118 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7067 Specs in 6157.518 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6705 Skipped
PASS

Ginkgo ran 1 suite in 1h42m38.166958785s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

