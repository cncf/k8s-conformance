I0118 15:06:13.289489      19 e2e.go:116] Starting e2e run "2f2ef4af-b7a6-4619-9e15-db1bd869d321" on Ginkgo node 1
Jan 18 15:06:13.312: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1674054373 - will randomize all specs

Will run 360 of 7067 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Jan 18 15:06:13.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 15:06:13.489: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 18 15:06:13.517: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 18 15:06:13.560: INFO: 24 / 24 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 18 15:06:13.560: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
Jan 18 15:06:13.560: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 18 15:06:13.570: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jan 18 15:06:13.570: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jan 18 15:06:13.570: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'nodelocaldns' (0 seconds elapsed)
Jan 18 15:06:13.570: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'openstack-cinder-csi-nodeplugin' (0 seconds elapsed)
Jan 18 15:06:13.570: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (0 seconds elapsed)
Jan 18 15:06:13.570: INFO: e2e test version: v1.25.4
Jan 18 15:06:13.572: INFO: kube-apiserver version: v1.25.4
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Jan 18 15:06:13.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 15:06:13.579: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.093 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jan 18 15:06:13.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 15:06:13.489: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Jan 18 15:06:13.517: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jan 18 15:06:13.560: INFO: 24 / 24 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jan 18 15:06:13.560: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
    Jan 18 15:06:13.560: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jan 18 15:06:13.570: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Jan 18 15:06:13.570: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Jan 18 15:06:13.570: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'nodelocaldns' (0 seconds elapsed)
    Jan 18 15:06:13.570: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'openstack-cinder-csi-nodeplugin' (0 seconds elapsed)
    Jan 18 15:06:13.570: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (0 seconds elapsed)
    Jan 18 15:06:13.570: INFO: e2e test version: v1.25.4
    Jan 18 15:06:13.572: INFO: kube-apiserver version: v1.25.4
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jan 18 15:06:13.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 15:06:13.579: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:06:13.631
Jan 18 15:06:13.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename podtemplate 01/18/23 15:06:13.632
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:06:13.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:06:13.66
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 01/18/23 15:06:13.668
Jan 18 15:06:13.675: INFO: created test-podtemplate-1
Jan 18 15:06:13.684: INFO: created test-podtemplate-2
Jan 18 15:06:13.690: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 01/18/23 15:06:13.69
STEP: delete collection of pod templates 01/18/23 15:06:13.696
Jan 18 15:06:13.696: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 01/18/23 15:06:13.741
Jan 18 15:06:13.742: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan 18 15:06:13.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7061" for this suite. 01/18/23 15:06:13.761
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":1,"skipped":14,"failed":0}
------------------------------
• [0.140 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:06:13.631
    Jan 18 15:06:13.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename podtemplate 01/18/23 15:06:13.632
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:06:13.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:06:13.66
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 01/18/23 15:06:13.668
    Jan 18 15:06:13.675: INFO: created test-podtemplate-1
    Jan 18 15:06:13.684: INFO: created test-podtemplate-2
    Jan 18 15:06:13.690: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 01/18/23 15:06:13.69
    STEP: delete collection of pod templates 01/18/23 15:06:13.696
    Jan 18 15:06:13.696: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 01/18/23 15:06:13.741
    Jan 18 15:06:13.742: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan 18 15:06:13.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-7061" for this suite. 01/18/23 15:06:13.761
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:06:13.774
Jan 18 15:06:13.775: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename daemonsets 01/18/23 15:06:13.776
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:06:13.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:06:13.803
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Jan 18 15:06:13.873: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 15:06:13.881
Jan 18 15:06:13.896: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:13.899: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 15:06:13.899: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 15:06:14.913: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:14.920: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 15:06:14.920: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 15:06:15.906: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:15.913: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 15:06:15.914: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 15:06:16.909: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:16.916: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 15:06:16.916: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 15:06:17.907: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:17.913: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 15:06:17.913: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 15:06:18.907: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:18.918: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 15:06:18.918: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 15:06:19.905: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:19.911: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 15:06:19.911: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 15:06:20.904: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:20.909: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 15:06:20.909: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 15:06:21.907: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:21.912: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 15:06:21.913: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 15:06:22.905: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:22.909: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 15:06:22.909: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 01/18/23 15:06:22.927
STEP: Check that daemon pods images are updated. 01/18/23 15:06:22.952
Jan 18 15:06:22.957: INFO: Wrong image for pod: daemon-set-4ntj9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 15:06:22.957: INFO: Wrong image for pod: daemon-set-dcngt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 15:06:22.966: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:23.974: INFO: Wrong image for pod: daemon-set-dcngt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 15:06:23.979: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:24.973: INFO: Wrong image for pod: daemon-set-dcngt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 15:06:24.978: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:25.974: INFO: Wrong image for pod: daemon-set-dcngt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 15:06:25.981: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:26.974: INFO: Wrong image for pod: daemon-set-dcngt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 15:06:26.974: INFO: Pod daemon-set-nlqtc is not available
Jan 18 15:06:26.983: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:27.976: INFO: Wrong image for pod: daemon-set-dcngt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 15:06:27.976: INFO: Pod daemon-set-nlqtc is not available
Jan 18 15:06:27.982: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:28.975: INFO: Wrong image for pod: daemon-set-dcngt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 15:06:28.975: INFO: Pod daemon-set-nlqtc is not available
Jan 18 15:06:28.981: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:29.975: INFO: Wrong image for pod: daemon-set-dcngt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 15:06:29.975: INFO: Pod daemon-set-nlqtc is not available
Jan 18 15:06:29.982: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:30.972: INFO: Wrong image for pod: daemon-set-dcngt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 15:06:30.972: INFO: Pod daemon-set-nlqtc is not available
Jan 18 15:06:30.977: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:31.976: INFO: Wrong image for pod: daemon-set-dcngt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 15:06:31.976: INFO: Pod daemon-set-nlqtc is not available
Jan 18 15:06:31.981: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:32.973: INFO: Wrong image for pod: daemon-set-dcngt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 15:06:32.973: INFO: Pod daemon-set-nlqtc is not available
Jan 18 15:06:32.982: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:33.980: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:34.977: INFO: Pod daemon-set-mnhx6 is not available
Jan 18 15:06:34.986: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 01/18/23 15:06:34.986
Jan 18 15:06:34.991: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:34.995: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 15:06:34.995: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
Jan 18 15:06:36.002: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:36.007: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 15:06:36.007: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
Jan 18 15:06:37.002: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:37.009: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 15:06:37.010: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
Jan 18 15:06:38.002: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:38.008: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 15:06:38.008: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
Jan 18 15:06:39.000: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:39.004: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 15:06:39.004: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
Jan 18 15:06:40.021: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:40.035: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 15:06:40.035: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
Jan 18 15:06:41.001: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:06:41.006: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 15:06:41.006: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/18/23 15:06:41.028
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7807, will wait for the garbage collector to delete the pods 01/18/23 15:06:41.029
Jan 18 15:06:41.094: INFO: Deleting DaemonSet.extensions daemon-set took: 10.472134ms
Jan 18 15:06:41.196: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.717657ms
Jan 18 15:06:43.901: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 15:06:43.902: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 15:06:43.907: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24842"},"items":null}

Jan 18 15:06:43.912: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24842"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 18 15:06:43.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7807" for this suite. 01/18/23 15:06:43.942
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":2,"skipped":40,"failed":0}
------------------------------
• [SLOW TEST] [30.175 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:06:13.774
    Jan 18 15:06:13.775: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename daemonsets 01/18/23 15:06:13.776
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:06:13.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:06:13.803
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Jan 18 15:06:13.873: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 15:06:13.881
    Jan 18 15:06:13.896: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:13.899: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 15:06:13.899: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 15:06:14.913: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:14.920: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 15:06:14.920: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 15:06:15.906: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:15.913: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 15:06:15.914: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 15:06:16.909: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:16.916: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 15:06:16.916: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 15:06:17.907: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:17.913: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 15:06:17.913: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 15:06:18.907: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:18.918: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 15:06:18.918: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 15:06:19.905: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:19.911: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 15:06:19.911: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 15:06:20.904: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:20.909: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 15:06:20.909: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 15:06:21.907: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:21.912: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 15:06:21.913: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 15:06:22.905: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:22.909: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 15:06:22.909: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 01/18/23 15:06:22.927
    STEP: Check that daemon pods images are updated. 01/18/23 15:06:22.952
    Jan 18 15:06:22.957: INFO: Wrong image for pod: daemon-set-4ntj9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 18 15:06:22.957: INFO: Wrong image for pod: daemon-set-dcngt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 18 15:06:22.966: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:23.974: INFO: Wrong image for pod: daemon-set-dcngt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 18 15:06:23.979: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:24.973: INFO: Wrong image for pod: daemon-set-dcngt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 18 15:06:24.978: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:25.974: INFO: Wrong image for pod: daemon-set-dcngt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 18 15:06:25.981: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:26.974: INFO: Wrong image for pod: daemon-set-dcngt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 18 15:06:26.974: INFO: Pod daemon-set-nlqtc is not available
    Jan 18 15:06:26.983: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:27.976: INFO: Wrong image for pod: daemon-set-dcngt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 18 15:06:27.976: INFO: Pod daemon-set-nlqtc is not available
    Jan 18 15:06:27.982: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:28.975: INFO: Wrong image for pod: daemon-set-dcngt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 18 15:06:28.975: INFO: Pod daemon-set-nlqtc is not available
    Jan 18 15:06:28.981: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:29.975: INFO: Wrong image for pod: daemon-set-dcngt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 18 15:06:29.975: INFO: Pod daemon-set-nlqtc is not available
    Jan 18 15:06:29.982: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:30.972: INFO: Wrong image for pod: daemon-set-dcngt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 18 15:06:30.972: INFO: Pod daemon-set-nlqtc is not available
    Jan 18 15:06:30.977: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:31.976: INFO: Wrong image for pod: daemon-set-dcngt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 18 15:06:31.976: INFO: Pod daemon-set-nlqtc is not available
    Jan 18 15:06:31.981: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:32.973: INFO: Wrong image for pod: daemon-set-dcngt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 18 15:06:32.973: INFO: Pod daemon-set-nlqtc is not available
    Jan 18 15:06:32.982: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:33.980: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:34.977: INFO: Pod daemon-set-mnhx6 is not available
    Jan 18 15:06:34.986: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 01/18/23 15:06:34.986
    Jan 18 15:06:34.991: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:34.995: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 15:06:34.995: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
    Jan 18 15:06:36.002: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:36.007: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 15:06:36.007: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
    Jan 18 15:06:37.002: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:37.009: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 15:06:37.010: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
    Jan 18 15:06:38.002: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:38.008: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 15:06:38.008: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
    Jan 18 15:06:39.000: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:39.004: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 15:06:39.004: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
    Jan 18 15:06:40.021: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:40.035: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 15:06:40.035: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
    Jan 18 15:06:41.001: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:06:41.006: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 15:06:41.006: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 15:06:41.028
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7807, will wait for the garbage collector to delete the pods 01/18/23 15:06:41.029
    Jan 18 15:06:41.094: INFO: Deleting DaemonSet.extensions daemon-set took: 10.472134ms
    Jan 18 15:06:41.196: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.717657ms
    Jan 18 15:06:43.901: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 15:06:43.902: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 15:06:43.907: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24842"},"items":null}

    Jan 18 15:06:43.912: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24842"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 15:06:43.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7807" for this suite. 01/18/23 15:06:43.942
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:06:43.953
Jan 18 15:06:43.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename webhook 01/18/23 15:06:43.955
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:06:43.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:06:43.995
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 15:06:44.039
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 15:06:44.717
STEP: Deploying the webhook pod 01/18/23 15:06:44.728
STEP: Wait for the deployment to be ready 01/18/23 15:06:44.741
Jan 18 15:06:44.765: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 18 15:06:46.807: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 6, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 6, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 6, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 6, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/18/23 15:06:48.813
STEP: Verifying the service has paired with the endpoint 01/18/23 15:06:48.823
Jan 18 15:06:49.824: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Jan 18 15:06:49.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/18/23 15:06:50.349
STEP: Creating a custom resource that should be denied by the webhook 01/18/23 15:06:50.372
STEP: Creating a custom resource whose deletion would be denied by the webhook 01/18/23 15:06:52.429
STEP: Updating the custom resource with disallowed data should be denied 01/18/23 15:06:52.439
STEP: Deleting the custom resource should be denied 01/18/23 15:06:52.452
STEP: Remove the offending key and value from the custom resource data 01/18/23 15:06:52.461
STEP: Deleting the updated custom resource should be successful 01/18/23 15:06:52.476
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 15:06:53.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1125" for this suite. 01/18/23 15:06:53.016
STEP: Destroying namespace "webhook-1125-markers" for this suite. 01/18/23 15:06:53.022
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":3,"skipped":63,"failed":0}
------------------------------
• [SLOW TEST] [9.159 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:06:43.953
    Jan 18 15:06:43.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename webhook 01/18/23 15:06:43.955
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:06:43.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:06:43.995
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 15:06:44.039
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 15:06:44.717
    STEP: Deploying the webhook pod 01/18/23 15:06:44.728
    STEP: Wait for the deployment to be ready 01/18/23 15:06:44.741
    Jan 18 15:06:44.765: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 18 15:06:46.807: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 6, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 6, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 6, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 6, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/18/23 15:06:48.813
    STEP: Verifying the service has paired with the endpoint 01/18/23 15:06:48.823
    Jan 18 15:06:49.824: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Jan 18 15:06:49.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/18/23 15:06:50.349
    STEP: Creating a custom resource that should be denied by the webhook 01/18/23 15:06:50.372
    STEP: Creating a custom resource whose deletion would be denied by the webhook 01/18/23 15:06:52.429
    STEP: Updating the custom resource with disallowed data should be denied 01/18/23 15:06:52.439
    STEP: Deleting the custom resource should be denied 01/18/23 15:06:52.452
    STEP: Remove the offending key and value from the custom resource data 01/18/23 15:06:52.461
    STEP: Deleting the updated custom resource should be successful 01/18/23 15:06:52.476
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 15:06:53.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1125" for this suite. 01/18/23 15:06:53.016
    STEP: Destroying namespace "webhook-1125-markers" for this suite. 01/18/23 15:06:53.022
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:06:53.126
Jan 18 15:06:53.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename certificates 01/18/23 15:06:53.129
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:06:53.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:06:53.185
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 01/18/23 15:06:54.359
STEP: getting /apis/certificates.k8s.io 01/18/23 15:06:54.363
STEP: getting /apis/certificates.k8s.io/v1 01/18/23 15:06:54.365
STEP: creating 01/18/23 15:06:54.366
STEP: getting 01/18/23 15:06:54.383
STEP: listing 01/18/23 15:06:54.388
STEP: watching 01/18/23 15:06:54.392
Jan 18 15:06:54.393: INFO: starting watch
STEP: patching 01/18/23 15:06:54.395
STEP: updating 01/18/23 15:06:54.402
Jan 18 15:06:54.414: INFO: waiting for watch events with expected annotations
Jan 18 15:06:54.414: INFO: saw patched and updated annotations
STEP: getting /approval 01/18/23 15:06:54.414
STEP: patching /approval 01/18/23 15:06:54.418
STEP: updating /approval 01/18/23 15:06:54.425
STEP: getting /status 01/18/23 15:06:54.432
STEP: patching /status 01/18/23 15:06:54.436
STEP: updating /status 01/18/23 15:06:54.443
STEP: deleting 01/18/23 15:06:54.453
STEP: deleting a collection 01/18/23 15:06:54.465
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 15:06:54.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-7978" for this suite. 01/18/23 15:06:54.487
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":4,"skipped":97,"failed":0}
------------------------------
• [1.372 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:06:53.126
    Jan 18 15:06:53.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename certificates 01/18/23 15:06:53.129
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:06:53.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:06:53.185
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 01/18/23 15:06:54.359
    STEP: getting /apis/certificates.k8s.io 01/18/23 15:06:54.363
    STEP: getting /apis/certificates.k8s.io/v1 01/18/23 15:06:54.365
    STEP: creating 01/18/23 15:06:54.366
    STEP: getting 01/18/23 15:06:54.383
    STEP: listing 01/18/23 15:06:54.388
    STEP: watching 01/18/23 15:06:54.392
    Jan 18 15:06:54.393: INFO: starting watch
    STEP: patching 01/18/23 15:06:54.395
    STEP: updating 01/18/23 15:06:54.402
    Jan 18 15:06:54.414: INFO: waiting for watch events with expected annotations
    Jan 18 15:06:54.414: INFO: saw patched and updated annotations
    STEP: getting /approval 01/18/23 15:06:54.414
    STEP: patching /approval 01/18/23 15:06:54.418
    STEP: updating /approval 01/18/23 15:06:54.425
    STEP: getting /status 01/18/23 15:06:54.432
    STEP: patching /status 01/18/23 15:06:54.436
    STEP: updating /status 01/18/23 15:06:54.443
    STEP: deleting 01/18/23 15:06:54.453
    STEP: deleting a collection 01/18/23 15:06:54.465
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 15:06:54.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-7978" for this suite. 01/18/23 15:06:54.487
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:06:54.5
Jan 18 15:06:54.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename ingress 01/18/23 15:06:54.502
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:06:54.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:06:54.556
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 01/18/23 15:06:54.562
STEP: getting /apis/networking.k8s.io 01/18/23 15:06:54.572
STEP: getting /apis/networking.k8s.iov1 01/18/23 15:06:54.578
STEP: creating 01/18/23 15:06:54.581
STEP: getting 01/18/23 15:06:54.601
STEP: listing 01/18/23 15:06:54.606
STEP: watching 01/18/23 15:06:54.611
Jan 18 15:06:54.611: INFO: starting watch
STEP: cluster-wide listing 01/18/23 15:06:54.613
STEP: cluster-wide watching 01/18/23 15:06:54.617
Jan 18 15:06:54.617: INFO: starting watch
STEP: patching 01/18/23 15:06:54.619
STEP: updating 01/18/23 15:06:54.631
Jan 18 15:06:54.644: INFO: waiting for watch events with expected annotations
Jan 18 15:06:54.644: INFO: saw patched and updated annotations
STEP: patching /status 01/18/23 15:06:54.644
STEP: updating /status 01/18/23 15:06:54.655
STEP: get /status 01/18/23 15:06:54.667
STEP: deleting 01/18/23 15:06:54.672
STEP: deleting a collection 01/18/23 15:06:54.69
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Jan 18 15:06:54.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-2723" for this suite. 01/18/23 15:06:54.716
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":5,"skipped":99,"failed":0}
------------------------------
• [0.227 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:06:54.5
    Jan 18 15:06:54.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename ingress 01/18/23 15:06:54.502
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:06:54.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:06:54.556
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 01/18/23 15:06:54.562
    STEP: getting /apis/networking.k8s.io 01/18/23 15:06:54.572
    STEP: getting /apis/networking.k8s.iov1 01/18/23 15:06:54.578
    STEP: creating 01/18/23 15:06:54.581
    STEP: getting 01/18/23 15:06:54.601
    STEP: listing 01/18/23 15:06:54.606
    STEP: watching 01/18/23 15:06:54.611
    Jan 18 15:06:54.611: INFO: starting watch
    STEP: cluster-wide listing 01/18/23 15:06:54.613
    STEP: cluster-wide watching 01/18/23 15:06:54.617
    Jan 18 15:06:54.617: INFO: starting watch
    STEP: patching 01/18/23 15:06:54.619
    STEP: updating 01/18/23 15:06:54.631
    Jan 18 15:06:54.644: INFO: waiting for watch events with expected annotations
    Jan 18 15:06:54.644: INFO: saw patched and updated annotations
    STEP: patching /status 01/18/23 15:06:54.644
    STEP: updating /status 01/18/23 15:06:54.655
    STEP: get /status 01/18/23 15:06:54.667
    STEP: deleting 01/18/23 15:06:54.672
    STEP: deleting a collection 01/18/23 15:06:54.69
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Jan 18 15:06:54.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-2723" for this suite. 01/18/23 15:06:54.716
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:06:54.727
Jan 18 15:06:54.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename gc 01/18/23 15:06:54.729
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:06:54.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:06:54.753
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 01/18/23 15:06:54.767
STEP: delete the rc 01/18/23 15:06:59.85
STEP: wait for the rc to be deleted 01/18/23 15:06:59.901
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/18/23 15:07:04.908
STEP: Gathering metrics 01/18/23 15:07:34.935
Jan 18 15:07:34.978: INFO: Waiting up to 5m0s for pod "kube-controller-manager-v1-25-1-18760-m" in namespace "kube-system" to be "running and ready"
Jan 18 15:07:34.985: INFO: Pod "kube-controller-manager-v1-25-1-18760-m": Phase="Running", Reason="", readiness=true. Elapsed: 6.836459ms
Jan 18 15:07:34.985: INFO: The phase of Pod kube-controller-manager-v1-25-1-18760-m is Running (Ready = true)
Jan 18 15:07:34.986: INFO: Pod "kube-controller-manager-v1-25-1-18760-m" satisfied condition "running and ready"
Jan 18 15:07:35.106: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 18 15:07:35.106: INFO: Deleting pod "simpletest.rc-2fqlm" in namespace "gc-5783"
Jan 18 15:07:35.120: INFO: Deleting pod "simpletest.rc-2htxn" in namespace "gc-5783"
Jan 18 15:07:35.139: INFO: Deleting pod "simpletest.rc-2z58z" in namespace "gc-5783"
Jan 18 15:07:35.160: INFO: Deleting pod "simpletest.rc-49tz6" in namespace "gc-5783"
Jan 18 15:07:35.176: INFO: Deleting pod "simpletest.rc-4v8hs" in namespace "gc-5783"
Jan 18 15:07:35.189: INFO: Deleting pod "simpletest.rc-4zthc" in namespace "gc-5783"
Jan 18 15:07:35.204: INFO: Deleting pod "simpletest.rc-5c9vz" in namespace "gc-5783"
Jan 18 15:07:35.220: INFO: Deleting pod "simpletest.rc-5f7z9" in namespace "gc-5783"
Jan 18 15:07:35.242: INFO: Deleting pod "simpletest.rc-5rpkb" in namespace "gc-5783"
Jan 18 15:07:35.264: INFO: Deleting pod "simpletest.rc-66dnv" in namespace "gc-5783"
Jan 18 15:07:35.307: INFO: Deleting pod "simpletest.rc-6cnmr" in namespace "gc-5783"
Jan 18 15:07:35.340: INFO: Deleting pod "simpletest.rc-6f2vz" in namespace "gc-5783"
Jan 18 15:07:35.363: INFO: Deleting pod "simpletest.rc-6l22t" in namespace "gc-5783"
Jan 18 15:07:35.402: INFO: Deleting pod "simpletest.rc-6nz9g" in namespace "gc-5783"
Jan 18 15:07:35.425: INFO: Deleting pod "simpletest.rc-6x5b5" in namespace "gc-5783"
Jan 18 15:07:35.438: INFO: Deleting pod "simpletest.rc-7l8tk" in namespace "gc-5783"
Jan 18 15:07:35.471: INFO: Deleting pod "simpletest.rc-8k4j5" in namespace "gc-5783"
Jan 18 15:07:35.503: INFO: Deleting pod "simpletest.rc-8sl7t" in namespace "gc-5783"
Jan 18 15:07:35.534: INFO: Deleting pod "simpletest.rc-96pj9" in namespace "gc-5783"
Jan 18 15:07:35.553: INFO: Deleting pod "simpletest.rc-99w8k" in namespace "gc-5783"
Jan 18 15:07:35.571: INFO: Deleting pod "simpletest.rc-9dgh7" in namespace "gc-5783"
Jan 18 15:07:35.606: INFO: Deleting pod "simpletest.rc-9ftbl" in namespace "gc-5783"
Jan 18 15:07:35.621: INFO: Deleting pod "simpletest.rc-9m4s2" in namespace "gc-5783"
Jan 18 15:07:35.640: INFO: Deleting pod "simpletest.rc-9pc74" in namespace "gc-5783"
Jan 18 15:07:35.670: INFO: Deleting pod "simpletest.rc-9vm5s" in namespace "gc-5783"
Jan 18 15:07:35.693: INFO: Deleting pod "simpletest.rc-b24nj" in namespace "gc-5783"
Jan 18 15:07:35.730: INFO: Deleting pod "simpletest.rc-b8xfd" in namespace "gc-5783"
Jan 18 15:07:35.745: INFO: Deleting pod "simpletest.rc-bflt5" in namespace "gc-5783"
Jan 18 15:07:35.767: INFO: Deleting pod "simpletest.rc-bm2ng" in namespace "gc-5783"
Jan 18 15:07:35.782: INFO: Deleting pod "simpletest.rc-bq9gl" in namespace "gc-5783"
Jan 18 15:07:35.823: INFO: Deleting pod "simpletest.rc-ch7j9" in namespace "gc-5783"
Jan 18 15:07:35.846: INFO: Deleting pod "simpletest.rc-d8pmm" in namespace "gc-5783"
Jan 18 15:07:35.864: INFO: Deleting pod "simpletest.rc-dnnds" in namespace "gc-5783"
Jan 18 15:07:35.889: INFO: Deleting pod "simpletest.rc-dw7b8" in namespace "gc-5783"
Jan 18 15:07:35.908: INFO: Deleting pod "simpletest.rc-dwfhb" in namespace "gc-5783"
Jan 18 15:07:35.933: INFO: Deleting pod "simpletest.rc-dxlk4" in namespace "gc-5783"
Jan 18 15:07:35.950: INFO: Deleting pod "simpletest.rc-g5klh" in namespace "gc-5783"
Jan 18 15:07:35.969: INFO: Deleting pod "simpletest.rc-gb8cd" in namespace "gc-5783"
Jan 18 15:07:35.982: INFO: Deleting pod "simpletest.rc-ggkpt" in namespace "gc-5783"
Jan 18 15:07:36.007: INFO: Deleting pod "simpletest.rc-ghmd7" in namespace "gc-5783"
Jan 18 15:07:36.021: INFO: Deleting pod "simpletest.rc-gscl5" in namespace "gc-5783"
Jan 18 15:07:36.042: INFO: Deleting pod "simpletest.rc-h5z7h" in namespace "gc-5783"
Jan 18 15:07:36.053: INFO: Deleting pod "simpletest.rc-hhtm4" in namespace "gc-5783"
Jan 18 15:07:36.069: INFO: Deleting pod "simpletest.rc-hqpql" in namespace "gc-5783"
Jan 18 15:07:36.083: INFO: Deleting pod "simpletest.rc-hzk7q" in namespace "gc-5783"
Jan 18 15:07:36.096: INFO: Deleting pod "simpletest.rc-jc255" in namespace "gc-5783"
Jan 18 15:07:36.110: INFO: Deleting pod "simpletest.rc-jcrms" in namespace "gc-5783"
Jan 18 15:07:36.130: INFO: Deleting pod "simpletest.rc-jvxgj" in namespace "gc-5783"
Jan 18 15:07:36.155: INFO: Deleting pod "simpletest.rc-k74qc" in namespace "gc-5783"
Jan 18 15:07:36.167: INFO: Deleting pod "simpletest.rc-kb8nb" in namespace "gc-5783"
Jan 18 15:07:36.185: INFO: Deleting pod "simpletest.rc-kd8wd" in namespace "gc-5783"
Jan 18 15:07:36.228: INFO: Deleting pod "simpletest.rc-kl47t" in namespace "gc-5783"
Jan 18 15:07:36.238: INFO: Deleting pod "simpletest.rc-kq9ks" in namespace "gc-5783"
Jan 18 15:07:36.254: INFO: Deleting pod "simpletest.rc-kr8lq" in namespace "gc-5783"
Jan 18 15:07:36.271: INFO: Deleting pod "simpletest.rc-lfltv" in namespace "gc-5783"
Jan 18 15:07:36.283: INFO: Deleting pod "simpletest.rc-lm4rn" in namespace "gc-5783"
Jan 18 15:07:36.304: INFO: Deleting pod "simpletest.rc-lsl7d" in namespace "gc-5783"
Jan 18 15:07:36.348: INFO: Deleting pod "simpletest.rc-m7pzx" in namespace "gc-5783"
Jan 18 15:07:36.383: INFO: Deleting pod "simpletest.rc-mh648" in namespace "gc-5783"
Jan 18 15:07:36.428: INFO: Deleting pod "simpletest.rc-n9nsb" in namespace "gc-5783"
Jan 18 15:07:36.520: INFO: Deleting pod "simpletest.rc-nbwx9" in namespace "gc-5783"
Jan 18 15:07:36.542: INFO: Deleting pod "simpletest.rc-ng97c" in namespace "gc-5783"
Jan 18 15:07:36.562: INFO: Deleting pod "simpletest.rc-nnhq8" in namespace "gc-5783"
Jan 18 15:07:36.587: INFO: Deleting pod "simpletest.rc-nxqgw" in namespace "gc-5783"
Jan 18 15:07:36.601: INFO: Deleting pod "simpletest.rc-p45m5" in namespace "gc-5783"
Jan 18 15:07:36.648: INFO: Deleting pod "simpletest.rc-pbtlm" in namespace "gc-5783"
Jan 18 15:07:36.671: INFO: Deleting pod "simpletest.rc-pc2h6" in namespace "gc-5783"
Jan 18 15:07:36.687: INFO: Deleting pod "simpletest.rc-pcjgz" in namespace "gc-5783"
Jan 18 15:07:36.719: INFO: Deleting pod "simpletest.rc-phjks" in namespace "gc-5783"
Jan 18 15:07:36.731: INFO: Deleting pod "simpletest.rc-q9mps" in namespace "gc-5783"
Jan 18 15:07:36.753: INFO: Deleting pod "simpletest.rc-qkx5c" in namespace "gc-5783"
Jan 18 15:07:36.835: INFO: Deleting pod "simpletest.rc-qngnl" in namespace "gc-5783"
Jan 18 15:07:36.853: INFO: Deleting pod "simpletest.rc-qpjz2" in namespace "gc-5783"
Jan 18 15:07:36.873: INFO: Deleting pod "simpletest.rc-qzrqp" in namespace "gc-5783"
Jan 18 15:07:36.893: INFO: Deleting pod "simpletest.rc-r2z66" in namespace "gc-5783"
Jan 18 15:07:36.926: INFO: Deleting pod "simpletest.rc-r58r6" in namespace "gc-5783"
Jan 18 15:07:36.941: INFO: Deleting pod "simpletest.rc-rv85m" in namespace "gc-5783"
Jan 18 15:07:36.956: INFO: Deleting pod "simpletest.rc-s4flv" in namespace "gc-5783"
Jan 18 15:07:36.969: INFO: Deleting pod "simpletest.rc-s6rzk" in namespace "gc-5783"
Jan 18 15:07:36.987: INFO: Deleting pod "simpletest.rc-s9vjd" in namespace "gc-5783"
Jan 18 15:07:37.006: INFO: Deleting pod "simpletest.rc-sb45s" in namespace "gc-5783"
Jan 18 15:07:37.067: INFO: Deleting pod "simpletest.rc-sc75l" in namespace "gc-5783"
Jan 18 15:07:37.155: INFO: Deleting pod "simpletest.rc-sfvzx" in namespace "gc-5783"
Jan 18 15:07:37.182: INFO: Deleting pod "simpletest.rc-stnz4" in namespace "gc-5783"
Jan 18 15:07:37.206: INFO: Deleting pod "simpletest.rc-t2nrk" in namespace "gc-5783"
Jan 18 15:07:37.237: INFO: Deleting pod "simpletest.rc-tjxzw" in namespace "gc-5783"
Jan 18 15:07:37.275: INFO: Deleting pod "simpletest.rc-txgrq" in namespace "gc-5783"
Jan 18 15:07:37.335: INFO: Deleting pod "simpletest.rc-v8qvh" in namespace "gc-5783"
Jan 18 15:07:37.366: INFO: Deleting pod "simpletest.rc-vfld7" in namespace "gc-5783"
Jan 18 15:07:37.426: INFO: Deleting pod "simpletest.rc-vpqlh" in namespace "gc-5783"
Jan 18 15:07:37.442: INFO: Deleting pod "simpletest.rc-vtcvd" in namespace "gc-5783"
Jan 18 15:07:37.455: INFO: Deleting pod "simpletest.rc-vv2dp" in namespace "gc-5783"
Jan 18 15:07:37.470: INFO: Deleting pod "simpletest.rc-w8lvz" in namespace "gc-5783"
Jan 18 15:07:37.486: INFO: Deleting pod "simpletest.rc-w9w4g" in namespace "gc-5783"
Jan 18 15:07:37.562: INFO: Deleting pod "simpletest.rc-x4b4v" in namespace "gc-5783"
Jan 18 15:07:37.580: INFO: Deleting pod "simpletest.rc-xb7qt" in namespace "gc-5783"
Jan 18 15:07:37.597: INFO: Deleting pod "simpletest.rc-xgqtc" in namespace "gc-5783"
Jan 18 15:07:37.619: INFO: Deleting pod "simpletest.rc-xwh5c" in namespace "gc-5783"
Jan 18 15:07:37.641: INFO: Deleting pod "simpletest.rc-zdt8l" in namespace "gc-5783"
Jan 18 15:07:37.664: INFO: Deleting pod "simpletest.rc-ztg5x" in namespace "gc-5783"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 18 15:07:37.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5783" for this suite. 01/18/23 15:07:37.701
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":6,"skipped":103,"failed":0}
------------------------------
• [SLOW TEST] [43.000 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:06:54.727
    Jan 18 15:06:54.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename gc 01/18/23 15:06:54.729
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:06:54.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:06:54.753
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 01/18/23 15:06:54.767
    STEP: delete the rc 01/18/23 15:06:59.85
    STEP: wait for the rc to be deleted 01/18/23 15:06:59.901
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/18/23 15:07:04.908
    STEP: Gathering metrics 01/18/23 15:07:34.935
    Jan 18 15:07:34.978: INFO: Waiting up to 5m0s for pod "kube-controller-manager-v1-25-1-18760-m" in namespace "kube-system" to be "running and ready"
    Jan 18 15:07:34.985: INFO: Pod "kube-controller-manager-v1-25-1-18760-m": Phase="Running", Reason="", readiness=true. Elapsed: 6.836459ms
    Jan 18 15:07:34.985: INFO: The phase of Pod kube-controller-manager-v1-25-1-18760-m is Running (Ready = true)
    Jan 18 15:07:34.986: INFO: Pod "kube-controller-manager-v1-25-1-18760-m" satisfied condition "running and ready"
    Jan 18 15:07:35.106: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 18 15:07:35.106: INFO: Deleting pod "simpletest.rc-2fqlm" in namespace "gc-5783"
    Jan 18 15:07:35.120: INFO: Deleting pod "simpletest.rc-2htxn" in namespace "gc-5783"
    Jan 18 15:07:35.139: INFO: Deleting pod "simpletest.rc-2z58z" in namespace "gc-5783"
    Jan 18 15:07:35.160: INFO: Deleting pod "simpletest.rc-49tz6" in namespace "gc-5783"
    Jan 18 15:07:35.176: INFO: Deleting pod "simpletest.rc-4v8hs" in namespace "gc-5783"
    Jan 18 15:07:35.189: INFO: Deleting pod "simpletest.rc-4zthc" in namespace "gc-5783"
    Jan 18 15:07:35.204: INFO: Deleting pod "simpletest.rc-5c9vz" in namespace "gc-5783"
    Jan 18 15:07:35.220: INFO: Deleting pod "simpletest.rc-5f7z9" in namespace "gc-5783"
    Jan 18 15:07:35.242: INFO: Deleting pod "simpletest.rc-5rpkb" in namespace "gc-5783"
    Jan 18 15:07:35.264: INFO: Deleting pod "simpletest.rc-66dnv" in namespace "gc-5783"
    Jan 18 15:07:35.307: INFO: Deleting pod "simpletest.rc-6cnmr" in namespace "gc-5783"
    Jan 18 15:07:35.340: INFO: Deleting pod "simpletest.rc-6f2vz" in namespace "gc-5783"
    Jan 18 15:07:35.363: INFO: Deleting pod "simpletest.rc-6l22t" in namespace "gc-5783"
    Jan 18 15:07:35.402: INFO: Deleting pod "simpletest.rc-6nz9g" in namespace "gc-5783"
    Jan 18 15:07:35.425: INFO: Deleting pod "simpletest.rc-6x5b5" in namespace "gc-5783"
    Jan 18 15:07:35.438: INFO: Deleting pod "simpletest.rc-7l8tk" in namespace "gc-5783"
    Jan 18 15:07:35.471: INFO: Deleting pod "simpletest.rc-8k4j5" in namespace "gc-5783"
    Jan 18 15:07:35.503: INFO: Deleting pod "simpletest.rc-8sl7t" in namespace "gc-5783"
    Jan 18 15:07:35.534: INFO: Deleting pod "simpletest.rc-96pj9" in namespace "gc-5783"
    Jan 18 15:07:35.553: INFO: Deleting pod "simpletest.rc-99w8k" in namespace "gc-5783"
    Jan 18 15:07:35.571: INFO: Deleting pod "simpletest.rc-9dgh7" in namespace "gc-5783"
    Jan 18 15:07:35.606: INFO: Deleting pod "simpletest.rc-9ftbl" in namespace "gc-5783"
    Jan 18 15:07:35.621: INFO: Deleting pod "simpletest.rc-9m4s2" in namespace "gc-5783"
    Jan 18 15:07:35.640: INFO: Deleting pod "simpletest.rc-9pc74" in namespace "gc-5783"
    Jan 18 15:07:35.670: INFO: Deleting pod "simpletest.rc-9vm5s" in namespace "gc-5783"
    Jan 18 15:07:35.693: INFO: Deleting pod "simpletest.rc-b24nj" in namespace "gc-5783"
    Jan 18 15:07:35.730: INFO: Deleting pod "simpletest.rc-b8xfd" in namespace "gc-5783"
    Jan 18 15:07:35.745: INFO: Deleting pod "simpletest.rc-bflt5" in namespace "gc-5783"
    Jan 18 15:07:35.767: INFO: Deleting pod "simpletest.rc-bm2ng" in namespace "gc-5783"
    Jan 18 15:07:35.782: INFO: Deleting pod "simpletest.rc-bq9gl" in namespace "gc-5783"
    Jan 18 15:07:35.823: INFO: Deleting pod "simpletest.rc-ch7j9" in namespace "gc-5783"
    Jan 18 15:07:35.846: INFO: Deleting pod "simpletest.rc-d8pmm" in namespace "gc-5783"
    Jan 18 15:07:35.864: INFO: Deleting pod "simpletest.rc-dnnds" in namespace "gc-5783"
    Jan 18 15:07:35.889: INFO: Deleting pod "simpletest.rc-dw7b8" in namespace "gc-5783"
    Jan 18 15:07:35.908: INFO: Deleting pod "simpletest.rc-dwfhb" in namespace "gc-5783"
    Jan 18 15:07:35.933: INFO: Deleting pod "simpletest.rc-dxlk4" in namespace "gc-5783"
    Jan 18 15:07:35.950: INFO: Deleting pod "simpletest.rc-g5klh" in namespace "gc-5783"
    Jan 18 15:07:35.969: INFO: Deleting pod "simpletest.rc-gb8cd" in namespace "gc-5783"
    Jan 18 15:07:35.982: INFO: Deleting pod "simpletest.rc-ggkpt" in namespace "gc-5783"
    Jan 18 15:07:36.007: INFO: Deleting pod "simpletest.rc-ghmd7" in namespace "gc-5783"
    Jan 18 15:07:36.021: INFO: Deleting pod "simpletest.rc-gscl5" in namespace "gc-5783"
    Jan 18 15:07:36.042: INFO: Deleting pod "simpletest.rc-h5z7h" in namespace "gc-5783"
    Jan 18 15:07:36.053: INFO: Deleting pod "simpletest.rc-hhtm4" in namespace "gc-5783"
    Jan 18 15:07:36.069: INFO: Deleting pod "simpletest.rc-hqpql" in namespace "gc-5783"
    Jan 18 15:07:36.083: INFO: Deleting pod "simpletest.rc-hzk7q" in namespace "gc-5783"
    Jan 18 15:07:36.096: INFO: Deleting pod "simpletest.rc-jc255" in namespace "gc-5783"
    Jan 18 15:07:36.110: INFO: Deleting pod "simpletest.rc-jcrms" in namespace "gc-5783"
    Jan 18 15:07:36.130: INFO: Deleting pod "simpletest.rc-jvxgj" in namespace "gc-5783"
    Jan 18 15:07:36.155: INFO: Deleting pod "simpletest.rc-k74qc" in namespace "gc-5783"
    Jan 18 15:07:36.167: INFO: Deleting pod "simpletest.rc-kb8nb" in namespace "gc-5783"
    Jan 18 15:07:36.185: INFO: Deleting pod "simpletest.rc-kd8wd" in namespace "gc-5783"
    Jan 18 15:07:36.228: INFO: Deleting pod "simpletest.rc-kl47t" in namespace "gc-5783"
    Jan 18 15:07:36.238: INFO: Deleting pod "simpletest.rc-kq9ks" in namespace "gc-5783"
    Jan 18 15:07:36.254: INFO: Deleting pod "simpletest.rc-kr8lq" in namespace "gc-5783"
    Jan 18 15:07:36.271: INFO: Deleting pod "simpletest.rc-lfltv" in namespace "gc-5783"
    Jan 18 15:07:36.283: INFO: Deleting pod "simpletest.rc-lm4rn" in namespace "gc-5783"
    Jan 18 15:07:36.304: INFO: Deleting pod "simpletest.rc-lsl7d" in namespace "gc-5783"
    Jan 18 15:07:36.348: INFO: Deleting pod "simpletest.rc-m7pzx" in namespace "gc-5783"
    Jan 18 15:07:36.383: INFO: Deleting pod "simpletest.rc-mh648" in namespace "gc-5783"
    Jan 18 15:07:36.428: INFO: Deleting pod "simpletest.rc-n9nsb" in namespace "gc-5783"
    Jan 18 15:07:36.520: INFO: Deleting pod "simpletest.rc-nbwx9" in namespace "gc-5783"
    Jan 18 15:07:36.542: INFO: Deleting pod "simpletest.rc-ng97c" in namespace "gc-5783"
    Jan 18 15:07:36.562: INFO: Deleting pod "simpletest.rc-nnhq8" in namespace "gc-5783"
    Jan 18 15:07:36.587: INFO: Deleting pod "simpletest.rc-nxqgw" in namespace "gc-5783"
    Jan 18 15:07:36.601: INFO: Deleting pod "simpletest.rc-p45m5" in namespace "gc-5783"
    Jan 18 15:07:36.648: INFO: Deleting pod "simpletest.rc-pbtlm" in namespace "gc-5783"
    Jan 18 15:07:36.671: INFO: Deleting pod "simpletest.rc-pc2h6" in namespace "gc-5783"
    Jan 18 15:07:36.687: INFO: Deleting pod "simpletest.rc-pcjgz" in namespace "gc-5783"
    Jan 18 15:07:36.719: INFO: Deleting pod "simpletest.rc-phjks" in namespace "gc-5783"
    Jan 18 15:07:36.731: INFO: Deleting pod "simpletest.rc-q9mps" in namespace "gc-5783"
    Jan 18 15:07:36.753: INFO: Deleting pod "simpletest.rc-qkx5c" in namespace "gc-5783"
    Jan 18 15:07:36.835: INFO: Deleting pod "simpletest.rc-qngnl" in namespace "gc-5783"
    Jan 18 15:07:36.853: INFO: Deleting pod "simpletest.rc-qpjz2" in namespace "gc-5783"
    Jan 18 15:07:36.873: INFO: Deleting pod "simpletest.rc-qzrqp" in namespace "gc-5783"
    Jan 18 15:07:36.893: INFO: Deleting pod "simpletest.rc-r2z66" in namespace "gc-5783"
    Jan 18 15:07:36.926: INFO: Deleting pod "simpletest.rc-r58r6" in namespace "gc-5783"
    Jan 18 15:07:36.941: INFO: Deleting pod "simpletest.rc-rv85m" in namespace "gc-5783"
    Jan 18 15:07:36.956: INFO: Deleting pod "simpletest.rc-s4flv" in namespace "gc-5783"
    Jan 18 15:07:36.969: INFO: Deleting pod "simpletest.rc-s6rzk" in namespace "gc-5783"
    Jan 18 15:07:36.987: INFO: Deleting pod "simpletest.rc-s9vjd" in namespace "gc-5783"
    Jan 18 15:07:37.006: INFO: Deleting pod "simpletest.rc-sb45s" in namespace "gc-5783"
    Jan 18 15:07:37.067: INFO: Deleting pod "simpletest.rc-sc75l" in namespace "gc-5783"
    Jan 18 15:07:37.155: INFO: Deleting pod "simpletest.rc-sfvzx" in namespace "gc-5783"
    Jan 18 15:07:37.182: INFO: Deleting pod "simpletest.rc-stnz4" in namespace "gc-5783"
    Jan 18 15:07:37.206: INFO: Deleting pod "simpletest.rc-t2nrk" in namespace "gc-5783"
    Jan 18 15:07:37.237: INFO: Deleting pod "simpletest.rc-tjxzw" in namespace "gc-5783"
    Jan 18 15:07:37.275: INFO: Deleting pod "simpletest.rc-txgrq" in namespace "gc-5783"
    Jan 18 15:07:37.335: INFO: Deleting pod "simpletest.rc-v8qvh" in namespace "gc-5783"
    Jan 18 15:07:37.366: INFO: Deleting pod "simpletest.rc-vfld7" in namespace "gc-5783"
    Jan 18 15:07:37.426: INFO: Deleting pod "simpletest.rc-vpqlh" in namespace "gc-5783"
    Jan 18 15:07:37.442: INFO: Deleting pod "simpletest.rc-vtcvd" in namespace "gc-5783"
    Jan 18 15:07:37.455: INFO: Deleting pod "simpletest.rc-vv2dp" in namespace "gc-5783"
    Jan 18 15:07:37.470: INFO: Deleting pod "simpletest.rc-w8lvz" in namespace "gc-5783"
    Jan 18 15:07:37.486: INFO: Deleting pod "simpletest.rc-w9w4g" in namespace "gc-5783"
    Jan 18 15:07:37.562: INFO: Deleting pod "simpletest.rc-x4b4v" in namespace "gc-5783"
    Jan 18 15:07:37.580: INFO: Deleting pod "simpletest.rc-xb7qt" in namespace "gc-5783"
    Jan 18 15:07:37.597: INFO: Deleting pod "simpletest.rc-xgqtc" in namespace "gc-5783"
    Jan 18 15:07:37.619: INFO: Deleting pod "simpletest.rc-xwh5c" in namespace "gc-5783"
    Jan 18 15:07:37.641: INFO: Deleting pod "simpletest.rc-zdt8l" in namespace "gc-5783"
    Jan 18 15:07:37.664: INFO: Deleting pod "simpletest.rc-ztg5x" in namespace "gc-5783"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 18 15:07:37.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5783" for this suite. 01/18/23 15:07:37.701
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:07:37.731
Jan 18 15:07:37.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename var-expansion 01/18/23 15:07:37.739
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:07:37.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:07:37.774
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 01/18/23 15:07:37.779
Jan 18 15:07:37.789: INFO: Waiting up to 5m0s for pod "var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d" in namespace "var-expansion-9930" to be "Succeeded or Failed"
Jan 18 15:07:37.797: INFO: Pod "var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.93857ms
Jan 18 15:07:39.802: INFO: Pod "var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012810615s
Jan 18 15:07:41.807: INFO: Pod "var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018131634s
Jan 18 15:07:43.805: INFO: Pod "var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016094098s
Jan 18 15:07:45.813: INFO: Pod "var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023624041s
Jan 18 15:07:47.814: INFO: Pod "var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.025333873s
Jan 18 15:07:49.821: INFO: Pod "var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.031795516s
Jan 18 15:07:51.802: INFO: Pod "var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.012632708s
Jan 18 15:07:53.803: INFO: Pod "var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.01403294s
Jan 18 15:07:55.804: INFO: Pod "var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d": Phase="Pending", Reason="", readiness=false. Elapsed: 18.014797146s
Jan 18 15:07:57.803: INFO: Pod "var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 20.01427441s
STEP: Saw pod success 01/18/23 15:07:57.803
Jan 18 15:07:57.804: INFO: Pod "var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d" satisfied condition "Succeeded or Failed"
Jan 18 15:07:57.808: INFO: Trying to get logs from node v1-25-1-18760-w2 pod var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d container dapi-container: <nil>
STEP: delete the pod 01/18/23 15:07:57.832
Jan 18 15:07:57.847: INFO: Waiting for pod var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d to disappear
Jan 18 15:07:57.851: INFO: Pod var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 18 15:07:57.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9930" for this suite. 01/18/23 15:07:57.856
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":7,"skipped":125,"failed":0}
------------------------------
• [SLOW TEST] [20.134 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:07:37.731
    Jan 18 15:07:37.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename var-expansion 01/18/23 15:07:37.739
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:07:37.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:07:37.774
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 01/18/23 15:07:37.779
    Jan 18 15:07:37.789: INFO: Waiting up to 5m0s for pod "var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d" in namespace "var-expansion-9930" to be "Succeeded or Failed"
    Jan 18 15:07:37.797: INFO: Pod "var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.93857ms
    Jan 18 15:07:39.802: INFO: Pod "var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012810615s
    Jan 18 15:07:41.807: INFO: Pod "var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018131634s
    Jan 18 15:07:43.805: INFO: Pod "var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016094098s
    Jan 18 15:07:45.813: INFO: Pod "var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023624041s
    Jan 18 15:07:47.814: INFO: Pod "var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.025333873s
    Jan 18 15:07:49.821: INFO: Pod "var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.031795516s
    Jan 18 15:07:51.802: INFO: Pod "var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.012632708s
    Jan 18 15:07:53.803: INFO: Pod "var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.01403294s
    Jan 18 15:07:55.804: INFO: Pod "var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d": Phase="Pending", Reason="", readiness=false. Elapsed: 18.014797146s
    Jan 18 15:07:57.803: INFO: Pod "var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 20.01427441s
    STEP: Saw pod success 01/18/23 15:07:57.803
    Jan 18 15:07:57.804: INFO: Pod "var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d" satisfied condition "Succeeded or Failed"
    Jan 18 15:07:57.808: INFO: Trying to get logs from node v1-25-1-18760-w2 pod var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d container dapi-container: <nil>
    STEP: delete the pod 01/18/23 15:07:57.832
    Jan 18 15:07:57.847: INFO: Waiting for pod var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d to disappear
    Jan 18 15:07:57.851: INFO: Pod var-expansion-7dd44309-3096-4485-9f5f-cf9df6e1e80d no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 18 15:07:57.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-9930" for this suite. 01/18/23 15:07:57.856
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:07:57.874
Jan 18 15:07:57.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename services 01/18/23 15:07:57.876
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:07:57.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:07:57.898
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 15:07:57.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2233" for this suite. 01/18/23 15:07:57.912
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":8,"skipped":161,"failed":0}
------------------------------
• [0.046 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:07:57.874
    Jan 18 15:07:57.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename services 01/18/23 15:07:57.876
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:07:57.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:07:57.898
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 15:07:57.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2233" for this suite. 01/18/23 15:07:57.912
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:07:57.92
Jan 18 15:07:57.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename configmap 01/18/23 15:07:57.922
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:07:57.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:07:57.997
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-6288/configmap-test-bdba2e70-36b6-4096-a333-58bf9c3f2fd5 01/18/23 15:07:58.002
STEP: Creating a pod to test consume configMaps 01/18/23 15:07:58.014
Jan 18 15:07:58.026: INFO: Waiting up to 5m0s for pod "pod-configmaps-97240a52-751b-4817-9f52-b56af329c2bd" in namespace "configmap-6288" to be "Succeeded or Failed"
Jan 18 15:07:58.031: INFO: Pod "pod-configmaps-97240a52-751b-4817-9f52-b56af329c2bd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.546881ms
Jan 18 15:08:00.038: INFO: Pod "pod-configmaps-97240a52-751b-4817-9f52-b56af329c2bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011300079s
Jan 18 15:08:02.038: INFO: Pod "pod-configmaps-97240a52-751b-4817-9f52-b56af329c2bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011308396s
STEP: Saw pod success 01/18/23 15:08:02.038
Jan 18 15:08:02.039: INFO: Pod "pod-configmaps-97240a52-751b-4817-9f52-b56af329c2bd" satisfied condition "Succeeded or Failed"
Jan 18 15:08:02.043: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-97240a52-751b-4817-9f52-b56af329c2bd container env-test: <nil>
STEP: delete the pod 01/18/23 15:08:02.051
Jan 18 15:08:02.071: INFO: Waiting for pod pod-configmaps-97240a52-751b-4817-9f52-b56af329c2bd to disappear
Jan 18 15:08:02.074: INFO: Pod pod-configmaps-97240a52-751b-4817-9f52-b56af329c2bd no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 15:08:02.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6288" for this suite. 01/18/23 15:08:02.086
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":9,"skipped":162,"failed":0}
------------------------------
• [4.178 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:07:57.92
    Jan 18 15:07:57.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename configmap 01/18/23 15:07:57.922
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:07:57.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:07:57.997
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-6288/configmap-test-bdba2e70-36b6-4096-a333-58bf9c3f2fd5 01/18/23 15:07:58.002
    STEP: Creating a pod to test consume configMaps 01/18/23 15:07:58.014
    Jan 18 15:07:58.026: INFO: Waiting up to 5m0s for pod "pod-configmaps-97240a52-751b-4817-9f52-b56af329c2bd" in namespace "configmap-6288" to be "Succeeded or Failed"
    Jan 18 15:07:58.031: INFO: Pod "pod-configmaps-97240a52-751b-4817-9f52-b56af329c2bd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.546881ms
    Jan 18 15:08:00.038: INFO: Pod "pod-configmaps-97240a52-751b-4817-9f52-b56af329c2bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011300079s
    Jan 18 15:08:02.038: INFO: Pod "pod-configmaps-97240a52-751b-4817-9f52-b56af329c2bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011308396s
    STEP: Saw pod success 01/18/23 15:08:02.038
    Jan 18 15:08:02.039: INFO: Pod "pod-configmaps-97240a52-751b-4817-9f52-b56af329c2bd" satisfied condition "Succeeded or Failed"
    Jan 18 15:08:02.043: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-97240a52-751b-4817-9f52-b56af329c2bd container env-test: <nil>
    STEP: delete the pod 01/18/23 15:08:02.051
    Jan 18 15:08:02.071: INFO: Waiting for pod pod-configmaps-97240a52-751b-4817-9f52-b56af329c2bd to disappear
    Jan 18 15:08:02.074: INFO: Pod pod-configmaps-97240a52-751b-4817-9f52-b56af329c2bd no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 15:08:02.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6288" for this suite. 01/18/23 15:08:02.086
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:08:02.115
Jan 18 15:08:02.115: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename secrets 01/18/23 15:08:02.12
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:08:02.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:08:02.15
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-7b8135de-339e-4ab3-855e-9a4477e5b4e0 01/18/23 15:08:02.155
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 18 15:08:02.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7845" for this suite. 01/18/23 15:08:02.166
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":10,"skipped":234,"failed":0}
------------------------------
• [0.061 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:08:02.115
    Jan 18 15:08:02.115: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename secrets 01/18/23 15:08:02.12
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:08:02.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:08:02.15
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-7b8135de-339e-4ab3-855e-9a4477e5b4e0 01/18/23 15:08:02.155
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 15:08:02.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7845" for this suite. 01/18/23 15:08:02.166
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:08:02.18
Jan 18 15:08:02.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename webhook 01/18/23 15:08:02.182
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:08:02.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:08:02.216
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 15:08:02.233
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 15:08:03.325
STEP: Deploying the webhook pod 01/18/23 15:08:03.34
STEP: Wait for the deployment to be ready 01/18/23 15:08:03.353
Jan 18 15:08:03.370: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/18/23 15:08:05.383
STEP: Verifying the service has paired with the endpoint 01/18/23 15:08:05.394
Jan 18 15:08:06.395: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 01/18/23 15:08:06.402
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/18/23 15:08:06.404
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/18/23 15:08:06.404
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/18/23 15:08:06.405
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/18/23 15:08:06.407
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/18/23 15:08:06.407
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/18/23 15:08:06.412
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 15:08:06.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3921" for this suite. 01/18/23 15:08:06.419
STEP: Destroying namespace "webhook-3921-markers" for this suite. 01/18/23 15:08:06.429
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":11,"skipped":245,"failed":0}
------------------------------
• [4.321 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:08:02.18
    Jan 18 15:08:02.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename webhook 01/18/23 15:08:02.182
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:08:02.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:08:02.216
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 15:08:02.233
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 15:08:03.325
    STEP: Deploying the webhook pod 01/18/23 15:08:03.34
    STEP: Wait for the deployment to be ready 01/18/23 15:08:03.353
    Jan 18 15:08:03.370: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/18/23 15:08:05.383
    STEP: Verifying the service has paired with the endpoint 01/18/23 15:08:05.394
    Jan 18 15:08:06.395: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 01/18/23 15:08:06.402
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/18/23 15:08:06.404
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/18/23 15:08:06.404
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/18/23 15:08:06.405
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/18/23 15:08:06.407
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/18/23 15:08:06.407
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/18/23 15:08:06.412
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 15:08:06.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3921" for this suite. 01/18/23 15:08:06.419
    STEP: Destroying namespace "webhook-3921-markers" for this suite. 01/18/23 15:08:06.429
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:08:06.508
Jan 18 15:08:06.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 15:08:06.512
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:08:06.58
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:08:06.586
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/18/23 15:08:06.613
Jan 18 15:08:06.626: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7917" to be "running and ready"
Jan 18 15:08:06.633: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.684215ms
Jan 18 15:08:06.633: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:08:08.638: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.01200699s
Jan 18 15:08:08.638: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 18 15:08:08.638: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 01/18/23 15:08:08.648
Jan 18 15:08:08.661: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-7917" to be "running and ready"
Jan 18 15:08:08.667: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.081246ms
Jan 18 15:08:08.667: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:08:10.672: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.010894516s
Jan 18 15:08:10.673: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jan 18 15:08:10.673: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/18/23 15:08:10.677
Jan 18 15:08:10.687: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 18 15:08:10.692: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 18 15:08:12.694: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 18 15:08:12.699: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 18 15:08:14.694: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 18 15:08:14.698: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 01/18/23 15:08:14.699
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 18 15:08:14.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7917" for this suite. 01/18/23 15:08:14.715
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":12,"skipped":259,"failed":0}
------------------------------
• [SLOW TEST] [8.217 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:08:06.508
    Jan 18 15:08:06.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 15:08:06.512
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:08:06.58
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:08:06.586
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/18/23 15:08:06.613
    Jan 18 15:08:06.626: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7917" to be "running and ready"
    Jan 18 15:08:06.633: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.684215ms
    Jan 18 15:08:06.633: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:08:08.638: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.01200699s
    Jan 18 15:08:08.638: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 18 15:08:08.638: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 01/18/23 15:08:08.648
    Jan 18 15:08:08.661: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-7917" to be "running and ready"
    Jan 18 15:08:08.667: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.081246ms
    Jan 18 15:08:08.667: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:08:10.672: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.010894516s
    Jan 18 15:08:10.673: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jan 18 15:08:10.673: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/18/23 15:08:10.677
    Jan 18 15:08:10.687: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 18 15:08:10.692: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 18 15:08:12.694: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 18 15:08:12.699: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 18 15:08:14.694: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 18 15:08:14.698: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 01/18/23 15:08:14.699
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 18 15:08:14.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-7917" for this suite. 01/18/23 15:08:14.715
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:08:14.726
Jan 18 15:08:14.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename svcaccounts 01/18/23 15:08:14.729
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:08:14.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:08:14.763
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Jan 18 15:08:14.780: INFO: Got root ca configmap in namespace "svcaccounts-1187"
Jan 18 15:08:14.786: INFO: Deleted root ca configmap in namespace "svcaccounts-1187"
STEP: waiting for a new root ca configmap created 01/18/23 15:08:15.287
Jan 18 15:08:15.294: INFO: Recreated root ca configmap in namespace "svcaccounts-1187"
Jan 18 15:08:15.302: INFO: Updated root ca configmap in namespace "svcaccounts-1187"
STEP: waiting for the root ca configmap reconciled 01/18/23 15:08:15.803
Jan 18 15:08:15.808: INFO: Reconciled root ca configmap in namespace "svcaccounts-1187"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 18 15:08:15.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1187" for this suite. 01/18/23 15:08:15.815
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":13,"skipped":266,"failed":0}
------------------------------
• [1.099 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:08:14.726
    Jan 18 15:08:14.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 15:08:14.729
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:08:14.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:08:14.763
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Jan 18 15:08:14.780: INFO: Got root ca configmap in namespace "svcaccounts-1187"
    Jan 18 15:08:14.786: INFO: Deleted root ca configmap in namespace "svcaccounts-1187"
    STEP: waiting for a new root ca configmap created 01/18/23 15:08:15.287
    Jan 18 15:08:15.294: INFO: Recreated root ca configmap in namespace "svcaccounts-1187"
    Jan 18 15:08:15.302: INFO: Updated root ca configmap in namespace "svcaccounts-1187"
    STEP: waiting for the root ca configmap reconciled 01/18/23 15:08:15.803
    Jan 18 15:08:15.808: INFO: Reconciled root ca configmap in namespace "svcaccounts-1187"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 18 15:08:15.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1187" for this suite. 01/18/23 15:08:15.815
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:08:15.827
Jan 18 15:08:15.827: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename downward-api 01/18/23 15:08:15.828
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:08:15.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:08:15.854
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 01/18/23 15:08:15.859
Jan 18 15:08:15.869: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bae2c8e7-47c4-4280-883b-4ef8ca9b8057" in namespace "downward-api-2109" to be "Succeeded or Failed"
Jan 18 15:08:15.873: INFO: Pod "downwardapi-volume-bae2c8e7-47c4-4280-883b-4ef8ca9b8057": Phase="Pending", Reason="", readiness=false. Elapsed: 4.081965ms
Jan 18 15:08:17.880: INFO: Pod "downwardapi-volume-bae2c8e7-47c4-4280-883b-4ef8ca9b8057": Phase="Running", Reason="", readiness=true. Elapsed: 2.010893463s
Jan 18 15:08:19.989: INFO: Pod "downwardapi-volume-bae2c8e7-47c4-4280-883b-4ef8ca9b8057": Phase="Running", Reason="", readiness=false. Elapsed: 4.119482786s
Jan 18 15:08:21.878: INFO: Pod "downwardapi-volume-bae2c8e7-47c4-4280-883b-4ef8ca9b8057": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008746412s
STEP: Saw pod success 01/18/23 15:08:21.878
Jan 18 15:08:21.878: INFO: Pod "downwardapi-volume-bae2c8e7-47c4-4280-883b-4ef8ca9b8057" satisfied condition "Succeeded or Failed"
Jan 18 15:08:21.884: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-bae2c8e7-47c4-4280-883b-4ef8ca9b8057 container client-container: <nil>
STEP: delete the pod 01/18/23 15:08:21.894
Jan 18 15:08:21.909: INFO: Waiting for pod downwardapi-volume-bae2c8e7-47c4-4280-883b-4ef8ca9b8057 to disappear
Jan 18 15:08:21.915: INFO: Pod downwardapi-volume-bae2c8e7-47c4-4280-883b-4ef8ca9b8057 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 15:08:21.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2109" for this suite. 01/18/23 15:08:21.919
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":14,"skipped":279,"failed":0}
------------------------------
• [SLOW TEST] [6.099 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:08:15.827
    Jan 18 15:08:15.827: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename downward-api 01/18/23 15:08:15.828
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:08:15.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:08:15.854
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 01/18/23 15:08:15.859
    Jan 18 15:08:15.869: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bae2c8e7-47c4-4280-883b-4ef8ca9b8057" in namespace "downward-api-2109" to be "Succeeded or Failed"
    Jan 18 15:08:15.873: INFO: Pod "downwardapi-volume-bae2c8e7-47c4-4280-883b-4ef8ca9b8057": Phase="Pending", Reason="", readiness=false. Elapsed: 4.081965ms
    Jan 18 15:08:17.880: INFO: Pod "downwardapi-volume-bae2c8e7-47c4-4280-883b-4ef8ca9b8057": Phase="Running", Reason="", readiness=true. Elapsed: 2.010893463s
    Jan 18 15:08:19.989: INFO: Pod "downwardapi-volume-bae2c8e7-47c4-4280-883b-4ef8ca9b8057": Phase="Running", Reason="", readiness=false. Elapsed: 4.119482786s
    Jan 18 15:08:21.878: INFO: Pod "downwardapi-volume-bae2c8e7-47c4-4280-883b-4ef8ca9b8057": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008746412s
    STEP: Saw pod success 01/18/23 15:08:21.878
    Jan 18 15:08:21.878: INFO: Pod "downwardapi-volume-bae2c8e7-47c4-4280-883b-4ef8ca9b8057" satisfied condition "Succeeded or Failed"
    Jan 18 15:08:21.884: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-bae2c8e7-47c4-4280-883b-4ef8ca9b8057 container client-container: <nil>
    STEP: delete the pod 01/18/23 15:08:21.894
    Jan 18 15:08:21.909: INFO: Waiting for pod downwardapi-volume-bae2c8e7-47c4-4280-883b-4ef8ca9b8057 to disappear
    Jan 18 15:08:21.915: INFO: Pod downwardapi-volume-bae2c8e7-47c4-4280-883b-4ef8ca9b8057 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 15:08:21.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2109" for this suite. 01/18/23 15:08:21.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:08:21.939
Jan 18 15:08:21.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename svc-latency 01/18/23 15:08:21.94
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:08:21.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:08:21.971
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jan 18 15:08:21.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1591 01/18/23 15:08:21.978
I0118 15:08:21.986594      19 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1591, replica count: 1
I0118 15:08:23.037771      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 15:08:24.038274      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 15:08:25.038519      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 15:08:26.039020      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 15:08:26.150: INFO: Created: latency-svc-g7pnv
Jan 18 15:08:26.168: INFO: Got endpoints: latency-svc-g7pnv [29.729639ms]
Jan 18 15:08:26.193: INFO: Created: latency-svc-4qjxc
Jan 18 15:08:26.428: INFO: Got endpoints: latency-svc-4qjxc [259.47152ms]
Jan 18 15:08:26.433: INFO: Created: latency-svc-9hpxt
Jan 18 15:08:26.457: INFO: Got endpoints: latency-svc-9hpxt [287.151707ms]
Jan 18 15:08:26.466: INFO: Created: latency-svc-76qkb
Jan 18 15:08:26.472: INFO: Got endpoints: latency-svc-76qkb [302.390664ms]
Jan 18 15:08:26.478: INFO: Created: latency-svc-4cm52
Jan 18 15:08:26.493: INFO: Got endpoints: latency-svc-4cm52 [323.573252ms]
Jan 18 15:08:26.625: INFO: Created: latency-svc-7bxvr
Jan 18 15:08:26.626: INFO: Created: latency-svc-zgkq2
Jan 18 15:08:26.626: INFO: Created: latency-svc-dzr8p
Jan 18 15:08:26.626: INFO: Created: latency-svc-m4x7v
Jan 18 15:08:26.626: INFO: Created: latency-svc-wmzrh
Jan 18 15:08:26.626: INFO: Created: latency-svc-g48js
Jan 18 15:08:26.626: INFO: Created: latency-svc-cpcbt
Jan 18 15:08:26.639: INFO: Created: latency-svc-hm74b
Jan 18 15:08:26.639: INFO: Created: latency-svc-jwf9m
Jan 18 15:08:26.640: INFO: Created: latency-svc-rpfll
Jan 18 15:08:26.645: INFO: Created: latency-svc-lf796
Jan 18 15:08:26.649: INFO: Created: latency-svc-9sv5c
Jan 18 15:08:26.649: INFO: Created: latency-svc-brvx9
Jan 18 15:08:26.649: INFO: Created: latency-svc-qj8wg
Jan 18 15:08:26.652: INFO: Created: latency-svc-nkkc9
Jan 18 15:08:26.775: INFO: Got endpoints: latency-svc-hm74b [604.95317ms]
Jan 18 15:08:26.777: INFO: Got endpoints: latency-svc-g48js [319.531482ms]
Jan 18 15:08:26.777: INFO: Got endpoints: latency-svc-dzr8p [608.102428ms]
Jan 18 15:08:26.780: INFO: Got endpoints: latency-svc-m4x7v [608.387114ms]
Jan 18 15:08:26.782: INFO: Got endpoints: latency-svc-wmzrh [611.005031ms]
Jan 18 15:08:26.820: INFO: Got endpoints: latency-svc-rpfll [327.722164ms]
Jan 18 15:08:26.989: INFO: Created: latency-svc-vkww8
Jan 18 15:08:26.997: INFO: Got endpoints: latency-svc-nkkc9 [826.858825ms]
Jan 18 15:08:26.998: INFO: Got endpoints: latency-svc-zgkq2 [826.508928ms]
Jan 18 15:08:26.999: INFO: Got endpoints: latency-svc-7bxvr [526.485233ms]
Jan 18 15:08:26.999: INFO: Got endpoints: latency-svc-jwf9m [829.090732ms]
Jan 18 15:08:27.007: INFO: Got endpoints: latency-svc-cpcbt [579.010539ms]
Jan 18 15:08:27.029: INFO: Created: latency-svc-cv6w6
Jan 18 15:08:27.074: INFO: Created: latency-svc-59zd6
Jan 18 15:08:27.093: INFO: Got endpoints: latency-svc-qj8wg [923.979403ms]
Jan 18 15:08:27.093: INFO: Got endpoints: latency-svc-brvx9 [922.886909ms]
Jan 18 15:08:27.093: INFO: Got endpoints: latency-svc-lf796 [923.882297ms]
Jan 18 15:08:27.105: INFO: Got endpoints: latency-svc-vkww8 [329.123978ms]
Jan 18 15:08:27.105: INFO: Got endpoints: latency-svc-9sv5c [933.956542ms]
Jan 18 15:08:27.146: INFO: Got endpoints: latency-svc-cv6w6 [369.301029ms]
Jan 18 15:08:27.154: INFO: Created: latency-svc-rwj75
Jan 18 15:08:27.175: INFO: Created: latency-svc-kng6g
Jan 18 15:08:27.178: INFO: Got endpoints: latency-svc-59zd6 [400.820893ms]
Jan 18 15:08:27.186: INFO: Created: latency-svc-jpn5k
Jan 18 15:08:27.212: INFO: Created: latency-svc-v54p8
Jan 18 15:08:27.220: INFO: Got endpoints: latency-svc-rwj75 [437.947665ms]
Jan 18 15:08:27.230: INFO: Got endpoints: latency-svc-kng6g [450.056562ms]
Jan 18 15:08:27.230: INFO: Got endpoints: latency-svc-jpn5k [409.929971ms]
Jan 18 15:08:27.239: INFO: Got endpoints: latency-svc-v54p8 [241.812239ms]
Jan 18 15:08:27.239: INFO: Created: latency-svc-kznvj
Jan 18 15:08:27.257: INFO: Got endpoints: latency-svc-kznvj [255.891139ms]
Jan 18 15:08:27.267: INFO: Created: latency-svc-66hk5
Jan 18 15:08:27.273: INFO: Created: latency-svc-qxrmb
Jan 18 15:08:27.280: INFO: Got endpoints: latency-svc-66hk5 [282.768048ms]
Jan 18 15:08:27.299: INFO: Created: latency-svc-brgzb
Jan 18 15:08:27.348: INFO: Got endpoints: latency-svc-brgzb [108.907576ms]
Jan 18 15:08:27.349: INFO: Got endpoints: latency-svc-qxrmb [117.9912ms]
Jan 18 15:08:27.350: INFO: Created: latency-svc-lg4d9
Jan 18 15:08:27.375: INFO: Got endpoints: latency-svc-lg4d9 [366.565832ms]
Jan 18 15:08:27.375: INFO: Created: latency-svc-dskz7
Jan 18 15:08:27.395: INFO: Got endpoints: latency-svc-dskz7 [301.246566ms]
Jan 18 15:08:27.635: INFO: Created: latency-svc-wh2dn
Jan 18 15:08:27.685: INFO: Created: latency-svc-9mcj9
Jan 18 15:08:27.686: INFO: Created: latency-svc-w7tgk
Jan 18 15:08:27.686: INFO: Created: latency-svc-hrzl8
Jan 18 15:08:27.686: INFO: Created: latency-svc-7g8sj
Jan 18 15:08:27.686: INFO: Created: latency-svc-jz28b
Jan 18 15:08:27.686: INFO: Created: latency-svc-9kmtj
Jan 18 15:08:27.686: INFO: Created: latency-svc-vvhnz
Jan 18 15:08:27.687: INFO: Created: latency-svc-swsv5
Jan 18 15:08:27.687: INFO: Created: latency-svc-tnrc4
Jan 18 15:08:27.688: INFO: Created: latency-svc-c5jjc
Jan 18 15:08:27.688: INFO: Created: latency-svc-mr8n7
Jan 18 15:08:27.689: INFO: Created: latency-svc-vptc6
Jan 18 15:08:27.730: INFO: Created: latency-svc-q6zzz
Jan 18 15:08:27.735: INFO: Created: latency-svc-h5ksw
Jan 18 15:08:27.824: INFO: Got endpoints: latency-svc-7g8sj [718.731968ms]
Jan 18 15:08:27.825: INFO: Got endpoints: latency-svc-vptc6 [594.791333ms]
Jan 18 15:08:27.825: INFO: Got endpoints: latency-svc-wh2dn [730.210657ms]
Jan 18 15:08:27.825: INFO: Got endpoints: latency-svc-mr8n7 [646.207127ms]
Jan 18 15:08:27.825: INFO: Got endpoints: latency-svc-vvhnz [678.16796ms]
Jan 18 15:08:27.910: INFO: Created: latency-svc-77vgm
Jan 18 15:08:27.949: INFO: Created: latency-svc-prc4m
Jan 18 15:08:27.967: INFO: Got endpoints: latency-svc-w7tgk [966.587356ms]
Jan 18 15:08:27.968: INFO: Got endpoints: latency-svc-hrzl8 [710.781101ms]
Jan 18 15:08:27.968: INFO: Got endpoints: latency-svc-tnrc4 [863.308805ms]
Jan 18 15:08:27.976: INFO: Got endpoints: latency-svc-jz28b [695.822672ms]
Jan 18 15:08:27.977: INFO: Got endpoints: latency-svc-c5jjc [756.601858ms]
Jan 18 15:08:27.990: INFO: Got endpoints: latency-svc-swsv5 [615.028085ms]
Jan 18 15:08:27.992: INFO: Got endpoints: latency-svc-h5ksw [596.650992ms]
Jan 18 15:08:27.993: INFO: Got endpoints: latency-svc-q6zzz [899.648627ms]
Jan 18 15:08:27.995: INFO: Got endpoints: latency-svc-9kmtj [645.759406ms]
Jan 18 15:08:27.996: INFO: Got endpoints: latency-svc-9mcj9 [647.328897ms]
Jan 18 15:08:28.044: INFO: Got endpoints: latency-svc-77vgm [219.212686ms]
Jan 18 15:08:28.045: INFO: Got endpoints: latency-svc-prc4m [218.704177ms]
Jan 18 15:08:28.067: INFO: Created: latency-svc-c65n5
Jan 18 15:08:28.087: INFO: Created: latency-svc-flvk5
Jan 18 15:08:28.087: INFO: Got endpoints: latency-svc-c65n5 [261.803403ms]
Jan 18 15:08:28.089: INFO: Created: latency-svc-vhwcz
Jan 18 15:08:28.118: INFO: Created: latency-svc-s522p
Jan 18 15:08:28.134: INFO: Created: latency-svc-vx5gr
Jan 18 15:08:28.142: INFO: Got endpoints: latency-svc-vhwcz [316.691642ms]
Jan 18 15:08:28.143: INFO: Got endpoints: latency-svc-flvk5 [316.947501ms]
Jan 18 15:08:28.159: INFO: Created: latency-svc-6mzwx
Jan 18 15:08:28.160: INFO: Got endpoints: latency-svc-s522p [192.639409ms]
Jan 18 15:08:28.192: INFO: Created: latency-svc-gtsrw
Jan 18 15:08:28.195: INFO: Got endpoints: latency-svc-vx5gr [226.817641ms]
Jan 18 15:08:28.217: INFO: Got endpoints: latency-svc-6mzwx [249.011181ms]
Jan 18 15:08:28.246: INFO: Got endpoints: latency-svc-gtsrw [268.853003ms]
Jan 18 15:08:28.254: INFO: Created: latency-svc-dhkqd
Jan 18 15:08:28.273: INFO: Created: latency-svc-rkvdm
Jan 18 15:08:28.285: INFO: Got endpoints: latency-svc-dhkqd [308.290408ms]
Jan 18 15:08:28.293: INFO: Created: latency-svc-sl6dh
Jan 18 15:08:28.318: INFO: Got endpoints: latency-svc-rkvdm [327.790915ms]
Jan 18 15:08:28.326: INFO: Got endpoints: latency-svc-sl6dh [334.5283ms]
Jan 18 15:08:28.577: INFO: Created: latency-svc-qgvks
Jan 18 15:08:28.578: INFO: Created: latency-svc-mzlvw
Jan 18 15:08:28.578: INFO: Created: latency-svc-5jkrv
Jan 18 15:08:28.579: INFO: Created: latency-svc-46hhb
Jan 18 15:08:28.579: INFO: Created: latency-svc-k2x6s
Jan 18 15:08:28.579: INFO: Created: latency-svc-6289w
Jan 18 15:08:28.579: INFO: Created: latency-svc-v8dbj
Jan 18 15:08:28.579: INFO: Created: latency-svc-6h7mg
Jan 18 15:08:28.580: INFO: Created: latency-svc-v78qj
Jan 18 15:08:28.578: INFO: Created: latency-svc-ttf7d
Jan 18 15:08:28.581: INFO: Created: latency-svc-hng2s
Jan 18 15:08:28.583: INFO: Created: latency-svc-s2lg6
Jan 18 15:08:28.583: INFO: Created: latency-svc-m9m6d
Jan 18 15:08:28.585: INFO: Created: latency-svc-p4djq
Jan 18 15:08:28.585: INFO: Created: latency-svc-wk4t2
Jan 18 15:08:28.675: INFO: Got endpoints: latency-svc-v78qj [679.102573ms]
Jan 18 15:08:28.678: INFO: Got endpoints: latency-svc-5jkrv [351.430323ms]
Jan 18 15:08:28.678: INFO: Got endpoints: latency-svc-m9m6d [358.874018ms]
Jan 18 15:08:28.712: INFO: Got endpoints: latency-svc-mzlvw [717.564502ms]
Jan 18 15:08:28.713: INFO: Got endpoints: latency-svc-46hhb [719.391078ms]
Jan 18 15:08:28.751: INFO: Got endpoints: latency-svc-k2x6s [705.258029ms]
Jan 18 15:08:28.751: INFO: Got endpoints: latency-svc-ttf7d [707.33411ms]
Jan 18 15:08:28.752: INFO: Got endpoints: latency-svc-6h7mg [664.581824ms]
Jan 18 15:08:28.769: INFO: Got endpoints: latency-svc-v8dbj [626.904259ms]
Jan 18 15:08:28.772: INFO: Got endpoints: latency-svc-wk4t2 [486.788161ms]
Jan 18 15:08:28.773: INFO: Created: latency-svc-rqwdx
Jan 18 15:08:28.798: INFO: Got endpoints: latency-svc-hng2s [580.478065ms]
Jan 18 15:08:28.807: INFO: Created: latency-svc-hznb2
Jan 18 15:08:28.807: INFO: Got endpoints: latency-svc-qgvks [664.344727ms]
Jan 18 15:08:28.807: INFO: Got endpoints: latency-svc-s2lg6 [561.58315ms]
Jan 18 15:08:28.808: INFO: Got endpoints: latency-svc-6289w [612.471963ms]
Jan 18 15:08:28.808: INFO: Got endpoints: latency-svc-p4djq [647.773531ms]
Jan 18 15:08:28.856: INFO: Got endpoints: latency-svc-hznb2 [176.353333ms]
Jan 18 15:08:28.856: INFO: Got endpoints: latency-svc-rqwdx [177.753208ms]
Jan 18 15:08:28.985: INFO: Created: latency-svc-tbxn4
Jan 18 15:08:28.997: INFO: Created: latency-svc-w4j5h
Jan 18 15:08:28.998: INFO: Created: latency-svc-4t8l4
Jan 18 15:08:28.999: INFO: Created: latency-svc-fxlhm
Jan 18 15:08:29.000: INFO: Created: latency-svc-tc7bx
Jan 18 15:08:29.001: INFO: Created: latency-svc-sbgkw
Jan 18 15:08:29.001: INFO: Created: latency-svc-zqn8g
Jan 18 15:08:29.001: INFO: Created: latency-svc-9pmjt
Jan 18 15:08:29.001: INFO: Created: latency-svc-ts54r
Jan 18 15:08:29.001: INFO: Created: latency-svc-k5s5w
Jan 18 15:08:29.002: INFO: Created: latency-svc-lcbxr
Jan 18 15:08:29.002: INFO: Created: latency-svc-5nllg
Jan 18 15:08:29.004: INFO: Created: latency-svc-rgm9q
Jan 18 15:08:29.004: INFO: Created: latency-svc-f8k8l
Jan 18 15:08:29.005: INFO: Created: latency-svc-fnqjc
Jan 18 15:08:29.008: INFO: Got endpoints: latency-svc-tbxn4 [236.66532ms]
Jan 18 15:08:29.009: INFO: Got endpoints: latency-svc-f8k8l [257.840473ms]
Jan 18 15:08:29.118: INFO: Got endpoints: latency-svc-4t8l4 [310.259089ms]
Jan 18 15:08:29.118: INFO: Got endpoints: latency-svc-5nllg [366.761314ms]
Jan 18 15:08:29.119: INFO: Got endpoints: latency-svc-tc7bx [311.71948ms]
Jan 18 15:08:29.120: INFO: Got endpoints: latency-svc-fxlhm [312.096328ms]
Jan 18 15:08:29.120: INFO: Got endpoints: latency-svc-w4j5h [322.16463ms]
Jan 18 15:08:29.142: INFO: Got endpoints: latency-svc-sbgkw [334.216717ms]
Jan 18 15:08:29.142: INFO: Created: latency-svc-mfs8v
Jan 18 15:08:29.149: INFO: Created: latency-svc-p4zhh
Jan 18 15:08:29.157: INFO: Created: latency-svc-phgtp
Jan 18 15:08:29.196: INFO: Got endpoints: latency-svc-ts54r [483.393934ms]
Jan 18 15:08:29.222: INFO: Created: latency-svc-c4pn8
Jan 18 15:08:29.223: INFO: Created: latency-svc-p6rk5
Jan 18 15:08:29.223: INFO: Created: latency-svc-b9jnd
Jan 18 15:08:29.231: INFO: Created: latency-svc-ph6b6
Jan 18 15:08:29.239: INFO: Created: latency-svc-tg2pg
Jan 18 15:08:29.246: INFO: Got endpoints: latency-svc-fnqjc [533.075479ms]
Jan 18 15:08:29.251: INFO: Created: latency-svc-9qx59
Jan 18 15:08:29.267: INFO: Created: latency-svc-pzm8f
Jan 18 15:08:29.294: INFO: Got endpoints: latency-svc-zqn8g [524.031038ms]
Jan 18 15:08:29.324: INFO: Created: latency-svc-7wl44
Jan 18 15:08:29.335: INFO: Got endpoints: latency-svc-lcbxr [479.072308ms]
Jan 18 15:08:29.349: INFO: Created: latency-svc-r2cb6
Jan 18 15:08:29.393: INFO: Got endpoints: latency-svc-k5s5w [713.100813ms]
Jan 18 15:08:29.417: INFO: Created: latency-svc-kvcw6
Jan 18 15:08:29.438: INFO: Got endpoints: latency-svc-9pmjt [581.758051ms]
Jan 18 15:08:29.452: INFO: Created: latency-svc-49t7v
Jan 18 15:08:29.490: INFO: Got endpoints: latency-svc-rgm9q [738.4812ms]
Jan 18 15:08:29.511: INFO: Created: latency-svc-k8ms4
Jan 18 15:08:29.547: INFO: Got endpoints: latency-svc-mfs8v [538.12009ms]
Jan 18 15:08:29.560: INFO: Created: latency-svc-lvq6t
Jan 18 15:08:29.597: INFO: Got endpoints: latency-svc-p4zhh [587.929861ms]
Jan 18 15:08:29.616: INFO: Created: latency-svc-kh7g5
Jan 18 15:08:29.646: INFO: Got endpoints: latency-svc-phgtp [526.182608ms]
Jan 18 15:08:29.697: INFO: Created: latency-svc-ff694
Jan 18 15:08:29.706: INFO: Got endpoints: latency-svc-b9jnd [588.461135ms]
Jan 18 15:08:29.731: INFO: Created: latency-svc-82xjj
Jan 18 15:08:29.736: INFO: Got endpoints: latency-svc-c4pn8 [617.295086ms]
Jan 18 15:08:29.751: INFO: Created: latency-svc-fswrl
Jan 18 15:08:29.787: INFO: Got endpoints: latency-svc-p6rk5 [667.671488ms]
Jan 18 15:08:29.805: INFO: Created: latency-svc-hhqcv
Jan 18 15:08:29.840: INFO: Got endpoints: latency-svc-ph6b6 [720.327387ms]
Jan 18 15:08:29.865: INFO: Created: latency-svc-lfb9w
Jan 18 15:08:29.903: INFO: Got endpoints: latency-svc-tg2pg [760.902285ms]
Jan 18 15:08:29.936: INFO: Created: latency-svc-8vjw6
Jan 18 15:08:29.943: INFO: Got endpoints: latency-svc-9qx59 [746.680414ms]
Jan 18 15:08:29.981: INFO: Created: latency-svc-gn86s
Jan 18 15:08:30.007: INFO: Got endpoints: latency-svc-pzm8f [760.965614ms]
Jan 18 15:08:30.025: INFO: Created: latency-svc-w5pd7
Jan 18 15:08:30.048: INFO: Got endpoints: latency-svc-7wl44 [754.164475ms]
Jan 18 15:08:30.069: INFO: Created: latency-svc-whkmn
Jan 18 15:08:30.091: INFO: Got endpoints: latency-svc-r2cb6 [755.825589ms]
Jan 18 15:08:30.109: INFO: Created: latency-svc-9d265
Jan 18 15:08:30.137: INFO: Got endpoints: latency-svc-kvcw6 [744.119142ms]
Jan 18 15:08:30.166: INFO: Created: latency-svc-w466n
Jan 18 15:08:30.188: INFO: Got endpoints: latency-svc-49t7v [749.773872ms]
Jan 18 15:08:30.204: INFO: Created: latency-svc-8xgn6
Jan 18 15:08:30.233: INFO: Got endpoints: latency-svc-k8ms4 [743.256458ms]
Jan 18 15:08:30.256: INFO: Created: latency-svc-92xrx
Jan 18 15:08:30.286: INFO: Got endpoints: latency-svc-lvq6t [739.690252ms]
Jan 18 15:08:30.302: INFO: Created: latency-svc-d69c7
Jan 18 15:08:30.341: INFO: Got endpoints: latency-svc-kh7g5 [744.448618ms]
Jan 18 15:08:30.360: INFO: Created: latency-svc-jwtv4
Jan 18 15:08:30.386: INFO: Got endpoints: latency-svc-ff694 [740.390767ms]
Jan 18 15:08:30.416: INFO: Created: latency-svc-ljss9
Jan 18 15:08:30.434: INFO: Got endpoints: latency-svc-82xjj [728.030404ms]
Jan 18 15:08:30.453: INFO: Created: latency-svc-7jtg6
Jan 18 15:08:30.489: INFO: Got endpoints: latency-svc-fswrl [752.795691ms]
Jan 18 15:08:30.508: INFO: Created: latency-svc-48gj5
Jan 18 15:08:30.541: INFO: Got endpoints: latency-svc-hhqcv [753.836516ms]
Jan 18 15:08:30.581: INFO: Created: latency-svc-rmmbr
Jan 18 15:08:30.596: INFO: Got endpoints: latency-svc-lfb9w [755.718137ms]
Jan 18 15:08:30.611: INFO: Created: latency-svc-z7vbj
Jan 18 15:08:30.634: INFO: Got endpoints: latency-svc-8vjw6 [731.781096ms]
Jan 18 15:08:30.652: INFO: Created: latency-svc-xbtsq
Jan 18 15:08:30.693: INFO: Got endpoints: latency-svc-gn86s [750.14901ms]
Jan 18 15:08:30.712: INFO: Created: latency-svc-4s47b
Jan 18 15:08:30.739: INFO: Got endpoints: latency-svc-w5pd7 [732.457315ms]
Jan 18 15:08:30.753: INFO: Created: latency-svc-qrckt
Jan 18 15:08:30.791: INFO: Got endpoints: latency-svc-whkmn [743.494824ms]
Jan 18 15:08:30.819: INFO: Created: latency-svc-xvqf2
Jan 18 15:08:30.842: INFO: Got endpoints: latency-svc-9d265 [751.076008ms]
Jan 18 15:08:30.862: INFO: Created: latency-svc-fcwf9
Jan 18 15:08:30.888: INFO: Got endpoints: latency-svc-w466n [750.893468ms]
Jan 18 15:08:30.907: INFO: Created: latency-svc-fhhn8
Jan 18 15:08:30.936: INFO: Got endpoints: latency-svc-8xgn6 [748.176899ms]
Jan 18 15:08:30.952: INFO: Created: latency-svc-hgtrx
Jan 18 15:08:30.999: INFO: Got endpoints: latency-svc-92xrx [765.657379ms]
Jan 18 15:08:31.023: INFO: Created: latency-svc-645kb
Jan 18 15:08:31.037: INFO: Got endpoints: latency-svc-d69c7 [749.991216ms]
Jan 18 15:08:31.053: INFO: Created: latency-svc-57lfn
Jan 18 15:08:31.085: INFO: Got endpoints: latency-svc-jwtv4 [743.196627ms]
Jan 18 15:08:31.114: INFO: Created: latency-svc-x7gnw
Jan 18 15:08:31.145: INFO: Got endpoints: latency-svc-ljss9 [758.641157ms]
Jan 18 15:08:31.182: INFO: Created: latency-svc-878tq
Jan 18 15:08:31.194: INFO: Got endpoints: latency-svc-7jtg6 [759.15093ms]
Jan 18 15:08:31.212: INFO: Created: latency-svc-4dbt9
Jan 18 15:08:31.237: INFO: Got endpoints: latency-svc-48gj5 [748.170198ms]
Jan 18 15:08:31.248: INFO: Created: latency-svc-qjszv
Jan 18 15:08:31.303: INFO: Got endpoints: latency-svc-rmmbr [761.209327ms]
Jan 18 15:08:31.318: INFO: Created: latency-svc-7qdgg
Jan 18 15:08:31.336: INFO: Got endpoints: latency-svc-z7vbj [739.429924ms]
Jan 18 15:08:31.355: INFO: Created: latency-svc-q8nbf
Jan 18 15:08:31.397: INFO: Got endpoints: latency-svc-xbtsq [762.066686ms]
Jan 18 15:08:31.418: INFO: Created: latency-svc-wrwqw
Jan 18 15:08:31.444: INFO: Got endpoints: latency-svc-4s47b [750.32742ms]
Jan 18 15:08:31.462: INFO: Created: latency-svc-5hh5j
Jan 18 15:08:31.485: INFO: Got endpoints: latency-svc-qrckt [745.680351ms]
Jan 18 15:08:31.516: INFO: Created: latency-svc-6nhzt
Jan 18 15:08:31.542: INFO: Got endpoints: latency-svc-xvqf2 [750.276154ms]
Jan 18 15:08:31.563: INFO: Created: latency-svc-hlqr4
Jan 18 15:08:31.586: INFO: Got endpoints: latency-svc-fcwf9 [744.046207ms]
Jan 18 15:08:31.606: INFO: Created: latency-svc-j7bfs
Jan 18 15:08:31.634: INFO: Got endpoints: latency-svc-fhhn8 [745.091702ms]
Jan 18 15:08:31.651: INFO: Created: latency-svc-jv7hb
Jan 18 15:08:31.688: INFO: Got endpoints: latency-svc-hgtrx [751.539254ms]
Jan 18 15:08:31.704: INFO: Created: latency-svc-56vtm
Jan 18 15:08:31.743: INFO: Got endpoints: latency-svc-645kb [743.842132ms]
Jan 18 15:08:31.755: INFO: Created: latency-svc-dvkk8
Jan 18 15:08:31.792: INFO: Got endpoints: latency-svc-57lfn [755.828951ms]
Jan 18 15:08:31.815: INFO: Created: latency-svc-7h9vc
Jan 18 15:08:31.841: INFO: Got endpoints: latency-svc-x7gnw [756.215758ms]
Jan 18 15:08:31.868: INFO: Created: latency-svc-rk6mn
Jan 18 15:08:31.893: INFO: Got endpoints: latency-svc-878tq [747.284199ms]
Jan 18 15:08:31.910: INFO: Created: latency-svc-5kf8s
Jan 18 15:08:31.938: INFO: Got endpoints: latency-svc-4dbt9 [742.404154ms]
Jan 18 15:08:31.952: INFO: Created: latency-svc-2bpbn
Jan 18 15:08:31.991: INFO: Got endpoints: latency-svc-qjszv [754.511545ms]
Jan 18 15:08:32.013: INFO: Created: latency-svc-4fnb5
Jan 18 15:08:32.040: INFO: Got endpoints: latency-svc-7qdgg [737.366963ms]
Jan 18 15:08:32.051: INFO: Created: latency-svc-pwhsd
Jan 18 15:08:32.087: INFO: Got endpoints: latency-svc-q8nbf [750.494014ms]
Jan 18 15:08:32.119: INFO: Created: latency-svc-hrvs8
Jan 18 15:08:32.134: INFO: Got endpoints: latency-svc-wrwqw [737.471849ms]
Jan 18 15:08:32.158: INFO: Created: latency-svc-w89q9
Jan 18 15:08:32.190: INFO: Got endpoints: latency-svc-5hh5j [746.417305ms]
Jan 18 15:08:32.230: INFO: Created: latency-svc-vhljj
Jan 18 15:08:32.243: INFO: Got endpoints: latency-svc-6nhzt [758.088852ms]
Jan 18 15:08:32.258: INFO: Created: latency-svc-s7xjt
Jan 18 15:08:32.294: INFO: Got endpoints: latency-svc-hlqr4 [752.07341ms]
Jan 18 15:08:32.319: INFO: Created: latency-svc-dg4mz
Jan 18 15:08:32.337: INFO: Got endpoints: latency-svc-j7bfs [750.61989ms]
Jan 18 15:08:32.355: INFO: Created: latency-svc-zj6zv
Jan 18 15:08:32.392: INFO: Got endpoints: latency-svc-jv7hb [758.459843ms]
Jan 18 15:08:32.412: INFO: Created: latency-svc-972wt
Jan 18 15:08:32.440: INFO: Got endpoints: latency-svc-56vtm [751.617659ms]
Jan 18 15:08:32.458: INFO: Created: latency-svc-p948k
Jan 18 15:08:32.492: INFO: Got endpoints: latency-svc-dvkk8 [749.005464ms]
Jan 18 15:08:32.505: INFO: Created: latency-svc-tnkd9
Jan 18 15:08:32.542: INFO: Got endpoints: latency-svc-7h9vc [749.450737ms]
Jan 18 15:08:32.553: INFO: Created: latency-svc-v5sbz
Jan 18 15:08:32.588: INFO: Got endpoints: latency-svc-rk6mn [746.948871ms]
Jan 18 15:08:32.600: INFO: Created: latency-svc-qtfkk
Jan 18 15:08:32.650: INFO: Got endpoints: latency-svc-5kf8s [756.772094ms]
Jan 18 15:08:32.664: INFO: Created: latency-svc-wsk98
Jan 18 15:08:32.688: INFO: Got endpoints: latency-svc-2bpbn [749.666759ms]
Jan 18 15:08:32.705: INFO: Created: latency-svc-wfqht
Jan 18 15:08:32.736: INFO: Got endpoints: latency-svc-4fnb5 [744.616053ms]
Jan 18 15:08:32.754: INFO: Created: latency-svc-8qfrl
Jan 18 15:08:32.790: INFO: Got endpoints: latency-svc-pwhsd [749.734051ms]
Jan 18 15:08:33.019: INFO: Got endpoints: latency-svc-s7xjt [775.301988ms]
Jan 18 15:08:33.019: INFO: Got endpoints: latency-svc-w89q9 [884.218844ms]
Jan 18 15:08:33.019: INFO: Got endpoints: latency-svc-hrvs8 [931.708661ms]
Jan 18 15:08:33.019: INFO: Got endpoints: latency-svc-vhljj [828.465581ms]
Jan 18 15:08:33.053: INFO: Got endpoints: latency-svc-dg4mz [757.972522ms]
Jan 18 15:08:33.053: INFO: Created: latency-svc-5ggdd
Jan 18 15:08:33.057: INFO: Created: latency-svc-h8w57
Jan 18 15:08:33.067: INFO: Created: latency-svc-zhjpm
Jan 18 15:08:33.077: INFO: Created: latency-svc-ntvqp
Jan 18 15:08:33.083: INFO: Created: latency-svc-ckrlp
Jan 18 15:08:33.093: INFO: Got endpoints: latency-svc-zj6zv [755.252178ms]
Jan 18 15:08:33.101: INFO: Created: latency-svc-rjw8w
Jan 18 15:08:33.127: INFO: Created: latency-svc-l8g5n
Jan 18 15:08:33.137: INFO: Got endpoints: latency-svc-972wt [745.119377ms]
Jan 18 15:08:33.155: INFO: Created: latency-svc-wxrkp
Jan 18 15:08:33.184: INFO: Got endpoints: latency-svc-p948k [744.448408ms]
Jan 18 15:08:33.197: INFO: Created: latency-svc-62cdt
Jan 18 15:08:33.237: INFO: Got endpoints: latency-svc-tnkd9 [744.53683ms]
Jan 18 15:08:33.254: INFO: Created: latency-svc-25qcv
Jan 18 15:08:33.289: INFO: Got endpoints: latency-svc-v5sbz [747.451091ms]
Jan 18 15:08:33.306: INFO: Created: latency-svc-q2qf9
Jan 18 15:08:33.343: INFO: Got endpoints: latency-svc-qtfkk [754.425789ms]
Jan 18 15:08:33.352: INFO: Created: latency-svc-phdvw
Jan 18 15:08:33.399: INFO: Got endpoints: latency-svc-wsk98 [748.360826ms]
Jan 18 15:08:33.422: INFO: Created: latency-svc-vvmb4
Jan 18 15:08:33.443: INFO: Got endpoints: latency-svc-wfqht [755.383101ms]
Jan 18 15:08:33.464: INFO: Created: latency-svc-v45rr
Jan 18 15:08:33.494: INFO: Got endpoints: latency-svc-8qfrl [757.904723ms]
Jan 18 15:08:33.514: INFO: Created: latency-svc-p88x5
Jan 18 15:08:33.779: INFO: Got endpoints: latency-svc-h8w57 [759.067895ms]
Jan 18 15:08:33.779: INFO: Got endpoints: latency-svc-ckrlp [758.565625ms]
Jan 18 15:08:33.781: INFO: Got endpoints: latency-svc-zhjpm [760.323861ms]
Jan 18 15:08:33.781: INFO: Got endpoints: latency-svc-ntvqp [760.229954ms]
Jan 18 15:08:33.781: INFO: Got endpoints: latency-svc-5ggdd [988.523182ms]
Jan 18 15:08:33.795: INFO: Got endpoints: latency-svc-rjw8w [742.743326ms]
Jan 18 15:08:33.804: INFO: Created: latency-svc-2wvvf
Jan 18 15:08:33.831: INFO: Created: latency-svc-d8xn5
Jan 18 15:08:33.844: INFO: Created: latency-svc-7q89f
Jan 18 15:08:33.860: INFO: Got endpoints: latency-svc-l8g5n [766.816814ms]
Jan 18 15:08:33.872: INFO: Created: latency-svc-m5r24
Jan 18 15:08:33.896: INFO: Got endpoints: latency-svc-wxrkp [758.20819ms]
Jan 18 15:08:33.902: INFO: Created: latency-svc-smsv9
Jan 18 15:08:33.903: INFO: Created: latency-svc-mvwrk
Jan 18 15:08:33.912: INFO: Created: latency-svc-bl27j
Jan 18 15:08:33.939: INFO: Created: latency-svc-5g2l8
Jan 18 15:08:33.946: INFO: Got endpoints: latency-svc-62cdt [761.257205ms]
Jan 18 15:08:33.963: INFO: Created: latency-svc-kj954
Jan 18 15:08:33.995: INFO: Got endpoints: latency-svc-25qcv [757.926675ms]
Jan 18 15:08:34.014: INFO: Created: latency-svc-8bpq8
Jan 18 15:08:34.036: INFO: Got endpoints: latency-svc-q2qf9 [746.543931ms]
Jan 18 15:08:34.062: INFO: Created: latency-svc-hq77c
Jan 18 15:08:34.085: INFO: Got endpoints: latency-svc-phdvw [741.934464ms]
Jan 18 15:08:34.107: INFO: Created: latency-svc-pbpn6
Jan 18 15:08:34.142: INFO: Got endpoints: latency-svc-vvmb4 [737.763658ms]
Jan 18 15:08:34.169: INFO: Created: latency-svc-m5fvn
Jan 18 15:08:34.190: INFO: Got endpoints: latency-svc-v45rr [746.975307ms]
Jan 18 15:08:34.214: INFO: Created: latency-svc-gz9d7
Jan 18 15:08:34.242: INFO: Got endpoints: latency-svc-p88x5 [747.877231ms]
Jan 18 15:08:34.287: INFO: Got endpoints: latency-svc-2wvvf [507.753827ms]
Jan 18 15:08:34.342: INFO: Got endpoints: latency-svc-d8xn5 [561.458168ms]
Jan 18 15:08:34.403: INFO: Got endpoints: latency-svc-7q89f [624.003438ms]
Jan 18 15:08:34.434: INFO: Got endpoints: latency-svc-m5r24 [653.555569ms]
Jan 18 15:08:34.489: INFO: Got endpoints: latency-svc-mvwrk [707.404666ms]
Jan 18 15:08:34.554: INFO: Got endpoints: latency-svc-smsv9 [758.734322ms]
Jan 18 15:08:34.595: INFO: Got endpoints: latency-svc-bl27j [735.101548ms]
Jan 18 15:08:34.660: INFO: Got endpoints: latency-svc-5g2l8 [764.545768ms]
Jan 18 15:08:34.687: INFO: Got endpoints: latency-svc-kj954 [740.772155ms]
Jan 18 15:08:34.742: INFO: Got endpoints: latency-svc-8bpq8 [747.135045ms]
Jan 18 15:08:34.789: INFO: Got endpoints: latency-svc-hq77c [752.192906ms]
Jan 18 15:08:34.845: INFO: Got endpoints: latency-svc-pbpn6 [760.23543ms]
Jan 18 15:08:34.894: INFO: Got endpoints: latency-svc-m5fvn [751.395013ms]
Jan 18 15:08:34.936: INFO: Got endpoints: latency-svc-gz9d7 [746.086888ms]
Jan 18 15:08:34.937: INFO: Latencies: [108.907576ms 117.9912ms 176.353333ms 177.753208ms 192.639409ms 218.704177ms 219.212686ms 226.817641ms 236.66532ms 241.812239ms 249.011181ms 255.891139ms 257.840473ms 259.47152ms 261.803403ms 268.853003ms 282.768048ms 287.151707ms 301.246566ms 302.390664ms 308.290408ms 310.259089ms 311.71948ms 312.096328ms 316.691642ms 316.947501ms 319.531482ms 322.16463ms 323.573252ms 327.722164ms 327.790915ms 329.123978ms 334.216717ms 334.5283ms 351.430323ms 358.874018ms 366.565832ms 366.761314ms 369.301029ms 400.820893ms 409.929971ms 437.947665ms 450.056562ms 479.072308ms 483.393934ms 486.788161ms 507.753827ms 524.031038ms 526.182608ms 526.485233ms 533.075479ms 538.12009ms 561.458168ms 561.58315ms 579.010539ms 580.478065ms 581.758051ms 587.929861ms 588.461135ms 594.791333ms 596.650992ms 604.95317ms 608.102428ms 608.387114ms 611.005031ms 612.471963ms 615.028085ms 617.295086ms 624.003438ms 626.904259ms 645.759406ms 646.207127ms 647.328897ms 647.773531ms 653.555569ms 664.344727ms 664.581824ms 667.671488ms 678.16796ms 679.102573ms 695.822672ms 705.258029ms 707.33411ms 707.404666ms 710.781101ms 713.100813ms 717.564502ms 718.731968ms 719.391078ms 720.327387ms 728.030404ms 730.210657ms 731.781096ms 732.457315ms 735.101548ms 737.366963ms 737.471849ms 737.763658ms 738.4812ms 739.429924ms 739.690252ms 740.390767ms 740.772155ms 741.934464ms 742.404154ms 742.743326ms 743.196627ms 743.256458ms 743.494824ms 743.842132ms 744.046207ms 744.119142ms 744.448408ms 744.448618ms 744.53683ms 744.616053ms 745.091702ms 745.119377ms 745.680351ms 746.086888ms 746.417305ms 746.543931ms 746.680414ms 746.948871ms 746.975307ms 747.135045ms 747.284199ms 747.451091ms 747.877231ms 748.170198ms 748.176899ms 748.360826ms 749.005464ms 749.450737ms 749.666759ms 749.734051ms 749.773872ms 749.991216ms 750.14901ms 750.276154ms 750.32742ms 750.494014ms 750.61989ms 750.893468ms 751.076008ms 751.395013ms 751.539254ms 751.617659ms 752.07341ms 752.192906ms 752.795691ms 753.836516ms 754.164475ms 754.425789ms 754.511545ms 755.252178ms 755.383101ms 755.718137ms 755.825589ms 755.828951ms 756.215758ms 756.601858ms 756.772094ms 757.904723ms 757.926675ms 757.972522ms 758.088852ms 758.20819ms 758.459843ms 758.565625ms 758.641157ms 758.734322ms 759.067895ms 759.15093ms 760.229954ms 760.23543ms 760.323861ms 760.902285ms 760.965614ms 761.209327ms 761.257205ms 762.066686ms 764.545768ms 765.657379ms 766.816814ms 775.301988ms 826.508928ms 826.858825ms 828.465581ms 829.090732ms 863.308805ms 884.218844ms 899.648627ms 922.886909ms 923.882297ms 923.979403ms 931.708661ms 933.956542ms 966.587356ms 988.523182ms]
Jan 18 15:08:34.938: INFO: 50 %ile: 739.690252ms
Jan 18 15:08:34.938: INFO: 90 %ile: 761.257205ms
Jan 18 15:08:34.938: INFO: 99 %ile: 966.587356ms
Jan 18 15:08:34.938: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Jan 18 15:08:34.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1591" for this suite. 01/18/23 15:08:34.958
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":15,"skipped":395,"failed":0}
------------------------------
• [SLOW TEST] [13.032 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:08:21.939
    Jan 18 15:08:21.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename svc-latency 01/18/23 15:08:21.94
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:08:21.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:08:21.971
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jan 18 15:08:21.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-1591 01/18/23 15:08:21.978
    I0118 15:08:21.986594      19 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1591, replica count: 1
    I0118 15:08:23.037771      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0118 15:08:24.038274      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0118 15:08:25.038519      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0118 15:08:26.039020      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 15:08:26.150: INFO: Created: latency-svc-g7pnv
    Jan 18 15:08:26.168: INFO: Got endpoints: latency-svc-g7pnv [29.729639ms]
    Jan 18 15:08:26.193: INFO: Created: latency-svc-4qjxc
    Jan 18 15:08:26.428: INFO: Got endpoints: latency-svc-4qjxc [259.47152ms]
    Jan 18 15:08:26.433: INFO: Created: latency-svc-9hpxt
    Jan 18 15:08:26.457: INFO: Got endpoints: latency-svc-9hpxt [287.151707ms]
    Jan 18 15:08:26.466: INFO: Created: latency-svc-76qkb
    Jan 18 15:08:26.472: INFO: Got endpoints: latency-svc-76qkb [302.390664ms]
    Jan 18 15:08:26.478: INFO: Created: latency-svc-4cm52
    Jan 18 15:08:26.493: INFO: Got endpoints: latency-svc-4cm52 [323.573252ms]
    Jan 18 15:08:26.625: INFO: Created: latency-svc-7bxvr
    Jan 18 15:08:26.626: INFO: Created: latency-svc-zgkq2
    Jan 18 15:08:26.626: INFO: Created: latency-svc-dzr8p
    Jan 18 15:08:26.626: INFO: Created: latency-svc-m4x7v
    Jan 18 15:08:26.626: INFO: Created: latency-svc-wmzrh
    Jan 18 15:08:26.626: INFO: Created: latency-svc-g48js
    Jan 18 15:08:26.626: INFO: Created: latency-svc-cpcbt
    Jan 18 15:08:26.639: INFO: Created: latency-svc-hm74b
    Jan 18 15:08:26.639: INFO: Created: latency-svc-jwf9m
    Jan 18 15:08:26.640: INFO: Created: latency-svc-rpfll
    Jan 18 15:08:26.645: INFO: Created: latency-svc-lf796
    Jan 18 15:08:26.649: INFO: Created: latency-svc-9sv5c
    Jan 18 15:08:26.649: INFO: Created: latency-svc-brvx9
    Jan 18 15:08:26.649: INFO: Created: latency-svc-qj8wg
    Jan 18 15:08:26.652: INFO: Created: latency-svc-nkkc9
    Jan 18 15:08:26.775: INFO: Got endpoints: latency-svc-hm74b [604.95317ms]
    Jan 18 15:08:26.777: INFO: Got endpoints: latency-svc-g48js [319.531482ms]
    Jan 18 15:08:26.777: INFO: Got endpoints: latency-svc-dzr8p [608.102428ms]
    Jan 18 15:08:26.780: INFO: Got endpoints: latency-svc-m4x7v [608.387114ms]
    Jan 18 15:08:26.782: INFO: Got endpoints: latency-svc-wmzrh [611.005031ms]
    Jan 18 15:08:26.820: INFO: Got endpoints: latency-svc-rpfll [327.722164ms]
    Jan 18 15:08:26.989: INFO: Created: latency-svc-vkww8
    Jan 18 15:08:26.997: INFO: Got endpoints: latency-svc-nkkc9 [826.858825ms]
    Jan 18 15:08:26.998: INFO: Got endpoints: latency-svc-zgkq2 [826.508928ms]
    Jan 18 15:08:26.999: INFO: Got endpoints: latency-svc-7bxvr [526.485233ms]
    Jan 18 15:08:26.999: INFO: Got endpoints: latency-svc-jwf9m [829.090732ms]
    Jan 18 15:08:27.007: INFO: Got endpoints: latency-svc-cpcbt [579.010539ms]
    Jan 18 15:08:27.029: INFO: Created: latency-svc-cv6w6
    Jan 18 15:08:27.074: INFO: Created: latency-svc-59zd6
    Jan 18 15:08:27.093: INFO: Got endpoints: latency-svc-qj8wg [923.979403ms]
    Jan 18 15:08:27.093: INFO: Got endpoints: latency-svc-brvx9 [922.886909ms]
    Jan 18 15:08:27.093: INFO: Got endpoints: latency-svc-lf796 [923.882297ms]
    Jan 18 15:08:27.105: INFO: Got endpoints: latency-svc-vkww8 [329.123978ms]
    Jan 18 15:08:27.105: INFO: Got endpoints: latency-svc-9sv5c [933.956542ms]
    Jan 18 15:08:27.146: INFO: Got endpoints: latency-svc-cv6w6 [369.301029ms]
    Jan 18 15:08:27.154: INFO: Created: latency-svc-rwj75
    Jan 18 15:08:27.175: INFO: Created: latency-svc-kng6g
    Jan 18 15:08:27.178: INFO: Got endpoints: latency-svc-59zd6 [400.820893ms]
    Jan 18 15:08:27.186: INFO: Created: latency-svc-jpn5k
    Jan 18 15:08:27.212: INFO: Created: latency-svc-v54p8
    Jan 18 15:08:27.220: INFO: Got endpoints: latency-svc-rwj75 [437.947665ms]
    Jan 18 15:08:27.230: INFO: Got endpoints: latency-svc-kng6g [450.056562ms]
    Jan 18 15:08:27.230: INFO: Got endpoints: latency-svc-jpn5k [409.929971ms]
    Jan 18 15:08:27.239: INFO: Got endpoints: latency-svc-v54p8 [241.812239ms]
    Jan 18 15:08:27.239: INFO: Created: latency-svc-kznvj
    Jan 18 15:08:27.257: INFO: Got endpoints: latency-svc-kznvj [255.891139ms]
    Jan 18 15:08:27.267: INFO: Created: latency-svc-66hk5
    Jan 18 15:08:27.273: INFO: Created: latency-svc-qxrmb
    Jan 18 15:08:27.280: INFO: Got endpoints: latency-svc-66hk5 [282.768048ms]
    Jan 18 15:08:27.299: INFO: Created: latency-svc-brgzb
    Jan 18 15:08:27.348: INFO: Got endpoints: latency-svc-brgzb [108.907576ms]
    Jan 18 15:08:27.349: INFO: Got endpoints: latency-svc-qxrmb [117.9912ms]
    Jan 18 15:08:27.350: INFO: Created: latency-svc-lg4d9
    Jan 18 15:08:27.375: INFO: Got endpoints: latency-svc-lg4d9 [366.565832ms]
    Jan 18 15:08:27.375: INFO: Created: latency-svc-dskz7
    Jan 18 15:08:27.395: INFO: Got endpoints: latency-svc-dskz7 [301.246566ms]
    Jan 18 15:08:27.635: INFO: Created: latency-svc-wh2dn
    Jan 18 15:08:27.685: INFO: Created: latency-svc-9mcj9
    Jan 18 15:08:27.686: INFO: Created: latency-svc-w7tgk
    Jan 18 15:08:27.686: INFO: Created: latency-svc-hrzl8
    Jan 18 15:08:27.686: INFO: Created: latency-svc-7g8sj
    Jan 18 15:08:27.686: INFO: Created: latency-svc-jz28b
    Jan 18 15:08:27.686: INFO: Created: latency-svc-9kmtj
    Jan 18 15:08:27.686: INFO: Created: latency-svc-vvhnz
    Jan 18 15:08:27.687: INFO: Created: latency-svc-swsv5
    Jan 18 15:08:27.687: INFO: Created: latency-svc-tnrc4
    Jan 18 15:08:27.688: INFO: Created: latency-svc-c5jjc
    Jan 18 15:08:27.688: INFO: Created: latency-svc-mr8n7
    Jan 18 15:08:27.689: INFO: Created: latency-svc-vptc6
    Jan 18 15:08:27.730: INFO: Created: latency-svc-q6zzz
    Jan 18 15:08:27.735: INFO: Created: latency-svc-h5ksw
    Jan 18 15:08:27.824: INFO: Got endpoints: latency-svc-7g8sj [718.731968ms]
    Jan 18 15:08:27.825: INFO: Got endpoints: latency-svc-vptc6 [594.791333ms]
    Jan 18 15:08:27.825: INFO: Got endpoints: latency-svc-wh2dn [730.210657ms]
    Jan 18 15:08:27.825: INFO: Got endpoints: latency-svc-mr8n7 [646.207127ms]
    Jan 18 15:08:27.825: INFO: Got endpoints: latency-svc-vvhnz [678.16796ms]
    Jan 18 15:08:27.910: INFO: Created: latency-svc-77vgm
    Jan 18 15:08:27.949: INFO: Created: latency-svc-prc4m
    Jan 18 15:08:27.967: INFO: Got endpoints: latency-svc-w7tgk [966.587356ms]
    Jan 18 15:08:27.968: INFO: Got endpoints: latency-svc-hrzl8 [710.781101ms]
    Jan 18 15:08:27.968: INFO: Got endpoints: latency-svc-tnrc4 [863.308805ms]
    Jan 18 15:08:27.976: INFO: Got endpoints: latency-svc-jz28b [695.822672ms]
    Jan 18 15:08:27.977: INFO: Got endpoints: latency-svc-c5jjc [756.601858ms]
    Jan 18 15:08:27.990: INFO: Got endpoints: latency-svc-swsv5 [615.028085ms]
    Jan 18 15:08:27.992: INFO: Got endpoints: latency-svc-h5ksw [596.650992ms]
    Jan 18 15:08:27.993: INFO: Got endpoints: latency-svc-q6zzz [899.648627ms]
    Jan 18 15:08:27.995: INFO: Got endpoints: latency-svc-9kmtj [645.759406ms]
    Jan 18 15:08:27.996: INFO: Got endpoints: latency-svc-9mcj9 [647.328897ms]
    Jan 18 15:08:28.044: INFO: Got endpoints: latency-svc-77vgm [219.212686ms]
    Jan 18 15:08:28.045: INFO: Got endpoints: latency-svc-prc4m [218.704177ms]
    Jan 18 15:08:28.067: INFO: Created: latency-svc-c65n5
    Jan 18 15:08:28.087: INFO: Created: latency-svc-flvk5
    Jan 18 15:08:28.087: INFO: Got endpoints: latency-svc-c65n5 [261.803403ms]
    Jan 18 15:08:28.089: INFO: Created: latency-svc-vhwcz
    Jan 18 15:08:28.118: INFO: Created: latency-svc-s522p
    Jan 18 15:08:28.134: INFO: Created: latency-svc-vx5gr
    Jan 18 15:08:28.142: INFO: Got endpoints: latency-svc-vhwcz [316.691642ms]
    Jan 18 15:08:28.143: INFO: Got endpoints: latency-svc-flvk5 [316.947501ms]
    Jan 18 15:08:28.159: INFO: Created: latency-svc-6mzwx
    Jan 18 15:08:28.160: INFO: Got endpoints: latency-svc-s522p [192.639409ms]
    Jan 18 15:08:28.192: INFO: Created: latency-svc-gtsrw
    Jan 18 15:08:28.195: INFO: Got endpoints: latency-svc-vx5gr [226.817641ms]
    Jan 18 15:08:28.217: INFO: Got endpoints: latency-svc-6mzwx [249.011181ms]
    Jan 18 15:08:28.246: INFO: Got endpoints: latency-svc-gtsrw [268.853003ms]
    Jan 18 15:08:28.254: INFO: Created: latency-svc-dhkqd
    Jan 18 15:08:28.273: INFO: Created: latency-svc-rkvdm
    Jan 18 15:08:28.285: INFO: Got endpoints: latency-svc-dhkqd [308.290408ms]
    Jan 18 15:08:28.293: INFO: Created: latency-svc-sl6dh
    Jan 18 15:08:28.318: INFO: Got endpoints: latency-svc-rkvdm [327.790915ms]
    Jan 18 15:08:28.326: INFO: Got endpoints: latency-svc-sl6dh [334.5283ms]
    Jan 18 15:08:28.577: INFO: Created: latency-svc-qgvks
    Jan 18 15:08:28.578: INFO: Created: latency-svc-mzlvw
    Jan 18 15:08:28.578: INFO: Created: latency-svc-5jkrv
    Jan 18 15:08:28.579: INFO: Created: latency-svc-46hhb
    Jan 18 15:08:28.579: INFO: Created: latency-svc-k2x6s
    Jan 18 15:08:28.579: INFO: Created: latency-svc-6289w
    Jan 18 15:08:28.579: INFO: Created: latency-svc-v8dbj
    Jan 18 15:08:28.579: INFO: Created: latency-svc-6h7mg
    Jan 18 15:08:28.580: INFO: Created: latency-svc-v78qj
    Jan 18 15:08:28.578: INFO: Created: latency-svc-ttf7d
    Jan 18 15:08:28.581: INFO: Created: latency-svc-hng2s
    Jan 18 15:08:28.583: INFO: Created: latency-svc-s2lg6
    Jan 18 15:08:28.583: INFO: Created: latency-svc-m9m6d
    Jan 18 15:08:28.585: INFO: Created: latency-svc-p4djq
    Jan 18 15:08:28.585: INFO: Created: latency-svc-wk4t2
    Jan 18 15:08:28.675: INFO: Got endpoints: latency-svc-v78qj [679.102573ms]
    Jan 18 15:08:28.678: INFO: Got endpoints: latency-svc-5jkrv [351.430323ms]
    Jan 18 15:08:28.678: INFO: Got endpoints: latency-svc-m9m6d [358.874018ms]
    Jan 18 15:08:28.712: INFO: Got endpoints: latency-svc-mzlvw [717.564502ms]
    Jan 18 15:08:28.713: INFO: Got endpoints: latency-svc-46hhb [719.391078ms]
    Jan 18 15:08:28.751: INFO: Got endpoints: latency-svc-k2x6s [705.258029ms]
    Jan 18 15:08:28.751: INFO: Got endpoints: latency-svc-ttf7d [707.33411ms]
    Jan 18 15:08:28.752: INFO: Got endpoints: latency-svc-6h7mg [664.581824ms]
    Jan 18 15:08:28.769: INFO: Got endpoints: latency-svc-v8dbj [626.904259ms]
    Jan 18 15:08:28.772: INFO: Got endpoints: latency-svc-wk4t2 [486.788161ms]
    Jan 18 15:08:28.773: INFO: Created: latency-svc-rqwdx
    Jan 18 15:08:28.798: INFO: Got endpoints: latency-svc-hng2s [580.478065ms]
    Jan 18 15:08:28.807: INFO: Created: latency-svc-hznb2
    Jan 18 15:08:28.807: INFO: Got endpoints: latency-svc-qgvks [664.344727ms]
    Jan 18 15:08:28.807: INFO: Got endpoints: latency-svc-s2lg6 [561.58315ms]
    Jan 18 15:08:28.808: INFO: Got endpoints: latency-svc-6289w [612.471963ms]
    Jan 18 15:08:28.808: INFO: Got endpoints: latency-svc-p4djq [647.773531ms]
    Jan 18 15:08:28.856: INFO: Got endpoints: latency-svc-hznb2 [176.353333ms]
    Jan 18 15:08:28.856: INFO: Got endpoints: latency-svc-rqwdx [177.753208ms]
    Jan 18 15:08:28.985: INFO: Created: latency-svc-tbxn4
    Jan 18 15:08:28.997: INFO: Created: latency-svc-w4j5h
    Jan 18 15:08:28.998: INFO: Created: latency-svc-4t8l4
    Jan 18 15:08:28.999: INFO: Created: latency-svc-fxlhm
    Jan 18 15:08:29.000: INFO: Created: latency-svc-tc7bx
    Jan 18 15:08:29.001: INFO: Created: latency-svc-sbgkw
    Jan 18 15:08:29.001: INFO: Created: latency-svc-zqn8g
    Jan 18 15:08:29.001: INFO: Created: latency-svc-9pmjt
    Jan 18 15:08:29.001: INFO: Created: latency-svc-ts54r
    Jan 18 15:08:29.001: INFO: Created: latency-svc-k5s5w
    Jan 18 15:08:29.002: INFO: Created: latency-svc-lcbxr
    Jan 18 15:08:29.002: INFO: Created: latency-svc-5nllg
    Jan 18 15:08:29.004: INFO: Created: latency-svc-rgm9q
    Jan 18 15:08:29.004: INFO: Created: latency-svc-f8k8l
    Jan 18 15:08:29.005: INFO: Created: latency-svc-fnqjc
    Jan 18 15:08:29.008: INFO: Got endpoints: latency-svc-tbxn4 [236.66532ms]
    Jan 18 15:08:29.009: INFO: Got endpoints: latency-svc-f8k8l [257.840473ms]
    Jan 18 15:08:29.118: INFO: Got endpoints: latency-svc-4t8l4 [310.259089ms]
    Jan 18 15:08:29.118: INFO: Got endpoints: latency-svc-5nllg [366.761314ms]
    Jan 18 15:08:29.119: INFO: Got endpoints: latency-svc-tc7bx [311.71948ms]
    Jan 18 15:08:29.120: INFO: Got endpoints: latency-svc-fxlhm [312.096328ms]
    Jan 18 15:08:29.120: INFO: Got endpoints: latency-svc-w4j5h [322.16463ms]
    Jan 18 15:08:29.142: INFO: Got endpoints: latency-svc-sbgkw [334.216717ms]
    Jan 18 15:08:29.142: INFO: Created: latency-svc-mfs8v
    Jan 18 15:08:29.149: INFO: Created: latency-svc-p4zhh
    Jan 18 15:08:29.157: INFO: Created: latency-svc-phgtp
    Jan 18 15:08:29.196: INFO: Got endpoints: latency-svc-ts54r [483.393934ms]
    Jan 18 15:08:29.222: INFO: Created: latency-svc-c4pn8
    Jan 18 15:08:29.223: INFO: Created: latency-svc-p6rk5
    Jan 18 15:08:29.223: INFO: Created: latency-svc-b9jnd
    Jan 18 15:08:29.231: INFO: Created: latency-svc-ph6b6
    Jan 18 15:08:29.239: INFO: Created: latency-svc-tg2pg
    Jan 18 15:08:29.246: INFO: Got endpoints: latency-svc-fnqjc [533.075479ms]
    Jan 18 15:08:29.251: INFO: Created: latency-svc-9qx59
    Jan 18 15:08:29.267: INFO: Created: latency-svc-pzm8f
    Jan 18 15:08:29.294: INFO: Got endpoints: latency-svc-zqn8g [524.031038ms]
    Jan 18 15:08:29.324: INFO: Created: latency-svc-7wl44
    Jan 18 15:08:29.335: INFO: Got endpoints: latency-svc-lcbxr [479.072308ms]
    Jan 18 15:08:29.349: INFO: Created: latency-svc-r2cb6
    Jan 18 15:08:29.393: INFO: Got endpoints: latency-svc-k5s5w [713.100813ms]
    Jan 18 15:08:29.417: INFO: Created: latency-svc-kvcw6
    Jan 18 15:08:29.438: INFO: Got endpoints: latency-svc-9pmjt [581.758051ms]
    Jan 18 15:08:29.452: INFO: Created: latency-svc-49t7v
    Jan 18 15:08:29.490: INFO: Got endpoints: latency-svc-rgm9q [738.4812ms]
    Jan 18 15:08:29.511: INFO: Created: latency-svc-k8ms4
    Jan 18 15:08:29.547: INFO: Got endpoints: latency-svc-mfs8v [538.12009ms]
    Jan 18 15:08:29.560: INFO: Created: latency-svc-lvq6t
    Jan 18 15:08:29.597: INFO: Got endpoints: latency-svc-p4zhh [587.929861ms]
    Jan 18 15:08:29.616: INFO: Created: latency-svc-kh7g5
    Jan 18 15:08:29.646: INFO: Got endpoints: latency-svc-phgtp [526.182608ms]
    Jan 18 15:08:29.697: INFO: Created: latency-svc-ff694
    Jan 18 15:08:29.706: INFO: Got endpoints: latency-svc-b9jnd [588.461135ms]
    Jan 18 15:08:29.731: INFO: Created: latency-svc-82xjj
    Jan 18 15:08:29.736: INFO: Got endpoints: latency-svc-c4pn8 [617.295086ms]
    Jan 18 15:08:29.751: INFO: Created: latency-svc-fswrl
    Jan 18 15:08:29.787: INFO: Got endpoints: latency-svc-p6rk5 [667.671488ms]
    Jan 18 15:08:29.805: INFO: Created: latency-svc-hhqcv
    Jan 18 15:08:29.840: INFO: Got endpoints: latency-svc-ph6b6 [720.327387ms]
    Jan 18 15:08:29.865: INFO: Created: latency-svc-lfb9w
    Jan 18 15:08:29.903: INFO: Got endpoints: latency-svc-tg2pg [760.902285ms]
    Jan 18 15:08:29.936: INFO: Created: latency-svc-8vjw6
    Jan 18 15:08:29.943: INFO: Got endpoints: latency-svc-9qx59 [746.680414ms]
    Jan 18 15:08:29.981: INFO: Created: latency-svc-gn86s
    Jan 18 15:08:30.007: INFO: Got endpoints: latency-svc-pzm8f [760.965614ms]
    Jan 18 15:08:30.025: INFO: Created: latency-svc-w5pd7
    Jan 18 15:08:30.048: INFO: Got endpoints: latency-svc-7wl44 [754.164475ms]
    Jan 18 15:08:30.069: INFO: Created: latency-svc-whkmn
    Jan 18 15:08:30.091: INFO: Got endpoints: latency-svc-r2cb6 [755.825589ms]
    Jan 18 15:08:30.109: INFO: Created: latency-svc-9d265
    Jan 18 15:08:30.137: INFO: Got endpoints: latency-svc-kvcw6 [744.119142ms]
    Jan 18 15:08:30.166: INFO: Created: latency-svc-w466n
    Jan 18 15:08:30.188: INFO: Got endpoints: latency-svc-49t7v [749.773872ms]
    Jan 18 15:08:30.204: INFO: Created: latency-svc-8xgn6
    Jan 18 15:08:30.233: INFO: Got endpoints: latency-svc-k8ms4 [743.256458ms]
    Jan 18 15:08:30.256: INFO: Created: latency-svc-92xrx
    Jan 18 15:08:30.286: INFO: Got endpoints: latency-svc-lvq6t [739.690252ms]
    Jan 18 15:08:30.302: INFO: Created: latency-svc-d69c7
    Jan 18 15:08:30.341: INFO: Got endpoints: latency-svc-kh7g5 [744.448618ms]
    Jan 18 15:08:30.360: INFO: Created: latency-svc-jwtv4
    Jan 18 15:08:30.386: INFO: Got endpoints: latency-svc-ff694 [740.390767ms]
    Jan 18 15:08:30.416: INFO: Created: latency-svc-ljss9
    Jan 18 15:08:30.434: INFO: Got endpoints: latency-svc-82xjj [728.030404ms]
    Jan 18 15:08:30.453: INFO: Created: latency-svc-7jtg6
    Jan 18 15:08:30.489: INFO: Got endpoints: latency-svc-fswrl [752.795691ms]
    Jan 18 15:08:30.508: INFO: Created: latency-svc-48gj5
    Jan 18 15:08:30.541: INFO: Got endpoints: latency-svc-hhqcv [753.836516ms]
    Jan 18 15:08:30.581: INFO: Created: latency-svc-rmmbr
    Jan 18 15:08:30.596: INFO: Got endpoints: latency-svc-lfb9w [755.718137ms]
    Jan 18 15:08:30.611: INFO: Created: latency-svc-z7vbj
    Jan 18 15:08:30.634: INFO: Got endpoints: latency-svc-8vjw6 [731.781096ms]
    Jan 18 15:08:30.652: INFO: Created: latency-svc-xbtsq
    Jan 18 15:08:30.693: INFO: Got endpoints: latency-svc-gn86s [750.14901ms]
    Jan 18 15:08:30.712: INFO: Created: latency-svc-4s47b
    Jan 18 15:08:30.739: INFO: Got endpoints: latency-svc-w5pd7 [732.457315ms]
    Jan 18 15:08:30.753: INFO: Created: latency-svc-qrckt
    Jan 18 15:08:30.791: INFO: Got endpoints: latency-svc-whkmn [743.494824ms]
    Jan 18 15:08:30.819: INFO: Created: latency-svc-xvqf2
    Jan 18 15:08:30.842: INFO: Got endpoints: latency-svc-9d265 [751.076008ms]
    Jan 18 15:08:30.862: INFO: Created: latency-svc-fcwf9
    Jan 18 15:08:30.888: INFO: Got endpoints: latency-svc-w466n [750.893468ms]
    Jan 18 15:08:30.907: INFO: Created: latency-svc-fhhn8
    Jan 18 15:08:30.936: INFO: Got endpoints: latency-svc-8xgn6 [748.176899ms]
    Jan 18 15:08:30.952: INFO: Created: latency-svc-hgtrx
    Jan 18 15:08:30.999: INFO: Got endpoints: latency-svc-92xrx [765.657379ms]
    Jan 18 15:08:31.023: INFO: Created: latency-svc-645kb
    Jan 18 15:08:31.037: INFO: Got endpoints: latency-svc-d69c7 [749.991216ms]
    Jan 18 15:08:31.053: INFO: Created: latency-svc-57lfn
    Jan 18 15:08:31.085: INFO: Got endpoints: latency-svc-jwtv4 [743.196627ms]
    Jan 18 15:08:31.114: INFO: Created: latency-svc-x7gnw
    Jan 18 15:08:31.145: INFO: Got endpoints: latency-svc-ljss9 [758.641157ms]
    Jan 18 15:08:31.182: INFO: Created: latency-svc-878tq
    Jan 18 15:08:31.194: INFO: Got endpoints: latency-svc-7jtg6 [759.15093ms]
    Jan 18 15:08:31.212: INFO: Created: latency-svc-4dbt9
    Jan 18 15:08:31.237: INFO: Got endpoints: latency-svc-48gj5 [748.170198ms]
    Jan 18 15:08:31.248: INFO: Created: latency-svc-qjszv
    Jan 18 15:08:31.303: INFO: Got endpoints: latency-svc-rmmbr [761.209327ms]
    Jan 18 15:08:31.318: INFO: Created: latency-svc-7qdgg
    Jan 18 15:08:31.336: INFO: Got endpoints: latency-svc-z7vbj [739.429924ms]
    Jan 18 15:08:31.355: INFO: Created: latency-svc-q8nbf
    Jan 18 15:08:31.397: INFO: Got endpoints: latency-svc-xbtsq [762.066686ms]
    Jan 18 15:08:31.418: INFO: Created: latency-svc-wrwqw
    Jan 18 15:08:31.444: INFO: Got endpoints: latency-svc-4s47b [750.32742ms]
    Jan 18 15:08:31.462: INFO: Created: latency-svc-5hh5j
    Jan 18 15:08:31.485: INFO: Got endpoints: latency-svc-qrckt [745.680351ms]
    Jan 18 15:08:31.516: INFO: Created: latency-svc-6nhzt
    Jan 18 15:08:31.542: INFO: Got endpoints: latency-svc-xvqf2 [750.276154ms]
    Jan 18 15:08:31.563: INFO: Created: latency-svc-hlqr4
    Jan 18 15:08:31.586: INFO: Got endpoints: latency-svc-fcwf9 [744.046207ms]
    Jan 18 15:08:31.606: INFO: Created: latency-svc-j7bfs
    Jan 18 15:08:31.634: INFO: Got endpoints: latency-svc-fhhn8 [745.091702ms]
    Jan 18 15:08:31.651: INFO: Created: latency-svc-jv7hb
    Jan 18 15:08:31.688: INFO: Got endpoints: latency-svc-hgtrx [751.539254ms]
    Jan 18 15:08:31.704: INFO: Created: latency-svc-56vtm
    Jan 18 15:08:31.743: INFO: Got endpoints: latency-svc-645kb [743.842132ms]
    Jan 18 15:08:31.755: INFO: Created: latency-svc-dvkk8
    Jan 18 15:08:31.792: INFO: Got endpoints: latency-svc-57lfn [755.828951ms]
    Jan 18 15:08:31.815: INFO: Created: latency-svc-7h9vc
    Jan 18 15:08:31.841: INFO: Got endpoints: latency-svc-x7gnw [756.215758ms]
    Jan 18 15:08:31.868: INFO: Created: latency-svc-rk6mn
    Jan 18 15:08:31.893: INFO: Got endpoints: latency-svc-878tq [747.284199ms]
    Jan 18 15:08:31.910: INFO: Created: latency-svc-5kf8s
    Jan 18 15:08:31.938: INFO: Got endpoints: latency-svc-4dbt9 [742.404154ms]
    Jan 18 15:08:31.952: INFO: Created: latency-svc-2bpbn
    Jan 18 15:08:31.991: INFO: Got endpoints: latency-svc-qjszv [754.511545ms]
    Jan 18 15:08:32.013: INFO: Created: latency-svc-4fnb5
    Jan 18 15:08:32.040: INFO: Got endpoints: latency-svc-7qdgg [737.366963ms]
    Jan 18 15:08:32.051: INFO: Created: latency-svc-pwhsd
    Jan 18 15:08:32.087: INFO: Got endpoints: latency-svc-q8nbf [750.494014ms]
    Jan 18 15:08:32.119: INFO: Created: latency-svc-hrvs8
    Jan 18 15:08:32.134: INFO: Got endpoints: latency-svc-wrwqw [737.471849ms]
    Jan 18 15:08:32.158: INFO: Created: latency-svc-w89q9
    Jan 18 15:08:32.190: INFO: Got endpoints: latency-svc-5hh5j [746.417305ms]
    Jan 18 15:08:32.230: INFO: Created: latency-svc-vhljj
    Jan 18 15:08:32.243: INFO: Got endpoints: latency-svc-6nhzt [758.088852ms]
    Jan 18 15:08:32.258: INFO: Created: latency-svc-s7xjt
    Jan 18 15:08:32.294: INFO: Got endpoints: latency-svc-hlqr4 [752.07341ms]
    Jan 18 15:08:32.319: INFO: Created: latency-svc-dg4mz
    Jan 18 15:08:32.337: INFO: Got endpoints: latency-svc-j7bfs [750.61989ms]
    Jan 18 15:08:32.355: INFO: Created: latency-svc-zj6zv
    Jan 18 15:08:32.392: INFO: Got endpoints: latency-svc-jv7hb [758.459843ms]
    Jan 18 15:08:32.412: INFO: Created: latency-svc-972wt
    Jan 18 15:08:32.440: INFO: Got endpoints: latency-svc-56vtm [751.617659ms]
    Jan 18 15:08:32.458: INFO: Created: latency-svc-p948k
    Jan 18 15:08:32.492: INFO: Got endpoints: latency-svc-dvkk8 [749.005464ms]
    Jan 18 15:08:32.505: INFO: Created: latency-svc-tnkd9
    Jan 18 15:08:32.542: INFO: Got endpoints: latency-svc-7h9vc [749.450737ms]
    Jan 18 15:08:32.553: INFO: Created: latency-svc-v5sbz
    Jan 18 15:08:32.588: INFO: Got endpoints: latency-svc-rk6mn [746.948871ms]
    Jan 18 15:08:32.600: INFO: Created: latency-svc-qtfkk
    Jan 18 15:08:32.650: INFO: Got endpoints: latency-svc-5kf8s [756.772094ms]
    Jan 18 15:08:32.664: INFO: Created: latency-svc-wsk98
    Jan 18 15:08:32.688: INFO: Got endpoints: latency-svc-2bpbn [749.666759ms]
    Jan 18 15:08:32.705: INFO: Created: latency-svc-wfqht
    Jan 18 15:08:32.736: INFO: Got endpoints: latency-svc-4fnb5 [744.616053ms]
    Jan 18 15:08:32.754: INFO: Created: latency-svc-8qfrl
    Jan 18 15:08:32.790: INFO: Got endpoints: latency-svc-pwhsd [749.734051ms]
    Jan 18 15:08:33.019: INFO: Got endpoints: latency-svc-s7xjt [775.301988ms]
    Jan 18 15:08:33.019: INFO: Got endpoints: latency-svc-w89q9 [884.218844ms]
    Jan 18 15:08:33.019: INFO: Got endpoints: latency-svc-hrvs8 [931.708661ms]
    Jan 18 15:08:33.019: INFO: Got endpoints: latency-svc-vhljj [828.465581ms]
    Jan 18 15:08:33.053: INFO: Got endpoints: latency-svc-dg4mz [757.972522ms]
    Jan 18 15:08:33.053: INFO: Created: latency-svc-5ggdd
    Jan 18 15:08:33.057: INFO: Created: latency-svc-h8w57
    Jan 18 15:08:33.067: INFO: Created: latency-svc-zhjpm
    Jan 18 15:08:33.077: INFO: Created: latency-svc-ntvqp
    Jan 18 15:08:33.083: INFO: Created: latency-svc-ckrlp
    Jan 18 15:08:33.093: INFO: Got endpoints: latency-svc-zj6zv [755.252178ms]
    Jan 18 15:08:33.101: INFO: Created: latency-svc-rjw8w
    Jan 18 15:08:33.127: INFO: Created: latency-svc-l8g5n
    Jan 18 15:08:33.137: INFO: Got endpoints: latency-svc-972wt [745.119377ms]
    Jan 18 15:08:33.155: INFO: Created: latency-svc-wxrkp
    Jan 18 15:08:33.184: INFO: Got endpoints: latency-svc-p948k [744.448408ms]
    Jan 18 15:08:33.197: INFO: Created: latency-svc-62cdt
    Jan 18 15:08:33.237: INFO: Got endpoints: latency-svc-tnkd9 [744.53683ms]
    Jan 18 15:08:33.254: INFO: Created: latency-svc-25qcv
    Jan 18 15:08:33.289: INFO: Got endpoints: latency-svc-v5sbz [747.451091ms]
    Jan 18 15:08:33.306: INFO: Created: latency-svc-q2qf9
    Jan 18 15:08:33.343: INFO: Got endpoints: latency-svc-qtfkk [754.425789ms]
    Jan 18 15:08:33.352: INFO: Created: latency-svc-phdvw
    Jan 18 15:08:33.399: INFO: Got endpoints: latency-svc-wsk98 [748.360826ms]
    Jan 18 15:08:33.422: INFO: Created: latency-svc-vvmb4
    Jan 18 15:08:33.443: INFO: Got endpoints: latency-svc-wfqht [755.383101ms]
    Jan 18 15:08:33.464: INFO: Created: latency-svc-v45rr
    Jan 18 15:08:33.494: INFO: Got endpoints: latency-svc-8qfrl [757.904723ms]
    Jan 18 15:08:33.514: INFO: Created: latency-svc-p88x5
    Jan 18 15:08:33.779: INFO: Got endpoints: latency-svc-h8w57 [759.067895ms]
    Jan 18 15:08:33.779: INFO: Got endpoints: latency-svc-ckrlp [758.565625ms]
    Jan 18 15:08:33.781: INFO: Got endpoints: latency-svc-zhjpm [760.323861ms]
    Jan 18 15:08:33.781: INFO: Got endpoints: latency-svc-ntvqp [760.229954ms]
    Jan 18 15:08:33.781: INFO: Got endpoints: latency-svc-5ggdd [988.523182ms]
    Jan 18 15:08:33.795: INFO: Got endpoints: latency-svc-rjw8w [742.743326ms]
    Jan 18 15:08:33.804: INFO: Created: latency-svc-2wvvf
    Jan 18 15:08:33.831: INFO: Created: latency-svc-d8xn5
    Jan 18 15:08:33.844: INFO: Created: latency-svc-7q89f
    Jan 18 15:08:33.860: INFO: Got endpoints: latency-svc-l8g5n [766.816814ms]
    Jan 18 15:08:33.872: INFO: Created: latency-svc-m5r24
    Jan 18 15:08:33.896: INFO: Got endpoints: latency-svc-wxrkp [758.20819ms]
    Jan 18 15:08:33.902: INFO: Created: latency-svc-smsv9
    Jan 18 15:08:33.903: INFO: Created: latency-svc-mvwrk
    Jan 18 15:08:33.912: INFO: Created: latency-svc-bl27j
    Jan 18 15:08:33.939: INFO: Created: latency-svc-5g2l8
    Jan 18 15:08:33.946: INFO: Got endpoints: latency-svc-62cdt [761.257205ms]
    Jan 18 15:08:33.963: INFO: Created: latency-svc-kj954
    Jan 18 15:08:33.995: INFO: Got endpoints: latency-svc-25qcv [757.926675ms]
    Jan 18 15:08:34.014: INFO: Created: latency-svc-8bpq8
    Jan 18 15:08:34.036: INFO: Got endpoints: latency-svc-q2qf9 [746.543931ms]
    Jan 18 15:08:34.062: INFO: Created: latency-svc-hq77c
    Jan 18 15:08:34.085: INFO: Got endpoints: latency-svc-phdvw [741.934464ms]
    Jan 18 15:08:34.107: INFO: Created: latency-svc-pbpn6
    Jan 18 15:08:34.142: INFO: Got endpoints: latency-svc-vvmb4 [737.763658ms]
    Jan 18 15:08:34.169: INFO: Created: latency-svc-m5fvn
    Jan 18 15:08:34.190: INFO: Got endpoints: latency-svc-v45rr [746.975307ms]
    Jan 18 15:08:34.214: INFO: Created: latency-svc-gz9d7
    Jan 18 15:08:34.242: INFO: Got endpoints: latency-svc-p88x5 [747.877231ms]
    Jan 18 15:08:34.287: INFO: Got endpoints: latency-svc-2wvvf [507.753827ms]
    Jan 18 15:08:34.342: INFO: Got endpoints: latency-svc-d8xn5 [561.458168ms]
    Jan 18 15:08:34.403: INFO: Got endpoints: latency-svc-7q89f [624.003438ms]
    Jan 18 15:08:34.434: INFO: Got endpoints: latency-svc-m5r24 [653.555569ms]
    Jan 18 15:08:34.489: INFO: Got endpoints: latency-svc-mvwrk [707.404666ms]
    Jan 18 15:08:34.554: INFO: Got endpoints: latency-svc-smsv9 [758.734322ms]
    Jan 18 15:08:34.595: INFO: Got endpoints: latency-svc-bl27j [735.101548ms]
    Jan 18 15:08:34.660: INFO: Got endpoints: latency-svc-5g2l8 [764.545768ms]
    Jan 18 15:08:34.687: INFO: Got endpoints: latency-svc-kj954 [740.772155ms]
    Jan 18 15:08:34.742: INFO: Got endpoints: latency-svc-8bpq8 [747.135045ms]
    Jan 18 15:08:34.789: INFO: Got endpoints: latency-svc-hq77c [752.192906ms]
    Jan 18 15:08:34.845: INFO: Got endpoints: latency-svc-pbpn6 [760.23543ms]
    Jan 18 15:08:34.894: INFO: Got endpoints: latency-svc-m5fvn [751.395013ms]
    Jan 18 15:08:34.936: INFO: Got endpoints: latency-svc-gz9d7 [746.086888ms]
    Jan 18 15:08:34.937: INFO: Latencies: [108.907576ms 117.9912ms 176.353333ms 177.753208ms 192.639409ms 218.704177ms 219.212686ms 226.817641ms 236.66532ms 241.812239ms 249.011181ms 255.891139ms 257.840473ms 259.47152ms 261.803403ms 268.853003ms 282.768048ms 287.151707ms 301.246566ms 302.390664ms 308.290408ms 310.259089ms 311.71948ms 312.096328ms 316.691642ms 316.947501ms 319.531482ms 322.16463ms 323.573252ms 327.722164ms 327.790915ms 329.123978ms 334.216717ms 334.5283ms 351.430323ms 358.874018ms 366.565832ms 366.761314ms 369.301029ms 400.820893ms 409.929971ms 437.947665ms 450.056562ms 479.072308ms 483.393934ms 486.788161ms 507.753827ms 524.031038ms 526.182608ms 526.485233ms 533.075479ms 538.12009ms 561.458168ms 561.58315ms 579.010539ms 580.478065ms 581.758051ms 587.929861ms 588.461135ms 594.791333ms 596.650992ms 604.95317ms 608.102428ms 608.387114ms 611.005031ms 612.471963ms 615.028085ms 617.295086ms 624.003438ms 626.904259ms 645.759406ms 646.207127ms 647.328897ms 647.773531ms 653.555569ms 664.344727ms 664.581824ms 667.671488ms 678.16796ms 679.102573ms 695.822672ms 705.258029ms 707.33411ms 707.404666ms 710.781101ms 713.100813ms 717.564502ms 718.731968ms 719.391078ms 720.327387ms 728.030404ms 730.210657ms 731.781096ms 732.457315ms 735.101548ms 737.366963ms 737.471849ms 737.763658ms 738.4812ms 739.429924ms 739.690252ms 740.390767ms 740.772155ms 741.934464ms 742.404154ms 742.743326ms 743.196627ms 743.256458ms 743.494824ms 743.842132ms 744.046207ms 744.119142ms 744.448408ms 744.448618ms 744.53683ms 744.616053ms 745.091702ms 745.119377ms 745.680351ms 746.086888ms 746.417305ms 746.543931ms 746.680414ms 746.948871ms 746.975307ms 747.135045ms 747.284199ms 747.451091ms 747.877231ms 748.170198ms 748.176899ms 748.360826ms 749.005464ms 749.450737ms 749.666759ms 749.734051ms 749.773872ms 749.991216ms 750.14901ms 750.276154ms 750.32742ms 750.494014ms 750.61989ms 750.893468ms 751.076008ms 751.395013ms 751.539254ms 751.617659ms 752.07341ms 752.192906ms 752.795691ms 753.836516ms 754.164475ms 754.425789ms 754.511545ms 755.252178ms 755.383101ms 755.718137ms 755.825589ms 755.828951ms 756.215758ms 756.601858ms 756.772094ms 757.904723ms 757.926675ms 757.972522ms 758.088852ms 758.20819ms 758.459843ms 758.565625ms 758.641157ms 758.734322ms 759.067895ms 759.15093ms 760.229954ms 760.23543ms 760.323861ms 760.902285ms 760.965614ms 761.209327ms 761.257205ms 762.066686ms 764.545768ms 765.657379ms 766.816814ms 775.301988ms 826.508928ms 826.858825ms 828.465581ms 829.090732ms 863.308805ms 884.218844ms 899.648627ms 922.886909ms 923.882297ms 923.979403ms 931.708661ms 933.956542ms 966.587356ms 988.523182ms]
    Jan 18 15:08:34.938: INFO: 50 %ile: 739.690252ms
    Jan 18 15:08:34.938: INFO: 90 %ile: 761.257205ms
    Jan 18 15:08:34.938: INFO: 99 %ile: 966.587356ms
    Jan 18 15:08:34.938: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Jan 18 15:08:34.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-1591" for this suite. 01/18/23 15:08:34.958
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:08:34.983
Jan 18 15:08:34.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename disruption 01/18/23 15:08:34.988
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:08:35.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:08:35.033
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 01/18/23 15:08:35.05
STEP: Updating PodDisruptionBudget status 01/18/23 15:08:37.067
STEP: Waiting for all pods to be running 01/18/23 15:08:37.084
Jan 18 15:08:37.097: INFO: running pods: 0 < 1
STEP: locating a running pod 01/18/23 15:08:39.107
STEP: Waiting for the pdb to be processed 01/18/23 15:08:39.123
STEP: Patching PodDisruptionBudget status 01/18/23 15:08:39.139
STEP: Waiting for the pdb to be processed 01/18/23 15:08:39.159
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 18 15:08:39.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3677" for this suite. 01/18/23 15:08:39.18
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":16,"skipped":410,"failed":0}
------------------------------
• [4.210 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:08:34.983
    Jan 18 15:08:34.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename disruption 01/18/23 15:08:34.988
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:08:35.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:08:35.033
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 01/18/23 15:08:35.05
    STEP: Updating PodDisruptionBudget status 01/18/23 15:08:37.067
    STEP: Waiting for all pods to be running 01/18/23 15:08:37.084
    Jan 18 15:08:37.097: INFO: running pods: 0 < 1
    STEP: locating a running pod 01/18/23 15:08:39.107
    STEP: Waiting for the pdb to be processed 01/18/23 15:08:39.123
    STEP: Patching PodDisruptionBudget status 01/18/23 15:08:39.139
    STEP: Waiting for the pdb to be processed 01/18/23 15:08:39.159
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 18 15:08:39.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-3677" for this suite. 01/18/23 15:08:39.18
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:08:39.242
Jan 18 15:08:39.242: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename pods 01/18/23 15:08:39.243
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:08:39.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:08:39.283
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Jan 18 15:08:39.323: INFO: Waiting up to 5m0s for pod "server-envvars-953a468e-dbf8-4ce3-bf6f-cdf0e4488cae" in namespace "pods-3322" to be "running and ready"
Jan 18 15:08:39.382: INFO: Pod "server-envvars-953a468e-dbf8-4ce3-bf6f-cdf0e4488cae": Phase="Pending", Reason="", readiness=false. Elapsed: 59.116616ms
Jan 18 15:08:39.382: INFO: The phase of Pod server-envvars-953a468e-dbf8-4ce3-bf6f-cdf0e4488cae is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:08:41.528: INFO: Pod "server-envvars-953a468e-dbf8-4ce3-bf6f-cdf0e4488cae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.205166041s
Jan 18 15:08:41.528: INFO: The phase of Pod server-envvars-953a468e-dbf8-4ce3-bf6f-cdf0e4488cae is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:08:43.390: INFO: Pod "server-envvars-953a468e-dbf8-4ce3-bf6f-cdf0e4488cae": Phase="Running", Reason="", readiness=true. Elapsed: 4.067259909s
Jan 18 15:08:43.390: INFO: The phase of Pod server-envvars-953a468e-dbf8-4ce3-bf6f-cdf0e4488cae is Running (Ready = true)
Jan 18 15:08:43.390: INFO: Pod "server-envvars-953a468e-dbf8-4ce3-bf6f-cdf0e4488cae" satisfied condition "running and ready"
Jan 18 15:08:43.428: INFO: Waiting up to 5m0s for pod "client-envvars-6414685c-c649-4ee8-bbda-5e2d6ba96404" in namespace "pods-3322" to be "Succeeded or Failed"
Jan 18 15:08:43.480: INFO: Pod "client-envvars-6414685c-c649-4ee8-bbda-5e2d6ba96404": Phase="Pending", Reason="", readiness=false. Elapsed: 52.124514ms
Jan 18 15:08:45.493: INFO: Pod "client-envvars-6414685c-c649-4ee8-bbda-5e2d6ba96404": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065364425s
Jan 18 15:08:47.494: INFO: Pod "client-envvars-6414685c-c649-4ee8-bbda-5e2d6ba96404": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066089114s
STEP: Saw pod success 01/18/23 15:08:47.494
Jan 18 15:08:47.495: INFO: Pod "client-envvars-6414685c-c649-4ee8-bbda-5e2d6ba96404" satisfied condition "Succeeded or Failed"
Jan 18 15:08:47.508: INFO: Trying to get logs from node v1-25-1-18760-w2 pod client-envvars-6414685c-c649-4ee8-bbda-5e2d6ba96404 container env3cont: <nil>
STEP: delete the pod 01/18/23 15:08:47.532
Jan 18 15:08:47.575: INFO: Waiting for pod client-envvars-6414685c-c649-4ee8-bbda-5e2d6ba96404 to disappear
Jan 18 15:08:47.589: INFO: Pod client-envvars-6414685c-c649-4ee8-bbda-5e2d6ba96404 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 15:08:47.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3322" for this suite. 01/18/23 15:08:47.597
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":17,"skipped":453,"failed":0}
------------------------------
• [SLOW TEST] [8.372 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:08:39.242
    Jan 18 15:08:39.242: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename pods 01/18/23 15:08:39.243
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:08:39.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:08:39.283
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Jan 18 15:08:39.323: INFO: Waiting up to 5m0s for pod "server-envvars-953a468e-dbf8-4ce3-bf6f-cdf0e4488cae" in namespace "pods-3322" to be "running and ready"
    Jan 18 15:08:39.382: INFO: Pod "server-envvars-953a468e-dbf8-4ce3-bf6f-cdf0e4488cae": Phase="Pending", Reason="", readiness=false. Elapsed: 59.116616ms
    Jan 18 15:08:39.382: INFO: The phase of Pod server-envvars-953a468e-dbf8-4ce3-bf6f-cdf0e4488cae is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:08:41.528: INFO: Pod "server-envvars-953a468e-dbf8-4ce3-bf6f-cdf0e4488cae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.205166041s
    Jan 18 15:08:41.528: INFO: The phase of Pod server-envvars-953a468e-dbf8-4ce3-bf6f-cdf0e4488cae is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:08:43.390: INFO: Pod "server-envvars-953a468e-dbf8-4ce3-bf6f-cdf0e4488cae": Phase="Running", Reason="", readiness=true. Elapsed: 4.067259909s
    Jan 18 15:08:43.390: INFO: The phase of Pod server-envvars-953a468e-dbf8-4ce3-bf6f-cdf0e4488cae is Running (Ready = true)
    Jan 18 15:08:43.390: INFO: Pod "server-envvars-953a468e-dbf8-4ce3-bf6f-cdf0e4488cae" satisfied condition "running and ready"
    Jan 18 15:08:43.428: INFO: Waiting up to 5m0s for pod "client-envvars-6414685c-c649-4ee8-bbda-5e2d6ba96404" in namespace "pods-3322" to be "Succeeded or Failed"
    Jan 18 15:08:43.480: INFO: Pod "client-envvars-6414685c-c649-4ee8-bbda-5e2d6ba96404": Phase="Pending", Reason="", readiness=false. Elapsed: 52.124514ms
    Jan 18 15:08:45.493: INFO: Pod "client-envvars-6414685c-c649-4ee8-bbda-5e2d6ba96404": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065364425s
    Jan 18 15:08:47.494: INFO: Pod "client-envvars-6414685c-c649-4ee8-bbda-5e2d6ba96404": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066089114s
    STEP: Saw pod success 01/18/23 15:08:47.494
    Jan 18 15:08:47.495: INFO: Pod "client-envvars-6414685c-c649-4ee8-bbda-5e2d6ba96404" satisfied condition "Succeeded or Failed"
    Jan 18 15:08:47.508: INFO: Trying to get logs from node v1-25-1-18760-w2 pod client-envvars-6414685c-c649-4ee8-bbda-5e2d6ba96404 container env3cont: <nil>
    STEP: delete the pod 01/18/23 15:08:47.532
    Jan 18 15:08:47.575: INFO: Waiting for pod client-envvars-6414685c-c649-4ee8-bbda-5e2d6ba96404 to disappear
    Jan 18 15:08:47.589: INFO: Pod client-envvars-6414685c-c649-4ee8-bbda-5e2d6ba96404 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 15:08:47.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3322" for this suite. 01/18/23 15:08:47.597
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:08:47.617
Jan 18 15:08:47.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename services 01/18/23 15:08:47.619
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:08:47.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:08:47.673
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-7192 01/18/23 15:08:47.682
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7192 to expose endpoints map[] 01/18/23 15:08:47.699
Jan 18 15:08:47.706: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jan 18 15:08:48.720: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jan 18 15:08:49.714: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jan 18 15:08:50.710: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jan 18 15:08:51.711: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jan 18 15:08:52.716: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jan 18 15:08:53.716: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jan 18 15:08:54.716: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jan 18 15:08:55.717: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jan 18 15:08:56.720: INFO: successfully validated that service multi-endpoint-test in namespace services-7192 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7192 01/18/23 15:08:56.721
Jan 18 15:08:56.732: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7192" to be "running and ready"
Jan 18 15:08:56.739: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.299834ms
Jan 18 15:08:56.739: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:08:58.745: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.012092186s
Jan 18 15:08:58.745: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 18 15:08:58.745: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7192 to expose endpoints map[pod1:[100]] 01/18/23 15:08:58.749
Jan 18 15:08:58.760: INFO: successfully validated that service multi-endpoint-test in namespace services-7192 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7192 01/18/23 15:08:58.76
Jan 18 15:08:58.766: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7192" to be "running and ready"
Jan 18 15:08:58.779: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.250309ms
Jan 18 15:08:58.780: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:09:00.785: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.018896151s
Jan 18 15:09:00.785: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 18 15:09:00.785: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7192 to expose endpoints map[pod1:[100] pod2:[101]] 01/18/23 15:09:00.788
Jan 18 15:09:00.803: INFO: successfully validated that service multi-endpoint-test in namespace services-7192 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 01/18/23 15:09:00.803
Jan 18 15:09:00.803: INFO: Creating new exec pod
Jan 18 15:09:00.813: INFO: Waiting up to 5m0s for pod "execpod7jw65" in namespace "services-7192" to be "running"
Jan 18 15:09:00.822: INFO: Pod "execpod7jw65": Phase="Pending", Reason="", readiness=false. Elapsed: 9.335277ms
Jan 18 15:09:02.839: INFO: Pod "execpod7jw65": Phase="Running", Reason="", readiness=true. Elapsed: 2.025848101s
Jan 18 15:09:02.839: INFO: Pod "execpod7jw65" satisfied condition "running"
Jan 18 15:09:03.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-7192 exec execpod7jw65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jan 18 15:09:04.130: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan 18 15:09:04.130: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 15:09:04.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-7192 exec execpod7jw65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.58.23 80'
Jan 18 15:09:04.337: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.58.23 80\nConnection to 10.233.58.23 80 port [tcp/http] succeeded!\n"
Jan 18 15:09:04.337: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 15:09:04.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-7192 exec execpod7jw65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jan 18 15:09:04.548: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan 18 15:09:04.548: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 15:09:04.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-7192 exec execpod7jw65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.58.23 81'
Jan 18 15:09:04.765: INFO: stderr: "+ nc -v -t -w 2 10.233.58.23 81\n+ echo hostName\nConnection to 10.233.58.23 81 port [tcp/*] succeeded!\n"
Jan 18 15:09:04.765: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-7192 01/18/23 15:09:04.765
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7192 to expose endpoints map[pod2:[101]] 01/18/23 15:09:04.79
Jan 18 15:09:05.858: INFO: successfully validated that service multi-endpoint-test in namespace services-7192 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7192 01/18/23 15:09:05.858
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7192 to expose endpoints map[] 01/18/23 15:09:05.891
Jan 18 15:09:05.922: INFO: successfully validated that service multi-endpoint-test in namespace services-7192 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 15:09:05.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7192" for this suite. 01/18/23 15:09:05.977
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":18,"skipped":485,"failed":0}
------------------------------
• [SLOW TEST] [18.370 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:08:47.617
    Jan 18 15:08:47.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename services 01/18/23 15:08:47.619
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:08:47.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:08:47.673
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-7192 01/18/23 15:08:47.682
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7192 to expose endpoints map[] 01/18/23 15:08:47.699
    Jan 18 15:08:47.706: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Jan 18 15:08:48.720: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Jan 18 15:08:49.714: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Jan 18 15:08:50.710: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Jan 18 15:08:51.711: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Jan 18 15:08:52.716: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Jan 18 15:08:53.716: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Jan 18 15:08:54.716: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Jan 18 15:08:55.717: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Jan 18 15:08:56.720: INFO: successfully validated that service multi-endpoint-test in namespace services-7192 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-7192 01/18/23 15:08:56.721
    Jan 18 15:08:56.732: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7192" to be "running and ready"
    Jan 18 15:08:56.739: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.299834ms
    Jan 18 15:08:56.739: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:08:58.745: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.012092186s
    Jan 18 15:08:58.745: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 18 15:08:58.745: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7192 to expose endpoints map[pod1:[100]] 01/18/23 15:08:58.749
    Jan 18 15:08:58.760: INFO: successfully validated that service multi-endpoint-test in namespace services-7192 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-7192 01/18/23 15:08:58.76
    Jan 18 15:08:58.766: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7192" to be "running and ready"
    Jan 18 15:08:58.779: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.250309ms
    Jan 18 15:08:58.780: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:09:00.785: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.018896151s
    Jan 18 15:09:00.785: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 18 15:09:00.785: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7192 to expose endpoints map[pod1:[100] pod2:[101]] 01/18/23 15:09:00.788
    Jan 18 15:09:00.803: INFO: successfully validated that service multi-endpoint-test in namespace services-7192 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 01/18/23 15:09:00.803
    Jan 18 15:09:00.803: INFO: Creating new exec pod
    Jan 18 15:09:00.813: INFO: Waiting up to 5m0s for pod "execpod7jw65" in namespace "services-7192" to be "running"
    Jan 18 15:09:00.822: INFO: Pod "execpod7jw65": Phase="Pending", Reason="", readiness=false. Elapsed: 9.335277ms
    Jan 18 15:09:02.839: INFO: Pod "execpod7jw65": Phase="Running", Reason="", readiness=true. Elapsed: 2.025848101s
    Jan 18 15:09:02.839: INFO: Pod "execpod7jw65" satisfied condition "running"
    Jan 18 15:09:03.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-7192 exec execpod7jw65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Jan 18 15:09:04.130: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jan 18 15:09:04.130: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 15:09:04.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-7192 exec execpod7jw65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.58.23 80'
    Jan 18 15:09:04.337: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.58.23 80\nConnection to 10.233.58.23 80 port [tcp/http] succeeded!\n"
    Jan 18 15:09:04.337: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 15:09:04.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-7192 exec execpod7jw65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Jan 18 15:09:04.548: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jan 18 15:09:04.548: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 15:09:04.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-7192 exec execpod7jw65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.58.23 81'
    Jan 18 15:09:04.765: INFO: stderr: "+ nc -v -t -w 2 10.233.58.23 81\n+ echo hostName\nConnection to 10.233.58.23 81 port [tcp/*] succeeded!\n"
    Jan 18 15:09:04.765: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-7192 01/18/23 15:09:04.765
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7192 to expose endpoints map[pod2:[101]] 01/18/23 15:09:04.79
    Jan 18 15:09:05.858: INFO: successfully validated that service multi-endpoint-test in namespace services-7192 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-7192 01/18/23 15:09:05.858
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7192 to expose endpoints map[] 01/18/23 15:09:05.891
    Jan 18 15:09:05.922: INFO: successfully validated that service multi-endpoint-test in namespace services-7192 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 15:09:05.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7192" for this suite. 01/18/23 15:09:05.977
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:09:05.994
Jan 18 15:09:05.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename kubelet-test 01/18/23 15:09:05.997
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:09:06.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:09:06.023
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 18 15:09:10.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1873" for this suite. 01/18/23 15:09:10.059
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":19,"skipped":487,"failed":0}
------------------------------
• [4.072 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:09:05.994
    Jan 18 15:09:05.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename kubelet-test 01/18/23 15:09:05.997
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:09:06.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:09:06.023
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 18 15:09:10.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-1873" for this suite. 01/18/23 15:09:10.059
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:09:10.073
Jan 18 15:09:10.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 15:09:10.074
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:09:10.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:09:10.101
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jan 18 15:09:10.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 15:09:13.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8653" for this suite. 01/18/23 15:09:13.48
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":20,"skipped":502,"failed":0}
------------------------------
• [3.442 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:09:10.073
    Jan 18 15:09:10.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 15:09:10.074
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:09:10.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:09:10.101
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jan 18 15:09:10.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 15:09:13.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-8653" for this suite. 01/18/23 15:09:13.48
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:09:13.518
Jan 18 15:09:13.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename services 01/18/23 15:09:13.523
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:09:13.598
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:09:13.604
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3351 01/18/23 15:09:13.609
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/18/23 15:09:13.631
STEP: creating service externalsvc in namespace services-3351 01/18/23 15:09:13.631
STEP: creating replication controller externalsvc in namespace services-3351 01/18/23 15:09:13.823
I0118 15:09:13.844774      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3351, replica count: 2
I0118 15:09:16.898540      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 01/18/23 15:09:16.903
Jan 18 15:09:16.925: INFO: Creating new exec pod
Jan 18 15:09:16.938: INFO: Waiting up to 5m0s for pod "execpodg4pmm" in namespace "services-3351" to be "running"
Jan 18 15:09:16.945: INFO: Pod "execpodg4pmm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.848221ms
Jan 18 15:09:18.951: INFO: Pod "execpodg4pmm": Phase="Running", Reason="", readiness=true. Elapsed: 2.012944442s
Jan 18 15:09:18.951: INFO: Pod "execpodg4pmm" satisfied condition "running"
Jan 18 15:09:18.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-3351 exec execpodg4pmm -- /bin/sh -x -c nslookup nodeport-service.services-3351.svc.cluster.local'
Jan 18 15:09:19.257: INFO: stderr: "+ nslookup nodeport-service.services-3351.svc.cluster.local\n"
Jan 18 15:09:19.257: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nnodeport-service.services-3351.svc.cluster.local\tcanonical name = externalsvc.services-3351.svc.cluster.local.\nName:\texternalsvc.services-3351.svc.cluster.local\nAddress: 10.233.39.202\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3351, will wait for the garbage collector to delete the pods 01/18/23 15:09:19.257
Jan 18 15:09:19.318: INFO: Deleting ReplicationController externalsvc took: 6.807586ms
Jan 18 15:09:19.420: INFO: Terminating ReplicationController externalsvc pods took: 101.820009ms
Jan 18 15:09:22.752: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 15:09:22.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3351" for this suite. 01/18/23 15:09:22.782
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":21,"skipped":512,"failed":0}
------------------------------
• [SLOW TEST] [9.291 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:09:13.518
    Jan 18 15:09:13.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename services 01/18/23 15:09:13.523
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:09:13.598
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:09:13.604
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-3351 01/18/23 15:09:13.609
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/18/23 15:09:13.631
    STEP: creating service externalsvc in namespace services-3351 01/18/23 15:09:13.631
    STEP: creating replication controller externalsvc in namespace services-3351 01/18/23 15:09:13.823
    I0118 15:09:13.844774      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3351, replica count: 2
    I0118 15:09:16.898540      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 01/18/23 15:09:16.903
    Jan 18 15:09:16.925: INFO: Creating new exec pod
    Jan 18 15:09:16.938: INFO: Waiting up to 5m0s for pod "execpodg4pmm" in namespace "services-3351" to be "running"
    Jan 18 15:09:16.945: INFO: Pod "execpodg4pmm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.848221ms
    Jan 18 15:09:18.951: INFO: Pod "execpodg4pmm": Phase="Running", Reason="", readiness=true. Elapsed: 2.012944442s
    Jan 18 15:09:18.951: INFO: Pod "execpodg4pmm" satisfied condition "running"
    Jan 18 15:09:18.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-3351 exec execpodg4pmm -- /bin/sh -x -c nslookup nodeport-service.services-3351.svc.cluster.local'
    Jan 18 15:09:19.257: INFO: stderr: "+ nslookup nodeport-service.services-3351.svc.cluster.local\n"
    Jan 18 15:09:19.257: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nnodeport-service.services-3351.svc.cluster.local\tcanonical name = externalsvc.services-3351.svc.cluster.local.\nName:\texternalsvc.services-3351.svc.cluster.local\nAddress: 10.233.39.202\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3351, will wait for the garbage collector to delete the pods 01/18/23 15:09:19.257
    Jan 18 15:09:19.318: INFO: Deleting ReplicationController externalsvc took: 6.807586ms
    Jan 18 15:09:19.420: INFO: Terminating ReplicationController externalsvc pods took: 101.820009ms
    Jan 18 15:09:22.752: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 15:09:22.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3351" for this suite. 01/18/23 15:09:22.782
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:09:22.835
Jan 18 15:09:22.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 15:09:22.838
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:09:22.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:09:22.883
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/18/23 15:09:22.895
Jan 18 15:09:22.914: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1487" to be "running and ready"
Jan 18 15:09:22.925: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 10.693151ms
Jan 18 15:09:22.925: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:09:24.933: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.01821448s
Jan 18 15:09:24.933: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 18 15:09:24.933: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 01/18/23 15:09:24.939
Jan 18 15:09:24.946: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-1487" to be "running and ready"
Jan 18 15:09:24.954: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00226ms
Jan 18 15:09:24.954: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:09:26.961: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.014464658s
Jan 18 15:09:26.961: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jan 18 15:09:26.961: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/18/23 15:09:26.966
STEP: delete the pod with lifecycle hook 01/18/23 15:09:26.974
Jan 18 15:09:27.000: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 18 15:09:27.005: INFO: Pod pod-with-poststart-http-hook still exists
Jan 18 15:09:29.006: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 18 15:09:29.015: INFO: Pod pod-with-poststart-http-hook still exists
Jan 18 15:09:31.006: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 18 15:09:31.012: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 18 15:09:31.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1487" for this suite. 01/18/23 15:09:31.018
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":22,"skipped":569,"failed":0}
------------------------------
• [SLOW TEST] [8.190 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:09:22.835
    Jan 18 15:09:22.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 15:09:22.838
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:09:22.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:09:22.883
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/18/23 15:09:22.895
    Jan 18 15:09:22.914: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1487" to be "running and ready"
    Jan 18 15:09:22.925: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 10.693151ms
    Jan 18 15:09:22.925: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:09:24.933: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.01821448s
    Jan 18 15:09:24.933: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 18 15:09:24.933: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 01/18/23 15:09:24.939
    Jan 18 15:09:24.946: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-1487" to be "running and ready"
    Jan 18 15:09:24.954: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00226ms
    Jan 18 15:09:24.954: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:09:26.961: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.014464658s
    Jan 18 15:09:26.961: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jan 18 15:09:26.961: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/18/23 15:09:26.966
    STEP: delete the pod with lifecycle hook 01/18/23 15:09:26.974
    Jan 18 15:09:27.000: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 18 15:09:27.005: INFO: Pod pod-with-poststart-http-hook still exists
    Jan 18 15:09:29.006: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 18 15:09:29.015: INFO: Pod pod-with-poststart-http-hook still exists
    Jan 18 15:09:31.006: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 18 15:09:31.012: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 18 15:09:31.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-1487" for this suite. 01/18/23 15:09:31.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:09:31.036
Jan 18 15:09:31.036: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 15:09:31.038
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:09:31.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:09:31.061
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Jan 18 15:09:31.066: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 15:09:36.833
Jan 18 15:09:36.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-3478 --namespace=crd-publish-openapi-3478 create -f -'
Jan 18 15:09:37.954: INFO: stderr: ""
Jan 18 15:09:37.954: INFO: stdout: "e2e-test-crd-publish-openapi-4631-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 18 15:09:37.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-3478 --namespace=crd-publish-openapi-3478 delete e2e-test-crd-publish-openapi-4631-crds test-cr'
Jan 18 15:09:38.071: INFO: stderr: ""
Jan 18 15:09:38.071: INFO: stdout: "e2e-test-crd-publish-openapi-4631-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 18 15:09:38.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-3478 --namespace=crd-publish-openapi-3478 apply -f -'
Jan 18 15:09:38.390: INFO: stderr: ""
Jan 18 15:09:38.391: INFO: stdout: "e2e-test-crd-publish-openapi-4631-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 18 15:09:38.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-3478 --namespace=crd-publish-openapi-3478 delete e2e-test-crd-publish-openapi-4631-crds test-cr'
Jan 18 15:09:38.501: INFO: stderr: ""
Jan 18 15:09:38.501: INFO: stdout: "e2e-test-crd-publish-openapi-4631-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/18/23 15:09:38.502
Jan 18 15:09:38.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-3478 explain e2e-test-crd-publish-openapi-4631-crds'
Jan 18 15:09:38.903: INFO: stderr: ""
Jan 18 15:09:38.903: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4631-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 15:09:41.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3478" for this suite. 01/18/23 15:09:41.775
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":23,"skipped":590,"failed":0}
------------------------------
• [SLOW TEST] [10.746 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:09:31.036
    Jan 18 15:09:31.036: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 15:09:31.038
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:09:31.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:09:31.061
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Jan 18 15:09:31.066: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 15:09:36.833
    Jan 18 15:09:36.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-3478 --namespace=crd-publish-openapi-3478 create -f -'
    Jan 18 15:09:37.954: INFO: stderr: ""
    Jan 18 15:09:37.954: INFO: stdout: "e2e-test-crd-publish-openapi-4631-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 18 15:09:37.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-3478 --namespace=crd-publish-openapi-3478 delete e2e-test-crd-publish-openapi-4631-crds test-cr'
    Jan 18 15:09:38.071: INFO: stderr: ""
    Jan 18 15:09:38.071: INFO: stdout: "e2e-test-crd-publish-openapi-4631-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jan 18 15:09:38.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-3478 --namespace=crd-publish-openapi-3478 apply -f -'
    Jan 18 15:09:38.390: INFO: stderr: ""
    Jan 18 15:09:38.391: INFO: stdout: "e2e-test-crd-publish-openapi-4631-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 18 15:09:38.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-3478 --namespace=crd-publish-openapi-3478 delete e2e-test-crd-publish-openapi-4631-crds test-cr'
    Jan 18 15:09:38.501: INFO: stderr: ""
    Jan 18 15:09:38.501: INFO: stdout: "e2e-test-crd-publish-openapi-4631-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/18/23 15:09:38.502
    Jan 18 15:09:38.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-3478 explain e2e-test-crd-publish-openapi-4631-crds'
    Jan 18 15:09:38.903: INFO: stderr: ""
    Jan 18 15:09:38.903: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4631-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 15:09:41.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3478" for this suite. 01/18/23 15:09:41.775
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:09:41.783
Jan 18 15:09:41.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename cronjob 01/18/23 15:09:41.785
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:09:41.804
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:09:41.809
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 01/18/23 15:09:41.813
STEP: creating 01/18/23 15:09:41.813
STEP: getting 01/18/23 15:09:41.822
STEP: listing 01/18/23 15:09:41.827
STEP: watching 01/18/23 15:09:41.831
Jan 18 15:09:41.831: INFO: starting watch
STEP: cluster-wide listing 01/18/23 15:09:41.836
STEP: cluster-wide watching 01/18/23 15:09:41.839
Jan 18 15:09:41.840: INFO: starting watch
STEP: patching 01/18/23 15:09:41.842
STEP: updating 01/18/23 15:09:41.849
Jan 18 15:09:41.859: INFO: waiting for watch events with expected annotations
Jan 18 15:09:41.859: INFO: saw patched and updated annotations
STEP: patching /status 01/18/23 15:09:41.859
STEP: updating /status 01/18/23 15:09:41.874
STEP: get /status 01/18/23 15:09:41.883
STEP: deleting 01/18/23 15:09:41.888
STEP: deleting a collection 01/18/23 15:09:41.904
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 18 15:09:41.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8312" for this suite. 01/18/23 15:09:41.924
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":24,"skipped":590,"failed":0}
------------------------------
• [0.148 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:09:41.783
    Jan 18 15:09:41.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename cronjob 01/18/23 15:09:41.785
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:09:41.804
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:09:41.809
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 01/18/23 15:09:41.813
    STEP: creating 01/18/23 15:09:41.813
    STEP: getting 01/18/23 15:09:41.822
    STEP: listing 01/18/23 15:09:41.827
    STEP: watching 01/18/23 15:09:41.831
    Jan 18 15:09:41.831: INFO: starting watch
    STEP: cluster-wide listing 01/18/23 15:09:41.836
    STEP: cluster-wide watching 01/18/23 15:09:41.839
    Jan 18 15:09:41.840: INFO: starting watch
    STEP: patching 01/18/23 15:09:41.842
    STEP: updating 01/18/23 15:09:41.849
    Jan 18 15:09:41.859: INFO: waiting for watch events with expected annotations
    Jan 18 15:09:41.859: INFO: saw patched and updated annotations
    STEP: patching /status 01/18/23 15:09:41.859
    STEP: updating /status 01/18/23 15:09:41.874
    STEP: get /status 01/18/23 15:09:41.883
    STEP: deleting 01/18/23 15:09:41.888
    STEP: deleting a collection 01/18/23 15:09:41.904
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 18 15:09:41.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-8312" for this suite. 01/18/23 15:09:41.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:09:41.932
Jan 18 15:09:41.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename statefulset 01/18/23 15:09:41.933
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:09:41.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:09:41.956
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3649 01/18/23 15:09:41.96
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 01/18/23 15:09:41.965
Jan 18 15:09:41.984: INFO: Found 0 stateful pods, waiting for 3
Jan 18 15:09:51.998: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 15:09:51.998: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 15:09:51.998: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/18/23 15:09:52.011
Jan 18 15:09:52.038: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/18/23 15:09:52.038
STEP: Not applying an update when the partition is greater than the number of replicas 01/18/23 15:10:02.061
STEP: Performing a canary update 01/18/23 15:10:02.061
Jan 18 15:10:02.085: INFO: Updating stateful set ss2
Jan 18 15:10:02.095: INFO: Waiting for Pod statefulset-3649/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 01/18/23 15:10:12.107
Jan 18 15:10:12.181: INFO: Found 1 stateful pods, waiting for 3
Jan 18 15:10:22.188: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 15:10:22.188: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 15:10:22.188: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 01/18/23 15:10:22.197
Jan 18 15:10:22.220: INFO: Updating stateful set ss2
Jan 18 15:10:22.268: INFO: Waiting for Pod statefulset-3649/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Jan 18 15:10:32.311: INFO: Updating stateful set ss2
Jan 18 15:10:32.337: INFO: Waiting for StatefulSet statefulset-3649/ss2 to complete update
Jan 18 15:10:32.337: INFO: Waiting for Pod statefulset-3649/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 15:10:42.358: INFO: Deleting all statefulset in ns statefulset-3649
Jan 18 15:10:42.363: INFO: Scaling statefulset ss2 to 0
Jan 18 15:10:52.404: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 15:10:52.409: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 18 15:10:52.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3649" for this suite. 01/18/23 15:10:52.439
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":25,"skipped":595,"failed":0}
------------------------------
• [SLOW TEST] [70.515 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:09:41.932
    Jan 18 15:09:41.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename statefulset 01/18/23 15:09:41.933
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:09:41.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:09:41.956
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3649 01/18/23 15:09:41.96
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 01/18/23 15:09:41.965
    Jan 18 15:09:41.984: INFO: Found 0 stateful pods, waiting for 3
    Jan 18 15:09:51.998: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 15:09:51.998: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 15:09:51.998: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/18/23 15:09:52.011
    Jan 18 15:09:52.038: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/18/23 15:09:52.038
    STEP: Not applying an update when the partition is greater than the number of replicas 01/18/23 15:10:02.061
    STEP: Performing a canary update 01/18/23 15:10:02.061
    Jan 18 15:10:02.085: INFO: Updating stateful set ss2
    Jan 18 15:10:02.095: INFO: Waiting for Pod statefulset-3649/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 01/18/23 15:10:12.107
    Jan 18 15:10:12.181: INFO: Found 1 stateful pods, waiting for 3
    Jan 18 15:10:22.188: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 15:10:22.188: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 15:10:22.188: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 01/18/23 15:10:22.197
    Jan 18 15:10:22.220: INFO: Updating stateful set ss2
    Jan 18 15:10:22.268: INFO: Waiting for Pod statefulset-3649/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Jan 18 15:10:32.311: INFO: Updating stateful set ss2
    Jan 18 15:10:32.337: INFO: Waiting for StatefulSet statefulset-3649/ss2 to complete update
    Jan 18 15:10:32.337: INFO: Waiting for Pod statefulset-3649/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 18 15:10:42.358: INFO: Deleting all statefulset in ns statefulset-3649
    Jan 18 15:10:42.363: INFO: Scaling statefulset ss2 to 0
    Jan 18 15:10:52.404: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 15:10:52.409: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 18 15:10:52.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3649" for this suite. 01/18/23 15:10:52.439
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:10:52.453
Jan 18 15:10:52.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename replication-controller 01/18/23 15:10:52.456
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:10:52.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:10:52.487
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 01/18/23 15:10:52.496
Jan 18 15:10:52.515: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6633" to be "running and ready"
Jan 18 15:10:52.522: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 6.515656ms
Jan 18 15:10:52.522: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:10:54.536: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.020538975s
Jan 18 15:10:54.536: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jan 18 15:10:54.536: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 01/18/23 15:10:54.543
STEP: Then the orphan pod is adopted 01/18/23 15:10:54.555
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 18 15:10:55.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6633" for this suite. 01/18/23 15:10:55.576
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":26,"skipped":611,"failed":0}
------------------------------
• [3.132 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:10:52.453
    Jan 18 15:10:52.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename replication-controller 01/18/23 15:10:52.456
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:10:52.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:10:52.487
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 01/18/23 15:10:52.496
    Jan 18 15:10:52.515: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6633" to be "running and ready"
    Jan 18 15:10:52.522: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 6.515656ms
    Jan 18 15:10:52.522: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:10:54.536: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.020538975s
    Jan 18 15:10:54.536: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jan 18 15:10:54.536: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 01/18/23 15:10:54.543
    STEP: Then the orphan pod is adopted 01/18/23 15:10:54.555
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 18 15:10:55.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-6633" for this suite. 01/18/23 15:10:55.576
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:10:55.585
Jan 18 15:10:55.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename sched-pred 01/18/23 15:10:55.587
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:10:55.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:10:55.622
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 18 15:10:55.628: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 18 15:10:55.640: INFO: Waiting for terminating namespaces to be deleted...
Jan 18 15:10:55.646: INFO: 
Logging pods the apiserver thinks is on node v1-25-1-18760-w before test
Jan 18 15:10:55.659: INFO: helm-controller-578bd944df-dzr2c from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 15:10:55.659: INFO: 	Container manager ready: true, restart count 0
Jan 18 15:10:55.659: INFO: notification-controller-6459696d4f-gdmj8 from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 15:10:55.659: INFO: 	Container manager ready: true, restart count 0
Jan 18 15:10:55.659: INFO: source-controller-84f6bcbfb8-ljjjm from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 15:10:55.659: INFO: 	Container manager ready: true, restart count 0
Jan 18 15:10:55.659: INFO: calico-node-59tts from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
Jan 18 15:10:55.659: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 15:10:55.659: INFO: coredns-coredns-5f9b955d9-pjqxb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 15:10:55.660: INFO: 	Container coredns ready: true, restart count 0
Jan 18 15:10:55.660: INFO: coredns-coredns-autoscaler-7d98f7496d-7zbwb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 15:10:55.660: INFO: 	Container autoscaler ready: true, restart count 0
Jan 18 15:10:55.660: INFO: kube-proxy-rzflq from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
Jan 18 15:10:55.660: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 15:10:55.660: INFO: metrics-server-58498b9c56-q88cp from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 15:10:55.660: INFO: 	Container metrics-server ready: true, restart count 0
Jan 18 15:10:55.660: INFO: nginx-proxy-v1-25-1-18760-w from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
Jan 18 15:10:55.660: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 18 15:10:55.660: INFO: nodelocaldns-skdxm from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
Jan 18 15:10:55.660: INFO: 	Container node-cache ready: true, restart count 0
Jan 18 15:10:55.660: INFO: openstack-cinder-csi-controllerplugin-55d57dd799-6496v from kube-system started at 2023-01-18 14:39:48 +0000 UTC (6 container statuses recorded)
Jan 18 15:10:55.660: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 18 15:10:55.660: INFO: 	Container csi-attacher ready: true, restart count 0
Jan 18 15:10:55.660: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan 18 15:10:55.660: INFO: 	Container csi-resizer ready: true, restart count 0
Jan 18 15:10:55.660: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan 18 15:10:55.660: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 18 15:10:55.661: INFO: openstack-cinder-csi-nodeplugin-w7sxz from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
Jan 18 15:10:55.661: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 18 15:10:55.661: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 18 15:10:55.661: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 18 15:10:55.661: INFO: sonobuoy-systemd-logs-daemon-set-e45d8d00b64540c2-j5v5z from sonobuoy started at 2023-01-18 15:05:41 +0000 UTC (2 container statuses recorded)
Jan 18 15:10:55.661: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 15:10:55.661: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 15:10:55.661: INFO: 
Logging pods the apiserver thinks is on node v1-25-1-18760-w2 before test
Jan 18 15:10:55.674: INFO: calico-node-qx4f9 from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
Jan 18 15:10:55.675: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 15:10:55.675: INFO: coredns-coredns-5f9b955d9-qggng from kube-system started at 2023-01-18 14:40:34 +0000 UTC (1 container statuses recorded)
Jan 18 15:10:55.675: INFO: 	Container coredns ready: true, restart count 0
Jan 18 15:10:55.675: INFO: kube-proxy-8r2pt from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
Jan 18 15:10:55.675: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 15:10:55.675: INFO: nginx-proxy-v1-25-1-18760-w2 from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
Jan 18 15:10:55.675: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 18 15:10:55.675: INFO: nodelocaldns-dzwbj from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
Jan 18 15:10:55.675: INFO: 	Container node-cache ready: true, restart count 0
Jan 18 15:10:55.675: INFO: openstack-cinder-csi-nodeplugin-qm575 from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
Jan 18 15:10:55.675: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 18 15:10:55.675: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 18 15:10:55.675: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 18 15:10:55.675: INFO: pod-adoption from replication-controller-6633 started at 2023-01-18 15:10:52 +0000 UTC (1 container statuses recorded)
Jan 18 15:10:55.675: INFO: 	Container pod-adoption ready: true, restart count 0
Jan 18 15:10:55.675: INFO: sonobuoy from sonobuoy started at 2023-01-18 15:05:32 +0000 UTC (1 container statuses recorded)
Jan 18 15:10:55.675: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 18 15:10:55.676: INFO: sonobuoy-e2e-job-97bb2aa868f3486c from sonobuoy started at 2023-01-18 15:05:41 +0000 UTC (2 container statuses recorded)
Jan 18 15:10:55.676: INFO: 	Container e2e ready: true, restart count 0
Jan 18 15:10:55.676: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 15:10:55.676: INFO: sonobuoy-systemd-logs-daemon-set-e45d8d00b64540c2-488l9 from sonobuoy started at 2023-01-18 15:05:41 +0000 UTC (2 container statuses recorded)
Jan 18 15:10:55.676: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 15:10:55.676: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 15:10:55.677
Jan 18 15:10:55.688: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-8913" to be "running"
Jan 18 15:10:55.708: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 19.680114ms
Jan 18 15:10:57.716: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.028101951s
Jan 18 15:10:57.716: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 15:10:57.722
STEP: Trying to apply a random label on the found node. 01/18/23 15:10:57.743
STEP: verifying the node has the label kubernetes.io/e2e-a2f7d45f-7c2e-4225-b1e9-0adab74a060c 95 01/18/23 15:10:57.765
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/18/23 15:10:57.772
Jan 18 15:10:57.778: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-8913" to be "not pending"
Jan 18 15:10:57.788: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.171409ms
Jan 18 15:10:59.796: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.016080908s
Jan 18 15:10:59.796: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.101.216 on the node which pod4 resides and expect not scheduled 01/18/23 15:10:59.796
Jan 18 15:10:59.804: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-8913" to be "not pending"
Jan 18 15:10:59.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.295175ms
Jan 18 15:11:01.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012843193s
Jan 18 15:11:03.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013720256s
Jan 18 15:11:05.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011687772s
Jan 18 15:11:07.824: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019529062s
Jan 18 15:11:09.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.01217827s
Jan 18 15:11:11.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011892991s
Jan 18 15:11:13.913: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.108324919s
Jan 18 15:11:15.881: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.076422635s
Jan 18 15:11:17.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.013974959s
Jan 18 15:11:19.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.012761796s
Jan 18 15:11:21.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.013622095s
Jan 18 15:11:23.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.015880449s
Jan 18 15:11:25.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.012880366s
Jan 18 15:11:27.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.013285359s
Jan 18 15:11:29.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.012998211s
Jan 18 15:11:31.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.011748963s
Jan 18 15:11:33.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.014426446s
Jan 18 15:11:35.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.01249159s
Jan 18 15:11:37.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.014286816s
Jan 18 15:11:39.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.014184784s
Jan 18 15:11:41.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.015059781s
Jan 18 15:11:43.823: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.018424395s
Jan 18 15:11:45.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.014101237s
Jan 18 15:11:47.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.011658967s
Jan 18 15:11:49.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.013153702s
Jan 18 15:11:51.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.015775279s
Jan 18 15:11:53.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.012544897s
Jan 18 15:11:55.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.013722958s
Jan 18 15:11:57.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.011929791s
Jan 18 15:11:59.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.013780521s
Jan 18 15:12:01.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.013736905s
Jan 18 15:12:03.827: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.02194694s
Jan 18 15:12:05.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.0129717s
Jan 18 15:12:07.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.01341045s
Jan 18 15:12:09.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.011595731s
Jan 18 15:12:11.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.012920071s
Jan 18 15:12:13.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.014183419s
Jan 18 15:12:15.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.014155759s
Jan 18 15:12:17.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.013787934s
Jan 18 15:12:19.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.012557576s
Jan 18 15:12:21.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.013265389s
Jan 18 15:12:23.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.011629556s
Jan 18 15:12:25.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.012717441s
Jan 18 15:12:27.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.015081164s
Jan 18 15:12:29.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.014129334s
Jan 18 15:12:31.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.015313621s
Jan 18 15:12:33.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.013574916s
Jan 18 15:12:35.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.013560275s
Jan 18 15:12:37.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.014114975s
Jan 18 15:12:39.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.013409619s
Jan 18 15:12:41.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.014444851s
Jan 18 15:12:43.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.014257705s
Jan 18 15:12:45.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.013850949s
Jan 18 15:12:47.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.012722342s
Jan 18 15:12:49.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.014228862s
Jan 18 15:12:51.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.012743366s
Jan 18 15:12:53.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.01457502s
Jan 18 15:12:55.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.015797881s
Jan 18 15:12:57.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.013980969s
Jan 18 15:12:59.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013403742s
Jan 18 15:13:01.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.013516473s
Jan 18 15:13:03.835: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.029953404s
Jan 18 15:13:05.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.01208747s
Jan 18 15:13:07.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.012428517s
Jan 18 15:13:09.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.01258028s
Jan 18 15:13:11.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.012486505s
Jan 18 15:13:13.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.012945674s
Jan 18 15:13:15.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.011387036s
Jan 18 15:13:17.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.012040729s
Jan 18 15:13:19.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.01269993s
Jan 18 15:13:21.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.012138964s
Jan 18 15:13:23.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.012796336s
Jan 18 15:13:25.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.012486677s
Jan 18 15:13:27.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.011577381s
Jan 18 15:13:29.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.013886191s
Jan 18 15:13:31.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.012683891s
Jan 18 15:13:33.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.012278516s
Jan 18 15:13:35.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.012460636s
Jan 18 15:13:37.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.011455114s
Jan 18 15:13:39.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.01110756s
Jan 18 15:13:41.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.01201064s
Jan 18 15:13:43.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.01370294s
Jan 18 15:13:45.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.011905799s
Jan 18 15:13:47.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.012871643s
Jan 18 15:13:49.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.013155494s
Jan 18 15:13:51.822: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.016884287s
Jan 18 15:13:53.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.013172129s
Jan 18 15:13:55.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.013075487s
Jan 18 15:13:57.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.013110557s
Jan 18 15:13:59.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.011831308s
Jan 18 15:14:01.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.013870687s
Jan 18 15:14:03.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.012100795s
Jan 18 15:14:05.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.012442965s
Jan 18 15:14:07.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.013072177s
Jan 18 15:14:09.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.01303352s
Jan 18 15:14:11.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.013636808s
Jan 18 15:14:13.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.013785478s
Jan 18 15:14:15.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.015138357s
Jan 18 15:14:17.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.012512097s
Jan 18 15:14:19.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.013739668s
Jan 18 15:14:21.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.013159696s
Jan 18 15:14:23.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.012494801s
Jan 18 15:14:25.824: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.019227089s
Jan 18 15:14:27.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.013094734s
Jan 18 15:14:29.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.012496973s
Jan 18 15:14:31.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.014049754s
Jan 18 15:14:33.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.014248984s
Jan 18 15:14:36.974: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m37.168865725s
Jan 18 15:14:37.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.012819061s
Jan 18 15:14:39.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.01169073s
Jan 18 15:14:41.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.012718962s
Jan 18 15:14:43.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.015769289s
Jan 18 15:14:45.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.012614404s
Jan 18 15:14:47.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.012817339s
Jan 18 15:14:49.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.013691733s
Jan 18 15:14:51.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.012188214s
Jan 18 15:14:53.836: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.031635862s
Jan 18 15:14:55.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.013462223s
Jan 18 15:14:57.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.011842672s
Jan 18 15:14:59.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.01212684s
Jan 18 15:15:01.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.013951753s
Jan 18 15:15:03.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.014197009s
Jan 18 15:15:05.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.012465348s
Jan 18 15:15:07.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.013743856s
Jan 18 15:15:09.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.014339382s
Jan 18 15:15:11.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.013641674s
Jan 18 15:15:13.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.012326893s
Jan 18 15:15:15.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.013381273s
Jan 18 15:15:17.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.011827807s
Jan 18 15:15:19.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.013895523s
Jan 18 15:15:21.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.01250324s
Jan 18 15:15:23.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.013933447s
Jan 18 15:15:25.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.01665502s
Jan 18 15:15:27.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.013260889s
Jan 18 15:15:29.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.013546173s
Jan 18 15:15:31.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.011927751s
Jan 18 15:15:33.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.014005432s
Jan 18 15:15:35.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.013971936s
Jan 18 15:15:37.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.013477202s
Jan 18 15:15:39.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.013039697s
Jan 18 15:15:41.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.014023332s
Jan 18 15:15:43.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.011881473s
Jan 18 15:15:45.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.011942862s
Jan 18 15:15:47.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.014999113s
Jan 18 15:15:49.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.012148561s
Jan 18 15:15:51.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.015145924s
Jan 18 15:15:53.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.013789712s
Jan 18 15:15:55.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.013111586s
Jan 18 15:15:57.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.014317243s
Jan 18 15:15:59.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.012340931s
Jan 18 15:15:59.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.016560222s
STEP: removing the label kubernetes.io/e2e-a2f7d45f-7c2e-4225-b1e9-0adab74a060c off the node v1-25-1-18760-w2 01/18/23 15:15:59.822
STEP: verifying the node doesn't have the label kubernetes.io/e2e-a2f7d45f-7c2e-4225-b1e9-0adab74a060c 01/18/23 15:15:59.861
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 18 15:15:59.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8913" for this suite. 01/18/23 15:15:59.876
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":27,"skipped":611,"failed":0}
------------------------------
• [SLOW TEST] [304.303 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:10:55.585
    Jan 18 15:10:55.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename sched-pred 01/18/23 15:10:55.587
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:10:55.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:10:55.622
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 18 15:10:55.628: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 18 15:10:55.640: INFO: Waiting for terminating namespaces to be deleted...
    Jan 18 15:10:55.646: INFO: 
    Logging pods the apiserver thinks is on node v1-25-1-18760-w before test
    Jan 18 15:10:55.659: INFO: helm-controller-578bd944df-dzr2c from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 15:10:55.659: INFO: 	Container manager ready: true, restart count 0
    Jan 18 15:10:55.659: INFO: notification-controller-6459696d4f-gdmj8 from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 15:10:55.659: INFO: 	Container manager ready: true, restart count 0
    Jan 18 15:10:55.659: INFO: source-controller-84f6bcbfb8-ljjjm from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 15:10:55.659: INFO: 	Container manager ready: true, restart count 0
    Jan 18 15:10:55.659: INFO: calico-node-59tts from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
    Jan 18 15:10:55.659: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 15:10:55.659: INFO: coredns-coredns-5f9b955d9-pjqxb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 15:10:55.660: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 15:10:55.660: INFO: coredns-coredns-autoscaler-7d98f7496d-7zbwb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 15:10:55.660: INFO: 	Container autoscaler ready: true, restart count 0
    Jan 18 15:10:55.660: INFO: kube-proxy-rzflq from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
    Jan 18 15:10:55.660: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 15:10:55.660: INFO: metrics-server-58498b9c56-q88cp from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 15:10:55.660: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 18 15:10:55.660: INFO: nginx-proxy-v1-25-1-18760-w from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
    Jan 18 15:10:55.660: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 18 15:10:55.660: INFO: nodelocaldns-skdxm from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
    Jan 18 15:10:55.660: INFO: 	Container node-cache ready: true, restart count 0
    Jan 18 15:10:55.660: INFO: openstack-cinder-csi-controllerplugin-55d57dd799-6496v from kube-system started at 2023-01-18 14:39:48 +0000 UTC (6 container statuses recorded)
    Jan 18 15:10:55.660: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 18 15:10:55.660: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan 18 15:10:55.660: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan 18 15:10:55.660: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan 18 15:10:55.660: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan 18 15:10:55.660: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 18 15:10:55.661: INFO: openstack-cinder-csi-nodeplugin-w7sxz from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
    Jan 18 15:10:55.661: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 18 15:10:55.661: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 18 15:10:55.661: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 18 15:10:55.661: INFO: sonobuoy-systemd-logs-daemon-set-e45d8d00b64540c2-j5v5z from sonobuoy started at 2023-01-18 15:05:41 +0000 UTC (2 container statuses recorded)
    Jan 18 15:10:55.661: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 15:10:55.661: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 18 15:10:55.661: INFO: 
    Logging pods the apiserver thinks is on node v1-25-1-18760-w2 before test
    Jan 18 15:10:55.674: INFO: calico-node-qx4f9 from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
    Jan 18 15:10:55.675: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 15:10:55.675: INFO: coredns-coredns-5f9b955d9-qggng from kube-system started at 2023-01-18 14:40:34 +0000 UTC (1 container statuses recorded)
    Jan 18 15:10:55.675: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 15:10:55.675: INFO: kube-proxy-8r2pt from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
    Jan 18 15:10:55.675: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 15:10:55.675: INFO: nginx-proxy-v1-25-1-18760-w2 from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
    Jan 18 15:10:55.675: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 18 15:10:55.675: INFO: nodelocaldns-dzwbj from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
    Jan 18 15:10:55.675: INFO: 	Container node-cache ready: true, restart count 0
    Jan 18 15:10:55.675: INFO: openstack-cinder-csi-nodeplugin-qm575 from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
    Jan 18 15:10:55.675: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 18 15:10:55.675: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 18 15:10:55.675: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 18 15:10:55.675: INFO: pod-adoption from replication-controller-6633 started at 2023-01-18 15:10:52 +0000 UTC (1 container statuses recorded)
    Jan 18 15:10:55.675: INFO: 	Container pod-adoption ready: true, restart count 0
    Jan 18 15:10:55.675: INFO: sonobuoy from sonobuoy started at 2023-01-18 15:05:32 +0000 UTC (1 container statuses recorded)
    Jan 18 15:10:55.675: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 18 15:10:55.676: INFO: sonobuoy-e2e-job-97bb2aa868f3486c from sonobuoy started at 2023-01-18 15:05:41 +0000 UTC (2 container statuses recorded)
    Jan 18 15:10:55.676: INFO: 	Container e2e ready: true, restart count 0
    Jan 18 15:10:55.676: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 15:10:55.676: INFO: sonobuoy-systemd-logs-daemon-set-e45d8d00b64540c2-488l9 from sonobuoy started at 2023-01-18 15:05:41 +0000 UTC (2 container statuses recorded)
    Jan 18 15:10:55.676: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 15:10:55.676: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 15:10:55.677
    Jan 18 15:10:55.688: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-8913" to be "running"
    Jan 18 15:10:55.708: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 19.680114ms
    Jan 18 15:10:57.716: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.028101951s
    Jan 18 15:10:57.716: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 15:10:57.722
    STEP: Trying to apply a random label on the found node. 01/18/23 15:10:57.743
    STEP: verifying the node has the label kubernetes.io/e2e-a2f7d45f-7c2e-4225-b1e9-0adab74a060c 95 01/18/23 15:10:57.765
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/18/23 15:10:57.772
    Jan 18 15:10:57.778: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-8913" to be "not pending"
    Jan 18 15:10:57.788: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.171409ms
    Jan 18 15:10:59.796: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.016080908s
    Jan 18 15:10:59.796: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.101.216 on the node which pod4 resides and expect not scheduled 01/18/23 15:10:59.796
    Jan 18 15:10:59.804: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-8913" to be "not pending"
    Jan 18 15:10:59.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.295175ms
    Jan 18 15:11:01.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012843193s
    Jan 18 15:11:03.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013720256s
    Jan 18 15:11:05.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011687772s
    Jan 18 15:11:07.824: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019529062s
    Jan 18 15:11:09.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.01217827s
    Jan 18 15:11:11.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011892991s
    Jan 18 15:11:13.913: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.108324919s
    Jan 18 15:11:15.881: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.076422635s
    Jan 18 15:11:17.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.013974959s
    Jan 18 15:11:19.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.012761796s
    Jan 18 15:11:21.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.013622095s
    Jan 18 15:11:23.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.015880449s
    Jan 18 15:11:25.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.012880366s
    Jan 18 15:11:27.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.013285359s
    Jan 18 15:11:29.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.012998211s
    Jan 18 15:11:31.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.011748963s
    Jan 18 15:11:33.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.014426446s
    Jan 18 15:11:35.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.01249159s
    Jan 18 15:11:37.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.014286816s
    Jan 18 15:11:39.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.014184784s
    Jan 18 15:11:41.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.015059781s
    Jan 18 15:11:43.823: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.018424395s
    Jan 18 15:11:45.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.014101237s
    Jan 18 15:11:47.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.011658967s
    Jan 18 15:11:49.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.013153702s
    Jan 18 15:11:51.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.015775279s
    Jan 18 15:11:53.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.012544897s
    Jan 18 15:11:55.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.013722958s
    Jan 18 15:11:57.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.011929791s
    Jan 18 15:11:59.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.013780521s
    Jan 18 15:12:01.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.013736905s
    Jan 18 15:12:03.827: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.02194694s
    Jan 18 15:12:05.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.0129717s
    Jan 18 15:12:07.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.01341045s
    Jan 18 15:12:09.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.011595731s
    Jan 18 15:12:11.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.012920071s
    Jan 18 15:12:13.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.014183419s
    Jan 18 15:12:15.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.014155759s
    Jan 18 15:12:17.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.013787934s
    Jan 18 15:12:19.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.012557576s
    Jan 18 15:12:21.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.013265389s
    Jan 18 15:12:23.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.011629556s
    Jan 18 15:12:25.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.012717441s
    Jan 18 15:12:27.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.015081164s
    Jan 18 15:12:29.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.014129334s
    Jan 18 15:12:31.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.015313621s
    Jan 18 15:12:33.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.013574916s
    Jan 18 15:12:35.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.013560275s
    Jan 18 15:12:37.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.014114975s
    Jan 18 15:12:39.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.013409619s
    Jan 18 15:12:41.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.014444851s
    Jan 18 15:12:43.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.014257705s
    Jan 18 15:12:45.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.013850949s
    Jan 18 15:12:47.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.012722342s
    Jan 18 15:12:49.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.014228862s
    Jan 18 15:12:51.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.012743366s
    Jan 18 15:12:53.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.01457502s
    Jan 18 15:12:55.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.015797881s
    Jan 18 15:12:57.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.013980969s
    Jan 18 15:12:59.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013403742s
    Jan 18 15:13:01.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.013516473s
    Jan 18 15:13:03.835: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.029953404s
    Jan 18 15:13:05.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.01208747s
    Jan 18 15:13:07.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.012428517s
    Jan 18 15:13:09.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.01258028s
    Jan 18 15:13:11.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.012486505s
    Jan 18 15:13:13.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.012945674s
    Jan 18 15:13:15.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.011387036s
    Jan 18 15:13:17.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.012040729s
    Jan 18 15:13:19.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.01269993s
    Jan 18 15:13:21.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.012138964s
    Jan 18 15:13:23.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.012796336s
    Jan 18 15:13:25.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.012486677s
    Jan 18 15:13:27.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.011577381s
    Jan 18 15:13:29.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.013886191s
    Jan 18 15:13:31.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.012683891s
    Jan 18 15:13:33.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.012278516s
    Jan 18 15:13:35.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.012460636s
    Jan 18 15:13:37.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.011455114s
    Jan 18 15:13:39.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.01110756s
    Jan 18 15:13:41.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.01201064s
    Jan 18 15:13:43.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.01370294s
    Jan 18 15:13:45.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.011905799s
    Jan 18 15:13:47.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.012871643s
    Jan 18 15:13:49.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.013155494s
    Jan 18 15:13:51.822: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.016884287s
    Jan 18 15:13:53.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.013172129s
    Jan 18 15:13:55.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.013075487s
    Jan 18 15:13:57.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.013110557s
    Jan 18 15:13:59.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.011831308s
    Jan 18 15:14:01.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.013870687s
    Jan 18 15:14:03.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.012100795s
    Jan 18 15:14:05.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.012442965s
    Jan 18 15:14:07.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.013072177s
    Jan 18 15:14:09.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.01303352s
    Jan 18 15:14:11.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.013636808s
    Jan 18 15:14:13.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.013785478s
    Jan 18 15:14:15.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.015138357s
    Jan 18 15:14:17.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.012512097s
    Jan 18 15:14:19.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.013739668s
    Jan 18 15:14:21.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.013159696s
    Jan 18 15:14:23.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.012494801s
    Jan 18 15:14:25.824: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.019227089s
    Jan 18 15:14:27.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.013094734s
    Jan 18 15:14:29.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.012496973s
    Jan 18 15:14:31.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.014049754s
    Jan 18 15:14:33.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.014248984s
    Jan 18 15:14:36.974: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m37.168865725s
    Jan 18 15:14:37.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.012819061s
    Jan 18 15:14:39.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.01169073s
    Jan 18 15:14:41.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.012718962s
    Jan 18 15:14:43.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.015769289s
    Jan 18 15:14:45.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.012614404s
    Jan 18 15:14:47.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.012817339s
    Jan 18 15:14:49.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.013691733s
    Jan 18 15:14:51.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.012188214s
    Jan 18 15:14:53.836: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.031635862s
    Jan 18 15:14:55.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.013462223s
    Jan 18 15:14:57.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.011842672s
    Jan 18 15:14:59.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.01212684s
    Jan 18 15:15:01.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.013951753s
    Jan 18 15:15:03.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.014197009s
    Jan 18 15:15:05.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.012465348s
    Jan 18 15:15:07.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.013743856s
    Jan 18 15:15:09.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.014339382s
    Jan 18 15:15:11.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.013641674s
    Jan 18 15:15:13.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.012326893s
    Jan 18 15:15:15.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.013381273s
    Jan 18 15:15:17.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.011827807s
    Jan 18 15:15:19.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.013895523s
    Jan 18 15:15:21.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.01250324s
    Jan 18 15:15:23.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.013933447s
    Jan 18 15:15:25.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.01665502s
    Jan 18 15:15:27.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.013260889s
    Jan 18 15:15:29.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.013546173s
    Jan 18 15:15:31.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.011927751s
    Jan 18 15:15:33.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.014005432s
    Jan 18 15:15:35.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.013971936s
    Jan 18 15:15:37.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.013477202s
    Jan 18 15:15:39.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.013039697s
    Jan 18 15:15:41.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.014023332s
    Jan 18 15:15:43.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.011881473s
    Jan 18 15:15:45.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.011942862s
    Jan 18 15:15:47.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.014999113s
    Jan 18 15:15:49.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.012148561s
    Jan 18 15:15:51.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.015145924s
    Jan 18 15:15:53.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.013789712s
    Jan 18 15:15:55.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.013111586s
    Jan 18 15:15:57.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.014317243s
    Jan 18 15:15:59.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.012340931s
    Jan 18 15:15:59.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.016560222s
    STEP: removing the label kubernetes.io/e2e-a2f7d45f-7c2e-4225-b1e9-0adab74a060c off the node v1-25-1-18760-w2 01/18/23 15:15:59.822
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-a2f7d45f-7c2e-4225-b1e9-0adab74a060c 01/18/23 15:15:59.861
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 15:15:59.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-8913" for this suite. 01/18/23 15:15:59.876
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:15:59.889
Jan 18 15:15:59.889: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 15:15:59.893
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:15:59.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:15:59.926
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Jan 18 15:15:59.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/18/23 15:16:04.542
Jan 18 15:16:04.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 --namespace=crd-publish-openapi-1984 create -f -'
Jan 18 15:16:06.042: INFO: stderr: ""
Jan 18 15:16:06.042: INFO: stdout: "e2e-test-crd-publish-openapi-7403-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 18 15:16:06.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 --namespace=crd-publish-openapi-1984 delete e2e-test-crd-publish-openapi-7403-crds test-foo'
Jan 18 15:16:06.236: INFO: stderr: ""
Jan 18 15:16:06.236: INFO: stdout: "e2e-test-crd-publish-openapi-7403-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 18 15:16:06.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 --namespace=crd-publish-openapi-1984 apply -f -'
Jan 18 15:16:06.588: INFO: stderr: ""
Jan 18 15:16:06.588: INFO: stdout: "e2e-test-crd-publish-openapi-7403-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 18 15:16:06.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 --namespace=crd-publish-openapi-1984 delete e2e-test-crd-publish-openapi-7403-crds test-foo'
Jan 18 15:16:06.699: INFO: stderr: ""
Jan 18 15:16:06.699: INFO: stdout: "e2e-test-crd-publish-openapi-7403-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/18/23 15:16:06.699
Jan 18 15:16:06.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 --namespace=crd-publish-openapi-1984 create -f -'
Jan 18 15:16:07.005: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/18/23 15:16:07.005
Jan 18 15:16:07.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 --namespace=crd-publish-openapi-1984 create -f -'
Jan 18 15:16:07.354: INFO: rc: 1
Jan 18 15:16:07.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 --namespace=crd-publish-openapi-1984 apply -f -'
Jan 18 15:16:07.671: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/18/23 15:16:07.671
Jan 18 15:16:07.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 --namespace=crd-publish-openapi-1984 create -f -'
Jan 18 15:16:07.977: INFO: rc: 1
Jan 18 15:16:07.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 --namespace=crd-publish-openapi-1984 apply -f -'
Jan 18 15:16:08.336: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 01/18/23 15:16:08.336
Jan 18 15:16:08.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 explain e2e-test-crd-publish-openapi-7403-crds'
Jan 18 15:16:08.674: INFO: stderr: ""
Jan 18 15:16:08.674: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7403-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 01/18/23 15:16:08.674
Jan 18 15:16:08.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 explain e2e-test-crd-publish-openapi-7403-crds.metadata'
Jan 18 15:16:09.028: INFO: stderr: ""
Jan 18 15:16:09.029: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7403-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 18 15:16:09.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 explain e2e-test-crd-publish-openapi-7403-crds.spec'
Jan 18 15:16:09.431: INFO: stderr: ""
Jan 18 15:16:09.431: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7403-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 18 15:16:09.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 explain e2e-test-crd-publish-openapi-7403-crds.spec.bars'
Jan 18 15:16:09.779: INFO: stderr: ""
Jan 18 15:16:09.779: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7403-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/18/23 15:16:09.779
Jan 18 15:16:09.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 explain e2e-test-crd-publish-openapi-7403-crds.spec.bars2'
Jan 18 15:16:10.101: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 15:16:14.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1984" for this suite. 01/18/23 15:16:14.848
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":28,"skipped":618,"failed":0}
------------------------------
• [SLOW TEST] [14.965 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:15:59.889
    Jan 18 15:15:59.889: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 15:15:59.893
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:15:59.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:15:59.926
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Jan 18 15:15:59.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/18/23 15:16:04.542
    Jan 18 15:16:04.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 --namespace=crd-publish-openapi-1984 create -f -'
    Jan 18 15:16:06.042: INFO: stderr: ""
    Jan 18 15:16:06.042: INFO: stdout: "e2e-test-crd-publish-openapi-7403-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 18 15:16:06.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 --namespace=crd-publish-openapi-1984 delete e2e-test-crd-publish-openapi-7403-crds test-foo'
    Jan 18 15:16:06.236: INFO: stderr: ""
    Jan 18 15:16:06.236: INFO: stdout: "e2e-test-crd-publish-openapi-7403-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jan 18 15:16:06.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 --namespace=crd-publish-openapi-1984 apply -f -'
    Jan 18 15:16:06.588: INFO: stderr: ""
    Jan 18 15:16:06.588: INFO: stdout: "e2e-test-crd-publish-openapi-7403-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 18 15:16:06.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 --namespace=crd-publish-openapi-1984 delete e2e-test-crd-publish-openapi-7403-crds test-foo'
    Jan 18 15:16:06.699: INFO: stderr: ""
    Jan 18 15:16:06.699: INFO: stdout: "e2e-test-crd-publish-openapi-7403-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/18/23 15:16:06.699
    Jan 18 15:16:06.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 --namespace=crd-publish-openapi-1984 create -f -'
    Jan 18 15:16:07.005: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/18/23 15:16:07.005
    Jan 18 15:16:07.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 --namespace=crd-publish-openapi-1984 create -f -'
    Jan 18 15:16:07.354: INFO: rc: 1
    Jan 18 15:16:07.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 --namespace=crd-publish-openapi-1984 apply -f -'
    Jan 18 15:16:07.671: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/18/23 15:16:07.671
    Jan 18 15:16:07.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 --namespace=crd-publish-openapi-1984 create -f -'
    Jan 18 15:16:07.977: INFO: rc: 1
    Jan 18 15:16:07.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 --namespace=crd-publish-openapi-1984 apply -f -'
    Jan 18 15:16:08.336: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 01/18/23 15:16:08.336
    Jan 18 15:16:08.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 explain e2e-test-crd-publish-openapi-7403-crds'
    Jan 18 15:16:08.674: INFO: stderr: ""
    Jan 18 15:16:08.674: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7403-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 01/18/23 15:16:08.674
    Jan 18 15:16:08.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 explain e2e-test-crd-publish-openapi-7403-crds.metadata'
    Jan 18 15:16:09.028: INFO: stderr: ""
    Jan 18 15:16:09.029: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7403-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jan 18 15:16:09.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 explain e2e-test-crd-publish-openapi-7403-crds.spec'
    Jan 18 15:16:09.431: INFO: stderr: ""
    Jan 18 15:16:09.431: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7403-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jan 18 15:16:09.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 explain e2e-test-crd-publish-openapi-7403-crds.spec.bars'
    Jan 18 15:16:09.779: INFO: stderr: ""
    Jan 18 15:16:09.779: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7403-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/18/23 15:16:09.779
    Jan 18 15:16:09.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-1984 explain e2e-test-crd-publish-openapi-7403-crds.spec.bars2'
    Jan 18 15:16:10.101: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 15:16:14.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1984" for this suite. 01/18/23 15:16:14.848
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:16:14.86
Jan 18 15:16:14.860: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename downward-api 01/18/23 15:16:14.862
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:16:14.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:16:14.883
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 01/18/23 15:16:14.888
Jan 18 15:16:14.898: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f0e493b9-2241-4545-86a1-5266c0591f09" in namespace "downward-api-6873" to be "Succeeded or Failed"
Jan 18 15:16:14.923: INFO: Pod "downwardapi-volume-f0e493b9-2241-4545-86a1-5266c0591f09": Phase="Pending", Reason="", readiness=false. Elapsed: 24.552182ms
Jan 18 15:16:16.937: INFO: Pod "downwardapi-volume-f0e493b9-2241-4545-86a1-5266c0591f09": Phase="Running", Reason="", readiness=true. Elapsed: 2.0386128s
Jan 18 15:16:18.937: INFO: Pod "downwardapi-volume-f0e493b9-2241-4545-86a1-5266c0591f09": Phase="Running", Reason="", readiness=false. Elapsed: 4.03849726s
Jan 18 15:16:20.937: INFO: Pod "downwardapi-volume-f0e493b9-2241-4545-86a1-5266c0591f09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038211361s
STEP: Saw pod success 01/18/23 15:16:20.937
Jan 18 15:16:20.937: INFO: Pod "downwardapi-volume-f0e493b9-2241-4545-86a1-5266c0591f09" satisfied condition "Succeeded or Failed"
Jan 18 15:16:20.942: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-f0e493b9-2241-4545-86a1-5266c0591f09 container client-container: <nil>
STEP: delete the pod 01/18/23 15:16:20.979
Jan 18 15:16:21.000: INFO: Waiting for pod downwardapi-volume-f0e493b9-2241-4545-86a1-5266c0591f09 to disappear
Jan 18 15:16:21.003: INFO: Pod downwardapi-volume-f0e493b9-2241-4545-86a1-5266c0591f09 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 15:16:21.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6873" for this suite. 01/18/23 15:16:21.008
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":29,"skipped":634,"failed":0}
------------------------------
• [SLOW TEST] [6.156 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:16:14.86
    Jan 18 15:16:14.860: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename downward-api 01/18/23 15:16:14.862
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:16:14.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:16:14.883
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 01/18/23 15:16:14.888
    Jan 18 15:16:14.898: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f0e493b9-2241-4545-86a1-5266c0591f09" in namespace "downward-api-6873" to be "Succeeded or Failed"
    Jan 18 15:16:14.923: INFO: Pod "downwardapi-volume-f0e493b9-2241-4545-86a1-5266c0591f09": Phase="Pending", Reason="", readiness=false. Elapsed: 24.552182ms
    Jan 18 15:16:16.937: INFO: Pod "downwardapi-volume-f0e493b9-2241-4545-86a1-5266c0591f09": Phase="Running", Reason="", readiness=true. Elapsed: 2.0386128s
    Jan 18 15:16:18.937: INFO: Pod "downwardapi-volume-f0e493b9-2241-4545-86a1-5266c0591f09": Phase="Running", Reason="", readiness=false. Elapsed: 4.03849726s
    Jan 18 15:16:20.937: INFO: Pod "downwardapi-volume-f0e493b9-2241-4545-86a1-5266c0591f09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038211361s
    STEP: Saw pod success 01/18/23 15:16:20.937
    Jan 18 15:16:20.937: INFO: Pod "downwardapi-volume-f0e493b9-2241-4545-86a1-5266c0591f09" satisfied condition "Succeeded or Failed"
    Jan 18 15:16:20.942: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-f0e493b9-2241-4545-86a1-5266c0591f09 container client-container: <nil>
    STEP: delete the pod 01/18/23 15:16:20.979
    Jan 18 15:16:21.000: INFO: Waiting for pod downwardapi-volume-f0e493b9-2241-4545-86a1-5266c0591f09 to disappear
    Jan 18 15:16:21.003: INFO: Pod downwardapi-volume-f0e493b9-2241-4545-86a1-5266c0591f09 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 15:16:21.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6873" for this suite. 01/18/23 15:16:21.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:16:21.018
Jan 18 15:16:21.018: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename webhook 01/18/23 15:16:21.02
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:16:21.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:16:21.06
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 15:16:21.08
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 15:16:21.807
STEP: Deploying the webhook pod 01/18/23 15:16:21.818
STEP: Wait for the deployment to be ready 01/18/23 15:16:21.834
Jan 18 15:16:21.856: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 15:16:23.869
STEP: Verifying the service has paired with the endpoint 01/18/23 15:16:23.884
Jan 18 15:16:24.887: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Jan 18 15:16:24.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-381-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 15:16:25.405
STEP: Creating a custom resource that should be mutated by the webhook 01/18/23 15:16:25.428
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 15:16:28.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9817" for this suite. 01/18/23 15:16:28.028
STEP: Destroying namespace "webhook-9817-markers" for this suite. 01/18/23 15:16:28.036
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":30,"skipped":653,"failed":0}
------------------------------
• [SLOW TEST] [7.085 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:16:21.018
    Jan 18 15:16:21.018: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename webhook 01/18/23 15:16:21.02
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:16:21.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:16:21.06
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 15:16:21.08
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 15:16:21.807
    STEP: Deploying the webhook pod 01/18/23 15:16:21.818
    STEP: Wait for the deployment to be ready 01/18/23 15:16:21.834
    Jan 18 15:16:21.856: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 15:16:23.869
    STEP: Verifying the service has paired with the endpoint 01/18/23 15:16:23.884
    Jan 18 15:16:24.887: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Jan 18 15:16:24.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-381-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 15:16:25.405
    STEP: Creating a custom resource that should be mutated by the webhook 01/18/23 15:16:25.428
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 15:16:28.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9817" for this suite. 01/18/23 15:16:28.028
    STEP: Destroying namespace "webhook-9817-markers" for this suite. 01/18/23 15:16:28.036
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:16:28.106
Jan 18 15:16:28.106: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 15:16:28.109
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:16:28.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:16:28.172
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-53f73ec2-c171-4cda-bcf4-e6e88919466b 01/18/23 15:16:28.193
STEP: Creating secret with name s-test-opt-upd-d66e9391-062b-4c88-ace3-2a35092d85ea 01/18/23 15:16:28.202
STEP: Creating the pod 01/18/23 15:16:28.211
Jan 18 15:16:28.233: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a4deacde-fbd7-43f5-81d8-8bf2ca3844f6" in namespace "projected-2391" to be "running and ready"
Jan 18 15:16:28.240: INFO: Pod "pod-projected-secrets-a4deacde-fbd7-43f5-81d8-8bf2ca3844f6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.490982ms
Jan 18 15:16:28.241: INFO: The phase of Pod pod-projected-secrets-a4deacde-fbd7-43f5-81d8-8bf2ca3844f6 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:16:30.256: INFO: Pod "pod-projected-secrets-a4deacde-fbd7-43f5-81d8-8bf2ca3844f6": Phase="Running", Reason="", readiness=true. Elapsed: 2.022174363s
Jan 18 15:16:30.256: INFO: The phase of Pod pod-projected-secrets-a4deacde-fbd7-43f5-81d8-8bf2ca3844f6 is Running (Ready = true)
Jan 18 15:16:30.256: INFO: Pod "pod-projected-secrets-a4deacde-fbd7-43f5-81d8-8bf2ca3844f6" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-53f73ec2-c171-4cda-bcf4-e6e88919466b 01/18/23 15:16:30.286
STEP: Updating secret s-test-opt-upd-d66e9391-062b-4c88-ace3-2a35092d85ea 01/18/23 15:16:30.292
STEP: Creating secret with name s-test-opt-create-74422955-b532-4e17-87ab-e909967855af 01/18/23 15:16:30.297
STEP: waiting to observe update in volume 01/18/23 15:16:30.302
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 18 15:16:32.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2391" for this suite. 01/18/23 15:16:32.346
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":31,"skipped":681,"failed":0}
------------------------------
• [4.247 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:16:28.106
    Jan 18 15:16:28.106: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 15:16:28.109
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:16:28.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:16:28.172
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-53f73ec2-c171-4cda-bcf4-e6e88919466b 01/18/23 15:16:28.193
    STEP: Creating secret with name s-test-opt-upd-d66e9391-062b-4c88-ace3-2a35092d85ea 01/18/23 15:16:28.202
    STEP: Creating the pod 01/18/23 15:16:28.211
    Jan 18 15:16:28.233: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a4deacde-fbd7-43f5-81d8-8bf2ca3844f6" in namespace "projected-2391" to be "running and ready"
    Jan 18 15:16:28.240: INFO: Pod "pod-projected-secrets-a4deacde-fbd7-43f5-81d8-8bf2ca3844f6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.490982ms
    Jan 18 15:16:28.241: INFO: The phase of Pod pod-projected-secrets-a4deacde-fbd7-43f5-81d8-8bf2ca3844f6 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:16:30.256: INFO: Pod "pod-projected-secrets-a4deacde-fbd7-43f5-81d8-8bf2ca3844f6": Phase="Running", Reason="", readiness=true. Elapsed: 2.022174363s
    Jan 18 15:16:30.256: INFO: The phase of Pod pod-projected-secrets-a4deacde-fbd7-43f5-81d8-8bf2ca3844f6 is Running (Ready = true)
    Jan 18 15:16:30.256: INFO: Pod "pod-projected-secrets-a4deacde-fbd7-43f5-81d8-8bf2ca3844f6" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-53f73ec2-c171-4cda-bcf4-e6e88919466b 01/18/23 15:16:30.286
    STEP: Updating secret s-test-opt-upd-d66e9391-062b-4c88-ace3-2a35092d85ea 01/18/23 15:16:30.292
    STEP: Creating secret with name s-test-opt-create-74422955-b532-4e17-87ab-e909967855af 01/18/23 15:16:30.297
    STEP: waiting to observe update in volume 01/18/23 15:16:30.302
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 18 15:16:32.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2391" for this suite. 01/18/23 15:16:32.346
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:16:32.356
Jan 18 15:16:32.356: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename emptydir 01/18/23 15:16:32.358
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:16:32.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:16:32.383
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 01/18/23 15:16:32.388
Jan 18 15:16:32.396: INFO: Waiting up to 5m0s for pod "pod-2c17f803-8e81-43b9-bae1-e64e9307bc1d" in namespace "emptydir-8590" to be "Succeeded or Failed"
Jan 18 15:16:32.400: INFO: Pod "pod-2c17f803-8e81-43b9-bae1-e64e9307bc1d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.743743ms
Jan 18 15:16:34.405: INFO: Pod "pod-2c17f803-8e81-43b9-bae1-e64e9307bc1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009312111s
Jan 18 15:16:36.407: INFO: Pod "pod-2c17f803-8e81-43b9-bae1-e64e9307bc1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011319819s
STEP: Saw pod success 01/18/23 15:16:36.407
Jan 18 15:16:36.408: INFO: Pod "pod-2c17f803-8e81-43b9-bae1-e64e9307bc1d" satisfied condition "Succeeded or Failed"
Jan 18 15:16:36.412: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-2c17f803-8e81-43b9-bae1-e64e9307bc1d container test-container: <nil>
STEP: delete the pod 01/18/23 15:16:36.42
Jan 18 15:16:36.440: INFO: Waiting for pod pod-2c17f803-8e81-43b9-bae1-e64e9307bc1d to disappear
Jan 18 15:16:36.443: INFO: Pod pod-2c17f803-8e81-43b9-bae1-e64e9307bc1d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 15:16:36.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8590" for this suite. 01/18/23 15:16:36.448
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":32,"skipped":692,"failed":0}
------------------------------
• [4.102 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:16:32.356
    Jan 18 15:16:32.356: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename emptydir 01/18/23 15:16:32.358
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:16:32.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:16:32.383
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/18/23 15:16:32.388
    Jan 18 15:16:32.396: INFO: Waiting up to 5m0s for pod "pod-2c17f803-8e81-43b9-bae1-e64e9307bc1d" in namespace "emptydir-8590" to be "Succeeded or Failed"
    Jan 18 15:16:32.400: INFO: Pod "pod-2c17f803-8e81-43b9-bae1-e64e9307bc1d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.743743ms
    Jan 18 15:16:34.405: INFO: Pod "pod-2c17f803-8e81-43b9-bae1-e64e9307bc1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009312111s
    Jan 18 15:16:36.407: INFO: Pod "pod-2c17f803-8e81-43b9-bae1-e64e9307bc1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011319819s
    STEP: Saw pod success 01/18/23 15:16:36.407
    Jan 18 15:16:36.408: INFO: Pod "pod-2c17f803-8e81-43b9-bae1-e64e9307bc1d" satisfied condition "Succeeded or Failed"
    Jan 18 15:16:36.412: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-2c17f803-8e81-43b9-bae1-e64e9307bc1d container test-container: <nil>
    STEP: delete the pod 01/18/23 15:16:36.42
    Jan 18 15:16:36.440: INFO: Waiting for pod pod-2c17f803-8e81-43b9-bae1-e64e9307bc1d to disappear
    Jan 18 15:16:36.443: INFO: Pod pod-2c17f803-8e81-43b9-bae1-e64e9307bc1d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 15:16:36.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8590" for this suite. 01/18/23 15:16:36.448
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:16:36.462
Jan 18 15:16:36.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename replication-controller 01/18/23 15:16:36.464
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:16:36.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:16:36.49
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Jan 18 15:16:36.495: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/18/23 15:16:37.52
STEP: Checking rc "condition-test" has the desired failure condition set 01/18/23 15:16:37.534
STEP: Scaling down rc "condition-test" to satisfy pod quota 01/18/23 15:16:38.628
Jan 18 15:16:38.640: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 01/18/23 15:16:38.64
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 18 15:16:39.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8914" for this suite. 01/18/23 15:16:39.655
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":33,"skipped":697,"failed":0}
------------------------------
• [3.203 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:16:36.462
    Jan 18 15:16:36.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename replication-controller 01/18/23 15:16:36.464
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:16:36.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:16:36.49
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Jan 18 15:16:36.495: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/18/23 15:16:37.52
    STEP: Checking rc "condition-test" has the desired failure condition set 01/18/23 15:16:37.534
    STEP: Scaling down rc "condition-test" to satisfy pod quota 01/18/23 15:16:38.628
    Jan 18 15:16:38.640: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 01/18/23 15:16:38.64
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 18 15:16:39.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-8914" for this suite. 01/18/23 15:16:39.655
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:16:39.668
Jan 18 15:16:39.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename replication-controller 01/18/23 15:16:39.669
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:16:39.687
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:16:39.692
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-cffea559-41f1-4116-87b1-cbcdb2398a5d 01/18/23 15:16:39.696
Jan 18 15:16:39.709: INFO: Pod name my-hostname-basic-cffea559-41f1-4116-87b1-cbcdb2398a5d: Found 0 pods out of 1
Jan 18 15:16:44.715: INFO: Pod name my-hostname-basic-cffea559-41f1-4116-87b1-cbcdb2398a5d: Found 1 pods out of 1
Jan 18 15:16:44.716: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-cffea559-41f1-4116-87b1-cbcdb2398a5d" are running
Jan 18 15:16:44.716: INFO: Waiting up to 5m0s for pod "my-hostname-basic-cffea559-41f1-4116-87b1-cbcdb2398a5d-98v8m" in namespace "replication-controller-7960" to be "running"
Jan 18 15:16:44.721: INFO: Pod "my-hostname-basic-cffea559-41f1-4116-87b1-cbcdb2398a5d-98v8m": Phase="Running", Reason="", readiness=true. Elapsed: 4.585732ms
Jan 18 15:16:44.721: INFO: Pod "my-hostname-basic-cffea559-41f1-4116-87b1-cbcdb2398a5d-98v8m" satisfied condition "running"
Jan 18 15:16:44.721: INFO: Pod "my-hostname-basic-cffea559-41f1-4116-87b1-cbcdb2398a5d-98v8m" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 15:16:39 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 15:16:41 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 15:16:41 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 15:16:39 +0000 UTC Reason: Message:}])
Jan 18 15:16:44.721: INFO: Trying to dial the pod
Jan 18 15:16:49.740: INFO: Controller my-hostname-basic-cffea559-41f1-4116-87b1-cbcdb2398a5d: Got expected result from replica 1 [my-hostname-basic-cffea559-41f1-4116-87b1-cbcdb2398a5d-98v8m]: "my-hostname-basic-cffea559-41f1-4116-87b1-cbcdb2398a5d-98v8m", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 18 15:16:49.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7960" for this suite. 01/18/23 15:16:49.745
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":34,"skipped":697,"failed":0}
------------------------------
• [SLOW TEST] [10.096 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:16:39.668
    Jan 18 15:16:39.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename replication-controller 01/18/23 15:16:39.669
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:16:39.687
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:16:39.692
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-cffea559-41f1-4116-87b1-cbcdb2398a5d 01/18/23 15:16:39.696
    Jan 18 15:16:39.709: INFO: Pod name my-hostname-basic-cffea559-41f1-4116-87b1-cbcdb2398a5d: Found 0 pods out of 1
    Jan 18 15:16:44.715: INFO: Pod name my-hostname-basic-cffea559-41f1-4116-87b1-cbcdb2398a5d: Found 1 pods out of 1
    Jan 18 15:16:44.716: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-cffea559-41f1-4116-87b1-cbcdb2398a5d" are running
    Jan 18 15:16:44.716: INFO: Waiting up to 5m0s for pod "my-hostname-basic-cffea559-41f1-4116-87b1-cbcdb2398a5d-98v8m" in namespace "replication-controller-7960" to be "running"
    Jan 18 15:16:44.721: INFO: Pod "my-hostname-basic-cffea559-41f1-4116-87b1-cbcdb2398a5d-98v8m": Phase="Running", Reason="", readiness=true. Elapsed: 4.585732ms
    Jan 18 15:16:44.721: INFO: Pod "my-hostname-basic-cffea559-41f1-4116-87b1-cbcdb2398a5d-98v8m" satisfied condition "running"
    Jan 18 15:16:44.721: INFO: Pod "my-hostname-basic-cffea559-41f1-4116-87b1-cbcdb2398a5d-98v8m" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 15:16:39 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 15:16:41 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 15:16:41 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 15:16:39 +0000 UTC Reason: Message:}])
    Jan 18 15:16:44.721: INFO: Trying to dial the pod
    Jan 18 15:16:49.740: INFO: Controller my-hostname-basic-cffea559-41f1-4116-87b1-cbcdb2398a5d: Got expected result from replica 1 [my-hostname-basic-cffea559-41f1-4116-87b1-cbcdb2398a5d-98v8m]: "my-hostname-basic-cffea559-41f1-4116-87b1-cbcdb2398a5d-98v8m", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 18 15:16:49.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-7960" for this suite. 01/18/23 15:16:49.745
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:16:49.765
Jan 18 15:16:49.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename emptydir 01/18/23 15:16:49.767
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:16:49.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:16:49.803
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/18/23 15:16:49.809
Jan 18 15:16:49.820: INFO: Waiting up to 5m0s for pod "pod-a3b801b3-d9fe-46f1-995f-03e908d05edb" in namespace "emptydir-3815" to be "Succeeded or Failed"
Jan 18 15:16:49.838: INFO: Pod "pod-a3b801b3-d9fe-46f1-995f-03e908d05edb": Phase="Pending", Reason="", readiness=false. Elapsed: 17.130091ms
Jan 18 15:16:51.844: INFO: Pod "pod-a3b801b3-d9fe-46f1-995f-03e908d05edb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023679964s
Jan 18 15:16:53.845: INFO: Pod "pod-a3b801b3-d9fe-46f1-995f-03e908d05edb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024070063s
STEP: Saw pod success 01/18/23 15:16:53.845
Jan 18 15:16:53.845: INFO: Pod "pod-a3b801b3-d9fe-46f1-995f-03e908d05edb" satisfied condition "Succeeded or Failed"
Jan 18 15:16:53.858: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-a3b801b3-d9fe-46f1-995f-03e908d05edb container test-container: <nil>
STEP: delete the pod 01/18/23 15:16:53.872
Jan 18 15:16:53.888: INFO: Waiting for pod pod-a3b801b3-d9fe-46f1-995f-03e908d05edb to disappear
Jan 18 15:16:53.892: INFO: Pod pod-a3b801b3-d9fe-46f1-995f-03e908d05edb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 15:16:53.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3815" for this suite. 01/18/23 15:16:53.911
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":35,"skipped":702,"failed":0}
------------------------------
• [4.158 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:16:49.765
    Jan 18 15:16:49.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename emptydir 01/18/23 15:16:49.767
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:16:49.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:16:49.803
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/18/23 15:16:49.809
    Jan 18 15:16:49.820: INFO: Waiting up to 5m0s for pod "pod-a3b801b3-d9fe-46f1-995f-03e908d05edb" in namespace "emptydir-3815" to be "Succeeded or Failed"
    Jan 18 15:16:49.838: INFO: Pod "pod-a3b801b3-d9fe-46f1-995f-03e908d05edb": Phase="Pending", Reason="", readiness=false. Elapsed: 17.130091ms
    Jan 18 15:16:51.844: INFO: Pod "pod-a3b801b3-d9fe-46f1-995f-03e908d05edb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023679964s
    Jan 18 15:16:53.845: INFO: Pod "pod-a3b801b3-d9fe-46f1-995f-03e908d05edb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024070063s
    STEP: Saw pod success 01/18/23 15:16:53.845
    Jan 18 15:16:53.845: INFO: Pod "pod-a3b801b3-d9fe-46f1-995f-03e908d05edb" satisfied condition "Succeeded or Failed"
    Jan 18 15:16:53.858: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-a3b801b3-d9fe-46f1-995f-03e908d05edb container test-container: <nil>
    STEP: delete the pod 01/18/23 15:16:53.872
    Jan 18 15:16:53.888: INFO: Waiting for pod pod-a3b801b3-d9fe-46f1-995f-03e908d05edb to disappear
    Jan 18 15:16:53.892: INFO: Pod pod-a3b801b3-d9fe-46f1-995f-03e908d05edb no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 15:16:53.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3815" for this suite. 01/18/23 15:16:53.911
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:16:53.925
Jan 18 15:16:53.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename container-probe 01/18/23 15:16:53.928
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:16:53.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:16:53.963
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-46c99426-b76b-42b1-a7a7-92efb1546733 in namespace container-probe-7669 01/18/23 15:16:53.97
Jan 18 15:16:53.982: INFO: Waiting up to 5m0s for pod "liveness-46c99426-b76b-42b1-a7a7-92efb1546733" in namespace "container-probe-7669" to be "not pending"
Jan 18 15:16:53.987: INFO: Pod "liveness-46c99426-b76b-42b1-a7a7-92efb1546733": Phase="Pending", Reason="", readiness=false. Elapsed: 5.02343ms
Jan 18 15:16:55.992: INFO: Pod "liveness-46c99426-b76b-42b1-a7a7-92efb1546733": Phase="Running", Reason="", readiness=true. Elapsed: 2.010678811s
Jan 18 15:16:55.992: INFO: Pod "liveness-46c99426-b76b-42b1-a7a7-92efb1546733" satisfied condition "not pending"
Jan 18 15:16:55.992: INFO: Started pod liveness-46c99426-b76b-42b1-a7a7-92efb1546733 in namespace container-probe-7669
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 15:16:55.992
Jan 18 15:16:55.997: INFO: Initial restart count of pod liveness-46c99426-b76b-42b1-a7a7-92efb1546733 is 0
STEP: deleting the pod 01/18/23 15:20:56.886
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 18 15:20:56.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7669" for this suite. 01/18/23 15:20:56.919
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":36,"skipped":714,"failed":0}
------------------------------
• [SLOW TEST] [243.001 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:16:53.925
    Jan 18 15:16:53.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename container-probe 01/18/23 15:16:53.928
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:16:53.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:16:53.963
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-46c99426-b76b-42b1-a7a7-92efb1546733 in namespace container-probe-7669 01/18/23 15:16:53.97
    Jan 18 15:16:53.982: INFO: Waiting up to 5m0s for pod "liveness-46c99426-b76b-42b1-a7a7-92efb1546733" in namespace "container-probe-7669" to be "not pending"
    Jan 18 15:16:53.987: INFO: Pod "liveness-46c99426-b76b-42b1-a7a7-92efb1546733": Phase="Pending", Reason="", readiness=false. Elapsed: 5.02343ms
    Jan 18 15:16:55.992: INFO: Pod "liveness-46c99426-b76b-42b1-a7a7-92efb1546733": Phase="Running", Reason="", readiness=true. Elapsed: 2.010678811s
    Jan 18 15:16:55.992: INFO: Pod "liveness-46c99426-b76b-42b1-a7a7-92efb1546733" satisfied condition "not pending"
    Jan 18 15:16:55.992: INFO: Started pod liveness-46c99426-b76b-42b1-a7a7-92efb1546733 in namespace container-probe-7669
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 15:16:55.992
    Jan 18 15:16:55.997: INFO: Initial restart count of pod liveness-46c99426-b76b-42b1-a7a7-92efb1546733 is 0
    STEP: deleting the pod 01/18/23 15:20:56.886
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 18 15:20:56.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7669" for this suite. 01/18/23 15:20:56.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:20:56.932
Jan 18 15:20:56.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename init-container 01/18/23 15:20:56.939
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:20:56.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:20:56.975
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 01/18/23 15:20:56.978
Jan 18 15:20:56.980: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 18 15:21:01.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4441" for this suite. 01/18/23 15:21:01.226
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":37,"skipped":744,"failed":0}
------------------------------
• [4.301 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:20:56.932
    Jan 18 15:20:56.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename init-container 01/18/23 15:20:56.939
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:20:56.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:20:56.975
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 01/18/23 15:20:56.978
    Jan 18 15:20:56.980: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 18 15:21:01.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-4441" for this suite. 01/18/23 15:21:01.226
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:21:01.237
Jan 18 15:21:01.237: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename pods 01/18/23 15:21:01.238
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:21:01.259
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:21:01.263
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 01/18/23 15:21:01.268
Jan 18 15:21:01.283: INFO: Waiting up to 5m0s for pod "pod-hostip-9d9a8639-0afb-4768-88b8-3ab0d0699bbe" in namespace "pods-8113" to be "running and ready"
Jan 18 15:21:01.302: INFO: Pod "pod-hostip-9d9a8639-0afb-4768-88b8-3ab0d0699bbe": Phase="Pending", Reason="", readiness=false. Elapsed: 19.11916ms
Jan 18 15:21:01.302: INFO: The phase of Pod pod-hostip-9d9a8639-0afb-4768-88b8-3ab0d0699bbe is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:21:03.311: INFO: Pod "pod-hostip-9d9a8639-0afb-4768-88b8-3ab0d0699bbe": Phase="Running", Reason="", readiness=true. Elapsed: 2.027786533s
Jan 18 15:21:03.311: INFO: The phase of Pod pod-hostip-9d9a8639-0afb-4768-88b8-3ab0d0699bbe is Running (Ready = true)
Jan 18 15:21:03.311: INFO: Pod "pod-hostip-9d9a8639-0afb-4768-88b8-3ab0d0699bbe" satisfied condition "running and ready"
Jan 18 15:21:03.323: INFO: Pod pod-hostip-9d9a8639-0afb-4768-88b8-3ab0d0699bbe has hostIP: 192.168.101.216
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 15:21:03.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8113" for this suite. 01/18/23 15:21:03.33
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":38,"skipped":778,"failed":0}
------------------------------
• [2.101 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:21:01.237
    Jan 18 15:21:01.237: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename pods 01/18/23 15:21:01.238
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:21:01.259
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:21:01.263
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 01/18/23 15:21:01.268
    Jan 18 15:21:01.283: INFO: Waiting up to 5m0s for pod "pod-hostip-9d9a8639-0afb-4768-88b8-3ab0d0699bbe" in namespace "pods-8113" to be "running and ready"
    Jan 18 15:21:01.302: INFO: Pod "pod-hostip-9d9a8639-0afb-4768-88b8-3ab0d0699bbe": Phase="Pending", Reason="", readiness=false. Elapsed: 19.11916ms
    Jan 18 15:21:01.302: INFO: The phase of Pod pod-hostip-9d9a8639-0afb-4768-88b8-3ab0d0699bbe is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:21:03.311: INFO: Pod "pod-hostip-9d9a8639-0afb-4768-88b8-3ab0d0699bbe": Phase="Running", Reason="", readiness=true. Elapsed: 2.027786533s
    Jan 18 15:21:03.311: INFO: The phase of Pod pod-hostip-9d9a8639-0afb-4768-88b8-3ab0d0699bbe is Running (Ready = true)
    Jan 18 15:21:03.311: INFO: Pod "pod-hostip-9d9a8639-0afb-4768-88b8-3ab0d0699bbe" satisfied condition "running and ready"
    Jan 18 15:21:03.323: INFO: Pod pod-hostip-9d9a8639-0afb-4768-88b8-3ab0d0699bbe has hostIP: 192.168.101.216
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 15:21:03.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8113" for this suite. 01/18/23 15:21:03.33
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:21:03.347
Jan 18 15:21:03.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename downward-api 01/18/23 15:21:03.348
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:21:03.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:21:03.375
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 01/18/23 15:21:03.38
Jan 18 15:21:03.390: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6dcdbee5-91c7-4ccc-add4-505161ab53b0" in namespace "downward-api-1821" to be "Succeeded or Failed"
Jan 18 15:21:03.395: INFO: Pod "downwardapi-volume-6dcdbee5-91c7-4ccc-add4-505161ab53b0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.853389ms
Jan 18 15:21:05.401: INFO: Pod "downwardapi-volume-6dcdbee5-91c7-4ccc-add4-505161ab53b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01104463s
Jan 18 15:21:07.401: INFO: Pod "downwardapi-volume-6dcdbee5-91c7-4ccc-add4-505161ab53b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010835107s
STEP: Saw pod success 01/18/23 15:21:07.401
Jan 18 15:21:07.402: INFO: Pod "downwardapi-volume-6dcdbee5-91c7-4ccc-add4-505161ab53b0" satisfied condition "Succeeded or Failed"
Jan 18 15:21:07.408: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-6dcdbee5-91c7-4ccc-add4-505161ab53b0 container client-container: <nil>
STEP: delete the pod 01/18/23 15:21:07.429
Jan 18 15:21:07.443: INFO: Waiting for pod downwardapi-volume-6dcdbee5-91c7-4ccc-add4-505161ab53b0 to disappear
Jan 18 15:21:07.446: INFO: Pod downwardapi-volume-6dcdbee5-91c7-4ccc-add4-505161ab53b0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 15:21:07.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1821" for this suite. 01/18/23 15:21:07.45
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":39,"skipped":820,"failed":0}
------------------------------
• [4.109 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:21:03.347
    Jan 18 15:21:03.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename downward-api 01/18/23 15:21:03.348
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:21:03.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:21:03.375
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 01/18/23 15:21:03.38
    Jan 18 15:21:03.390: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6dcdbee5-91c7-4ccc-add4-505161ab53b0" in namespace "downward-api-1821" to be "Succeeded or Failed"
    Jan 18 15:21:03.395: INFO: Pod "downwardapi-volume-6dcdbee5-91c7-4ccc-add4-505161ab53b0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.853389ms
    Jan 18 15:21:05.401: INFO: Pod "downwardapi-volume-6dcdbee5-91c7-4ccc-add4-505161ab53b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01104463s
    Jan 18 15:21:07.401: INFO: Pod "downwardapi-volume-6dcdbee5-91c7-4ccc-add4-505161ab53b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010835107s
    STEP: Saw pod success 01/18/23 15:21:07.401
    Jan 18 15:21:07.402: INFO: Pod "downwardapi-volume-6dcdbee5-91c7-4ccc-add4-505161ab53b0" satisfied condition "Succeeded or Failed"
    Jan 18 15:21:07.408: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-6dcdbee5-91c7-4ccc-add4-505161ab53b0 container client-container: <nil>
    STEP: delete the pod 01/18/23 15:21:07.429
    Jan 18 15:21:07.443: INFO: Waiting for pod downwardapi-volume-6dcdbee5-91c7-4ccc-add4-505161ab53b0 to disappear
    Jan 18 15:21:07.446: INFO: Pod downwardapi-volume-6dcdbee5-91c7-4ccc-add4-505161ab53b0 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 15:21:07.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1821" for this suite. 01/18/23 15:21:07.45
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:21:07.457
Jan 18 15:21:07.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename webhook 01/18/23 15:21:07.459
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:21:07.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:21:07.607
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 15:21:07.638
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 15:21:08.104
STEP: Deploying the webhook pod 01/18/23 15:21:08.118
STEP: Wait for the deployment to be ready 01/18/23 15:21:08.129
Jan 18 15:21:08.147: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 18 15:21:10.162: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 21, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 21, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 21, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 21, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/18/23 15:21:12.166
STEP: Verifying the service has paired with the endpoint 01/18/23 15:21:12.187
Jan 18 15:21:13.188: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 01/18/23 15:21:13.194
STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/18/23 15:21:13.221
STEP: Creating a configMap that should not be mutated 01/18/23 15:21:13.229
STEP: Patching a mutating webhook configuration's rules to include the create operation 01/18/23 15:21:13.243
STEP: Creating a configMap that should be mutated 01/18/23 15:21:13.252
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 15:21:13.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1400" for this suite. 01/18/23 15:21:13.282
STEP: Destroying namespace "webhook-1400-markers" for this suite. 01/18/23 15:21:13.288
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":40,"skipped":821,"failed":0}
------------------------------
• [SLOW TEST] [5.900 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:21:07.457
    Jan 18 15:21:07.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename webhook 01/18/23 15:21:07.459
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:21:07.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:21:07.607
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 15:21:07.638
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 15:21:08.104
    STEP: Deploying the webhook pod 01/18/23 15:21:08.118
    STEP: Wait for the deployment to be ready 01/18/23 15:21:08.129
    Jan 18 15:21:08.147: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 18 15:21:10.162: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 21, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 21, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 21, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 21, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/18/23 15:21:12.166
    STEP: Verifying the service has paired with the endpoint 01/18/23 15:21:12.187
    Jan 18 15:21:13.188: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 01/18/23 15:21:13.194
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/18/23 15:21:13.221
    STEP: Creating a configMap that should not be mutated 01/18/23 15:21:13.229
    STEP: Patching a mutating webhook configuration's rules to include the create operation 01/18/23 15:21:13.243
    STEP: Creating a configMap that should be mutated 01/18/23 15:21:13.252
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 15:21:13.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1400" for this suite. 01/18/23 15:21:13.282
    STEP: Destroying namespace "webhook-1400-markers" for this suite. 01/18/23 15:21:13.288
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:21:13.382
Jan 18 15:21:13.382: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename resourcequota 01/18/23 15:21:13.385
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:21:13.442
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:21:13.448
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 01/18/23 15:21:13.454
STEP: Creating a ResourceQuota 01/18/23 15:21:18.464
STEP: Ensuring resource quota status is calculated 01/18/23 15:21:18.472
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 15:21:20.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-35" for this suite. 01/18/23 15:21:20.486
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":41,"skipped":868,"failed":0}
------------------------------
• [SLOW TEST] [7.113 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:21:13.382
    Jan 18 15:21:13.382: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename resourcequota 01/18/23 15:21:13.385
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:21:13.442
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:21:13.448
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 01/18/23 15:21:13.454
    STEP: Creating a ResourceQuota 01/18/23 15:21:18.464
    STEP: Ensuring resource quota status is calculated 01/18/23 15:21:18.472
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 15:21:20.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-35" for this suite. 01/18/23 15:21:20.486
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:21:20.502
Jan 18 15:21:20.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename webhook 01/18/23 15:21:20.504
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:21:20.592
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:21:20.599
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 15:21:20.628
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 15:21:21.222
STEP: Deploying the webhook pod 01/18/23 15:21:21.23
STEP: Wait for the deployment to be ready 01/18/23 15:21:21.242
Jan 18 15:21:21.254: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 15:21:23.267
STEP: Verifying the service has paired with the endpoint 01/18/23 15:21:23.279
Jan 18 15:21:24.280: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/18/23 15:21:24.286
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/18/23 15:21:24.313
STEP: Creating a dummy validating-webhook-configuration object 01/18/23 15:21:24.331
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/18/23 15:21:24.344
STEP: Creating a dummy mutating-webhook-configuration object 01/18/23 15:21:24.35
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/18/23 15:21:24.364
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 15:21:24.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4815" for this suite. 01/18/23 15:21:24.395
STEP: Destroying namespace "webhook-4815-markers" for this suite. 01/18/23 15:21:24.404
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":42,"skipped":887,"failed":0}
------------------------------
• [4.012 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:21:20.502
    Jan 18 15:21:20.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename webhook 01/18/23 15:21:20.504
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:21:20.592
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:21:20.599
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 15:21:20.628
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 15:21:21.222
    STEP: Deploying the webhook pod 01/18/23 15:21:21.23
    STEP: Wait for the deployment to be ready 01/18/23 15:21:21.242
    Jan 18 15:21:21.254: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 15:21:23.267
    STEP: Verifying the service has paired with the endpoint 01/18/23 15:21:23.279
    Jan 18 15:21:24.280: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/18/23 15:21:24.286
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/18/23 15:21:24.313
    STEP: Creating a dummy validating-webhook-configuration object 01/18/23 15:21:24.331
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/18/23 15:21:24.344
    STEP: Creating a dummy mutating-webhook-configuration object 01/18/23 15:21:24.35
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/18/23 15:21:24.364
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 15:21:24.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4815" for this suite. 01/18/23 15:21:24.395
    STEP: Destroying namespace "webhook-4815-markers" for this suite. 01/18/23 15:21:24.404
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:21:24.514
Jan 18 15:21:24.514: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename discovery 01/18/23 15:21:24.516
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:21:24.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:21:24.583
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 01/18/23 15:21:24.591
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jan 18 15:21:25.237: INFO: Checking APIGroup: apiregistration.k8s.io
Jan 18 15:21:25.239: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan 18 15:21:25.240: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan 18 15:21:25.240: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan 18 15:21:25.240: INFO: Checking APIGroup: apps
Jan 18 15:21:25.242: INFO: PreferredVersion.GroupVersion: apps/v1
Jan 18 15:21:25.242: INFO: Versions found [{apps/v1 v1}]
Jan 18 15:21:25.242: INFO: apps/v1 matches apps/v1
Jan 18 15:21:25.242: INFO: Checking APIGroup: events.k8s.io
Jan 18 15:21:25.243: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan 18 15:21:25.243: INFO: Versions found [{events.k8s.io/v1 v1}]
Jan 18 15:21:25.243: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan 18 15:21:25.243: INFO: Checking APIGroup: authentication.k8s.io
Jan 18 15:21:25.245: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan 18 15:21:25.245: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan 18 15:21:25.245: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan 18 15:21:25.245: INFO: Checking APIGroup: authorization.k8s.io
Jan 18 15:21:25.246: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan 18 15:21:25.246: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan 18 15:21:25.246: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan 18 15:21:25.246: INFO: Checking APIGroup: autoscaling
Jan 18 15:21:25.247: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan 18 15:21:25.247: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Jan 18 15:21:25.247: INFO: autoscaling/v2 matches autoscaling/v2
Jan 18 15:21:25.247: INFO: Checking APIGroup: batch
Jan 18 15:21:25.249: INFO: PreferredVersion.GroupVersion: batch/v1
Jan 18 15:21:25.249: INFO: Versions found [{batch/v1 v1}]
Jan 18 15:21:25.249: INFO: batch/v1 matches batch/v1
Jan 18 15:21:25.249: INFO: Checking APIGroup: certificates.k8s.io
Jan 18 15:21:25.250: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan 18 15:21:25.251: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan 18 15:21:25.251: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan 18 15:21:25.251: INFO: Checking APIGroup: networking.k8s.io
Jan 18 15:21:25.252: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan 18 15:21:25.252: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jan 18 15:21:25.252: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan 18 15:21:25.253: INFO: Checking APIGroup: policy
Jan 18 15:21:25.254: INFO: PreferredVersion.GroupVersion: policy/v1
Jan 18 15:21:25.254: INFO: Versions found [{policy/v1 v1}]
Jan 18 15:21:25.255: INFO: policy/v1 matches policy/v1
Jan 18 15:21:25.255: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan 18 15:21:25.256: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan 18 15:21:25.256: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan 18 15:21:25.256: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan 18 15:21:25.256: INFO: Checking APIGroup: storage.k8s.io
Jan 18 15:21:25.258: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan 18 15:21:25.258: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan 18 15:21:25.258: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan 18 15:21:25.258: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan 18 15:21:25.260: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan 18 15:21:25.260: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan 18 15:21:25.261: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan 18 15:21:25.261: INFO: Checking APIGroup: apiextensions.k8s.io
Jan 18 15:21:25.263: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan 18 15:21:25.263: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan 18 15:21:25.263: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan 18 15:21:25.263: INFO: Checking APIGroup: scheduling.k8s.io
Jan 18 15:21:25.265: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan 18 15:21:25.266: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan 18 15:21:25.266: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan 18 15:21:25.266: INFO: Checking APIGroup: coordination.k8s.io
Jan 18 15:21:25.268: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan 18 15:21:25.268: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan 18 15:21:25.268: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan 18 15:21:25.268: INFO: Checking APIGroup: node.k8s.io
Jan 18 15:21:25.270: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan 18 15:21:25.270: INFO: Versions found [{node.k8s.io/v1 v1}]
Jan 18 15:21:25.270: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan 18 15:21:25.270: INFO: Checking APIGroup: discovery.k8s.io
Jan 18 15:21:25.272: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan 18 15:21:25.272: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jan 18 15:21:25.272: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan 18 15:21:25.272: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan 18 15:21:25.273: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Jan 18 15:21:25.274: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jan 18 15:21:25.274: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Jan 18 15:21:25.274: INFO: Checking APIGroup: crd.projectcalico.org
Jan 18 15:21:25.276: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jan 18 15:21:25.276: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jan 18 15:21:25.276: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jan 18 15:21:25.276: INFO: Checking APIGroup: notification.toolkit.fluxcd.io
Jan 18 15:21:25.277: INFO: PreferredVersion.GroupVersion: notification.toolkit.fluxcd.io/v1beta2
Jan 18 15:21:25.277: INFO: Versions found [{notification.toolkit.fluxcd.io/v1beta2 v1beta2} {notification.toolkit.fluxcd.io/v1beta1 v1beta1}]
Jan 18 15:21:25.278: INFO: notification.toolkit.fluxcd.io/v1beta2 matches notification.toolkit.fluxcd.io/v1beta2
Jan 18 15:21:25.278: INFO: Checking APIGroup: source.toolkit.fluxcd.io
Jan 18 15:21:25.279: INFO: PreferredVersion.GroupVersion: source.toolkit.fluxcd.io/v1beta2
Jan 18 15:21:25.279: INFO: Versions found [{source.toolkit.fluxcd.io/v1beta2 v1beta2} {source.toolkit.fluxcd.io/v1beta1 v1beta1}]
Jan 18 15:21:25.279: INFO: source.toolkit.fluxcd.io/v1beta2 matches source.toolkit.fluxcd.io/v1beta2
Jan 18 15:21:25.279: INFO: Checking APIGroup: helm.toolkit.fluxcd.io
Jan 18 15:21:25.281: INFO: PreferredVersion.GroupVersion: helm.toolkit.fluxcd.io/v2beta1
Jan 18 15:21:25.281: INFO: Versions found [{helm.toolkit.fluxcd.io/v2beta1 v2beta1}]
Jan 18 15:21:25.281: INFO: helm.toolkit.fluxcd.io/v2beta1 matches helm.toolkit.fluxcd.io/v2beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Jan 18 15:21:25.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-6118" for this suite. 01/18/23 15:21:25.287
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":43,"skipped":887,"failed":0}
------------------------------
• [0.782 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:21:24.514
    Jan 18 15:21:24.514: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename discovery 01/18/23 15:21:24.516
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:21:24.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:21:24.583
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 01/18/23 15:21:24.591
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jan 18 15:21:25.237: INFO: Checking APIGroup: apiregistration.k8s.io
    Jan 18 15:21:25.239: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jan 18 15:21:25.240: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jan 18 15:21:25.240: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jan 18 15:21:25.240: INFO: Checking APIGroup: apps
    Jan 18 15:21:25.242: INFO: PreferredVersion.GroupVersion: apps/v1
    Jan 18 15:21:25.242: INFO: Versions found [{apps/v1 v1}]
    Jan 18 15:21:25.242: INFO: apps/v1 matches apps/v1
    Jan 18 15:21:25.242: INFO: Checking APIGroup: events.k8s.io
    Jan 18 15:21:25.243: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jan 18 15:21:25.243: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jan 18 15:21:25.243: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jan 18 15:21:25.243: INFO: Checking APIGroup: authentication.k8s.io
    Jan 18 15:21:25.245: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jan 18 15:21:25.245: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jan 18 15:21:25.245: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jan 18 15:21:25.245: INFO: Checking APIGroup: authorization.k8s.io
    Jan 18 15:21:25.246: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jan 18 15:21:25.246: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jan 18 15:21:25.246: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jan 18 15:21:25.246: INFO: Checking APIGroup: autoscaling
    Jan 18 15:21:25.247: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jan 18 15:21:25.247: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Jan 18 15:21:25.247: INFO: autoscaling/v2 matches autoscaling/v2
    Jan 18 15:21:25.247: INFO: Checking APIGroup: batch
    Jan 18 15:21:25.249: INFO: PreferredVersion.GroupVersion: batch/v1
    Jan 18 15:21:25.249: INFO: Versions found [{batch/v1 v1}]
    Jan 18 15:21:25.249: INFO: batch/v1 matches batch/v1
    Jan 18 15:21:25.249: INFO: Checking APIGroup: certificates.k8s.io
    Jan 18 15:21:25.250: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jan 18 15:21:25.251: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jan 18 15:21:25.251: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jan 18 15:21:25.251: INFO: Checking APIGroup: networking.k8s.io
    Jan 18 15:21:25.252: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jan 18 15:21:25.252: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jan 18 15:21:25.252: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jan 18 15:21:25.253: INFO: Checking APIGroup: policy
    Jan 18 15:21:25.254: INFO: PreferredVersion.GroupVersion: policy/v1
    Jan 18 15:21:25.254: INFO: Versions found [{policy/v1 v1}]
    Jan 18 15:21:25.255: INFO: policy/v1 matches policy/v1
    Jan 18 15:21:25.255: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jan 18 15:21:25.256: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jan 18 15:21:25.256: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jan 18 15:21:25.256: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jan 18 15:21:25.256: INFO: Checking APIGroup: storage.k8s.io
    Jan 18 15:21:25.258: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jan 18 15:21:25.258: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jan 18 15:21:25.258: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jan 18 15:21:25.258: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jan 18 15:21:25.260: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jan 18 15:21:25.260: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jan 18 15:21:25.261: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jan 18 15:21:25.261: INFO: Checking APIGroup: apiextensions.k8s.io
    Jan 18 15:21:25.263: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jan 18 15:21:25.263: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jan 18 15:21:25.263: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jan 18 15:21:25.263: INFO: Checking APIGroup: scheduling.k8s.io
    Jan 18 15:21:25.265: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jan 18 15:21:25.266: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jan 18 15:21:25.266: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jan 18 15:21:25.266: INFO: Checking APIGroup: coordination.k8s.io
    Jan 18 15:21:25.268: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jan 18 15:21:25.268: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jan 18 15:21:25.268: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jan 18 15:21:25.268: INFO: Checking APIGroup: node.k8s.io
    Jan 18 15:21:25.270: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jan 18 15:21:25.270: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jan 18 15:21:25.270: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jan 18 15:21:25.270: INFO: Checking APIGroup: discovery.k8s.io
    Jan 18 15:21:25.272: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jan 18 15:21:25.272: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jan 18 15:21:25.272: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jan 18 15:21:25.272: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jan 18 15:21:25.273: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Jan 18 15:21:25.274: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Jan 18 15:21:25.274: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Jan 18 15:21:25.274: INFO: Checking APIGroup: crd.projectcalico.org
    Jan 18 15:21:25.276: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Jan 18 15:21:25.276: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Jan 18 15:21:25.276: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Jan 18 15:21:25.276: INFO: Checking APIGroup: notification.toolkit.fluxcd.io
    Jan 18 15:21:25.277: INFO: PreferredVersion.GroupVersion: notification.toolkit.fluxcd.io/v1beta2
    Jan 18 15:21:25.277: INFO: Versions found [{notification.toolkit.fluxcd.io/v1beta2 v1beta2} {notification.toolkit.fluxcd.io/v1beta1 v1beta1}]
    Jan 18 15:21:25.278: INFO: notification.toolkit.fluxcd.io/v1beta2 matches notification.toolkit.fluxcd.io/v1beta2
    Jan 18 15:21:25.278: INFO: Checking APIGroup: source.toolkit.fluxcd.io
    Jan 18 15:21:25.279: INFO: PreferredVersion.GroupVersion: source.toolkit.fluxcd.io/v1beta2
    Jan 18 15:21:25.279: INFO: Versions found [{source.toolkit.fluxcd.io/v1beta2 v1beta2} {source.toolkit.fluxcd.io/v1beta1 v1beta1}]
    Jan 18 15:21:25.279: INFO: source.toolkit.fluxcd.io/v1beta2 matches source.toolkit.fluxcd.io/v1beta2
    Jan 18 15:21:25.279: INFO: Checking APIGroup: helm.toolkit.fluxcd.io
    Jan 18 15:21:25.281: INFO: PreferredVersion.GroupVersion: helm.toolkit.fluxcd.io/v2beta1
    Jan 18 15:21:25.281: INFO: Versions found [{helm.toolkit.fluxcd.io/v2beta1 v2beta1}]
    Jan 18 15:21:25.281: INFO: helm.toolkit.fluxcd.io/v2beta1 matches helm.toolkit.fluxcd.io/v2beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Jan 18 15:21:25.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-6118" for this suite. 01/18/23 15:21:25.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:21:25.3
Jan 18 15:21:25.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename runtimeclass 01/18/23 15:21:25.302
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:21:25.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:21:25.335
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 01/18/23 15:21:25.339
STEP: getting /apis/node.k8s.io 01/18/23 15:21:25.35
STEP: getting /apis/node.k8s.io/v1 01/18/23 15:21:25.353
STEP: creating 01/18/23 15:21:25.36
STEP: watching 01/18/23 15:21:25.382
Jan 18 15:21:25.382: INFO: starting watch
STEP: getting 01/18/23 15:21:25.396
STEP: listing 01/18/23 15:21:25.399
STEP: patching 01/18/23 15:21:25.403
STEP: updating 01/18/23 15:21:25.412
Jan 18 15:21:25.420: INFO: waiting for watch events with expected annotations
STEP: deleting 01/18/23 15:21:25.42
STEP: deleting a collection 01/18/23 15:21:25.436
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 18 15:21:25.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1130" for this suite. 01/18/23 15:21:25.461
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":44,"skipped":904,"failed":0}
------------------------------
• [0.170 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:21:25.3
    Jan 18 15:21:25.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename runtimeclass 01/18/23 15:21:25.302
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:21:25.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:21:25.335
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 01/18/23 15:21:25.339
    STEP: getting /apis/node.k8s.io 01/18/23 15:21:25.35
    STEP: getting /apis/node.k8s.io/v1 01/18/23 15:21:25.353
    STEP: creating 01/18/23 15:21:25.36
    STEP: watching 01/18/23 15:21:25.382
    Jan 18 15:21:25.382: INFO: starting watch
    STEP: getting 01/18/23 15:21:25.396
    STEP: listing 01/18/23 15:21:25.399
    STEP: patching 01/18/23 15:21:25.403
    STEP: updating 01/18/23 15:21:25.412
    Jan 18 15:21:25.420: INFO: waiting for watch events with expected annotations
    STEP: deleting 01/18/23 15:21:25.42
    STEP: deleting a collection 01/18/23 15:21:25.436
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 18 15:21:25.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-1130" for this suite. 01/18/23 15:21:25.461
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:21:25.484
Jan 18 15:21:25.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename replicaset 01/18/23 15:21:25.486
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:21:25.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:21:25.538
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/18/23 15:21:25.551
Jan 18 15:21:25.578: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 18 15:21:30.589: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 15:21:30.589
STEP: getting scale subresource 01/18/23 15:21:30.59
STEP: updating a scale subresource 01/18/23 15:21:30.601
STEP: verifying the replicaset Spec.Replicas was modified 01/18/23 15:21:30.618
STEP: Patch a scale subresource 01/18/23 15:21:30.642
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 18 15:21:30.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9269" for this suite. 01/18/23 15:21:30.693
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":45,"skipped":951,"failed":0}
------------------------------
• [SLOW TEST] [5.221 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:21:25.484
    Jan 18 15:21:25.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename replicaset 01/18/23 15:21:25.486
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:21:25.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:21:25.538
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/18/23 15:21:25.551
    Jan 18 15:21:25.578: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 18 15:21:30.589: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 15:21:30.589
    STEP: getting scale subresource 01/18/23 15:21:30.59
    STEP: updating a scale subresource 01/18/23 15:21:30.601
    STEP: verifying the replicaset Spec.Replicas was modified 01/18/23 15:21:30.618
    STEP: Patch a scale subresource 01/18/23 15:21:30.642
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 18 15:21:30.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-9269" for this suite. 01/18/23 15:21:30.693
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:21:30.711
Jan 18 15:21:30.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 15:21:30.712
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:21:30.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:21:30.764
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-d73cfff2-10cd-4306-ae9b-618183944421 01/18/23 15:21:30.771
STEP: Creating secret with name secret-projected-all-test-volume-04904256-8a22-4d54-b092-bd07f40692f9 01/18/23 15:21:30.776
STEP: Creating a pod to test Check all projections for projected volume plugin 01/18/23 15:21:30.784
Jan 18 15:21:30.796: INFO: Waiting up to 5m0s for pod "projected-volume-f83bc0c1-d126-478c-bd93-0ab25782ba36" in namespace "projected-3153" to be "Succeeded or Failed"
Jan 18 15:21:30.809: INFO: Pod "projected-volume-f83bc0c1-d126-478c-bd93-0ab25782ba36": Phase="Pending", Reason="", readiness=false. Elapsed: 12.801411ms
Jan 18 15:21:32.815: INFO: Pod "projected-volume-f83bc0c1-d126-478c-bd93-0ab25782ba36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019015329s
Jan 18 15:21:34.815: INFO: Pod "projected-volume-f83bc0c1-d126-478c-bd93-0ab25782ba36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018798316s
STEP: Saw pod success 01/18/23 15:21:34.817
Jan 18 15:21:34.817: INFO: Pod "projected-volume-f83bc0c1-d126-478c-bd93-0ab25782ba36" satisfied condition "Succeeded or Failed"
Jan 18 15:21:34.824: INFO: Trying to get logs from node v1-25-1-18760-w2 pod projected-volume-f83bc0c1-d126-478c-bd93-0ab25782ba36 container projected-all-volume-test: <nil>
STEP: delete the pod 01/18/23 15:21:34.844
Jan 18 15:21:34.863: INFO: Waiting for pod projected-volume-f83bc0c1-d126-478c-bd93-0ab25782ba36 to disappear
Jan 18 15:21:34.867: INFO: Pod projected-volume-f83bc0c1-d126-478c-bd93-0ab25782ba36 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Jan 18 15:21:34.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3153" for this suite. 01/18/23 15:21:34.872
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":46,"skipped":952,"failed":0}
------------------------------
• [4.168 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:21:30.711
    Jan 18 15:21:30.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 15:21:30.712
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:21:30.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:21:30.764
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-d73cfff2-10cd-4306-ae9b-618183944421 01/18/23 15:21:30.771
    STEP: Creating secret with name secret-projected-all-test-volume-04904256-8a22-4d54-b092-bd07f40692f9 01/18/23 15:21:30.776
    STEP: Creating a pod to test Check all projections for projected volume plugin 01/18/23 15:21:30.784
    Jan 18 15:21:30.796: INFO: Waiting up to 5m0s for pod "projected-volume-f83bc0c1-d126-478c-bd93-0ab25782ba36" in namespace "projected-3153" to be "Succeeded or Failed"
    Jan 18 15:21:30.809: INFO: Pod "projected-volume-f83bc0c1-d126-478c-bd93-0ab25782ba36": Phase="Pending", Reason="", readiness=false. Elapsed: 12.801411ms
    Jan 18 15:21:32.815: INFO: Pod "projected-volume-f83bc0c1-d126-478c-bd93-0ab25782ba36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019015329s
    Jan 18 15:21:34.815: INFO: Pod "projected-volume-f83bc0c1-d126-478c-bd93-0ab25782ba36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018798316s
    STEP: Saw pod success 01/18/23 15:21:34.817
    Jan 18 15:21:34.817: INFO: Pod "projected-volume-f83bc0c1-d126-478c-bd93-0ab25782ba36" satisfied condition "Succeeded or Failed"
    Jan 18 15:21:34.824: INFO: Trying to get logs from node v1-25-1-18760-w2 pod projected-volume-f83bc0c1-d126-478c-bd93-0ab25782ba36 container projected-all-volume-test: <nil>
    STEP: delete the pod 01/18/23 15:21:34.844
    Jan 18 15:21:34.863: INFO: Waiting for pod projected-volume-f83bc0c1-d126-478c-bd93-0ab25782ba36 to disappear
    Jan 18 15:21:34.867: INFO: Pod projected-volume-f83bc0c1-d126-478c-bd93-0ab25782ba36 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Jan 18 15:21:34.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3153" for this suite. 01/18/23 15:21:34.872
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:21:34.881
Jan 18 15:21:34.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename init-container 01/18/23 15:21:34.882
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:21:34.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:21:34.955
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 01/18/23 15:21:34.963
Jan 18 15:21:34.963: INFO: PodSpec: initContainers in spec.initContainers
Jan 18 15:22:16.545: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-bb4e6f7f-c94e-4497-a252-6d4a289a6d42", GenerateName:"", Namespace:"init-container-5093", SelfLink:"", UID:"62d177f3-5d01-4e04-bc9d-c435373fe0a3", ResourceVersion:"34391", Generation:0, CreationTimestamp:time.Date(2023, time.January, 18, 15, 21, 34, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"963845935"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"aa89547478d46900650d12de9b2783377dd6833fa315a796e68072991d17e04f", "cni.projectcalico.org/podIP":"10.233.68.99/32", "cni.projectcalico.org/podIPs":"10.233.68.99/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 15, 21, 34, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000c39248), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 15, 21, 35, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000c392a8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 15, 22, 16, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000c392f0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-bgzzc", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003646c00), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bgzzc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bgzzc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bgzzc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00320c660), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"v1-25-1-18760-w2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00059c7e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00320c840)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00320c860)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00320c868), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00320c86c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00129dfd0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 15, 21, 34, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 15, 21, 34, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 15, 21, 34, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 15, 21, 34, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.101.216", PodIP:"10.233.68.99", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.68.99"}}, StartTime:time.Date(2023, time.January, 18, 15, 21, 34, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00059c930)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00059c9a0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://a357e976d1595d0b3329b757751f42711983297e3b127d9ed06a4607476ebc2b", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003646c80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003646c60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc00320ca1f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 18 15:22:16.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5093" for this suite. 01/18/23 15:22:16.572
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":47,"skipped":968,"failed":0}
------------------------------
• [SLOW TEST] [41.707 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:21:34.881
    Jan 18 15:21:34.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename init-container 01/18/23 15:21:34.882
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:21:34.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:21:34.955
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 01/18/23 15:21:34.963
    Jan 18 15:21:34.963: INFO: PodSpec: initContainers in spec.initContainers
    Jan 18 15:22:16.545: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-bb4e6f7f-c94e-4497-a252-6d4a289a6d42", GenerateName:"", Namespace:"init-container-5093", SelfLink:"", UID:"62d177f3-5d01-4e04-bc9d-c435373fe0a3", ResourceVersion:"34391", Generation:0, CreationTimestamp:time.Date(2023, time.January, 18, 15, 21, 34, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"963845935"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"aa89547478d46900650d12de9b2783377dd6833fa315a796e68072991d17e04f", "cni.projectcalico.org/podIP":"10.233.68.99/32", "cni.projectcalico.org/podIPs":"10.233.68.99/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 15, 21, 34, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000c39248), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 15, 21, 35, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000c392a8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 15, 22, 16, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000c392f0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-bgzzc", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003646c00), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bgzzc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bgzzc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bgzzc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00320c660), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"v1-25-1-18760-w2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00059c7e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00320c840)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00320c860)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00320c868), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00320c86c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00129dfd0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 15, 21, 34, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 15, 21, 34, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 15, 21, 34, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 15, 21, 34, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.101.216", PodIP:"10.233.68.99", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.68.99"}}, StartTime:time.Date(2023, time.January, 18, 15, 21, 34, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00059c930)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00059c9a0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://a357e976d1595d0b3329b757751f42711983297e3b127d9ed06a4607476ebc2b", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003646c80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003646c60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc00320ca1f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 18 15:22:16.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-5093" for this suite. 01/18/23 15:22:16.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:22:16.589
Jan 18 15:22:16.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename downward-api 01/18/23 15:22:16.591
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:22:16.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:22:16.631
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 01/18/23 15:22:16.641
Jan 18 15:22:16.653: INFO: Waiting up to 5m0s for pod "downwardapi-volume-41e09ff7-b5c3-40cc-ba97-18dd513f580b" in namespace "downward-api-414" to be "Succeeded or Failed"
Jan 18 15:22:16.668: INFO: Pod "downwardapi-volume-41e09ff7-b5c3-40cc-ba97-18dd513f580b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.995695ms
Jan 18 15:22:18.675: INFO: Pod "downwardapi-volume-41e09ff7-b5c3-40cc-ba97-18dd513f580b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022937405s
Jan 18 15:22:20.674: INFO: Pod "downwardapi-volume-41e09ff7-b5c3-40cc-ba97-18dd513f580b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021587137s
STEP: Saw pod success 01/18/23 15:22:20.674
Jan 18 15:22:20.675: INFO: Pod "downwardapi-volume-41e09ff7-b5c3-40cc-ba97-18dd513f580b" satisfied condition "Succeeded or Failed"
Jan 18 15:22:20.681: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-41e09ff7-b5c3-40cc-ba97-18dd513f580b container client-container: <nil>
STEP: delete the pod 01/18/23 15:22:20.689
Jan 18 15:22:20.712: INFO: Waiting for pod downwardapi-volume-41e09ff7-b5c3-40cc-ba97-18dd513f580b to disappear
Jan 18 15:22:20.715: INFO: Pod downwardapi-volume-41e09ff7-b5c3-40cc-ba97-18dd513f580b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 15:22:20.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-414" for this suite. 01/18/23 15:22:20.721
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":48,"skipped":979,"failed":0}
------------------------------
• [4.139 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:22:16.589
    Jan 18 15:22:16.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename downward-api 01/18/23 15:22:16.591
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:22:16.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:22:16.631
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 01/18/23 15:22:16.641
    Jan 18 15:22:16.653: INFO: Waiting up to 5m0s for pod "downwardapi-volume-41e09ff7-b5c3-40cc-ba97-18dd513f580b" in namespace "downward-api-414" to be "Succeeded or Failed"
    Jan 18 15:22:16.668: INFO: Pod "downwardapi-volume-41e09ff7-b5c3-40cc-ba97-18dd513f580b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.995695ms
    Jan 18 15:22:18.675: INFO: Pod "downwardapi-volume-41e09ff7-b5c3-40cc-ba97-18dd513f580b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022937405s
    Jan 18 15:22:20.674: INFO: Pod "downwardapi-volume-41e09ff7-b5c3-40cc-ba97-18dd513f580b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021587137s
    STEP: Saw pod success 01/18/23 15:22:20.674
    Jan 18 15:22:20.675: INFO: Pod "downwardapi-volume-41e09ff7-b5c3-40cc-ba97-18dd513f580b" satisfied condition "Succeeded or Failed"
    Jan 18 15:22:20.681: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-41e09ff7-b5c3-40cc-ba97-18dd513f580b container client-container: <nil>
    STEP: delete the pod 01/18/23 15:22:20.689
    Jan 18 15:22:20.712: INFO: Waiting for pod downwardapi-volume-41e09ff7-b5c3-40cc-ba97-18dd513f580b to disappear
    Jan 18 15:22:20.715: INFO: Pod downwardapi-volume-41e09ff7-b5c3-40cc-ba97-18dd513f580b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 15:22:20.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-414" for this suite. 01/18/23 15:22:20.721
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:22:20.733
Jan 18 15:22:20.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename proxy 01/18/23 15:22:20.735
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:22:20.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:22:20.773
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 01/18/23 15:22:20.788
STEP: creating replication controller proxy-service-2f5xq in namespace proxy-1293 01/18/23 15:22:20.789
I0118 15:22:20.800219      19 runners.go:193] Created replication controller with name: proxy-service-2f5xq, namespace: proxy-1293, replica count: 1
I0118 15:22:21.851004      19 runners.go:193] proxy-service-2f5xq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 15:22:22.851930      19 runners.go:193] proxy-service-2f5xq Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 15:22:22.856: INFO: setup took 2.07816991s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/18/23 15:22:22.857
Jan 18 15:22:22.879: INFO: (0) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 20.731808ms)
Jan 18 15:22:22.879: INFO: (0) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 20.566911ms)
Jan 18 15:22:22.879: INFO: (0) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 20.968984ms)
Jan 18 15:22:22.885: INFO: (0) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 26.840854ms)
Jan 18 15:22:22.886: INFO: (0) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 27.017479ms)
Jan 18 15:22:22.886: INFO: (0) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 27.986463ms)
Jan 18 15:22:22.886: INFO: (0) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 28.057241ms)
Jan 18 15:22:22.895: INFO: (0) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 36.494795ms)
Jan 18 15:22:22.895: INFO: (0) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 37.366087ms)
Jan 18 15:22:22.895: INFO: (0) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 38.490831ms)
Jan 18 15:22:22.895: INFO: (0) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 37.256423ms)
Jan 18 15:22:22.895: INFO: (0) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 37.042136ms)
Jan 18 15:22:22.895: INFO: (0) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 37.465603ms)
Jan 18 15:22:22.896: INFO: (0) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 37.240666ms)
Jan 18 15:22:22.896: INFO: (0) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 37.805171ms)
Jan 18 15:22:22.896: INFO: (0) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 37.283812ms)
Jan 18 15:22:22.903: INFO: (1) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 6.635977ms)
Jan 18 15:22:22.903: INFO: (1) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 6.413604ms)
Jan 18 15:22:22.912: INFO: (1) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 14.189984ms)
Jan 18 15:22:22.912: INFO: (1) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 14.374408ms)
Jan 18 15:22:22.912: INFO: (1) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 14.496762ms)
Jan 18 15:22:22.912: INFO: (1) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 15.398448ms)
Jan 18 15:22:22.914: INFO: (1) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 17.291311ms)
Jan 18 15:22:22.914: INFO: (1) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 17.056752ms)
Jan 18 15:22:22.923: INFO: (1) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 26.431237ms)
Jan 18 15:22:22.926: INFO: (1) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 29.162211ms)
Jan 18 15:22:22.927: INFO: (1) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 30.321138ms)
Jan 18 15:22:22.932: INFO: (1) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 34.422836ms)
Jan 18 15:22:22.932: INFO: (1) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 34.644245ms)
Jan 18 15:22:22.932: INFO: (1) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 34.598519ms)
Jan 18 15:22:22.932: INFO: (1) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 35.080469ms)
Jan 18 15:22:22.932: INFO: (1) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 34.766992ms)
Jan 18 15:22:22.942: INFO: (2) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 9.614209ms)
Jan 18 15:22:22.947: INFO: (2) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 14.344197ms)
Jan 18 15:22:22.948: INFO: (2) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 15.872018ms)
Jan 18 15:22:22.951: INFO: (2) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 18.110133ms)
Jan 18 15:22:22.952: INFO: (2) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 19.224848ms)
Jan 18 15:22:22.952: INFO: (2) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 18.721293ms)
Jan 18 15:22:22.953: INFO: (2) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 19.344253ms)
Jan 18 15:22:22.953: INFO: (2) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 19.251764ms)
Jan 18 15:22:22.953: INFO: (2) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 19.340395ms)
Jan 18 15:22:22.954: INFO: (2) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 21.851176ms)
Jan 18 15:22:22.955: INFO: (2) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 23.09891ms)
Jan 18 15:22:22.956: INFO: (2) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 23.135756ms)
Jan 18 15:22:22.956: INFO: (2) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 22.284824ms)
Jan 18 15:22:22.956: INFO: (2) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 22.472177ms)
Jan 18 15:22:22.956: INFO: (2) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 23.088326ms)
Jan 18 15:22:22.957: INFO: (2) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 24.29342ms)
Jan 18 15:22:22.961: INFO: (3) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 4.44013ms)
Jan 18 15:22:22.967: INFO: (3) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 9.551792ms)
Jan 18 15:22:22.975: INFO: (3) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 18.035343ms)
Jan 18 15:22:22.976: INFO: (3) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 18.578996ms)
Jan 18 15:22:22.976: INFO: (3) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 18.295295ms)
Jan 18 15:22:22.977: INFO: (3) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 19.346253ms)
Jan 18 15:22:22.977: INFO: (3) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 19.677146ms)
Jan 18 15:22:22.977: INFO: (3) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 19.465824ms)
Jan 18 15:22:22.977: INFO: (3) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 20.194465ms)
Jan 18 15:22:22.978: INFO: (3) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 20.2829ms)
Jan 18 15:22:22.978: INFO: (3) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 20.253719ms)
Jan 18 15:22:22.978: INFO: (3) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 20.626776ms)
Jan 18 15:22:22.978: INFO: (3) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 20.678172ms)
Jan 18 15:22:22.979: INFO: (3) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 21.330169ms)
Jan 18 15:22:22.979: INFO: (3) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 20.980169ms)
Jan 18 15:22:22.979: INFO: (3) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 20.843121ms)
Jan 18 15:22:22.992: INFO: (4) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 12.850358ms)
Jan 18 15:22:22.993: INFO: (4) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 13.511147ms)
Jan 18 15:22:22.994: INFO: (4) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 14.3745ms)
Jan 18 15:22:22.994: INFO: (4) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 13.633996ms)
Jan 18 15:22:22.994: INFO: (4) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 13.560324ms)
Jan 18 15:22:22.995: INFO: (4) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 14.746749ms)
Jan 18 15:22:22.995: INFO: (4) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 15.166223ms)
Jan 18 15:22:22.995: INFO: (4) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 15.030588ms)
Jan 18 15:22:22.995: INFO: (4) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 15.352723ms)
Jan 18 15:22:22.996: INFO: (4) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 15.228457ms)
Jan 18 15:22:22.996: INFO: (4) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 15.115463ms)
Jan 18 15:22:22.997: INFO: (4) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 16.303117ms)
Jan 18 15:22:22.997: INFO: (4) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 16.694415ms)
Jan 18 15:22:22.997: INFO: (4) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 17.865484ms)
Jan 18 15:22:22.998: INFO: (4) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 17.716737ms)
Jan 18 15:22:22.998: INFO: (4) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 17.298457ms)
Jan 18 15:22:23.008: INFO: (5) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 10.208827ms)
Jan 18 15:22:23.039: INFO: (5) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 41.047625ms)
Jan 18 15:22:23.042: INFO: (5) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 43.01027ms)
Jan 18 15:22:23.042: INFO: (5) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 44.00708ms)
Jan 18 15:22:23.042: INFO: (5) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 43.315559ms)
Jan 18 15:22:23.043: INFO: (5) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 44.723157ms)
Jan 18 15:22:23.043: INFO: (5) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 44.36114ms)
Jan 18 15:22:23.044: INFO: (5) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 45.517492ms)
Jan 18 15:22:23.045: INFO: (5) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 45.521888ms)
Jan 18 15:22:23.045: INFO: (5) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 45.776761ms)
Jan 18 15:22:23.045: INFO: (5) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 45.652448ms)
Jan 18 15:22:23.045: INFO: (5) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 46.749962ms)
Jan 18 15:22:23.046: INFO: (5) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 47.019707ms)
Jan 18 15:22:23.051: INFO: (5) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 52.736162ms)
Jan 18 15:22:23.052: INFO: (5) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 53.415421ms)
Jan 18 15:22:23.052: INFO: (5) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 52.88562ms)
Jan 18 15:22:23.063: INFO: (6) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 10.794851ms)
Jan 18 15:22:23.064: INFO: (6) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 9.549967ms)
Jan 18 15:22:23.065: INFO: (6) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 11.819838ms)
Jan 18 15:22:23.065: INFO: (6) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 11.74299ms)
Jan 18 15:22:23.065: INFO: (6) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 12.546351ms)
Jan 18 15:22:23.065: INFO: (6) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 11.208594ms)
Jan 18 15:22:23.065: INFO: (6) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 11.937232ms)
Jan 18 15:22:23.065: INFO: (6) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 11.621803ms)
Jan 18 15:22:23.070: INFO: (6) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 16.425084ms)
Jan 18 15:22:23.072: INFO: (6) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 18.461876ms)
Jan 18 15:22:23.072: INFO: (6) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 19.384107ms)
Jan 18 15:22:23.072: INFO: (6) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 19.228002ms)
Jan 18 15:22:23.072: INFO: (6) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 19.505293ms)
Jan 18 15:22:23.072: INFO: (6) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 20.161709ms)
Jan 18 15:22:23.072: INFO: (6) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 18.428832ms)
Jan 18 15:22:23.076: INFO: (6) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 21.760961ms)
Jan 18 15:22:23.085: INFO: (7) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 8.950251ms)
Jan 18 15:22:23.093: INFO: (7) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 16.055223ms)
Jan 18 15:22:23.093: INFO: (7) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 16.723158ms)
Jan 18 15:22:23.093: INFO: (7) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 17.094706ms)
Jan 18 15:22:23.094: INFO: (7) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 17.523279ms)
Jan 18 15:22:23.094: INFO: (7) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 17.311866ms)
Jan 18 15:22:23.095: INFO: (7) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 18.136801ms)
Jan 18 15:22:23.095: INFO: (7) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 18.497286ms)
Jan 18 15:22:23.095: INFO: (7) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 18.524027ms)
Jan 18 15:22:23.096: INFO: (7) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 19.737878ms)
Jan 18 15:22:23.097: INFO: (7) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 20.003204ms)
Jan 18 15:22:23.098: INFO: (7) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 21.334193ms)
Jan 18 15:22:23.103: INFO: (7) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 26.572622ms)
Jan 18 15:22:23.103: INFO: (7) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 26.449224ms)
Jan 18 15:22:23.104: INFO: (7) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 27.080889ms)
Jan 18 15:22:23.104: INFO: (7) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 27.197924ms)
Jan 18 15:22:23.124: INFO: (8) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 19.358334ms)
Jan 18 15:22:23.138: INFO: (8) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 33.432354ms)
Jan 18 15:22:23.138: INFO: (8) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 32.845091ms)
Jan 18 15:22:23.140: INFO: (8) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 34.699119ms)
Jan 18 15:22:23.140: INFO: (8) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 35.7657ms)
Jan 18 15:22:23.148: INFO: (8) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 43.074803ms)
Jan 18 15:22:23.148: INFO: (8) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 43.098341ms)
Jan 18 15:22:23.148: INFO: (8) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 43.818778ms)
Jan 18 15:22:23.153: INFO: (8) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 48.138351ms)
Jan 18 15:22:23.153: INFO: (8) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 48.122007ms)
Jan 18 15:22:23.153: INFO: (8) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 48.128341ms)
Jan 18 15:22:23.154: INFO: (8) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 48.102005ms)
Jan 18 15:22:23.154: INFO: (8) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 48.427711ms)
Jan 18 15:22:23.154: INFO: (8) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 48.191374ms)
Jan 18 15:22:23.154: INFO: (8) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 48.500873ms)
Jan 18 15:22:23.154: INFO: (8) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 48.743393ms)
Jan 18 15:22:23.168: INFO: (9) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 13.459947ms)
Jan 18 15:22:23.168: INFO: (9) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 12.473311ms)
Jan 18 15:22:23.169: INFO: (9) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 12.129095ms)
Jan 18 15:22:23.170: INFO: (9) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 13.285319ms)
Jan 18 15:22:23.170: INFO: (9) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 14.716663ms)
Jan 18 15:22:23.171: INFO: (9) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 13.941848ms)
Jan 18 15:22:23.171: INFO: (9) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 14.710145ms)
Jan 18 15:22:23.171: INFO: (9) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 13.904025ms)
Jan 18 15:22:23.171: INFO: (9) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 14.337517ms)
Jan 18 15:22:23.171: INFO: (9) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 14.477995ms)
Jan 18 15:22:23.179: INFO: (9) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 21.755754ms)
Jan 18 15:22:23.179: INFO: (9) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 22.316231ms)
Jan 18 15:22:23.179: INFO: (9) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 23.873455ms)
Jan 18 15:22:23.179: INFO: (9) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 22.966156ms)
Jan 18 15:22:23.180: INFO: (9) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 24.623966ms)
Jan 18 15:22:23.180: INFO: (9) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 23.341438ms)
Jan 18 15:22:23.200: INFO: (10) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 19.803184ms)
Jan 18 15:22:23.201: INFO: (10) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 20.73363ms)
Jan 18 15:22:23.201: INFO: (10) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 20.938098ms)
Jan 18 15:22:23.201: INFO: (10) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 21.262977ms)
Jan 18 15:22:23.203: INFO: (10) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 22.27581ms)
Jan 18 15:22:23.203: INFO: (10) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 22.299411ms)
Jan 18 15:22:23.206: INFO: (10) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 25.469446ms)
Jan 18 15:22:23.207: INFO: (10) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 26.566985ms)
Jan 18 15:22:23.208: INFO: (10) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 27.51397ms)
Jan 18 15:22:23.208: INFO: (10) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 27.842486ms)
Jan 18 15:22:23.210: INFO: (10) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 30.110159ms)
Jan 18 15:22:23.210: INFO: (10) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 29.733397ms)
Jan 18 15:22:23.210: INFO: (10) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 30.001274ms)
Jan 18 15:22:23.210: INFO: (10) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 29.715322ms)
Jan 18 15:22:23.210: INFO: (10) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 29.767128ms)
Jan 18 15:22:23.210: INFO: (10) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 30.20709ms)
Jan 18 15:22:23.232: INFO: (11) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 21.219743ms)
Jan 18 15:22:23.232: INFO: (11) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 21.924395ms)
Jan 18 15:22:23.238: INFO: (11) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 25.785108ms)
Jan 18 15:22:23.238: INFO: (11) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 25.018371ms)
Jan 18 15:22:23.238: INFO: (11) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 26.632881ms)
Jan 18 15:22:23.238: INFO: (11) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 26.496357ms)
Jan 18 15:22:23.238: INFO: (11) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 25.293254ms)
Jan 18 15:22:23.239: INFO: (11) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 26.433073ms)
Jan 18 15:22:23.239: INFO: (11) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 28.078497ms)
Jan 18 15:22:23.240: INFO: (11) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 27.877098ms)
Jan 18 15:22:23.241: INFO: (11) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 30.304315ms)
Jan 18 15:22:23.241: INFO: (11) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 28.980571ms)
Jan 18 15:22:23.241: INFO: (11) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 27.892169ms)
Jan 18 15:22:23.241: INFO: (11) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 28.625423ms)
Jan 18 15:22:23.242: INFO: (11) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 28.839536ms)
Jan 18 15:22:23.246: INFO: (11) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 35.326204ms)
Jan 18 15:22:23.259: INFO: (12) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 11.649268ms)
Jan 18 15:22:23.259: INFO: (12) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 12.373791ms)
Jan 18 15:22:23.259: INFO: (12) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 12.845903ms)
Jan 18 15:22:23.259: INFO: (12) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 12.038089ms)
Jan 18 15:22:23.260: INFO: (12) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 13.477375ms)
Jan 18 15:22:23.260: INFO: (12) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 13.283199ms)
Jan 18 15:22:23.260: INFO: (12) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 13.97093ms)
Jan 18 15:22:23.260: INFO: (12) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 12.863659ms)
Jan 18 15:22:23.260: INFO: (12) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 13.21559ms)
Jan 18 15:22:23.262: INFO: (12) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 14.669559ms)
Jan 18 15:22:23.266: INFO: (12) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 18.891091ms)
Jan 18 15:22:23.266: INFO: (12) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 19.272189ms)
Jan 18 15:22:23.266: INFO: (12) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 19.698992ms)
Jan 18 15:22:23.267: INFO: (12) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 19.779288ms)
Jan 18 15:22:23.267: INFO: (12) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 20.24702ms)
Jan 18 15:22:23.268: INFO: (12) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 20.505637ms)
Jan 18 15:22:23.276: INFO: (13) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 7.507555ms)
Jan 18 15:22:23.282: INFO: (13) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 12.660239ms)
Jan 18 15:22:23.282: INFO: (13) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 13.069168ms)
Jan 18 15:22:23.282: INFO: (13) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 13.635306ms)
Jan 18 15:22:23.283: INFO: (13) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 13.047357ms)
Jan 18 15:22:23.283: INFO: (13) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 14.3006ms)
Jan 18 15:22:23.283: INFO: (13) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 14.207659ms)
Jan 18 15:22:23.283: INFO: (13) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 13.406071ms)
Jan 18 15:22:23.283: INFO: (13) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 13.976201ms)
Jan 18 15:22:23.283: INFO: (13) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 15.140129ms)
Jan 18 15:22:23.283: INFO: (13) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 13.840117ms)
Jan 18 15:22:23.283: INFO: (13) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 13.6466ms)
Jan 18 15:22:23.283: INFO: (13) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 14.133276ms)
Jan 18 15:22:23.284: INFO: (13) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 15.9075ms)
Jan 18 15:22:23.284: INFO: (13) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 14.927808ms)
Jan 18 15:22:23.284: INFO: (13) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 16.213081ms)
Jan 18 15:22:23.297: INFO: (14) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 11.809002ms)
Jan 18 15:22:23.297: INFO: (14) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 12.35896ms)
Jan 18 15:22:23.297: INFO: (14) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 12.465581ms)
Jan 18 15:22:23.298: INFO: (14) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 12.845391ms)
Jan 18 15:22:23.298: INFO: (14) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 12.771528ms)
Jan 18 15:22:23.298: INFO: (14) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 13.083496ms)
Jan 18 15:22:23.299: INFO: (14) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 14.127341ms)
Jan 18 15:22:23.300: INFO: (14) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 15.259609ms)
Jan 18 15:22:23.300: INFO: (14) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 15.369274ms)
Jan 18 15:22:23.300: INFO: (14) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 15.468869ms)
Jan 18 15:22:23.301: INFO: (14) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 16.560304ms)
Jan 18 15:22:23.302: INFO: (14) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 16.627464ms)
Jan 18 15:22:23.302: INFO: (14) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 16.998156ms)
Jan 18 15:22:23.302: INFO: (14) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 17.145346ms)
Jan 18 15:22:23.302: INFO: (14) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 17.482638ms)
Jan 18 15:22:23.302: INFO: (14) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 17.283791ms)
Jan 18 15:22:23.317: INFO: (15) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 14.885168ms)
Jan 18 15:22:23.317: INFO: (15) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 14.995989ms)
Jan 18 15:22:23.317: INFO: (15) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 15.306796ms)
Jan 18 15:22:23.317: INFO: (15) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 14.801525ms)
Jan 18 15:22:23.318: INFO: (15) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 15.330619ms)
Jan 18 15:22:23.318: INFO: (15) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 15.128812ms)
Jan 18 15:22:23.318: INFO: (15) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 15.539044ms)
Jan 18 15:22:23.318: INFO: (15) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 16.127791ms)
Jan 18 15:22:23.318: INFO: (15) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 15.821356ms)
Jan 18 15:22:23.320: INFO: (15) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 17.419893ms)
Jan 18 15:22:23.323: INFO: (15) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 20.755032ms)
Jan 18 15:22:23.324: INFO: (15) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 21.458395ms)
Jan 18 15:22:23.324: INFO: (15) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 21.744392ms)
Jan 18 15:22:23.324: INFO: (15) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 21.95546ms)
Jan 18 15:22:23.324: INFO: (15) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 21.88472ms)
Jan 18 15:22:23.325: INFO: (15) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 22.018592ms)
Jan 18 15:22:23.335: INFO: (16) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 10.029125ms)
Jan 18 15:22:23.336: INFO: (16) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 10.649527ms)
Jan 18 15:22:23.336: INFO: (16) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 10.838395ms)
Jan 18 15:22:23.336: INFO: (16) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 10.875208ms)
Jan 18 15:22:23.336: INFO: (16) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 11.194629ms)
Jan 18 15:22:23.341: INFO: (16) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 15.866761ms)
Jan 18 15:22:23.342: INFO: (16) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 15.955297ms)
Jan 18 15:22:23.342: INFO: (16) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 16.601788ms)
Jan 18 15:22:23.342: INFO: (16) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 16.675219ms)
Jan 18 15:22:23.342: INFO: (16) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 17.145392ms)
Jan 18 15:22:23.342: INFO: (16) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 17.377055ms)
Jan 18 15:22:23.342: INFO: (16) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 16.742908ms)
Jan 18 15:22:23.343: INFO: (16) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 17.774368ms)
Jan 18 15:22:23.343: INFO: (16) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 18.169791ms)
Jan 18 15:22:23.345: INFO: (16) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 19.347488ms)
Jan 18 15:22:23.345: INFO: (16) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 19.318482ms)
Jan 18 15:22:23.351: INFO: (17) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 5.870912ms)
Jan 18 15:22:23.351: INFO: (17) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 6.126533ms)
Jan 18 15:22:23.351: INFO: (17) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 6.177873ms)
Jan 18 15:22:23.363: INFO: (17) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 17.227768ms)
Jan 18 15:22:23.364: INFO: (17) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 18.277851ms)
Jan 18 15:22:23.364: INFO: (17) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 18.375783ms)
Jan 18 15:22:23.364: INFO: (17) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 18.655207ms)
Jan 18 15:22:23.365: INFO: (17) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 19.003976ms)
Jan 18 15:22:23.365: INFO: (17) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 18.963328ms)
Jan 18 15:22:23.365: INFO: (17) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 19.184241ms)
Jan 18 15:22:23.365: INFO: (17) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 19.320748ms)
Jan 18 15:22:23.365: INFO: (17) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 19.461102ms)
Jan 18 15:22:23.365: INFO: (17) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 19.422273ms)
Jan 18 15:22:23.365: INFO: (17) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 19.389149ms)
Jan 18 15:22:23.365: INFO: (17) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 19.65542ms)
Jan 18 15:22:23.365: INFO: (17) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 19.734894ms)
Jan 18 15:22:23.374: INFO: (18) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 8.242292ms)
Jan 18 15:22:23.374: INFO: (18) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 8.156757ms)
Jan 18 15:22:23.374: INFO: (18) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 8.472981ms)
Jan 18 15:22:23.374: INFO: (18) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 8.355165ms)
Jan 18 15:22:23.381: INFO: (18) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 14.508644ms)
Jan 18 15:22:23.381: INFO: (18) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 14.172887ms)
Jan 18 15:22:23.381: INFO: (18) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 14.331384ms)
Jan 18 15:22:23.381: INFO: (18) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 14.225457ms)
Jan 18 15:22:23.381: INFO: (18) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 14.40442ms)
Jan 18 15:22:23.381: INFO: (18) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 15.011077ms)
Jan 18 15:22:23.381: INFO: (18) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 14.431265ms)
Jan 18 15:22:23.384: INFO: (18) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 17.845452ms)
Jan 18 15:22:23.384: INFO: (18) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 17.453051ms)
Jan 18 15:22:23.386: INFO: (18) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 19.541313ms)
Jan 18 15:22:23.386: INFO: (18) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 19.319312ms)
Jan 18 15:22:23.386: INFO: (18) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 19.448636ms)
Jan 18 15:22:23.393: INFO: (19) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 6.626847ms)
Jan 18 15:22:23.393: INFO: (19) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 6.815666ms)
Jan 18 15:22:23.396: INFO: (19) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 9.959017ms)
Jan 18 15:22:23.398: INFO: (19) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 11.533254ms)
Jan 18 15:22:23.398: INFO: (19) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 11.642607ms)
Jan 18 15:22:23.399: INFO: (19) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 12.909266ms)
Jan 18 15:22:23.401: INFO: (19) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 14.122966ms)
Jan 18 15:22:23.402: INFO: (19) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 14.900736ms)
Jan 18 15:22:23.402: INFO: (19) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 14.989623ms)
Jan 18 15:22:23.402: INFO: (19) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 15.533259ms)
Jan 18 15:22:23.402: INFO: (19) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 15.514971ms)
Jan 18 15:22:23.402: INFO: (19) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 15.913661ms)
Jan 18 15:22:23.402: INFO: (19) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 15.692131ms)
Jan 18 15:22:23.403: INFO: (19) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 16.966684ms)
Jan 18 15:22:23.404: INFO: (19) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 17.146257ms)
Jan 18 15:22:23.405: INFO: (19) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 18.666227ms)
STEP: deleting ReplicationController proxy-service-2f5xq in namespace proxy-1293, will wait for the garbage collector to delete the pods 01/18/23 15:22:23.405
Jan 18 15:22:23.474: INFO: Deleting ReplicationController proxy-service-2f5xq took: 14.560163ms
Jan 18 15:22:23.574: INFO: Terminating ReplicationController proxy-service-2f5xq pods took: 100.572129ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan 18 15:22:25.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1293" for this suite. 01/18/23 15:22:25.688
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":49,"skipped":982,"failed":0}
------------------------------
• [4.962 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:22:20.733
    Jan 18 15:22:20.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename proxy 01/18/23 15:22:20.735
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:22:20.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:22:20.773
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 01/18/23 15:22:20.788
    STEP: creating replication controller proxy-service-2f5xq in namespace proxy-1293 01/18/23 15:22:20.789
    I0118 15:22:20.800219      19 runners.go:193] Created replication controller with name: proxy-service-2f5xq, namespace: proxy-1293, replica count: 1
    I0118 15:22:21.851004      19 runners.go:193] proxy-service-2f5xq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0118 15:22:22.851930      19 runners.go:193] proxy-service-2f5xq Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 15:22:22.856: INFO: setup took 2.07816991s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/18/23 15:22:22.857
    Jan 18 15:22:22.879: INFO: (0) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 20.731808ms)
    Jan 18 15:22:22.879: INFO: (0) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 20.566911ms)
    Jan 18 15:22:22.879: INFO: (0) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 20.968984ms)
    Jan 18 15:22:22.885: INFO: (0) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 26.840854ms)
    Jan 18 15:22:22.886: INFO: (0) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 27.017479ms)
    Jan 18 15:22:22.886: INFO: (0) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 27.986463ms)
    Jan 18 15:22:22.886: INFO: (0) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 28.057241ms)
    Jan 18 15:22:22.895: INFO: (0) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 36.494795ms)
    Jan 18 15:22:22.895: INFO: (0) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 37.366087ms)
    Jan 18 15:22:22.895: INFO: (0) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 38.490831ms)
    Jan 18 15:22:22.895: INFO: (0) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 37.256423ms)
    Jan 18 15:22:22.895: INFO: (0) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 37.042136ms)
    Jan 18 15:22:22.895: INFO: (0) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 37.465603ms)
    Jan 18 15:22:22.896: INFO: (0) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 37.240666ms)
    Jan 18 15:22:22.896: INFO: (0) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 37.805171ms)
    Jan 18 15:22:22.896: INFO: (0) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 37.283812ms)
    Jan 18 15:22:22.903: INFO: (1) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 6.635977ms)
    Jan 18 15:22:22.903: INFO: (1) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 6.413604ms)
    Jan 18 15:22:22.912: INFO: (1) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 14.189984ms)
    Jan 18 15:22:22.912: INFO: (1) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 14.374408ms)
    Jan 18 15:22:22.912: INFO: (1) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 14.496762ms)
    Jan 18 15:22:22.912: INFO: (1) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 15.398448ms)
    Jan 18 15:22:22.914: INFO: (1) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 17.291311ms)
    Jan 18 15:22:22.914: INFO: (1) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 17.056752ms)
    Jan 18 15:22:22.923: INFO: (1) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 26.431237ms)
    Jan 18 15:22:22.926: INFO: (1) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 29.162211ms)
    Jan 18 15:22:22.927: INFO: (1) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 30.321138ms)
    Jan 18 15:22:22.932: INFO: (1) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 34.422836ms)
    Jan 18 15:22:22.932: INFO: (1) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 34.644245ms)
    Jan 18 15:22:22.932: INFO: (1) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 34.598519ms)
    Jan 18 15:22:22.932: INFO: (1) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 35.080469ms)
    Jan 18 15:22:22.932: INFO: (1) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 34.766992ms)
    Jan 18 15:22:22.942: INFO: (2) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 9.614209ms)
    Jan 18 15:22:22.947: INFO: (2) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 14.344197ms)
    Jan 18 15:22:22.948: INFO: (2) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 15.872018ms)
    Jan 18 15:22:22.951: INFO: (2) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 18.110133ms)
    Jan 18 15:22:22.952: INFO: (2) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 19.224848ms)
    Jan 18 15:22:22.952: INFO: (2) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 18.721293ms)
    Jan 18 15:22:22.953: INFO: (2) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 19.344253ms)
    Jan 18 15:22:22.953: INFO: (2) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 19.251764ms)
    Jan 18 15:22:22.953: INFO: (2) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 19.340395ms)
    Jan 18 15:22:22.954: INFO: (2) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 21.851176ms)
    Jan 18 15:22:22.955: INFO: (2) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 23.09891ms)
    Jan 18 15:22:22.956: INFO: (2) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 23.135756ms)
    Jan 18 15:22:22.956: INFO: (2) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 22.284824ms)
    Jan 18 15:22:22.956: INFO: (2) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 22.472177ms)
    Jan 18 15:22:22.956: INFO: (2) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 23.088326ms)
    Jan 18 15:22:22.957: INFO: (2) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 24.29342ms)
    Jan 18 15:22:22.961: INFO: (3) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 4.44013ms)
    Jan 18 15:22:22.967: INFO: (3) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 9.551792ms)
    Jan 18 15:22:22.975: INFO: (3) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 18.035343ms)
    Jan 18 15:22:22.976: INFO: (3) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 18.578996ms)
    Jan 18 15:22:22.976: INFO: (3) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 18.295295ms)
    Jan 18 15:22:22.977: INFO: (3) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 19.346253ms)
    Jan 18 15:22:22.977: INFO: (3) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 19.677146ms)
    Jan 18 15:22:22.977: INFO: (3) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 19.465824ms)
    Jan 18 15:22:22.977: INFO: (3) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 20.194465ms)
    Jan 18 15:22:22.978: INFO: (3) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 20.2829ms)
    Jan 18 15:22:22.978: INFO: (3) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 20.253719ms)
    Jan 18 15:22:22.978: INFO: (3) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 20.626776ms)
    Jan 18 15:22:22.978: INFO: (3) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 20.678172ms)
    Jan 18 15:22:22.979: INFO: (3) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 21.330169ms)
    Jan 18 15:22:22.979: INFO: (3) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 20.980169ms)
    Jan 18 15:22:22.979: INFO: (3) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 20.843121ms)
    Jan 18 15:22:22.992: INFO: (4) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 12.850358ms)
    Jan 18 15:22:22.993: INFO: (4) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 13.511147ms)
    Jan 18 15:22:22.994: INFO: (4) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 14.3745ms)
    Jan 18 15:22:22.994: INFO: (4) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 13.633996ms)
    Jan 18 15:22:22.994: INFO: (4) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 13.560324ms)
    Jan 18 15:22:22.995: INFO: (4) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 14.746749ms)
    Jan 18 15:22:22.995: INFO: (4) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 15.166223ms)
    Jan 18 15:22:22.995: INFO: (4) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 15.030588ms)
    Jan 18 15:22:22.995: INFO: (4) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 15.352723ms)
    Jan 18 15:22:22.996: INFO: (4) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 15.228457ms)
    Jan 18 15:22:22.996: INFO: (4) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 15.115463ms)
    Jan 18 15:22:22.997: INFO: (4) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 16.303117ms)
    Jan 18 15:22:22.997: INFO: (4) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 16.694415ms)
    Jan 18 15:22:22.997: INFO: (4) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 17.865484ms)
    Jan 18 15:22:22.998: INFO: (4) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 17.716737ms)
    Jan 18 15:22:22.998: INFO: (4) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 17.298457ms)
    Jan 18 15:22:23.008: INFO: (5) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 10.208827ms)
    Jan 18 15:22:23.039: INFO: (5) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 41.047625ms)
    Jan 18 15:22:23.042: INFO: (5) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 43.01027ms)
    Jan 18 15:22:23.042: INFO: (5) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 44.00708ms)
    Jan 18 15:22:23.042: INFO: (5) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 43.315559ms)
    Jan 18 15:22:23.043: INFO: (5) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 44.723157ms)
    Jan 18 15:22:23.043: INFO: (5) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 44.36114ms)
    Jan 18 15:22:23.044: INFO: (5) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 45.517492ms)
    Jan 18 15:22:23.045: INFO: (5) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 45.521888ms)
    Jan 18 15:22:23.045: INFO: (5) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 45.776761ms)
    Jan 18 15:22:23.045: INFO: (5) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 45.652448ms)
    Jan 18 15:22:23.045: INFO: (5) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 46.749962ms)
    Jan 18 15:22:23.046: INFO: (5) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 47.019707ms)
    Jan 18 15:22:23.051: INFO: (5) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 52.736162ms)
    Jan 18 15:22:23.052: INFO: (5) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 53.415421ms)
    Jan 18 15:22:23.052: INFO: (5) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 52.88562ms)
    Jan 18 15:22:23.063: INFO: (6) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 10.794851ms)
    Jan 18 15:22:23.064: INFO: (6) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 9.549967ms)
    Jan 18 15:22:23.065: INFO: (6) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 11.819838ms)
    Jan 18 15:22:23.065: INFO: (6) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 11.74299ms)
    Jan 18 15:22:23.065: INFO: (6) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 12.546351ms)
    Jan 18 15:22:23.065: INFO: (6) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 11.208594ms)
    Jan 18 15:22:23.065: INFO: (6) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 11.937232ms)
    Jan 18 15:22:23.065: INFO: (6) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 11.621803ms)
    Jan 18 15:22:23.070: INFO: (6) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 16.425084ms)
    Jan 18 15:22:23.072: INFO: (6) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 18.461876ms)
    Jan 18 15:22:23.072: INFO: (6) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 19.384107ms)
    Jan 18 15:22:23.072: INFO: (6) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 19.228002ms)
    Jan 18 15:22:23.072: INFO: (6) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 19.505293ms)
    Jan 18 15:22:23.072: INFO: (6) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 20.161709ms)
    Jan 18 15:22:23.072: INFO: (6) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 18.428832ms)
    Jan 18 15:22:23.076: INFO: (6) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 21.760961ms)
    Jan 18 15:22:23.085: INFO: (7) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 8.950251ms)
    Jan 18 15:22:23.093: INFO: (7) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 16.055223ms)
    Jan 18 15:22:23.093: INFO: (7) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 16.723158ms)
    Jan 18 15:22:23.093: INFO: (7) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 17.094706ms)
    Jan 18 15:22:23.094: INFO: (7) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 17.523279ms)
    Jan 18 15:22:23.094: INFO: (7) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 17.311866ms)
    Jan 18 15:22:23.095: INFO: (7) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 18.136801ms)
    Jan 18 15:22:23.095: INFO: (7) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 18.497286ms)
    Jan 18 15:22:23.095: INFO: (7) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 18.524027ms)
    Jan 18 15:22:23.096: INFO: (7) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 19.737878ms)
    Jan 18 15:22:23.097: INFO: (7) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 20.003204ms)
    Jan 18 15:22:23.098: INFO: (7) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 21.334193ms)
    Jan 18 15:22:23.103: INFO: (7) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 26.572622ms)
    Jan 18 15:22:23.103: INFO: (7) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 26.449224ms)
    Jan 18 15:22:23.104: INFO: (7) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 27.080889ms)
    Jan 18 15:22:23.104: INFO: (7) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 27.197924ms)
    Jan 18 15:22:23.124: INFO: (8) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 19.358334ms)
    Jan 18 15:22:23.138: INFO: (8) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 33.432354ms)
    Jan 18 15:22:23.138: INFO: (8) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 32.845091ms)
    Jan 18 15:22:23.140: INFO: (8) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 34.699119ms)
    Jan 18 15:22:23.140: INFO: (8) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 35.7657ms)
    Jan 18 15:22:23.148: INFO: (8) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 43.074803ms)
    Jan 18 15:22:23.148: INFO: (8) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 43.098341ms)
    Jan 18 15:22:23.148: INFO: (8) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 43.818778ms)
    Jan 18 15:22:23.153: INFO: (8) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 48.138351ms)
    Jan 18 15:22:23.153: INFO: (8) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 48.122007ms)
    Jan 18 15:22:23.153: INFO: (8) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 48.128341ms)
    Jan 18 15:22:23.154: INFO: (8) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 48.102005ms)
    Jan 18 15:22:23.154: INFO: (8) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 48.427711ms)
    Jan 18 15:22:23.154: INFO: (8) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 48.191374ms)
    Jan 18 15:22:23.154: INFO: (8) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 48.500873ms)
    Jan 18 15:22:23.154: INFO: (8) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 48.743393ms)
    Jan 18 15:22:23.168: INFO: (9) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 13.459947ms)
    Jan 18 15:22:23.168: INFO: (9) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 12.473311ms)
    Jan 18 15:22:23.169: INFO: (9) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 12.129095ms)
    Jan 18 15:22:23.170: INFO: (9) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 13.285319ms)
    Jan 18 15:22:23.170: INFO: (9) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 14.716663ms)
    Jan 18 15:22:23.171: INFO: (9) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 13.941848ms)
    Jan 18 15:22:23.171: INFO: (9) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 14.710145ms)
    Jan 18 15:22:23.171: INFO: (9) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 13.904025ms)
    Jan 18 15:22:23.171: INFO: (9) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 14.337517ms)
    Jan 18 15:22:23.171: INFO: (9) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 14.477995ms)
    Jan 18 15:22:23.179: INFO: (9) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 21.755754ms)
    Jan 18 15:22:23.179: INFO: (9) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 22.316231ms)
    Jan 18 15:22:23.179: INFO: (9) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 23.873455ms)
    Jan 18 15:22:23.179: INFO: (9) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 22.966156ms)
    Jan 18 15:22:23.180: INFO: (9) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 24.623966ms)
    Jan 18 15:22:23.180: INFO: (9) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 23.341438ms)
    Jan 18 15:22:23.200: INFO: (10) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 19.803184ms)
    Jan 18 15:22:23.201: INFO: (10) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 20.73363ms)
    Jan 18 15:22:23.201: INFO: (10) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 20.938098ms)
    Jan 18 15:22:23.201: INFO: (10) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 21.262977ms)
    Jan 18 15:22:23.203: INFO: (10) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 22.27581ms)
    Jan 18 15:22:23.203: INFO: (10) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 22.299411ms)
    Jan 18 15:22:23.206: INFO: (10) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 25.469446ms)
    Jan 18 15:22:23.207: INFO: (10) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 26.566985ms)
    Jan 18 15:22:23.208: INFO: (10) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 27.51397ms)
    Jan 18 15:22:23.208: INFO: (10) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 27.842486ms)
    Jan 18 15:22:23.210: INFO: (10) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 30.110159ms)
    Jan 18 15:22:23.210: INFO: (10) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 29.733397ms)
    Jan 18 15:22:23.210: INFO: (10) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 30.001274ms)
    Jan 18 15:22:23.210: INFO: (10) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 29.715322ms)
    Jan 18 15:22:23.210: INFO: (10) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 29.767128ms)
    Jan 18 15:22:23.210: INFO: (10) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 30.20709ms)
    Jan 18 15:22:23.232: INFO: (11) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 21.219743ms)
    Jan 18 15:22:23.232: INFO: (11) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 21.924395ms)
    Jan 18 15:22:23.238: INFO: (11) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 25.785108ms)
    Jan 18 15:22:23.238: INFO: (11) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 25.018371ms)
    Jan 18 15:22:23.238: INFO: (11) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 26.632881ms)
    Jan 18 15:22:23.238: INFO: (11) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 26.496357ms)
    Jan 18 15:22:23.238: INFO: (11) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 25.293254ms)
    Jan 18 15:22:23.239: INFO: (11) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 26.433073ms)
    Jan 18 15:22:23.239: INFO: (11) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 28.078497ms)
    Jan 18 15:22:23.240: INFO: (11) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 27.877098ms)
    Jan 18 15:22:23.241: INFO: (11) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 30.304315ms)
    Jan 18 15:22:23.241: INFO: (11) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 28.980571ms)
    Jan 18 15:22:23.241: INFO: (11) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 27.892169ms)
    Jan 18 15:22:23.241: INFO: (11) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 28.625423ms)
    Jan 18 15:22:23.242: INFO: (11) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 28.839536ms)
    Jan 18 15:22:23.246: INFO: (11) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 35.326204ms)
    Jan 18 15:22:23.259: INFO: (12) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 11.649268ms)
    Jan 18 15:22:23.259: INFO: (12) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 12.373791ms)
    Jan 18 15:22:23.259: INFO: (12) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 12.845903ms)
    Jan 18 15:22:23.259: INFO: (12) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 12.038089ms)
    Jan 18 15:22:23.260: INFO: (12) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 13.477375ms)
    Jan 18 15:22:23.260: INFO: (12) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 13.283199ms)
    Jan 18 15:22:23.260: INFO: (12) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 13.97093ms)
    Jan 18 15:22:23.260: INFO: (12) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 12.863659ms)
    Jan 18 15:22:23.260: INFO: (12) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 13.21559ms)
    Jan 18 15:22:23.262: INFO: (12) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 14.669559ms)
    Jan 18 15:22:23.266: INFO: (12) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 18.891091ms)
    Jan 18 15:22:23.266: INFO: (12) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 19.272189ms)
    Jan 18 15:22:23.266: INFO: (12) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 19.698992ms)
    Jan 18 15:22:23.267: INFO: (12) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 19.779288ms)
    Jan 18 15:22:23.267: INFO: (12) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 20.24702ms)
    Jan 18 15:22:23.268: INFO: (12) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 20.505637ms)
    Jan 18 15:22:23.276: INFO: (13) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 7.507555ms)
    Jan 18 15:22:23.282: INFO: (13) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 12.660239ms)
    Jan 18 15:22:23.282: INFO: (13) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 13.069168ms)
    Jan 18 15:22:23.282: INFO: (13) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 13.635306ms)
    Jan 18 15:22:23.283: INFO: (13) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 13.047357ms)
    Jan 18 15:22:23.283: INFO: (13) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 14.3006ms)
    Jan 18 15:22:23.283: INFO: (13) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 14.207659ms)
    Jan 18 15:22:23.283: INFO: (13) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 13.406071ms)
    Jan 18 15:22:23.283: INFO: (13) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 13.976201ms)
    Jan 18 15:22:23.283: INFO: (13) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 15.140129ms)
    Jan 18 15:22:23.283: INFO: (13) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 13.840117ms)
    Jan 18 15:22:23.283: INFO: (13) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 13.6466ms)
    Jan 18 15:22:23.283: INFO: (13) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 14.133276ms)
    Jan 18 15:22:23.284: INFO: (13) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 15.9075ms)
    Jan 18 15:22:23.284: INFO: (13) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 14.927808ms)
    Jan 18 15:22:23.284: INFO: (13) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 16.213081ms)
    Jan 18 15:22:23.297: INFO: (14) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 11.809002ms)
    Jan 18 15:22:23.297: INFO: (14) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 12.35896ms)
    Jan 18 15:22:23.297: INFO: (14) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 12.465581ms)
    Jan 18 15:22:23.298: INFO: (14) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 12.845391ms)
    Jan 18 15:22:23.298: INFO: (14) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 12.771528ms)
    Jan 18 15:22:23.298: INFO: (14) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 13.083496ms)
    Jan 18 15:22:23.299: INFO: (14) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 14.127341ms)
    Jan 18 15:22:23.300: INFO: (14) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 15.259609ms)
    Jan 18 15:22:23.300: INFO: (14) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 15.369274ms)
    Jan 18 15:22:23.300: INFO: (14) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 15.468869ms)
    Jan 18 15:22:23.301: INFO: (14) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 16.560304ms)
    Jan 18 15:22:23.302: INFO: (14) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 16.627464ms)
    Jan 18 15:22:23.302: INFO: (14) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 16.998156ms)
    Jan 18 15:22:23.302: INFO: (14) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 17.145346ms)
    Jan 18 15:22:23.302: INFO: (14) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 17.482638ms)
    Jan 18 15:22:23.302: INFO: (14) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 17.283791ms)
    Jan 18 15:22:23.317: INFO: (15) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 14.885168ms)
    Jan 18 15:22:23.317: INFO: (15) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 14.995989ms)
    Jan 18 15:22:23.317: INFO: (15) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 15.306796ms)
    Jan 18 15:22:23.317: INFO: (15) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 14.801525ms)
    Jan 18 15:22:23.318: INFO: (15) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 15.330619ms)
    Jan 18 15:22:23.318: INFO: (15) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 15.128812ms)
    Jan 18 15:22:23.318: INFO: (15) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 15.539044ms)
    Jan 18 15:22:23.318: INFO: (15) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 16.127791ms)
    Jan 18 15:22:23.318: INFO: (15) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 15.821356ms)
    Jan 18 15:22:23.320: INFO: (15) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 17.419893ms)
    Jan 18 15:22:23.323: INFO: (15) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 20.755032ms)
    Jan 18 15:22:23.324: INFO: (15) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 21.458395ms)
    Jan 18 15:22:23.324: INFO: (15) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 21.744392ms)
    Jan 18 15:22:23.324: INFO: (15) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 21.95546ms)
    Jan 18 15:22:23.324: INFO: (15) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 21.88472ms)
    Jan 18 15:22:23.325: INFO: (15) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 22.018592ms)
    Jan 18 15:22:23.335: INFO: (16) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 10.029125ms)
    Jan 18 15:22:23.336: INFO: (16) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 10.649527ms)
    Jan 18 15:22:23.336: INFO: (16) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 10.838395ms)
    Jan 18 15:22:23.336: INFO: (16) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 10.875208ms)
    Jan 18 15:22:23.336: INFO: (16) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 11.194629ms)
    Jan 18 15:22:23.341: INFO: (16) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 15.866761ms)
    Jan 18 15:22:23.342: INFO: (16) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 15.955297ms)
    Jan 18 15:22:23.342: INFO: (16) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 16.601788ms)
    Jan 18 15:22:23.342: INFO: (16) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 16.675219ms)
    Jan 18 15:22:23.342: INFO: (16) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 17.145392ms)
    Jan 18 15:22:23.342: INFO: (16) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 17.377055ms)
    Jan 18 15:22:23.342: INFO: (16) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 16.742908ms)
    Jan 18 15:22:23.343: INFO: (16) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 17.774368ms)
    Jan 18 15:22:23.343: INFO: (16) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 18.169791ms)
    Jan 18 15:22:23.345: INFO: (16) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 19.347488ms)
    Jan 18 15:22:23.345: INFO: (16) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 19.318482ms)
    Jan 18 15:22:23.351: INFO: (17) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 5.870912ms)
    Jan 18 15:22:23.351: INFO: (17) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 6.126533ms)
    Jan 18 15:22:23.351: INFO: (17) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 6.177873ms)
    Jan 18 15:22:23.363: INFO: (17) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 17.227768ms)
    Jan 18 15:22:23.364: INFO: (17) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 18.277851ms)
    Jan 18 15:22:23.364: INFO: (17) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 18.375783ms)
    Jan 18 15:22:23.364: INFO: (17) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 18.655207ms)
    Jan 18 15:22:23.365: INFO: (17) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 19.003976ms)
    Jan 18 15:22:23.365: INFO: (17) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 18.963328ms)
    Jan 18 15:22:23.365: INFO: (17) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 19.184241ms)
    Jan 18 15:22:23.365: INFO: (17) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 19.320748ms)
    Jan 18 15:22:23.365: INFO: (17) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 19.461102ms)
    Jan 18 15:22:23.365: INFO: (17) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 19.422273ms)
    Jan 18 15:22:23.365: INFO: (17) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 19.389149ms)
    Jan 18 15:22:23.365: INFO: (17) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 19.65542ms)
    Jan 18 15:22:23.365: INFO: (17) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 19.734894ms)
    Jan 18 15:22:23.374: INFO: (18) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 8.242292ms)
    Jan 18 15:22:23.374: INFO: (18) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 8.156757ms)
    Jan 18 15:22:23.374: INFO: (18) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 8.472981ms)
    Jan 18 15:22:23.374: INFO: (18) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 8.355165ms)
    Jan 18 15:22:23.381: INFO: (18) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 14.508644ms)
    Jan 18 15:22:23.381: INFO: (18) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 14.172887ms)
    Jan 18 15:22:23.381: INFO: (18) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 14.331384ms)
    Jan 18 15:22:23.381: INFO: (18) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 14.225457ms)
    Jan 18 15:22:23.381: INFO: (18) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 14.40442ms)
    Jan 18 15:22:23.381: INFO: (18) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 15.011077ms)
    Jan 18 15:22:23.381: INFO: (18) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 14.431265ms)
    Jan 18 15:22:23.384: INFO: (18) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 17.845452ms)
    Jan 18 15:22:23.384: INFO: (18) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 17.453051ms)
    Jan 18 15:22:23.386: INFO: (18) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 19.541313ms)
    Jan 18 15:22:23.386: INFO: (18) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 19.319312ms)
    Jan 18 15:22:23.386: INFO: (18) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 19.448636ms)
    Jan 18 15:22:23.393: INFO: (19) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:462/proxy/: tls qux (200; 6.626847ms)
    Jan 18 15:22:23.393: INFO: (19) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 6.815666ms)
    Jan 18 15:22:23.396: INFO: (19) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:460/proxy/: tls baz (200; 9.959017ms)
    Jan 18 15:22:23.398: INFO: (19) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">test<... (200; 11.533254ms)
    Jan 18 15:22:23.398: INFO: (19) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw/proxy/rewriteme">test</a> (200; 11.642607ms)
    Jan 18 15:22:23.399: INFO: (19) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:1080/proxy/rewriteme">... (200; 12.909266ms)
    Jan 18 15:22:23.401: INFO: (19) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:162/proxy/: bar (200; 14.122966ms)
    Jan 18 15:22:23.402: INFO: (19) /api/v1/namespaces/proxy-1293/pods/http:proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 14.900736ms)
    Jan 18 15:22:23.402: INFO: (19) /api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/: <a href="/api/v1/namespaces/proxy-1293/pods/https:proxy-service-2f5xq-lpkhw:443/proxy/tlsrewritem... (200; 14.989623ms)
    Jan 18 15:22:23.402: INFO: (19) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname1/proxy/: foo (200; 15.533259ms)
    Jan 18 15:22:23.402: INFO: (19) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname2/proxy/: tls qux (200; 15.514971ms)
    Jan 18 15:22:23.402: INFO: (19) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname1/proxy/: foo (200; 15.913661ms)
    Jan 18 15:22:23.402: INFO: (19) /api/v1/namespaces/proxy-1293/pods/proxy-service-2f5xq-lpkhw:160/proxy/: foo (200; 15.692131ms)
    Jan 18 15:22:23.403: INFO: (19) /api/v1/namespaces/proxy-1293/services/proxy-service-2f5xq:portname2/proxy/: bar (200; 16.966684ms)
    Jan 18 15:22:23.404: INFO: (19) /api/v1/namespaces/proxy-1293/services/https:proxy-service-2f5xq:tlsportname1/proxy/: tls baz (200; 17.146257ms)
    Jan 18 15:22:23.405: INFO: (19) /api/v1/namespaces/proxy-1293/services/http:proxy-service-2f5xq:portname2/proxy/: bar (200; 18.666227ms)
    STEP: deleting ReplicationController proxy-service-2f5xq in namespace proxy-1293, will wait for the garbage collector to delete the pods 01/18/23 15:22:23.405
    Jan 18 15:22:23.474: INFO: Deleting ReplicationController proxy-service-2f5xq took: 14.560163ms
    Jan 18 15:22:23.574: INFO: Terminating ReplicationController proxy-service-2f5xq pods took: 100.572129ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan 18 15:22:25.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-1293" for this suite. 01/18/23 15:22:25.688
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:22:25.702
Jan 18 15:22:25.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename csistoragecapacity 01/18/23 15:22:25.703
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:22:25.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:22:25.727
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 01/18/23 15:22:25.733
STEP: getting /apis/storage.k8s.io 01/18/23 15:22:25.737
STEP: getting /apis/storage.k8s.io/v1 01/18/23 15:22:25.744
STEP: creating 01/18/23 15:22:25.747
STEP: watching 01/18/23 15:22:25.78
Jan 18 15:22:25.780: INFO: starting watch
STEP: getting 01/18/23 15:22:25.795
STEP: listing in namespace 01/18/23 15:22:25.802
STEP: listing across namespaces 01/18/23 15:22:25.808
STEP: patching 01/18/23 15:22:25.815
STEP: updating 01/18/23 15:22:25.824
Jan 18 15:22:25.841: INFO: waiting for watch events with expected annotations in namespace
Jan 18 15:22:25.841: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 01/18/23 15:22:25.841
STEP: deleting a collection 01/18/23 15:22:25.855
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Jan 18 15:22:25.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-97" for this suite. 01/18/23 15:22:25.911
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":50,"skipped":999,"failed":0}
------------------------------
• [0.216 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:22:25.702
    Jan 18 15:22:25.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename csistoragecapacity 01/18/23 15:22:25.703
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:22:25.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:22:25.727
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 01/18/23 15:22:25.733
    STEP: getting /apis/storage.k8s.io 01/18/23 15:22:25.737
    STEP: getting /apis/storage.k8s.io/v1 01/18/23 15:22:25.744
    STEP: creating 01/18/23 15:22:25.747
    STEP: watching 01/18/23 15:22:25.78
    Jan 18 15:22:25.780: INFO: starting watch
    STEP: getting 01/18/23 15:22:25.795
    STEP: listing in namespace 01/18/23 15:22:25.802
    STEP: listing across namespaces 01/18/23 15:22:25.808
    STEP: patching 01/18/23 15:22:25.815
    STEP: updating 01/18/23 15:22:25.824
    Jan 18 15:22:25.841: INFO: waiting for watch events with expected annotations in namespace
    Jan 18 15:22:25.841: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 01/18/23 15:22:25.841
    STEP: deleting a collection 01/18/23 15:22:25.855
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Jan 18 15:22:25.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-97" for this suite. 01/18/23 15:22:25.911
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:22:25.922
Jan 18 15:22:25.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename aggregator 01/18/23 15:22:25.923
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:22:25.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:22:25.959
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jan 18 15:22:25.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 01/18/23 15:22:25.966
Jan 18 15:22:26.611: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Jan 18 15:22:28.714: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 15:22:30.719: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 15:22:32.720: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 15:22:34.720: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 15:22:36.721: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 15:22:38.719: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 15:22:40.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 15:22:42.720: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 15:22:44.719: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 15:22:46.720: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 15:22:48.721: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 15:22:50.720: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 15:22:52.991: INFO: Waited 245.54692ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 01/18/23 15:22:53.116
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/18/23 15:22:53.12
STEP: List APIServices 01/18/23 15:22:53.133
Jan 18 15:22:53.145: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Jan 18 15:22:53.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9680" for this suite. 01/18/23 15:22:53.475
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":51,"skipped":1001,"failed":0}
------------------------------
• [SLOW TEST] [27.597 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:22:25.922
    Jan 18 15:22:25.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename aggregator 01/18/23 15:22:25.923
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:22:25.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:22:25.959
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jan 18 15:22:25.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 01/18/23 15:22:25.966
    Jan 18 15:22:26.611: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
    Jan 18 15:22:28.714: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 15:22:30.719: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 15:22:32.720: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 15:22:34.720: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 15:22:36.721: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 15:22:38.719: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 15:22:40.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 15:22:42.720: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 15:22:44.719: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 15:22:46.720: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 15:22:48.721: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 15:22:50.720: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 15:22:52.991: INFO: Waited 245.54692ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 01/18/23 15:22:53.116
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/18/23 15:22:53.12
    STEP: List APIServices 01/18/23 15:22:53.133
    Jan 18 15:22:53.145: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Jan 18 15:22:53.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-9680" for this suite. 01/18/23 15:22:53.475
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:22:53.517
Jan 18 15:22:53.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename podtemplate 01/18/23 15:22:53.52
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:22:53.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:22:53.556
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan 18 15:22:53.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-357" for this suite. 01/18/23 15:22:53.642
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":52,"skipped":1006,"failed":0}
------------------------------
• [0.133 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:22:53.517
    Jan 18 15:22:53.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename podtemplate 01/18/23 15:22:53.52
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:22:53.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:22:53.556
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan 18 15:22:53.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-357" for this suite. 01/18/23 15:22:53.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:22:53.66
Jan 18 15:22:53.660: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename resourcequota 01/18/23 15:22:53.661
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:22:53.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:22:53.709
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 01/18/23 15:22:53.715
STEP: Creating a ResourceQuota 01/18/23 15:22:58.72
STEP: Ensuring resource quota status is calculated 01/18/23 15:22:58.726
STEP: Creating a Service 01/18/23 15:23:00.731
STEP: Creating a NodePort Service 01/18/23 15:23:00.752
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/18/23 15:23:00.787
STEP: Ensuring resource quota status captures service creation 01/18/23 15:23:00.81
STEP: Deleting Services 01/18/23 15:23:02.816
STEP: Ensuring resource quota status released usage 01/18/23 15:23:02.873
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 15:23:04.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9645" for this suite. 01/18/23 15:23:04.886
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":53,"skipped":1031,"failed":0}
------------------------------
• [SLOW TEST] [11.235 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:22:53.66
    Jan 18 15:22:53.660: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename resourcequota 01/18/23 15:22:53.661
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:22:53.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:22:53.709
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 01/18/23 15:22:53.715
    STEP: Creating a ResourceQuota 01/18/23 15:22:58.72
    STEP: Ensuring resource quota status is calculated 01/18/23 15:22:58.726
    STEP: Creating a Service 01/18/23 15:23:00.731
    STEP: Creating a NodePort Service 01/18/23 15:23:00.752
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/18/23 15:23:00.787
    STEP: Ensuring resource quota status captures service creation 01/18/23 15:23:00.81
    STEP: Deleting Services 01/18/23 15:23:02.816
    STEP: Ensuring resource quota status released usage 01/18/23 15:23:02.873
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 15:23:04.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9645" for this suite. 01/18/23 15:23:04.886
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:23:04.896
Jan 18 15:23:04.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename secrets 01/18/23 15:23:04.898
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:23:04.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:23:04.931
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-ba7a8c0e-7c8b-4a5a-8ee8-4e6b9bf2ed1a 01/18/23 15:23:04.946
STEP: Creating a pod to test consume secrets 01/18/23 15:23:04.955
Jan 18 15:23:04.964: INFO: Waiting up to 5m0s for pod "pod-secrets-63eab8e5-7bf4-4160-a80e-37094bf133ca" in namespace "secrets-8139" to be "Succeeded or Failed"
Jan 18 15:23:04.973: INFO: Pod "pod-secrets-63eab8e5-7bf4-4160-a80e-37094bf133ca": Phase="Pending", Reason="", readiness=false. Elapsed: 8.786389ms
Jan 18 15:23:06.979: INFO: Pod "pod-secrets-63eab8e5-7bf4-4160-a80e-37094bf133ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014378652s
Jan 18 15:23:08.980: INFO: Pod "pod-secrets-63eab8e5-7bf4-4160-a80e-37094bf133ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015526312s
STEP: Saw pod success 01/18/23 15:23:08.98
Jan 18 15:23:08.981: INFO: Pod "pod-secrets-63eab8e5-7bf4-4160-a80e-37094bf133ca" satisfied condition "Succeeded or Failed"
Jan 18 15:23:08.984: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-63eab8e5-7bf4-4160-a80e-37094bf133ca container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 15:23:08.992
Jan 18 15:23:09.003: INFO: Waiting for pod pod-secrets-63eab8e5-7bf4-4160-a80e-37094bf133ca to disappear
Jan 18 15:23:09.007: INFO: Pod pod-secrets-63eab8e5-7bf4-4160-a80e-37094bf133ca no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 18 15:23:09.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8139" for this suite. 01/18/23 15:23:09.016
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":54,"skipped":1064,"failed":0}
------------------------------
• [4.128 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:23:04.896
    Jan 18 15:23:04.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename secrets 01/18/23 15:23:04.898
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:23:04.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:23:04.931
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-ba7a8c0e-7c8b-4a5a-8ee8-4e6b9bf2ed1a 01/18/23 15:23:04.946
    STEP: Creating a pod to test consume secrets 01/18/23 15:23:04.955
    Jan 18 15:23:04.964: INFO: Waiting up to 5m0s for pod "pod-secrets-63eab8e5-7bf4-4160-a80e-37094bf133ca" in namespace "secrets-8139" to be "Succeeded or Failed"
    Jan 18 15:23:04.973: INFO: Pod "pod-secrets-63eab8e5-7bf4-4160-a80e-37094bf133ca": Phase="Pending", Reason="", readiness=false. Elapsed: 8.786389ms
    Jan 18 15:23:06.979: INFO: Pod "pod-secrets-63eab8e5-7bf4-4160-a80e-37094bf133ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014378652s
    Jan 18 15:23:08.980: INFO: Pod "pod-secrets-63eab8e5-7bf4-4160-a80e-37094bf133ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015526312s
    STEP: Saw pod success 01/18/23 15:23:08.98
    Jan 18 15:23:08.981: INFO: Pod "pod-secrets-63eab8e5-7bf4-4160-a80e-37094bf133ca" satisfied condition "Succeeded or Failed"
    Jan 18 15:23:08.984: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-63eab8e5-7bf4-4160-a80e-37094bf133ca container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 15:23:08.992
    Jan 18 15:23:09.003: INFO: Waiting for pod pod-secrets-63eab8e5-7bf4-4160-a80e-37094bf133ca to disappear
    Jan 18 15:23:09.007: INFO: Pod pod-secrets-63eab8e5-7bf4-4160-a80e-37094bf133ca no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 15:23:09.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8139" for this suite. 01/18/23 15:23:09.016
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:23:09.025
Jan 18 15:23:09.026: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 15:23:09.027
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:23:09.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:23:09.055
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 01/18/23 15:23:09.059
Jan 18 15:23:09.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: rename a version 01/18/23 15:23:21.132
STEP: check the new version name is served 01/18/23 15:23:21.152
STEP: check the old version name is removed 01/18/23 15:23:26.039
STEP: check the other version is not changed 01/18/23 15:23:28.065
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 15:23:37.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9232" for this suite. 01/18/23 15:23:37.586
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":55,"skipped":1066,"failed":0}
------------------------------
• [SLOW TEST] [28.566 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:23:09.025
    Jan 18 15:23:09.026: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 15:23:09.027
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:23:09.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:23:09.055
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 01/18/23 15:23:09.059
    Jan 18 15:23:09.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: rename a version 01/18/23 15:23:21.132
    STEP: check the new version name is served 01/18/23 15:23:21.152
    STEP: check the old version name is removed 01/18/23 15:23:26.039
    STEP: check the other version is not changed 01/18/23 15:23:28.065
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 15:23:37.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9232" for this suite. 01/18/23 15:23:37.586
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:23:37.598
Jan 18 15:23:37.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename namespaces 01/18/23 15:23:37.6
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:23:37.619
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:23:37.624
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 01/18/23 15:23:37.629
STEP: patching the Namespace 01/18/23 15:23:37.649
STEP: get the Namespace and ensuring it has the label 01/18/23 15:23:37.657
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 18 15:23:37.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8792" for this suite. 01/18/23 15:23:37.668
STEP: Destroying namespace "nspatchtest-c2aedb4b-24fe-4737-87b6-39a00b628989-5231" for this suite. 01/18/23 15:23:37.674
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":56,"skipped":1072,"failed":0}
------------------------------
• [0.082 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:23:37.598
    Jan 18 15:23:37.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename namespaces 01/18/23 15:23:37.6
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:23:37.619
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:23:37.624
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 01/18/23 15:23:37.629
    STEP: patching the Namespace 01/18/23 15:23:37.649
    STEP: get the Namespace and ensuring it has the label 01/18/23 15:23:37.657
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 15:23:37.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-8792" for this suite. 01/18/23 15:23:37.668
    STEP: Destroying namespace "nspatchtest-c2aedb4b-24fe-4737-87b6-39a00b628989-5231" for this suite. 01/18/23 15:23:37.674
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:23:37.682
Jan 18 15:23:37.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename resourcequota 01/18/23 15:23:37.683
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:23:37.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:23:37.715
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 01/18/23 15:23:37.724
STEP: Creating a ResourceQuota 01/18/23 15:23:42.73
STEP: Ensuring resource quota status is calculated 01/18/23 15:23:42.736
STEP: Creating a ReplicationController 01/18/23 15:23:44.743
STEP: Ensuring resource quota status captures replication controller creation 01/18/23 15:23:44.756
STEP: Deleting a ReplicationController 01/18/23 15:23:46.762
STEP: Ensuring resource quota status released usage 01/18/23 15:23:46.769
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 15:23:48.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3768" for this suite. 01/18/23 15:23:48.781
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":57,"skipped":1074,"failed":0}
------------------------------
• [SLOW TEST] [11.107 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:23:37.682
    Jan 18 15:23:37.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename resourcequota 01/18/23 15:23:37.683
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:23:37.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:23:37.715
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 01/18/23 15:23:37.724
    STEP: Creating a ResourceQuota 01/18/23 15:23:42.73
    STEP: Ensuring resource quota status is calculated 01/18/23 15:23:42.736
    STEP: Creating a ReplicationController 01/18/23 15:23:44.743
    STEP: Ensuring resource quota status captures replication controller creation 01/18/23 15:23:44.756
    STEP: Deleting a ReplicationController 01/18/23 15:23:46.762
    STEP: Ensuring resource quota status released usage 01/18/23 15:23:46.769
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 15:23:48.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3768" for this suite. 01/18/23 15:23:48.781
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:23:48.792
Jan 18 15:23:48.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename container-probe 01/18/23 15:23:48.794
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:23:48.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:23:48.825
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-b3347794-2932-44cb-a00f-90a064a73717 in namespace container-probe-8275 01/18/23 15:23:48.835
Jan 18 15:23:48.846: INFO: Waiting up to 5m0s for pod "busybox-b3347794-2932-44cb-a00f-90a064a73717" in namespace "container-probe-8275" to be "not pending"
Jan 18 15:23:48.854: INFO: Pod "busybox-b3347794-2932-44cb-a00f-90a064a73717": Phase="Pending", Reason="", readiness=false. Elapsed: 8.695261ms
Jan 18 15:23:50.880: INFO: Pod "busybox-b3347794-2932-44cb-a00f-90a064a73717": Phase="Running", Reason="", readiness=true. Elapsed: 2.034321815s
Jan 18 15:23:50.880: INFO: Pod "busybox-b3347794-2932-44cb-a00f-90a064a73717" satisfied condition "not pending"
Jan 18 15:23:50.880: INFO: Started pod busybox-b3347794-2932-44cb-a00f-90a064a73717 in namespace container-probe-8275
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 15:23:50.88
Jan 18 15:23:50.884: INFO: Initial restart count of pod busybox-b3347794-2932-44cb-a00f-90a064a73717 is 0
Jan 18 15:24:41.035: INFO: Restart count of pod container-probe-8275/busybox-b3347794-2932-44cb-a00f-90a064a73717 is now 1 (50.150174419s elapsed)
STEP: deleting the pod 01/18/23 15:24:41.035
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 18 15:24:41.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8275" for this suite. 01/18/23 15:24:41.079
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":58,"skipped":1095,"failed":0}
------------------------------
• [SLOW TEST] [52.295 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:23:48.792
    Jan 18 15:23:48.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename container-probe 01/18/23 15:23:48.794
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:23:48.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:23:48.825
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-b3347794-2932-44cb-a00f-90a064a73717 in namespace container-probe-8275 01/18/23 15:23:48.835
    Jan 18 15:23:48.846: INFO: Waiting up to 5m0s for pod "busybox-b3347794-2932-44cb-a00f-90a064a73717" in namespace "container-probe-8275" to be "not pending"
    Jan 18 15:23:48.854: INFO: Pod "busybox-b3347794-2932-44cb-a00f-90a064a73717": Phase="Pending", Reason="", readiness=false. Elapsed: 8.695261ms
    Jan 18 15:23:50.880: INFO: Pod "busybox-b3347794-2932-44cb-a00f-90a064a73717": Phase="Running", Reason="", readiness=true. Elapsed: 2.034321815s
    Jan 18 15:23:50.880: INFO: Pod "busybox-b3347794-2932-44cb-a00f-90a064a73717" satisfied condition "not pending"
    Jan 18 15:23:50.880: INFO: Started pod busybox-b3347794-2932-44cb-a00f-90a064a73717 in namespace container-probe-8275
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 15:23:50.88
    Jan 18 15:23:50.884: INFO: Initial restart count of pod busybox-b3347794-2932-44cb-a00f-90a064a73717 is 0
    Jan 18 15:24:41.035: INFO: Restart count of pod container-probe-8275/busybox-b3347794-2932-44cb-a00f-90a064a73717 is now 1 (50.150174419s elapsed)
    STEP: deleting the pod 01/18/23 15:24:41.035
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 18 15:24:41.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8275" for this suite. 01/18/23 15:24:41.079
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:24:41.093
Jan 18 15:24:41.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename deployment 01/18/23 15:24:41.096
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:24:41.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:24:41.12
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 01/18/23 15:24:41.13
STEP: waiting for Deployment to be created 01/18/23 15:24:41.136
STEP: waiting for all Replicas to be Ready 01/18/23 15:24:41.138
Jan 18 15:24:41.141: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 15:24:41.141: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 15:24:41.157: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 15:24:41.157: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 15:24:41.191: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 15:24:41.191: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 15:24:41.241: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 15:24:41.242: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 15:24:43.176: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 18 15:24:43.177: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 18 15:24:43.188: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 01/18/23 15:24:43.188
W0118 15:24:43.205194      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 18 15:24:43.211: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 01/18/23 15:24:43.211
Jan 18 15:24:43.214: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
Jan 18 15:24:43.215: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
Jan 18 15:24:43.215: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
Jan 18 15:24:43.215: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
Jan 18 15:24:43.215: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
Jan 18 15:24:43.215: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
Jan 18 15:24:43.216: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
Jan 18 15:24:43.216: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
Jan 18 15:24:43.216: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
Jan 18 15:24:43.216: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
Jan 18 15:24:43.216: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan 18 15:24:43.216: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan 18 15:24:43.216: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan 18 15:24:43.216: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan 18 15:24:43.222: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan 18 15:24:43.223: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan 18 15:24:43.252: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan 18 15:24:43.252: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan 18 15:24:43.306: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
Jan 18 15:24:43.307: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
Jan 18 15:24:43.322: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
Jan 18 15:24:43.322: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
Jan 18 15:24:45.219: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan 18 15:24:45.220: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan 18 15:24:45.252: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
STEP: listing Deployments 01/18/23 15:24:45.253
Jan 18 15:24:45.259: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 01/18/23 15:24:45.259
Jan 18 15:24:45.280: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 01/18/23 15:24:45.281
Jan 18 15:24:45.318: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 15:24:45.318: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 15:24:45.354: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 15:24:45.410: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 15:24:45.441: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 15:24:47.227: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 15:24:47.254: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 15:24:47.263: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 15:24:47.295: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 15:24:49.113: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 01/18/23 15:24:49.234
STEP: fetching the DeploymentStatus 01/18/23 15:24:49.421
Jan 18 15:24:49.429: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
Jan 18 15:24:49.429: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
Jan 18 15:24:49.429: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
Jan 18 15:24:49.430: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
Jan 18 15:24:49.430: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
Jan 18 15:24:49.430: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan 18 15:24:49.430: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan 18 15:24:49.431: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan 18 15:24:49.431: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
Jan 18 15:24:49.431: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 3
STEP: deleting the Deployment 01/18/23 15:24:49.431
Jan 18 15:24:49.445: INFO: observed event type MODIFIED
Jan 18 15:24:49.445: INFO: observed event type MODIFIED
Jan 18 15:24:49.446: INFO: observed event type MODIFIED
Jan 18 15:24:49.446: INFO: observed event type MODIFIED
Jan 18 15:24:49.446: INFO: observed event type MODIFIED
Jan 18 15:24:49.447: INFO: observed event type MODIFIED
Jan 18 15:24:49.447: INFO: observed event type MODIFIED
Jan 18 15:24:49.447: INFO: observed event type MODIFIED
Jan 18 15:24:49.447: INFO: observed event type MODIFIED
Jan 18 15:24:49.448: INFO: observed event type MODIFIED
Jan 18 15:24:49.448: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 15:24:49.454: INFO: Log out all the ReplicaSets if there is no deployment created
Jan 18 15:24:49.464: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-5772  667a2084-a69d-4cf1-b61f-55a80b59ba8c 35483 4 2023-01-18 15:24:43 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 96cc2709-3a7e-43d0-aa0a-9cbbe28017b2 0xc009d37ea7 0xc009d37ea8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 15:24:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96cc2709-3a7e-43d0-aa0a-9cbbe28017b2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 15:24:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009d37f60 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 18 15:24:49.472: INFO: pod: "test-deployment-54cc775c4b-pkqg2":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-pkqg2 test-deployment-54cc775c4b- deployment-5772  c11d7b97-4931-4aba-ae4f-9c00c5ad8159 35479 0 2023-01-18 15:24:43 +0000 UTC 2023-01-18 15:24:50 +0000 UTC 0xc009d82a98 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:8a035c5e1211f89f156dab2bd1d34e0fdd831320aa7b1627d91bf6bf2b782f89 cni.projectcalico.org/podIP:10.233.68.107/32 cni.projectcalico.org/podIPs:10.233.68.107/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 667a2084-a69d-4cf1-b61f-55a80b59ba8c 0xc009d82af7 0xc009d82af8}] [] [{calico Update v1 2023-01-18 15:24:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-18 15:24:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"667a2084-a69d-4cf1-b61f-55a80b59ba8c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 15:24:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-58mrr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-58mrr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:24:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:24:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:24:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:24:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.107,StartTime:2023-01-18 15:24:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 15:24:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://83b4f5ae36e5fd1c2ef5cd4494e7309109af00f25377eb336112ce49d84b0b9a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.107,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 18 15:24:49.472: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-5772  8322f88c-e2ed-4797-83a8-556e177e9701 35475 2 2023-01-18 15:24:45 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 96cc2709-3a7e-43d0-aa0a-9cbbe28017b2 0xc009d37fe7 0xc009d37fe8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 15:24:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96cc2709-3a7e-43d0-aa0a-9cbbe28017b2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 15:24:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009da4080 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jan 18 15:24:49.478: INFO: pod: "test-deployment-7c7d8d58c8-hqnqd":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-hqnqd test-deployment-7c7d8d58c8- deployment-5772  7cb4ba2f-6149-47b0-97f9-870fdd665a87 35430 0 2023-01-18 15:24:45 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:59467770a8573175315ed2f3fceb345ccbd37547b378d6c16f369b78fa031540 cni.projectcalico.org/podIP:10.233.68.108/32 cni.projectcalico.org/podIPs:10.233.68.108/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 8322f88c-e2ed-4797-83a8-556e177e9701 0xc009d83b07 0xc009d83b08}] [] [{kube-controller-manager Update v1 2023-01-18 15:24:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8322f88c-e2ed-4797-83a8-556e177e9701\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 15:24:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 15:24:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5v4gf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5v4gf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:24:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:24:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:24:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:24:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.108,StartTime:2023-01-18 15:24:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 15:24:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f819213b229f46aa6b8a0c5015c78e34404593040ad34621809382201800801c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.108,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 18 15:24:49.479: INFO: pod: "test-deployment-7c7d8d58c8-hv9dx":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-hv9dx test-deployment-7c7d8d58c8- deployment-5772  841fc6bf-14e6-4013-9205-06f7ea01985e 35474 0 2023-01-18 15:24:47 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:bc14f7bf621358fe1271c6ced45fa0919383a0c4613c1e0953a6a99f39c4f17d cni.projectcalico.org/podIP:10.233.78.63/32 cni.projectcalico.org/podIPs:10.233.78.63/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 8322f88c-e2ed-4797-83a8-556e177e9701 0xc009d83dc7 0xc009d83dc8}] [] [{calico Update v1 2023-01-18 15:24:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-18 15:24:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8322f88c-e2ed-4797-83a8-556e177e9701\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 15:24:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-54ffr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-54ffr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:24:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:24:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:10.233.78.63,StartTime:2023-01-18 15:24:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 15:24:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c52960a32f073480eb5612a99fb05493b809663287407082198573021ab6b9cd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 18 15:24:49.480: INFO: ReplicaSet "test-deployment-8594bb6fdd":
&ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-5772  9bc3c2b3-db49-420e-9f20-71bf1f772006 35363 3 2023-01-18 15:24:41 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 96cc2709-3a7e-43d0-aa0a-9cbbe28017b2 0xc009da40f7 0xc009da40f8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 15:24:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96cc2709-3a7e-43d0-aa0a-9cbbe28017b2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 15:24:45 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009da4180 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 18 15:24:49.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5772" for this suite. 01/18/23 15:24:49.494
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":59,"skipped":1110,"failed":0}
------------------------------
• [SLOW TEST] [8.408 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:24:41.093
    Jan 18 15:24:41.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename deployment 01/18/23 15:24:41.096
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:24:41.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:24:41.12
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 01/18/23 15:24:41.13
    STEP: waiting for Deployment to be created 01/18/23 15:24:41.136
    STEP: waiting for all Replicas to be Ready 01/18/23 15:24:41.138
    Jan 18 15:24:41.141: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 15:24:41.141: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 15:24:41.157: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 15:24:41.157: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 15:24:41.191: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 15:24:41.191: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 15:24:41.241: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 15:24:41.242: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 15:24:43.176: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 18 15:24:43.177: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 18 15:24:43.188: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 01/18/23 15:24:43.188
    W0118 15:24:43.205194      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 18 15:24:43.211: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 01/18/23 15:24:43.211
    Jan 18 15:24:43.214: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
    Jan 18 15:24:43.215: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
    Jan 18 15:24:43.215: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
    Jan 18 15:24:43.215: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
    Jan 18 15:24:43.215: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
    Jan 18 15:24:43.215: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
    Jan 18 15:24:43.216: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
    Jan 18 15:24:43.216: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 0
    Jan 18 15:24:43.216: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
    Jan 18 15:24:43.216: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
    Jan 18 15:24:43.216: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan 18 15:24:43.216: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan 18 15:24:43.216: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan 18 15:24:43.216: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan 18 15:24:43.222: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan 18 15:24:43.223: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan 18 15:24:43.252: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan 18 15:24:43.252: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan 18 15:24:43.306: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
    Jan 18 15:24:43.307: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
    Jan 18 15:24:43.322: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
    Jan 18 15:24:43.322: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
    Jan 18 15:24:45.219: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan 18 15:24:45.220: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan 18 15:24:45.252: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
    STEP: listing Deployments 01/18/23 15:24:45.253
    Jan 18 15:24:45.259: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 01/18/23 15:24:45.259
    Jan 18 15:24:45.280: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 01/18/23 15:24:45.281
    Jan 18 15:24:45.318: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 15:24:45.318: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 15:24:45.354: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 15:24:45.410: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 15:24:45.441: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 15:24:47.227: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 15:24:47.254: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 15:24:47.263: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 15:24:47.295: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 15:24:49.113: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 01/18/23 15:24:49.234
    STEP: fetching the DeploymentStatus 01/18/23 15:24:49.421
    Jan 18 15:24:49.429: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
    Jan 18 15:24:49.429: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
    Jan 18 15:24:49.429: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
    Jan 18 15:24:49.430: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
    Jan 18 15:24:49.430: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 1
    Jan 18 15:24:49.430: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan 18 15:24:49.430: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan 18 15:24:49.431: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan 18 15:24:49.431: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 2
    Jan 18 15:24:49.431: INFO: observed Deployment test-deployment in namespace deployment-5772 with ReadyReplicas 3
    STEP: deleting the Deployment 01/18/23 15:24:49.431
    Jan 18 15:24:49.445: INFO: observed event type MODIFIED
    Jan 18 15:24:49.445: INFO: observed event type MODIFIED
    Jan 18 15:24:49.446: INFO: observed event type MODIFIED
    Jan 18 15:24:49.446: INFO: observed event type MODIFIED
    Jan 18 15:24:49.446: INFO: observed event type MODIFIED
    Jan 18 15:24:49.447: INFO: observed event type MODIFIED
    Jan 18 15:24:49.447: INFO: observed event type MODIFIED
    Jan 18 15:24:49.447: INFO: observed event type MODIFIED
    Jan 18 15:24:49.447: INFO: observed event type MODIFIED
    Jan 18 15:24:49.448: INFO: observed event type MODIFIED
    Jan 18 15:24:49.448: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 15:24:49.454: INFO: Log out all the ReplicaSets if there is no deployment created
    Jan 18 15:24:49.464: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-5772  667a2084-a69d-4cf1-b61f-55a80b59ba8c 35483 4 2023-01-18 15:24:43 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 96cc2709-3a7e-43d0-aa0a-9cbbe28017b2 0xc009d37ea7 0xc009d37ea8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 15:24:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96cc2709-3a7e-43d0-aa0a-9cbbe28017b2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 15:24:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009d37f60 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Jan 18 15:24:49.472: INFO: pod: "test-deployment-54cc775c4b-pkqg2":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-pkqg2 test-deployment-54cc775c4b- deployment-5772  c11d7b97-4931-4aba-ae4f-9c00c5ad8159 35479 0 2023-01-18 15:24:43 +0000 UTC 2023-01-18 15:24:50 +0000 UTC 0xc009d82a98 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:8a035c5e1211f89f156dab2bd1d34e0fdd831320aa7b1627d91bf6bf2b782f89 cni.projectcalico.org/podIP:10.233.68.107/32 cni.projectcalico.org/podIPs:10.233.68.107/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 667a2084-a69d-4cf1-b61f-55a80b59ba8c 0xc009d82af7 0xc009d82af8}] [] [{calico Update v1 2023-01-18 15:24:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-18 15:24:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"667a2084-a69d-4cf1-b61f-55a80b59ba8c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 15:24:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-58mrr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-58mrr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:24:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:24:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:24:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:24:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.107,StartTime:2023-01-18 15:24:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 15:24:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://83b4f5ae36e5fd1c2ef5cd4494e7309109af00f25377eb336112ce49d84b0b9a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.107,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 18 15:24:49.472: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-5772  8322f88c-e2ed-4797-83a8-556e177e9701 35475 2 2023-01-18 15:24:45 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 96cc2709-3a7e-43d0-aa0a-9cbbe28017b2 0xc009d37fe7 0xc009d37fe8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 15:24:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96cc2709-3a7e-43d0-aa0a-9cbbe28017b2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 15:24:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009da4080 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Jan 18 15:24:49.478: INFO: pod: "test-deployment-7c7d8d58c8-hqnqd":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-hqnqd test-deployment-7c7d8d58c8- deployment-5772  7cb4ba2f-6149-47b0-97f9-870fdd665a87 35430 0 2023-01-18 15:24:45 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:59467770a8573175315ed2f3fceb345ccbd37547b378d6c16f369b78fa031540 cni.projectcalico.org/podIP:10.233.68.108/32 cni.projectcalico.org/podIPs:10.233.68.108/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 8322f88c-e2ed-4797-83a8-556e177e9701 0xc009d83b07 0xc009d83b08}] [] [{kube-controller-manager Update v1 2023-01-18 15:24:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8322f88c-e2ed-4797-83a8-556e177e9701\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 15:24:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 15:24:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5v4gf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5v4gf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:24:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:24:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:24:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:24:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.108,StartTime:2023-01-18 15:24:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 15:24:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f819213b229f46aa6b8a0c5015c78e34404593040ad34621809382201800801c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.108,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 18 15:24:49.479: INFO: pod: "test-deployment-7c7d8d58c8-hv9dx":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-hv9dx test-deployment-7c7d8d58c8- deployment-5772  841fc6bf-14e6-4013-9205-06f7ea01985e 35474 0 2023-01-18 15:24:47 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:bc14f7bf621358fe1271c6ced45fa0919383a0c4613c1e0953a6a99f39c4f17d cni.projectcalico.org/podIP:10.233.78.63/32 cni.projectcalico.org/podIPs:10.233.78.63/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 8322f88c-e2ed-4797-83a8-556e177e9701 0xc009d83dc7 0xc009d83dc8}] [] [{calico Update v1 2023-01-18 15:24:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-18 15:24:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8322f88c-e2ed-4797-83a8-556e177e9701\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 15:24:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-54ffr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-54ffr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:24:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:24:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:10.233.78.63,StartTime:2023-01-18 15:24:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 15:24:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c52960a32f073480eb5612a99fb05493b809663287407082198573021ab6b9cd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 18 15:24:49.480: INFO: ReplicaSet "test-deployment-8594bb6fdd":
    &ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-5772  9bc3c2b3-db49-420e-9f20-71bf1f772006 35363 3 2023-01-18 15:24:41 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 96cc2709-3a7e-43d0-aa0a-9cbbe28017b2 0xc009da40f7 0xc009da40f8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 15:24:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96cc2709-3a7e-43d0-aa0a-9cbbe28017b2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 15:24:45 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009da4180 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 18 15:24:49.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5772" for this suite. 01/18/23 15:24:49.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:24:49.506
Jan 18 15:24:49.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename svcaccounts 01/18/23 15:24:49.51
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:24:49.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:24:49.549
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 01/18/23 15:24:49.555
STEP: watching for the ServiceAccount to be added 01/18/23 15:24:49.569
STEP: patching the ServiceAccount 01/18/23 15:24:49.572
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/18/23 15:24:49.584
STEP: deleting the ServiceAccount 01/18/23 15:24:49.592
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 18 15:24:49.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9342" for this suite. 01/18/23 15:24:49.622
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":60,"skipped":1116,"failed":0}
------------------------------
• [0.126 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:24:49.506
    Jan 18 15:24:49.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 15:24:49.51
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:24:49.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:24:49.549
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 01/18/23 15:24:49.555
    STEP: watching for the ServiceAccount to be added 01/18/23 15:24:49.569
    STEP: patching the ServiceAccount 01/18/23 15:24:49.572
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/18/23 15:24:49.584
    STEP: deleting the ServiceAccount 01/18/23 15:24:49.592
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 18 15:24:49.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-9342" for this suite. 01/18/23 15:24:49.622
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:24:49.637
Jan 18 15:24:49.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename downward-api 01/18/23 15:24:49.639
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:24:49.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:24:49.675
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 01/18/23 15:24:49.685
Jan 18 15:24:49.695: INFO: Waiting up to 5m0s for pod "downwardapi-volume-589ba856-2ec4-4a1a-b80d-19ccdaf2487f" in namespace "downward-api-220" to be "Succeeded or Failed"
Jan 18 15:24:49.700: INFO: Pod "downwardapi-volume-589ba856-2ec4-4a1a-b80d-19ccdaf2487f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.478607ms
Jan 18 15:24:51.706: INFO: Pod "downwardapi-volume-589ba856-2ec4-4a1a-b80d-19ccdaf2487f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011142643s
Jan 18 15:24:53.705: INFO: Pod "downwardapi-volume-589ba856-2ec4-4a1a-b80d-19ccdaf2487f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010205118s
STEP: Saw pod success 01/18/23 15:24:53.706
Jan 18 15:24:53.706: INFO: Pod "downwardapi-volume-589ba856-2ec4-4a1a-b80d-19ccdaf2487f" satisfied condition "Succeeded or Failed"
Jan 18 15:24:53.711: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-589ba856-2ec4-4a1a-b80d-19ccdaf2487f container client-container: <nil>
STEP: delete the pod 01/18/23 15:24:53.731
Jan 18 15:24:53.744: INFO: Waiting for pod downwardapi-volume-589ba856-2ec4-4a1a-b80d-19ccdaf2487f to disappear
Jan 18 15:24:53.748: INFO: Pod downwardapi-volume-589ba856-2ec4-4a1a-b80d-19ccdaf2487f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 15:24:53.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-220" for this suite. 01/18/23 15:24:53.754
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":61,"skipped":1160,"failed":0}
------------------------------
• [4.129 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:24:49.637
    Jan 18 15:24:49.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename downward-api 01/18/23 15:24:49.639
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:24:49.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:24:49.675
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 01/18/23 15:24:49.685
    Jan 18 15:24:49.695: INFO: Waiting up to 5m0s for pod "downwardapi-volume-589ba856-2ec4-4a1a-b80d-19ccdaf2487f" in namespace "downward-api-220" to be "Succeeded or Failed"
    Jan 18 15:24:49.700: INFO: Pod "downwardapi-volume-589ba856-2ec4-4a1a-b80d-19ccdaf2487f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.478607ms
    Jan 18 15:24:51.706: INFO: Pod "downwardapi-volume-589ba856-2ec4-4a1a-b80d-19ccdaf2487f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011142643s
    Jan 18 15:24:53.705: INFO: Pod "downwardapi-volume-589ba856-2ec4-4a1a-b80d-19ccdaf2487f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010205118s
    STEP: Saw pod success 01/18/23 15:24:53.706
    Jan 18 15:24:53.706: INFO: Pod "downwardapi-volume-589ba856-2ec4-4a1a-b80d-19ccdaf2487f" satisfied condition "Succeeded or Failed"
    Jan 18 15:24:53.711: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-589ba856-2ec4-4a1a-b80d-19ccdaf2487f container client-container: <nil>
    STEP: delete the pod 01/18/23 15:24:53.731
    Jan 18 15:24:53.744: INFO: Waiting for pod downwardapi-volume-589ba856-2ec4-4a1a-b80d-19ccdaf2487f to disappear
    Jan 18 15:24:53.748: INFO: Pod downwardapi-volume-589ba856-2ec4-4a1a-b80d-19ccdaf2487f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 15:24:53.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-220" for this suite. 01/18/23 15:24:53.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:24:53.774
Jan 18 15:24:53.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename kubectl 01/18/23 15:24:53.776
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:24:53.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:24:53.818
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 01/18/23 15:24:53.825
Jan 18 15:24:53.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-5732 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 18 15:24:53.960: INFO: stderr: ""
Jan 18 15:24:53.960: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 01/18/23 15:24:53.96
Jan 18 15:24:53.961: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 18 15:24:53.961: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5732" to be "running and ready, or succeeded"
Jan 18 15:24:53.967: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.295398ms
Jan 18 15:24:53.970: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'v1-25-1-18760-w2' to be 'Running' but was 'Pending'
Jan 18 15:24:55.975: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014615261s
Jan 18 15:24:55.975: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'v1-25-1-18760-w2' to be 'Running' but was 'Pending'
Jan 18 15:24:57.974: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.013046057s
Jan 18 15:24:57.974: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 18 15:24:57.974: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 01/18/23 15:24:57.974
Jan 18 15:24:57.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-5732 logs logs-generator logs-generator'
Jan 18 15:24:58.103: INFO: stderr: ""
Jan 18 15:24:58.103: INFO: stdout: "I0118 15:24:55.518921       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/zqtr 358\nI0118 15:24:55.719095       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/46r 334\nI0118 15:24:55.919841       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/76f 292\nI0118 15:24:56.119159       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/flm 380\nI0118 15:24:56.319715       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/m4c 279\nI0118 15:24:56.519071       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/fcz 568\nI0118 15:24:56.719542       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/qk9 320\nI0118 15:24:56.918982       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/x76 370\nI0118 15:24:57.119495       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/x4fr 447\nI0118 15:24:57.319887       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/7c4 381\nI0118 15:24:57.519318       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/hvb2 454\nI0118 15:24:57.719787       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/z5l2 342\nI0118 15:24:57.919114       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/dkv 409\n"
STEP: limiting log lines 01/18/23 15:24:58.103
Jan 18 15:24:58.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-5732 logs logs-generator logs-generator --tail=1'
Jan 18 15:24:58.226: INFO: stderr: ""
Jan 18 15:24:58.226: INFO: stdout: "I0118 15:24:58.119593       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/dtj8 263\n"
Jan 18 15:24:58.226: INFO: got output "I0118 15:24:58.119593       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/dtj8 263\n"
STEP: limiting log bytes 01/18/23 15:24:58.226
Jan 18 15:24:58.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-5732 logs logs-generator logs-generator --limit-bytes=1'
Jan 18 15:24:58.335: INFO: stderr: ""
Jan 18 15:24:58.335: INFO: stdout: "I"
Jan 18 15:24:58.335: INFO: got output "I"
STEP: exposing timestamps 01/18/23 15:24:58.335
Jan 18 15:24:58.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-5732 logs logs-generator logs-generator --tail=1 --timestamps'
Jan 18 15:24:58.450: INFO: stderr: ""
Jan 18 15:24:58.450: INFO: stdout: "2023-01-18T15:24:58.319258084Z I0118 15:24:58.319065       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/bl7 338\n"
Jan 18 15:24:58.450: INFO: got output "2023-01-18T15:24:58.319258084Z I0118 15:24:58.319065       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/bl7 338\n"
STEP: restricting to a time range 01/18/23 15:24:58.45
Jan 18 15:25:00.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-5732 logs logs-generator logs-generator --since=1s'
Jan 18 15:25:01.065: INFO: stderr: ""
Jan 18 15:25:01.065: INFO: stdout: "I0118 15:25:00.119011       1 logs_generator.go:76] 23 GET /api/v1/namespaces/default/pods/hxpp 563\nI0118 15:25:00.319411       1 logs_generator.go:76] 24 GET /api/v1/namespaces/ns/pods/bjxz 259\nI0118 15:25:00.519793       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/default/pods/dzh 214\nI0118 15:25:00.719037       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/c45 404\nI0118 15:25:00.919496       1 logs_generator.go:76] 27 GET /api/v1/namespaces/default/pods/9rp2 353\n"
Jan 18 15:25:01.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-5732 logs logs-generator logs-generator --since=24h'
Jan 18 15:25:01.248: INFO: stderr: ""
Jan 18 15:25:01.248: INFO: stdout: "I0118 15:24:55.518921       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/zqtr 358\nI0118 15:24:55.719095       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/46r 334\nI0118 15:24:55.919841       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/76f 292\nI0118 15:24:56.119159       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/flm 380\nI0118 15:24:56.319715       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/m4c 279\nI0118 15:24:56.519071       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/fcz 568\nI0118 15:24:56.719542       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/qk9 320\nI0118 15:24:56.918982       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/x76 370\nI0118 15:24:57.119495       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/x4fr 447\nI0118 15:24:57.319887       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/7c4 381\nI0118 15:24:57.519318       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/hvb2 454\nI0118 15:24:57.719787       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/z5l2 342\nI0118 15:24:57.919114       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/dkv 409\nI0118 15:24:58.119593       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/dtj8 263\nI0118 15:24:58.319065       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/bl7 338\nI0118 15:24:58.519512       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/s85 439\nI0118 15:24:58.721575       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/fj6w 479\nI0118 15:24:58.919938       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/8ph7 468\nI0118 15:24:59.119275       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/mjw 405\nI0118 15:24:59.319458       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/x7pg 357\nI0118 15:24:59.519843       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/hcwx 550\nI0118 15:24:59.719294       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/rcz 297\nI0118 15:24:59.919877       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/kube-system/pods/plqf 273\nI0118 15:25:00.119011       1 logs_generator.go:76] 23 GET /api/v1/namespaces/default/pods/hxpp 563\nI0118 15:25:00.319411       1 logs_generator.go:76] 24 GET /api/v1/namespaces/ns/pods/bjxz 259\nI0118 15:25:00.519793       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/default/pods/dzh 214\nI0118 15:25:00.719037       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/c45 404\nI0118 15:25:00.919496       1 logs_generator.go:76] 27 GET /api/v1/namespaces/default/pods/9rp2 353\nI0118 15:25:01.126539       1 logs_generator.go:76] 28 POST /api/v1/namespaces/default/pods/8ks5 207\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Jan 18 15:25:01.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-5732 delete pod logs-generator'
Jan 18 15:25:02.299: INFO: stderr: ""
Jan 18 15:25:02.299: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 15:25:02.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5732" for this suite. 01/18/23 15:25:02.305
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":62,"skipped":1200,"failed":0}
------------------------------
• [SLOW TEST] [8.539 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:24:53.774
    Jan 18 15:24:53.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename kubectl 01/18/23 15:24:53.776
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:24:53.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:24:53.818
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 01/18/23 15:24:53.825
    Jan 18 15:24:53.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-5732 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jan 18 15:24:53.960: INFO: stderr: ""
    Jan 18 15:24:53.960: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 01/18/23 15:24:53.96
    Jan 18 15:24:53.961: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jan 18 15:24:53.961: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5732" to be "running and ready, or succeeded"
    Jan 18 15:24:53.967: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.295398ms
    Jan 18 15:24:53.970: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'v1-25-1-18760-w2' to be 'Running' but was 'Pending'
    Jan 18 15:24:55.975: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014615261s
    Jan 18 15:24:55.975: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'v1-25-1-18760-w2' to be 'Running' but was 'Pending'
    Jan 18 15:24:57.974: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.013046057s
    Jan 18 15:24:57.974: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jan 18 15:24:57.974: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 01/18/23 15:24:57.974
    Jan 18 15:24:57.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-5732 logs logs-generator logs-generator'
    Jan 18 15:24:58.103: INFO: stderr: ""
    Jan 18 15:24:58.103: INFO: stdout: "I0118 15:24:55.518921       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/zqtr 358\nI0118 15:24:55.719095       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/46r 334\nI0118 15:24:55.919841       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/76f 292\nI0118 15:24:56.119159       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/flm 380\nI0118 15:24:56.319715       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/m4c 279\nI0118 15:24:56.519071       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/fcz 568\nI0118 15:24:56.719542       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/qk9 320\nI0118 15:24:56.918982       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/x76 370\nI0118 15:24:57.119495       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/x4fr 447\nI0118 15:24:57.319887       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/7c4 381\nI0118 15:24:57.519318       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/hvb2 454\nI0118 15:24:57.719787       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/z5l2 342\nI0118 15:24:57.919114       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/dkv 409\n"
    STEP: limiting log lines 01/18/23 15:24:58.103
    Jan 18 15:24:58.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-5732 logs logs-generator logs-generator --tail=1'
    Jan 18 15:24:58.226: INFO: stderr: ""
    Jan 18 15:24:58.226: INFO: stdout: "I0118 15:24:58.119593       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/dtj8 263\n"
    Jan 18 15:24:58.226: INFO: got output "I0118 15:24:58.119593       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/dtj8 263\n"
    STEP: limiting log bytes 01/18/23 15:24:58.226
    Jan 18 15:24:58.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-5732 logs logs-generator logs-generator --limit-bytes=1'
    Jan 18 15:24:58.335: INFO: stderr: ""
    Jan 18 15:24:58.335: INFO: stdout: "I"
    Jan 18 15:24:58.335: INFO: got output "I"
    STEP: exposing timestamps 01/18/23 15:24:58.335
    Jan 18 15:24:58.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-5732 logs logs-generator logs-generator --tail=1 --timestamps'
    Jan 18 15:24:58.450: INFO: stderr: ""
    Jan 18 15:24:58.450: INFO: stdout: "2023-01-18T15:24:58.319258084Z I0118 15:24:58.319065       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/bl7 338\n"
    Jan 18 15:24:58.450: INFO: got output "2023-01-18T15:24:58.319258084Z I0118 15:24:58.319065       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/bl7 338\n"
    STEP: restricting to a time range 01/18/23 15:24:58.45
    Jan 18 15:25:00.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-5732 logs logs-generator logs-generator --since=1s'
    Jan 18 15:25:01.065: INFO: stderr: ""
    Jan 18 15:25:01.065: INFO: stdout: "I0118 15:25:00.119011       1 logs_generator.go:76] 23 GET /api/v1/namespaces/default/pods/hxpp 563\nI0118 15:25:00.319411       1 logs_generator.go:76] 24 GET /api/v1/namespaces/ns/pods/bjxz 259\nI0118 15:25:00.519793       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/default/pods/dzh 214\nI0118 15:25:00.719037       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/c45 404\nI0118 15:25:00.919496       1 logs_generator.go:76] 27 GET /api/v1/namespaces/default/pods/9rp2 353\n"
    Jan 18 15:25:01.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-5732 logs logs-generator logs-generator --since=24h'
    Jan 18 15:25:01.248: INFO: stderr: ""
    Jan 18 15:25:01.248: INFO: stdout: "I0118 15:24:55.518921       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/zqtr 358\nI0118 15:24:55.719095       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/46r 334\nI0118 15:24:55.919841       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/76f 292\nI0118 15:24:56.119159       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/flm 380\nI0118 15:24:56.319715       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/m4c 279\nI0118 15:24:56.519071       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/fcz 568\nI0118 15:24:56.719542       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/qk9 320\nI0118 15:24:56.918982       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/x76 370\nI0118 15:24:57.119495       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/x4fr 447\nI0118 15:24:57.319887       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/7c4 381\nI0118 15:24:57.519318       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/hvb2 454\nI0118 15:24:57.719787       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/z5l2 342\nI0118 15:24:57.919114       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/dkv 409\nI0118 15:24:58.119593       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/dtj8 263\nI0118 15:24:58.319065       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/bl7 338\nI0118 15:24:58.519512       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/s85 439\nI0118 15:24:58.721575       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/fj6w 479\nI0118 15:24:58.919938       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/8ph7 468\nI0118 15:24:59.119275       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/mjw 405\nI0118 15:24:59.319458       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/x7pg 357\nI0118 15:24:59.519843       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/hcwx 550\nI0118 15:24:59.719294       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/rcz 297\nI0118 15:24:59.919877       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/kube-system/pods/plqf 273\nI0118 15:25:00.119011       1 logs_generator.go:76] 23 GET /api/v1/namespaces/default/pods/hxpp 563\nI0118 15:25:00.319411       1 logs_generator.go:76] 24 GET /api/v1/namespaces/ns/pods/bjxz 259\nI0118 15:25:00.519793       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/default/pods/dzh 214\nI0118 15:25:00.719037       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/c45 404\nI0118 15:25:00.919496       1 logs_generator.go:76] 27 GET /api/v1/namespaces/default/pods/9rp2 353\nI0118 15:25:01.126539       1 logs_generator.go:76] 28 POST /api/v1/namespaces/default/pods/8ks5 207\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Jan 18 15:25:01.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-5732 delete pod logs-generator'
    Jan 18 15:25:02.299: INFO: stderr: ""
    Jan 18 15:25:02.299: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 15:25:02.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5732" for this suite. 01/18/23 15:25:02.305
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:25:02.313
Jan 18 15:25:02.313: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename statefulset 01/18/23 15:25:02.315
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:25:02.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:25:02.351
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4046 01/18/23 15:25:02.356
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 01/18/23 15:25:02.363
STEP: Creating stateful set ss in namespace statefulset-4046 01/18/23 15:25:02.37
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4046 01/18/23 15:25:02.38
Jan 18 15:25:02.390: INFO: Found 0 stateful pods, waiting for 1
Jan 18 15:25:12.396: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/18/23 15:25:12.396
Jan 18 15:25:12.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-4046 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 15:25:12.642: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 15:25:12.642: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 15:25:12.642: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 15:25:12.646: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 18 15:25:22.654: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 15:25:22.655: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 15:25:22.689: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999353s
Jan 18 15:25:23.695: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993980407s
Jan 18 15:25:24.699: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.989291364s
Jan 18 15:25:25.704: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.984586464s
Jan 18 15:25:26.712: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.979287772s
Jan 18 15:25:27.717: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.971920401s
Jan 18 15:25:28.723: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.965889962s
Jan 18 15:25:29.729: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.960957564s
Jan 18 15:25:30.734: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.955015577s
Jan 18 15:25:31.743: INFO: Verifying statefulset ss doesn't scale past 1 for another 948.821159ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4046 01/18/23 15:25:32.743
Jan 18 15:25:32.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-4046 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 15:25:32.955: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 15:25:32.955: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 15:25:32.955: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 15:25:32.959: INFO: Found 1 stateful pods, waiting for 3
Jan 18 15:25:42.967: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 15:25:42.968: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 15:25:42.968: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 01/18/23 15:25:42.968
STEP: Scale down will halt with unhealthy stateful pod 01/18/23 15:25:42.968
Jan 18 15:25:42.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-4046 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 15:25:43.173: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 15:25:43.173: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 15:25:43.173: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 15:25:43.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-4046 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 15:25:43.387: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 15:25:43.387: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 15:25:43.387: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 15:25:43.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-4046 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 15:25:43.583: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 15:25:43.583: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 15:25:43.583: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 15:25:43.583: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 15:25:43.587: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 18 15:25:53.598: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 15:25:53.599: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 15:25:53.599: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 15:25:53.614: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999432s
Jan 18 15:25:54.620: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995242531s
Jan 18 15:25:55.631: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988855299s
Jan 18 15:25:56.637: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.978027467s
Jan 18 15:25:57.643: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.971564199s
Jan 18 15:25:58.650: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.96567181s
Jan 18 15:25:59.658: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.958509167s
Jan 18 15:26:00.666: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.950793001s
Jan 18 15:26:01.684: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.942875654s
Jan 18 15:26:02.690: INFO: Verifying statefulset ss doesn't scale past 3 for another 924.963491ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4046 01/18/23 15:26:03.691
Jan 18 15:26:03.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-4046 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 15:26:03.941: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 15:26:03.941: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 15:26:03.941: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 15:26:03.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-4046 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 15:26:04.155: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 15:26:04.155: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 15:26:04.155: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 15:26:04.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-4046 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 15:26:04.356: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 15:26:04.356: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 15:26:04.356: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 15:26:04.356: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 01/18/23 15:26:14.377
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 15:26:14.378: INFO: Deleting all statefulset in ns statefulset-4046
Jan 18 15:26:14.382: INFO: Scaling statefulset ss to 0
Jan 18 15:26:14.395: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 15:26:14.398: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 18 15:26:14.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4046" for this suite. 01/18/23 15:26:14.434
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":63,"skipped":1200,"failed":0}
------------------------------
• [SLOW TEST] [72.129 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:25:02.313
    Jan 18 15:25:02.313: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename statefulset 01/18/23 15:25:02.315
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:25:02.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:25:02.351
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4046 01/18/23 15:25:02.356
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 01/18/23 15:25:02.363
    STEP: Creating stateful set ss in namespace statefulset-4046 01/18/23 15:25:02.37
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4046 01/18/23 15:25:02.38
    Jan 18 15:25:02.390: INFO: Found 0 stateful pods, waiting for 1
    Jan 18 15:25:12.396: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/18/23 15:25:12.396
    Jan 18 15:25:12.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-4046 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 15:25:12.642: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 15:25:12.642: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 15:25:12.642: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 15:25:12.646: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 18 15:25:22.654: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 15:25:22.655: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 15:25:22.689: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999353s
    Jan 18 15:25:23.695: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993980407s
    Jan 18 15:25:24.699: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.989291364s
    Jan 18 15:25:25.704: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.984586464s
    Jan 18 15:25:26.712: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.979287772s
    Jan 18 15:25:27.717: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.971920401s
    Jan 18 15:25:28.723: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.965889962s
    Jan 18 15:25:29.729: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.960957564s
    Jan 18 15:25:30.734: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.955015577s
    Jan 18 15:25:31.743: INFO: Verifying statefulset ss doesn't scale past 1 for another 948.821159ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4046 01/18/23 15:25:32.743
    Jan 18 15:25:32.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-4046 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 15:25:32.955: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 15:25:32.955: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 15:25:32.955: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 15:25:32.959: INFO: Found 1 stateful pods, waiting for 3
    Jan 18 15:25:42.967: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 15:25:42.968: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 15:25:42.968: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 01/18/23 15:25:42.968
    STEP: Scale down will halt with unhealthy stateful pod 01/18/23 15:25:42.968
    Jan 18 15:25:42.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-4046 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 15:25:43.173: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 15:25:43.173: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 15:25:43.173: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 15:25:43.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-4046 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 15:25:43.387: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 15:25:43.387: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 15:25:43.387: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 15:25:43.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-4046 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 15:25:43.583: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 15:25:43.583: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 15:25:43.583: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 15:25:43.583: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 15:25:43.587: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jan 18 15:25:53.598: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 15:25:53.599: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 15:25:53.599: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 15:25:53.614: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999432s
    Jan 18 15:25:54.620: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995242531s
    Jan 18 15:25:55.631: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988855299s
    Jan 18 15:25:56.637: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.978027467s
    Jan 18 15:25:57.643: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.971564199s
    Jan 18 15:25:58.650: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.96567181s
    Jan 18 15:25:59.658: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.958509167s
    Jan 18 15:26:00.666: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.950793001s
    Jan 18 15:26:01.684: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.942875654s
    Jan 18 15:26:02.690: INFO: Verifying statefulset ss doesn't scale past 3 for another 924.963491ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4046 01/18/23 15:26:03.691
    Jan 18 15:26:03.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-4046 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 15:26:03.941: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 15:26:03.941: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 15:26:03.941: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 15:26:03.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-4046 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 15:26:04.155: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 15:26:04.155: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 15:26:04.155: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 15:26:04.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-4046 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 15:26:04.356: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 15:26:04.356: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 15:26:04.356: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 15:26:04.356: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 01/18/23 15:26:14.377
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 18 15:26:14.378: INFO: Deleting all statefulset in ns statefulset-4046
    Jan 18 15:26:14.382: INFO: Scaling statefulset ss to 0
    Jan 18 15:26:14.395: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 15:26:14.398: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 18 15:26:14.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4046" for this suite. 01/18/23 15:26:14.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:26:14.445
Jan 18 15:26:14.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 15:26:14.449
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:26:14.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:26:14.479
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/18/23 15:26:14.483
Jan 18 15:26:14.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/18/23 15:26:32.761
Jan 18 15:26:32.763: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 15:26:35.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 15:26:52.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1364" for this suite. 01/18/23 15:26:52.779
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":64,"skipped":1209,"failed":0}
------------------------------
• [SLOW TEST] [38.340 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:26:14.445
    Jan 18 15:26:14.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 15:26:14.449
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:26:14.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:26:14.479
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/18/23 15:26:14.483
    Jan 18 15:26:14.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/18/23 15:26:32.761
    Jan 18 15:26:32.763: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 15:26:35.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 15:26:52.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1364" for this suite. 01/18/23 15:26:52.779
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:26:52.799
Jan 18 15:26:52.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename resourcequota 01/18/23 15:26:52.802
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:26:52.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:26:52.84
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 01/18/23 15:26:52.847
STEP: Getting a ResourceQuota 01/18/23 15:26:52.864
STEP: Listing all ResourceQuotas with LabelSelector 01/18/23 15:26:52.869
STEP: Patching the ResourceQuota 01/18/23 15:26:52.875
STEP: Deleting a Collection of ResourceQuotas 01/18/23 15:26:52.884
STEP: Verifying the deleted ResourceQuota 01/18/23 15:26:52.897
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 15:26:52.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4447" for this suite. 01/18/23 15:26:52.907
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":65,"skipped":1261,"failed":0}
------------------------------
• [0.116 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:26:52.799
    Jan 18 15:26:52.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename resourcequota 01/18/23 15:26:52.802
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:26:52.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:26:52.84
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 01/18/23 15:26:52.847
    STEP: Getting a ResourceQuota 01/18/23 15:26:52.864
    STEP: Listing all ResourceQuotas with LabelSelector 01/18/23 15:26:52.869
    STEP: Patching the ResourceQuota 01/18/23 15:26:52.875
    STEP: Deleting a Collection of ResourceQuotas 01/18/23 15:26:52.884
    STEP: Verifying the deleted ResourceQuota 01/18/23 15:26:52.897
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 15:26:52.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4447" for this suite. 01/18/23 15:26:52.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:26:52.918
Jan 18 15:26:52.918: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename svcaccounts 01/18/23 15:26:52.92
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:26:52.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:26:52.942
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Jan 18 15:26:52.966: INFO: created pod pod-service-account-defaultsa
Jan 18 15:26:52.966: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 18 15:26:52.973: INFO: created pod pod-service-account-mountsa
Jan 18 15:26:52.973: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 18 15:26:52.984: INFO: created pod pod-service-account-nomountsa
Jan 18 15:26:52.984: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 18 15:26:53.000: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 18 15:26:53.001: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 18 15:26:53.011: INFO: created pod pod-service-account-mountsa-mountspec
Jan 18 15:26:53.011: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 18 15:26:53.233: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 18 15:26:53.233: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 18 15:26:53.250: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 18 15:26:53.250: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 18 15:26:53.264: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 18 15:26:53.264: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 18 15:26:53.295: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 18 15:26:53.295: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 18 15:26:53.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6342" for this suite. 01/18/23 15:26:53.311
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":66,"skipped":1283,"failed":0}
------------------------------
• [0.407 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:26:52.918
    Jan 18 15:26:52.918: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 15:26:52.92
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:26:52.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:26:52.942
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Jan 18 15:26:52.966: INFO: created pod pod-service-account-defaultsa
    Jan 18 15:26:52.966: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jan 18 15:26:52.973: INFO: created pod pod-service-account-mountsa
    Jan 18 15:26:52.973: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jan 18 15:26:52.984: INFO: created pod pod-service-account-nomountsa
    Jan 18 15:26:52.984: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jan 18 15:26:53.000: INFO: created pod pod-service-account-defaultsa-mountspec
    Jan 18 15:26:53.001: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jan 18 15:26:53.011: INFO: created pod pod-service-account-mountsa-mountspec
    Jan 18 15:26:53.011: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jan 18 15:26:53.233: INFO: created pod pod-service-account-nomountsa-mountspec
    Jan 18 15:26:53.233: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jan 18 15:26:53.250: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jan 18 15:26:53.250: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jan 18 15:26:53.264: INFO: created pod pod-service-account-mountsa-nomountspec
    Jan 18 15:26:53.264: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jan 18 15:26:53.295: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jan 18 15:26:53.295: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 18 15:26:53.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-6342" for this suite. 01/18/23 15:26:53.311
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:26:53.327
Jan 18 15:26:53.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename dns 01/18/23 15:26:53.328
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:26:53.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:26:53.364
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7905.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7905.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 01/18/23 15:26:53.37
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7905.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7905.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 01/18/23 15:26:53.37
STEP: creating a pod to probe /etc/hosts 01/18/23 15:26:53.37
STEP: submitting the pod to kubernetes 01/18/23 15:26:53.37
Jan 18 15:26:53.379: INFO: Waiting up to 15m0s for pod "dns-test-1567377e-8a96-46dc-9cf4-243504ec3cd5" in namespace "dns-7905" to be "running"
Jan 18 15:26:53.384: INFO: Pod "dns-test-1567377e-8a96-46dc-9cf4-243504ec3cd5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.270049ms
Jan 18 15:26:55.391: INFO: Pod "dns-test-1567377e-8a96-46dc-9cf4-243504ec3cd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012238695s
Jan 18 15:26:57.402: INFO: Pod "dns-test-1567377e-8a96-46dc-9cf4-243504ec3cd5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022955737s
Jan 18 15:26:59.390: INFO: Pod "dns-test-1567377e-8a96-46dc-9cf4-243504ec3cd5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011492089s
Jan 18 15:27:01.389: INFO: Pod "dns-test-1567377e-8a96-46dc-9cf4-243504ec3cd5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010378444s
Jan 18 15:27:03.395: INFO: Pod "dns-test-1567377e-8a96-46dc-9cf4-243504ec3cd5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.01613793s
Jan 18 15:27:05.393: INFO: Pod "dns-test-1567377e-8a96-46dc-9cf4-243504ec3cd5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.014573947s
Jan 18 15:27:07.391: INFO: Pod "dns-test-1567377e-8a96-46dc-9cf4-243504ec3cd5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.012404141s
Jan 18 15:27:09.391: INFO: Pod "dns-test-1567377e-8a96-46dc-9cf4-243504ec3cd5": Phase="Running", Reason="", readiness=true. Elapsed: 16.011873019s
Jan 18 15:27:09.391: INFO: Pod "dns-test-1567377e-8a96-46dc-9cf4-243504ec3cd5" satisfied condition "running"
STEP: retrieving the pod 01/18/23 15:27:09.391
STEP: looking for the results for each expected name from probers 01/18/23 15:27:09.397
Jan 18 15:27:09.435: INFO: DNS probes using dns-7905/dns-test-1567377e-8a96-46dc-9cf4-243504ec3cd5 succeeded

STEP: deleting the pod 01/18/23 15:27:09.435
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 18 15:27:09.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7905" for this suite. 01/18/23 15:27:09.459
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":67,"skipped":1316,"failed":0}
------------------------------
• [SLOW TEST] [16.142 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:26:53.327
    Jan 18 15:26:53.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename dns 01/18/23 15:26:53.328
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:26:53.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:26:53.364
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7905.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7905.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     01/18/23 15:26:53.37
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7905.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7905.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     01/18/23 15:26:53.37
    STEP: creating a pod to probe /etc/hosts 01/18/23 15:26:53.37
    STEP: submitting the pod to kubernetes 01/18/23 15:26:53.37
    Jan 18 15:26:53.379: INFO: Waiting up to 15m0s for pod "dns-test-1567377e-8a96-46dc-9cf4-243504ec3cd5" in namespace "dns-7905" to be "running"
    Jan 18 15:26:53.384: INFO: Pod "dns-test-1567377e-8a96-46dc-9cf4-243504ec3cd5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.270049ms
    Jan 18 15:26:55.391: INFO: Pod "dns-test-1567377e-8a96-46dc-9cf4-243504ec3cd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012238695s
    Jan 18 15:26:57.402: INFO: Pod "dns-test-1567377e-8a96-46dc-9cf4-243504ec3cd5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022955737s
    Jan 18 15:26:59.390: INFO: Pod "dns-test-1567377e-8a96-46dc-9cf4-243504ec3cd5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011492089s
    Jan 18 15:27:01.389: INFO: Pod "dns-test-1567377e-8a96-46dc-9cf4-243504ec3cd5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010378444s
    Jan 18 15:27:03.395: INFO: Pod "dns-test-1567377e-8a96-46dc-9cf4-243504ec3cd5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.01613793s
    Jan 18 15:27:05.393: INFO: Pod "dns-test-1567377e-8a96-46dc-9cf4-243504ec3cd5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.014573947s
    Jan 18 15:27:07.391: INFO: Pod "dns-test-1567377e-8a96-46dc-9cf4-243504ec3cd5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.012404141s
    Jan 18 15:27:09.391: INFO: Pod "dns-test-1567377e-8a96-46dc-9cf4-243504ec3cd5": Phase="Running", Reason="", readiness=true. Elapsed: 16.011873019s
    Jan 18 15:27:09.391: INFO: Pod "dns-test-1567377e-8a96-46dc-9cf4-243504ec3cd5" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 15:27:09.391
    STEP: looking for the results for each expected name from probers 01/18/23 15:27:09.397
    Jan 18 15:27:09.435: INFO: DNS probes using dns-7905/dns-test-1567377e-8a96-46dc-9cf4-243504ec3cd5 succeeded

    STEP: deleting the pod 01/18/23 15:27:09.435
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 18 15:27:09.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-7905" for this suite. 01/18/23 15:27:09.459
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:27:09.471
Jan 18 15:27:09.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename replication-controller 01/18/23 15:27:09.474
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:27:09.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:27:09.509
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 01/18/23 15:27:09.523
STEP: When the matched label of one of its pods change 01/18/23 15:27:09.529
Jan 18 15:27:09.534: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 18 15:27:14.540: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 01/18/23 15:27:14.557
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 18 15:27:15.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6935" for this suite. 01/18/23 15:27:15.579
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":68,"skipped":1318,"failed":0}
------------------------------
• [SLOW TEST] [6.119 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:27:09.471
    Jan 18 15:27:09.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename replication-controller 01/18/23 15:27:09.474
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:27:09.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:27:09.509
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 01/18/23 15:27:09.523
    STEP: When the matched label of one of its pods change 01/18/23 15:27:09.529
    Jan 18 15:27:09.534: INFO: Pod name pod-release: Found 0 pods out of 1
    Jan 18 15:27:14.540: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/18/23 15:27:14.557
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 18 15:27:15.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-6935" for this suite. 01/18/23 15:27:15.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:27:15.593
Jan 18 15:27:15.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename pods 01/18/23 15:27:15.608
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:27:15.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:27:15.649
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 01/18/23 15:27:15.666
STEP: watching for Pod to be ready 01/18/23 15:27:15.677
Jan 18 15:27:15.684: INFO: observed Pod pod-test in namespace pods-4918 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan 18 15:27:15.691: INFO: observed Pod pod-test in namespace pods-4918 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 15:27:15 +0000 UTC  }]
Jan 18 15:27:15.708: INFO: observed Pod pod-test in namespace pods-4918 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 15:27:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 15:27:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 15:27:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 15:27:15 +0000 UTC  }]
Jan 18 15:27:16.658: INFO: observed Pod pod-test in namespace pods-4918 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 15:27:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 15:27:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 15:27:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 15:27:15 +0000 UTC  }]
Jan 18 15:27:17.096: INFO: Found Pod pod-test in namespace pods-4918 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 15:27:15 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 15:27:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 15:27:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 15:27:15 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 01/18/23 15:27:17.102
STEP: getting the Pod and ensuring that it's patched 01/18/23 15:27:17.122
STEP: replacing the Pod's status Ready condition to False 01/18/23 15:27:17.129
STEP: check the Pod again to ensure its Ready conditions are False 01/18/23 15:27:17.145
STEP: deleting the Pod via a Collection with a LabelSelector 01/18/23 15:27:17.146
STEP: watching for the Pod to be deleted 01/18/23 15:27:17.165
Jan 18 15:27:17.170: INFO: observed event type MODIFIED
Jan 18 15:27:19.105: INFO: observed event type MODIFIED
Jan 18 15:27:19.710: INFO: observed event type MODIFIED
Jan 18 15:27:20.125: INFO: observed event type MODIFIED
Jan 18 15:27:20.131: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 15:27:20.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4918" for this suite. 01/18/23 15:27:20.152
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":69,"skipped":1375,"failed":0}
------------------------------
• [4.570 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:27:15.593
    Jan 18 15:27:15.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename pods 01/18/23 15:27:15.608
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:27:15.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:27:15.649
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 01/18/23 15:27:15.666
    STEP: watching for Pod to be ready 01/18/23 15:27:15.677
    Jan 18 15:27:15.684: INFO: observed Pod pod-test in namespace pods-4918 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jan 18 15:27:15.691: INFO: observed Pod pod-test in namespace pods-4918 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 15:27:15 +0000 UTC  }]
    Jan 18 15:27:15.708: INFO: observed Pod pod-test in namespace pods-4918 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 15:27:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 15:27:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 15:27:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 15:27:15 +0000 UTC  }]
    Jan 18 15:27:16.658: INFO: observed Pod pod-test in namespace pods-4918 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 15:27:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 15:27:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 15:27:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 15:27:15 +0000 UTC  }]
    Jan 18 15:27:17.096: INFO: Found Pod pod-test in namespace pods-4918 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 15:27:15 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 15:27:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 15:27:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 15:27:15 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 01/18/23 15:27:17.102
    STEP: getting the Pod and ensuring that it's patched 01/18/23 15:27:17.122
    STEP: replacing the Pod's status Ready condition to False 01/18/23 15:27:17.129
    STEP: check the Pod again to ensure its Ready conditions are False 01/18/23 15:27:17.145
    STEP: deleting the Pod via a Collection with a LabelSelector 01/18/23 15:27:17.146
    STEP: watching for the Pod to be deleted 01/18/23 15:27:17.165
    Jan 18 15:27:17.170: INFO: observed event type MODIFIED
    Jan 18 15:27:19.105: INFO: observed event type MODIFIED
    Jan 18 15:27:19.710: INFO: observed event type MODIFIED
    Jan 18 15:27:20.125: INFO: observed event type MODIFIED
    Jan 18 15:27:20.131: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 15:27:20.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4918" for this suite. 01/18/23 15:27:20.152
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:27:20.17
Jan 18 15:27:20.170: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename emptydir 01/18/23 15:27:20.172
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:27:20.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:27:20.199
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 01/18/23 15:27:20.202
Jan 18 15:27:20.211: INFO: Waiting up to 5m0s for pod "pod-bbbcfb53-8743-4146-9532-6a774e0c1b4c" in namespace "emptydir-4653" to be "Succeeded or Failed"
Jan 18 15:27:20.222: INFO: Pod "pod-bbbcfb53-8743-4146-9532-6a774e0c1b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.174383ms
Jan 18 15:27:22.232: INFO: Pod "pod-bbbcfb53-8743-4146-9532-6a774e0c1b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021240164s
Jan 18 15:27:24.228: INFO: Pod "pod-bbbcfb53-8743-4146-9532-6a774e0c1b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017222243s
Jan 18 15:27:26.227: INFO: Pod "pod-bbbcfb53-8743-4146-9532-6a774e0c1b4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016148806s
STEP: Saw pod success 01/18/23 15:27:26.227
Jan 18 15:27:26.228: INFO: Pod "pod-bbbcfb53-8743-4146-9532-6a774e0c1b4c" satisfied condition "Succeeded or Failed"
Jan 18 15:27:26.232: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-bbbcfb53-8743-4146-9532-6a774e0c1b4c container test-container: <nil>
STEP: delete the pod 01/18/23 15:27:26.257
Jan 18 15:27:26.312: INFO: Waiting for pod pod-bbbcfb53-8743-4146-9532-6a774e0c1b4c to disappear
Jan 18 15:27:26.328: INFO: Pod pod-bbbcfb53-8743-4146-9532-6a774e0c1b4c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 15:27:26.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4653" for this suite. 01/18/23 15:27:26.335
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":70,"skipped":1377,"failed":0}
------------------------------
• [SLOW TEST] [6.172 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:27:20.17
    Jan 18 15:27:20.170: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename emptydir 01/18/23 15:27:20.172
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:27:20.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:27:20.199
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/18/23 15:27:20.202
    Jan 18 15:27:20.211: INFO: Waiting up to 5m0s for pod "pod-bbbcfb53-8743-4146-9532-6a774e0c1b4c" in namespace "emptydir-4653" to be "Succeeded or Failed"
    Jan 18 15:27:20.222: INFO: Pod "pod-bbbcfb53-8743-4146-9532-6a774e0c1b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.174383ms
    Jan 18 15:27:22.232: INFO: Pod "pod-bbbcfb53-8743-4146-9532-6a774e0c1b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021240164s
    Jan 18 15:27:24.228: INFO: Pod "pod-bbbcfb53-8743-4146-9532-6a774e0c1b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017222243s
    Jan 18 15:27:26.227: INFO: Pod "pod-bbbcfb53-8743-4146-9532-6a774e0c1b4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016148806s
    STEP: Saw pod success 01/18/23 15:27:26.227
    Jan 18 15:27:26.228: INFO: Pod "pod-bbbcfb53-8743-4146-9532-6a774e0c1b4c" satisfied condition "Succeeded or Failed"
    Jan 18 15:27:26.232: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-bbbcfb53-8743-4146-9532-6a774e0c1b4c container test-container: <nil>
    STEP: delete the pod 01/18/23 15:27:26.257
    Jan 18 15:27:26.312: INFO: Waiting for pod pod-bbbcfb53-8743-4146-9532-6a774e0c1b4c to disappear
    Jan 18 15:27:26.328: INFO: Pod pod-bbbcfb53-8743-4146-9532-6a774e0c1b4c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 15:27:26.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4653" for this suite. 01/18/23 15:27:26.335
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:27:26.342
Jan 18 15:27:26.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 15:27:26.345
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:27:26.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:27:26.377
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Jan 18 15:27:26.381: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 15:27:31.574
Jan 18 15:27:31.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-638 --namespace=crd-publish-openapi-638 create -f -'
Jan 18 15:27:33.222: INFO: stderr: ""
Jan 18 15:27:33.222: INFO: stdout: "e2e-test-crd-publish-openapi-2536-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 18 15:27:33.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-638 --namespace=crd-publish-openapi-638 delete e2e-test-crd-publish-openapi-2536-crds test-cr'
Jan 18 15:27:33.335: INFO: stderr: ""
Jan 18 15:27:33.335: INFO: stdout: "e2e-test-crd-publish-openapi-2536-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 18 15:27:33.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-638 --namespace=crd-publish-openapi-638 apply -f -'
Jan 18 15:27:33.637: INFO: stderr: ""
Jan 18 15:27:33.637: INFO: stdout: "e2e-test-crd-publish-openapi-2536-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 18 15:27:33.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-638 --namespace=crd-publish-openapi-638 delete e2e-test-crd-publish-openapi-2536-crds test-cr'
Jan 18 15:27:33.749: INFO: stderr: ""
Jan 18 15:27:33.749: INFO: stdout: "e2e-test-crd-publish-openapi-2536-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 01/18/23 15:27:33.749
Jan 18 15:27:33.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-638 explain e2e-test-crd-publish-openapi-2536-crds'
Jan 18 15:27:34.111: INFO: stderr: ""
Jan 18 15:27:34.111: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2536-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 15:27:39.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-638" for this suite. 01/18/23 15:27:39.302
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":71,"skipped":1378,"failed":0}
------------------------------
• [SLOW TEST] [12.977 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:27:26.342
    Jan 18 15:27:26.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 15:27:26.345
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:27:26.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:27:26.377
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Jan 18 15:27:26.381: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 15:27:31.574
    Jan 18 15:27:31.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-638 --namespace=crd-publish-openapi-638 create -f -'
    Jan 18 15:27:33.222: INFO: stderr: ""
    Jan 18 15:27:33.222: INFO: stdout: "e2e-test-crd-publish-openapi-2536-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 18 15:27:33.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-638 --namespace=crd-publish-openapi-638 delete e2e-test-crd-publish-openapi-2536-crds test-cr'
    Jan 18 15:27:33.335: INFO: stderr: ""
    Jan 18 15:27:33.335: INFO: stdout: "e2e-test-crd-publish-openapi-2536-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jan 18 15:27:33.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-638 --namespace=crd-publish-openapi-638 apply -f -'
    Jan 18 15:27:33.637: INFO: stderr: ""
    Jan 18 15:27:33.637: INFO: stdout: "e2e-test-crd-publish-openapi-2536-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 18 15:27:33.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-638 --namespace=crd-publish-openapi-638 delete e2e-test-crd-publish-openapi-2536-crds test-cr'
    Jan 18 15:27:33.749: INFO: stderr: ""
    Jan 18 15:27:33.749: INFO: stdout: "e2e-test-crd-publish-openapi-2536-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 01/18/23 15:27:33.749
    Jan 18 15:27:33.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-638 explain e2e-test-crd-publish-openapi-2536-crds'
    Jan 18 15:27:34.111: INFO: stderr: ""
    Jan 18 15:27:34.111: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2536-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 15:27:39.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-638" for this suite. 01/18/23 15:27:39.302
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:27:39.321
Jan 18 15:27:39.321: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename statefulset 01/18/23 15:27:39.323
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:27:39.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:27:39.346
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3525 01/18/23 15:27:39.351
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 01/18/23 15:27:39.357
Jan 18 15:27:39.376: INFO: Found 0 stateful pods, waiting for 3
Jan 18 15:27:49.383: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 15:27:49.383: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 15:27:49.383: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 15:27:49.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-3525 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 15:27:49.624: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 15:27:49.624: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 15:27:49.624: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/18/23 15:27:59.642
Jan 18 15:27:59.666: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/18/23 15:27:59.666
STEP: Updating Pods in reverse ordinal order 01/18/23 15:28:09.691
Jan 18 15:28:09.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-3525 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 15:28:09.885: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 15:28:09.885: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 15:28:09.885: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 15:28:19.919: INFO: Waiting for StatefulSet statefulset-3525/ss2 to complete update
STEP: Rolling back to a previous revision 01/18/23 15:28:29.933
Jan 18 15:28:29.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-3525 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 15:28:30.169: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 15:28:30.169: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 15:28:30.169: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 15:28:40.217: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 01/18/23 15:28:50.243
Jan 18 15:28:50.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-3525 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 15:28:50.440: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 15:28:50.440: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 15:28:50.440: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 15:29:00.479: INFO: Waiting for StatefulSet statefulset-3525/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 15:29:10.493: INFO: Deleting all statefulset in ns statefulset-3525
Jan 18 15:29:10.497: INFO: Scaling statefulset ss2 to 0
Jan 18 15:29:20.535: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 15:29:20.540: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 18 15:29:20.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3525" for this suite. 01/18/23 15:29:20.566
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":72,"skipped":1394,"failed":0}
------------------------------
• [SLOW TEST] [101.262 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:27:39.321
    Jan 18 15:27:39.321: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename statefulset 01/18/23 15:27:39.323
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:27:39.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:27:39.346
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3525 01/18/23 15:27:39.351
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 01/18/23 15:27:39.357
    Jan 18 15:27:39.376: INFO: Found 0 stateful pods, waiting for 3
    Jan 18 15:27:49.383: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 15:27:49.383: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 15:27:49.383: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 15:27:49.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-3525 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 15:27:49.624: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 15:27:49.624: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 15:27:49.624: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/18/23 15:27:59.642
    Jan 18 15:27:59.666: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/18/23 15:27:59.666
    STEP: Updating Pods in reverse ordinal order 01/18/23 15:28:09.691
    Jan 18 15:28:09.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-3525 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 15:28:09.885: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 15:28:09.885: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 15:28:09.885: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 15:28:19.919: INFO: Waiting for StatefulSet statefulset-3525/ss2 to complete update
    STEP: Rolling back to a previous revision 01/18/23 15:28:29.933
    Jan 18 15:28:29.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-3525 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 15:28:30.169: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 15:28:30.169: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 15:28:30.169: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 15:28:40.217: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 01/18/23 15:28:50.243
    Jan 18 15:28:50.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-3525 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 15:28:50.440: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 15:28:50.440: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 15:28:50.440: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 15:29:00.479: INFO: Waiting for StatefulSet statefulset-3525/ss2 to complete update
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 18 15:29:10.493: INFO: Deleting all statefulset in ns statefulset-3525
    Jan 18 15:29:10.497: INFO: Scaling statefulset ss2 to 0
    Jan 18 15:29:20.535: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 15:29:20.540: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 18 15:29:20.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3525" for this suite. 01/18/23 15:29:20.566
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:29:20.6
Jan 18 15:29:20.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename kubectl 01/18/23 15:29:20.602
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:29:20.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:29:20.635
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 01/18/23 15:29:20.641
Jan 18 15:29:20.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1230 create -f -'
Jan 18 15:29:22.135: INFO: stderr: ""
Jan 18 15:29:22.135: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 15:29:22.135
Jan 18 15:29:22.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1230 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 15:29:22.305: INFO: stderr: ""
Jan 18 15:29:22.305: INFO: stdout: "update-demo-nautilus-ns7d9 update-demo-nautilus-t5fwm "
Jan 18 15:29:22.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1230 get pods update-demo-nautilus-ns7d9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 15:29:22.415: INFO: stderr: ""
Jan 18 15:29:22.415: INFO: stdout: ""
Jan 18 15:29:22.415: INFO: update-demo-nautilus-ns7d9 is created but not running
Jan 18 15:29:27.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1230 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 15:29:27.602: INFO: stderr: ""
Jan 18 15:29:27.602: INFO: stdout: "update-demo-nautilus-ns7d9 update-demo-nautilus-t5fwm "
Jan 18 15:29:27.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1230 get pods update-demo-nautilus-ns7d9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 15:29:27.736: INFO: stderr: ""
Jan 18 15:29:27.736: INFO: stdout: ""
Jan 18 15:29:27.736: INFO: update-demo-nautilus-ns7d9 is created but not running
Jan 18 15:29:32.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1230 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 15:29:32.857: INFO: stderr: ""
Jan 18 15:29:32.857: INFO: stdout: "update-demo-nautilus-ns7d9 update-demo-nautilus-t5fwm "
Jan 18 15:29:32.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1230 get pods update-demo-nautilus-ns7d9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 15:29:32.962: INFO: stderr: ""
Jan 18 15:29:32.962: INFO: stdout: "true"
Jan 18 15:29:32.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1230 get pods update-demo-nautilus-ns7d9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 15:29:33.061: INFO: stderr: ""
Jan 18 15:29:33.061: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 18 15:29:33.061: INFO: validating pod update-demo-nautilus-ns7d9
Jan 18 15:29:33.069: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 15:29:33.069: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 15:29:33.069: INFO: update-demo-nautilus-ns7d9 is verified up and running
Jan 18 15:29:33.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1230 get pods update-demo-nautilus-t5fwm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 15:29:33.166: INFO: stderr: ""
Jan 18 15:29:33.166: INFO: stdout: "true"
Jan 18 15:29:33.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1230 get pods update-demo-nautilus-t5fwm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 15:29:33.274: INFO: stderr: ""
Jan 18 15:29:33.274: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 18 15:29:33.274: INFO: validating pod update-demo-nautilus-t5fwm
Jan 18 15:29:33.280: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 15:29:33.280: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 15:29:33.280: INFO: update-demo-nautilus-t5fwm is verified up and running
STEP: using delete to clean up resources 01/18/23 15:29:33.28
Jan 18 15:29:33.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1230 delete --grace-period=0 --force -f -'
Jan 18 15:29:33.432: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 15:29:33.432: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 18 15:29:33.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1230 get rc,svc -l name=update-demo --no-headers'
Jan 18 15:29:33.637: INFO: stderr: "No resources found in kubectl-1230 namespace.\n"
Jan 18 15:29:33.637: INFO: stdout: ""
Jan 18 15:29:33.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1230 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 18 15:29:33.788: INFO: stderr: ""
Jan 18 15:29:33.788: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 15:29:33.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1230" for this suite. 01/18/23 15:29:33.794
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":73,"skipped":1469,"failed":0}
------------------------------
• [SLOW TEST] [13.205 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:29:20.6
    Jan 18 15:29:20.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename kubectl 01/18/23 15:29:20.602
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:29:20.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:29:20.635
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 01/18/23 15:29:20.641
    Jan 18 15:29:20.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1230 create -f -'
    Jan 18 15:29:22.135: INFO: stderr: ""
    Jan 18 15:29:22.135: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 15:29:22.135
    Jan 18 15:29:22.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1230 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 15:29:22.305: INFO: stderr: ""
    Jan 18 15:29:22.305: INFO: stdout: "update-demo-nautilus-ns7d9 update-demo-nautilus-t5fwm "
    Jan 18 15:29:22.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1230 get pods update-demo-nautilus-ns7d9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 15:29:22.415: INFO: stderr: ""
    Jan 18 15:29:22.415: INFO: stdout: ""
    Jan 18 15:29:22.415: INFO: update-demo-nautilus-ns7d9 is created but not running
    Jan 18 15:29:27.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1230 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 15:29:27.602: INFO: stderr: ""
    Jan 18 15:29:27.602: INFO: stdout: "update-demo-nautilus-ns7d9 update-demo-nautilus-t5fwm "
    Jan 18 15:29:27.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1230 get pods update-demo-nautilus-ns7d9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 15:29:27.736: INFO: stderr: ""
    Jan 18 15:29:27.736: INFO: stdout: ""
    Jan 18 15:29:27.736: INFO: update-demo-nautilus-ns7d9 is created but not running
    Jan 18 15:29:32.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1230 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 15:29:32.857: INFO: stderr: ""
    Jan 18 15:29:32.857: INFO: stdout: "update-demo-nautilus-ns7d9 update-demo-nautilus-t5fwm "
    Jan 18 15:29:32.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1230 get pods update-demo-nautilus-ns7d9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 15:29:32.962: INFO: stderr: ""
    Jan 18 15:29:32.962: INFO: stdout: "true"
    Jan 18 15:29:32.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1230 get pods update-demo-nautilus-ns7d9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 15:29:33.061: INFO: stderr: ""
    Jan 18 15:29:33.061: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 18 15:29:33.061: INFO: validating pod update-demo-nautilus-ns7d9
    Jan 18 15:29:33.069: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 15:29:33.069: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 15:29:33.069: INFO: update-demo-nautilus-ns7d9 is verified up and running
    Jan 18 15:29:33.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1230 get pods update-demo-nautilus-t5fwm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 15:29:33.166: INFO: stderr: ""
    Jan 18 15:29:33.166: INFO: stdout: "true"
    Jan 18 15:29:33.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1230 get pods update-demo-nautilus-t5fwm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 15:29:33.274: INFO: stderr: ""
    Jan 18 15:29:33.274: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 18 15:29:33.274: INFO: validating pod update-demo-nautilus-t5fwm
    Jan 18 15:29:33.280: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 15:29:33.280: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 15:29:33.280: INFO: update-demo-nautilus-t5fwm is verified up and running
    STEP: using delete to clean up resources 01/18/23 15:29:33.28
    Jan 18 15:29:33.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1230 delete --grace-period=0 --force -f -'
    Jan 18 15:29:33.432: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 15:29:33.432: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 18 15:29:33.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1230 get rc,svc -l name=update-demo --no-headers'
    Jan 18 15:29:33.637: INFO: stderr: "No resources found in kubectl-1230 namespace.\n"
    Jan 18 15:29:33.637: INFO: stdout: ""
    Jan 18 15:29:33.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1230 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 18 15:29:33.788: INFO: stderr: ""
    Jan 18 15:29:33.788: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 15:29:33.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1230" for this suite. 01/18/23 15:29:33.794
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:29:33.805
Jan 18 15:29:33.806: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename security-context-test 01/18/23 15:29:33.808
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:29:33.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:29:33.844
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Jan 18 15:29:33.867: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-bf352afd-d197-4b78-9f36-20a8a0981d6c" in namespace "security-context-test-219" to be "Succeeded or Failed"
Jan 18 15:29:33.874: INFO: Pod "busybox-privileged-false-bf352afd-d197-4b78-9f36-20a8a0981d6c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.822637ms
Jan 18 15:29:35.883: INFO: Pod "busybox-privileged-false-bf352afd-d197-4b78-9f36-20a8a0981d6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015721799s
Jan 18 15:29:37.884: INFO: Pod "busybox-privileged-false-bf352afd-d197-4b78-9f36-20a8a0981d6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016477032s
Jan 18 15:29:37.884: INFO: Pod "busybox-privileged-false-bf352afd-d197-4b78-9f36-20a8a0981d6c" satisfied condition "Succeeded or Failed"
Jan 18 15:29:37.913: INFO: Got logs for pod "busybox-privileged-false-bf352afd-d197-4b78-9f36-20a8a0981d6c": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 18 15:29:37.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-219" for this suite. 01/18/23 15:29:37.921
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":74,"skipped":1469,"failed":0}
------------------------------
• [4.123 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:29:33.805
    Jan 18 15:29:33.806: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename security-context-test 01/18/23 15:29:33.808
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:29:33.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:29:33.844
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Jan 18 15:29:33.867: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-bf352afd-d197-4b78-9f36-20a8a0981d6c" in namespace "security-context-test-219" to be "Succeeded or Failed"
    Jan 18 15:29:33.874: INFO: Pod "busybox-privileged-false-bf352afd-d197-4b78-9f36-20a8a0981d6c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.822637ms
    Jan 18 15:29:35.883: INFO: Pod "busybox-privileged-false-bf352afd-d197-4b78-9f36-20a8a0981d6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015721799s
    Jan 18 15:29:37.884: INFO: Pod "busybox-privileged-false-bf352afd-d197-4b78-9f36-20a8a0981d6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016477032s
    Jan 18 15:29:37.884: INFO: Pod "busybox-privileged-false-bf352afd-d197-4b78-9f36-20a8a0981d6c" satisfied condition "Succeeded or Failed"
    Jan 18 15:29:37.913: INFO: Got logs for pod "busybox-privileged-false-bf352afd-d197-4b78-9f36-20a8a0981d6c": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 18 15:29:37.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-219" for this suite. 01/18/23 15:29:37.921
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:29:37.929
Jan 18 15:29:37.930: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename runtimeclass 01/18/23 15:29:37.932
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:29:37.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:29:37.961
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jan 18 15:29:37.986: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3074 to be scheduled
Jan 18 15:29:37.993: INFO: 1 pods are not scheduled: [runtimeclass-3074/test-runtimeclass-runtimeclass-3074-preconfigured-handler-brx4w(beb69314-7fe2-4351-a4b6-b359a237ead7)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 18 15:29:40.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3074" for this suite. 01/18/23 15:29:40.021
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":75,"skipped":1471,"failed":0}
------------------------------
• [2.100 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:29:37.929
    Jan 18 15:29:37.930: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename runtimeclass 01/18/23 15:29:37.932
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:29:37.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:29:37.961
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jan 18 15:29:37.986: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3074 to be scheduled
    Jan 18 15:29:37.993: INFO: 1 pods are not scheduled: [runtimeclass-3074/test-runtimeclass-runtimeclass-3074-preconfigured-handler-brx4w(beb69314-7fe2-4351-a4b6-b359a237ead7)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 18 15:29:40.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3074" for this suite. 01/18/23 15:29:40.021
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:29:40.036
Jan 18 15:29:40.036: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename replicaset 01/18/23 15:29:40.038
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:29:40.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:29:40.068
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jan 18 15:29:40.072: INFO: Creating ReplicaSet my-hostname-basic-cb2fc420-3b70-4320-b80a-caed8a93acc9
Jan 18 15:29:40.081: INFO: Pod name my-hostname-basic-cb2fc420-3b70-4320-b80a-caed8a93acc9: Found 0 pods out of 1
Jan 18 15:29:45.088: INFO: Pod name my-hostname-basic-cb2fc420-3b70-4320-b80a-caed8a93acc9: Found 1 pods out of 1
Jan 18 15:29:45.088: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-cb2fc420-3b70-4320-b80a-caed8a93acc9" is running
Jan 18 15:29:45.088: INFO: Waiting up to 5m0s for pod "my-hostname-basic-cb2fc420-3b70-4320-b80a-caed8a93acc9-2h7vt" in namespace "replicaset-9523" to be "running"
Jan 18 15:29:45.092: INFO: Pod "my-hostname-basic-cb2fc420-3b70-4320-b80a-caed8a93acc9-2h7vt": Phase="Running", Reason="", readiness=true. Elapsed: 4.387579ms
Jan 18 15:29:45.092: INFO: Pod "my-hostname-basic-cb2fc420-3b70-4320-b80a-caed8a93acc9-2h7vt" satisfied condition "running"
Jan 18 15:29:45.092: INFO: Pod "my-hostname-basic-cb2fc420-3b70-4320-b80a-caed8a93acc9-2h7vt" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 15:29:40 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 15:29:41 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 15:29:41 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 15:29:40 +0000 UTC Reason: Message:}])
Jan 18 15:29:45.092: INFO: Trying to dial the pod
Jan 18 15:29:50.106: INFO: Controller my-hostname-basic-cb2fc420-3b70-4320-b80a-caed8a93acc9: Got expected result from replica 1 [my-hostname-basic-cb2fc420-3b70-4320-b80a-caed8a93acc9-2h7vt]: "my-hostname-basic-cb2fc420-3b70-4320-b80a-caed8a93acc9-2h7vt", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 18 15:29:50.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9523" for this suite. 01/18/23 15:29:50.113
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":76,"skipped":1483,"failed":0}
------------------------------
• [SLOW TEST] [10.086 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:29:40.036
    Jan 18 15:29:40.036: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename replicaset 01/18/23 15:29:40.038
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:29:40.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:29:40.068
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jan 18 15:29:40.072: INFO: Creating ReplicaSet my-hostname-basic-cb2fc420-3b70-4320-b80a-caed8a93acc9
    Jan 18 15:29:40.081: INFO: Pod name my-hostname-basic-cb2fc420-3b70-4320-b80a-caed8a93acc9: Found 0 pods out of 1
    Jan 18 15:29:45.088: INFO: Pod name my-hostname-basic-cb2fc420-3b70-4320-b80a-caed8a93acc9: Found 1 pods out of 1
    Jan 18 15:29:45.088: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-cb2fc420-3b70-4320-b80a-caed8a93acc9" is running
    Jan 18 15:29:45.088: INFO: Waiting up to 5m0s for pod "my-hostname-basic-cb2fc420-3b70-4320-b80a-caed8a93acc9-2h7vt" in namespace "replicaset-9523" to be "running"
    Jan 18 15:29:45.092: INFO: Pod "my-hostname-basic-cb2fc420-3b70-4320-b80a-caed8a93acc9-2h7vt": Phase="Running", Reason="", readiness=true. Elapsed: 4.387579ms
    Jan 18 15:29:45.092: INFO: Pod "my-hostname-basic-cb2fc420-3b70-4320-b80a-caed8a93acc9-2h7vt" satisfied condition "running"
    Jan 18 15:29:45.092: INFO: Pod "my-hostname-basic-cb2fc420-3b70-4320-b80a-caed8a93acc9-2h7vt" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 15:29:40 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 15:29:41 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 15:29:41 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 15:29:40 +0000 UTC Reason: Message:}])
    Jan 18 15:29:45.092: INFO: Trying to dial the pod
    Jan 18 15:29:50.106: INFO: Controller my-hostname-basic-cb2fc420-3b70-4320-b80a-caed8a93acc9: Got expected result from replica 1 [my-hostname-basic-cb2fc420-3b70-4320-b80a-caed8a93acc9-2h7vt]: "my-hostname-basic-cb2fc420-3b70-4320-b80a-caed8a93acc9-2h7vt", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 18 15:29:50.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-9523" for this suite. 01/18/23 15:29:50.113
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:29:50.125
Jan 18 15:29:50.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename kubectl 01/18/23 15:29:50.126
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:29:50.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:29:50.154
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 01/18/23 15:29:50.159
Jan 18 15:29:50.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-9046 create -f -'
Jan 18 15:29:50.507: INFO: stderr: ""
Jan 18 15:29:50.507: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/18/23 15:29:50.507
Jan 18 15:29:51.513: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 15:29:51.513: INFO: Found 0 / 1
Jan 18 15:29:52.514: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 15:29:52.514: INFO: Found 1 / 1
Jan 18 15:29:52.514: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 01/18/23 15:29:52.514
Jan 18 15:29:52.518: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 15:29:52.518: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 18 15:29:52.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-9046 patch pod agnhost-primary-vwx2q -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 18 15:29:52.639: INFO: stderr: ""
Jan 18 15:29:52.639: INFO: stdout: "pod/agnhost-primary-vwx2q patched\n"
STEP: checking annotations 01/18/23 15:29:52.639
Jan 18 15:29:52.671: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 15:29:52.671: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 15:29:52.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9046" for this suite. 01/18/23 15:29:52.677
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":77,"skipped":1484,"failed":0}
------------------------------
• [2.565 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:29:50.125
    Jan 18 15:29:50.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename kubectl 01/18/23 15:29:50.126
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:29:50.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:29:50.154
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 01/18/23 15:29:50.159
    Jan 18 15:29:50.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-9046 create -f -'
    Jan 18 15:29:50.507: INFO: stderr: ""
    Jan 18 15:29:50.507: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/18/23 15:29:50.507
    Jan 18 15:29:51.513: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 15:29:51.513: INFO: Found 0 / 1
    Jan 18 15:29:52.514: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 15:29:52.514: INFO: Found 1 / 1
    Jan 18 15:29:52.514: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 01/18/23 15:29:52.514
    Jan 18 15:29:52.518: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 15:29:52.518: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 18 15:29:52.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-9046 patch pod agnhost-primary-vwx2q -p {"metadata":{"annotations":{"x":"y"}}}'
    Jan 18 15:29:52.639: INFO: stderr: ""
    Jan 18 15:29:52.639: INFO: stdout: "pod/agnhost-primary-vwx2q patched\n"
    STEP: checking annotations 01/18/23 15:29:52.639
    Jan 18 15:29:52.671: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 15:29:52.671: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 15:29:52.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9046" for this suite. 01/18/23 15:29:52.677
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:29:52.689
Jan 18 15:29:52.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename watch 01/18/23 15:29:52.696
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:29:52.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:29:52.721
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 01/18/23 15:29:52.725
STEP: creating a new configmap 01/18/23 15:29:52.727
STEP: modifying the configmap once 01/18/23 15:29:52.732
STEP: closing the watch once it receives two notifications 01/18/23 15:29:52.742
Jan 18 15:29:52.743: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5082  54398655-d505-4995-a64d-fa40fdf8c657 37996 0 2023-01-18 15:29:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 15:29:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 15:29:52.743: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5082  54398655-d505-4995-a64d-fa40fdf8c657 37997 0 2023-01-18 15:29:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 15:29:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 01/18/23 15:29:52.743
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/18/23 15:29:52.754
STEP: deleting the configmap 01/18/23 15:29:52.757
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/18/23 15:29:52.768
Jan 18 15:29:52.769: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5082  54398655-d505-4995-a64d-fa40fdf8c657 37998 0 2023-01-18 15:29:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 15:29:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 15:29:52.769: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5082  54398655-d505-4995-a64d-fa40fdf8c657 38000 0 2023-01-18 15:29:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 15:29:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 18 15:29:52.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5082" for this suite. 01/18/23 15:29:52.775
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":78,"skipped":1486,"failed":0}
------------------------------
• [0.092 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:29:52.689
    Jan 18 15:29:52.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename watch 01/18/23 15:29:52.696
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:29:52.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:29:52.721
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 01/18/23 15:29:52.725
    STEP: creating a new configmap 01/18/23 15:29:52.727
    STEP: modifying the configmap once 01/18/23 15:29:52.732
    STEP: closing the watch once it receives two notifications 01/18/23 15:29:52.742
    Jan 18 15:29:52.743: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5082  54398655-d505-4995-a64d-fa40fdf8c657 37996 0 2023-01-18 15:29:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 15:29:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 15:29:52.743: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5082  54398655-d505-4995-a64d-fa40fdf8c657 37997 0 2023-01-18 15:29:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 15:29:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 01/18/23 15:29:52.743
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/18/23 15:29:52.754
    STEP: deleting the configmap 01/18/23 15:29:52.757
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/18/23 15:29:52.768
    Jan 18 15:29:52.769: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5082  54398655-d505-4995-a64d-fa40fdf8c657 37998 0 2023-01-18 15:29:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 15:29:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 15:29:52.769: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5082  54398655-d505-4995-a64d-fa40fdf8c657 38000 0 2023-01-18 15:29:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 15:29:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 18 15:29:52.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-5082" for this suite. 01/18/23 15:29:52.775
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:29:52.787
Jan 18 15:29:52.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 15:29:52.788
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:29:52.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:29:52.814
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-ecb75981-9a8e-4acb-8eb1-62d7c844f507 01/18/23 15:29:52.819
STEP: Creating a pod to test consume secrets 01/18/23 15:29:52.825
Jan 18 15:29:52.837: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2e894869-e76e-4c14-8a1c-895dbd5944fe" in namespace "projected-4811" to be "Succeeded or Failed"
Jan 18 15:29:52.841: INFO: Pod "pod-projected-secrets-2e894869-e76e-4c14-8a1c-895dbd5944fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051298ms
Jan 18 15:29:54.847: INFO: Pod "pod-projected-secrets-2e894869-e76e-4c14-8a1c-895dbd5944fe": Phase="Running", Reason="", readiness=true. Elapsed: 2.010056554s
Jan 18 15:29:56.846: INFO: Pod "pod-projected-secrets-2e894869-e76e-4c14-8a1c-895dbd5944fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0091781s
STEP: Saw pod success 01/18/23 15:29:56.847
Jan 18 15:29:56.847: INFO: Pod "pod-projected-secrets-2e894869-e76e-4c14-8a1c-895dbd5944fe" satisfied condition "Succeeded or Failed"
Jan 18 15:29:56.852: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-secrets-2e894869-e76e-4c14-8a1c-895dbd5944fe container projected-secret-volume-test: <nil>
STEP: delete the pod 01/18/23 15:29:56.861
Jan 18 15:29:56.880: INFO: Waiting for pod pod-projected-secrets-2e894869-e76e-4c14-8a1c-895dbd5944fe to disappear
Jan 18 15:29:56.883: INFO: Pod pod-projected-secrets-2e894869-e76e-4c14-8a1c-895dbd5944fe no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 18 15:29:56.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4811" for this suite. 01/18/23 15:29:56.888
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":79,"skipped":1512,"failed":0}
------------------------------
• [4.110 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:29:52.787
    Jan 18 15:29:52.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 15:29:52.788
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:29:52.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:29:52.814
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-ecb75981-9a8e-4acb-8eb1-62d7c844f507 01/18/23 15:29:52.819
    STEP: Creating a pod to test consume secrets 01/18/23 15:29:52.825
    Jan 18 15:29:52.837: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2e894869-e76e-4c14-8a1c-895dbd5944fe" in namespace "projected-4811" to be "Succeeded or Failed"
    Jan 18 15:29:52.841: INFO: Pod "pod-projected-secrets-2e894869-e76e-4c14-8a1c-895dbd5944fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051298ms
    Jan 18 15:29:54.847: INFO: Pod "pod-projected-secrets-2e894869-e76e-4c14-8a1c-895dbd5944fe": Phase="Running", Reason="", readiness=true. Elapsed: 2.010056554s
    Jan 18 15:29:56.846: INFO: Pod "pod-projected-secrets-2e894869-e76e-4c14-8a1c-895dbd5944fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0091781s
    STEP: Saw pod success 01/18/23 15:29:56.847
    Jan 18 15:29:56.847: INFO: Pod "pod-projected-secrets-2e894869-e76e-4c14-8a1c-895dbd5944fe" satisfied condition "Succeeded or Failed"
    Jan 18 15:29:56.852: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-secrets-2e894869-e76e-4c14-8a1c-895dbd5944fe container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 15:29:56.861
    Jan 18 15:29:56.880: INFO: Waiting for pod pod-projected-secrets-2e894869-e76e-4c14-8a1c-895dbd5944fe to disappear
    Jan 18 15:29:56.883: INFO: Pod pod-projected-secrets-2e894869-e76e-4c14-8a1c-895dbd5944fe no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 18 15:29:56.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4811" for this suite. 01/18/23 15:29:56.888
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:29:56.904
Jan 18 15:29:56.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename webhook 01/18/23 15:29:56.907
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:29:56.927
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:29:56.931
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 15:29:56.95
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 15:29:57.399
STEP: Deploying the webhook pod 01/18/23 15:29:57.408
STEP: Wait for the deployment to be ready 01/18/23 15:29:57.423
Jan 18 15:29:57.430: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/18/23 15:29:59.447
STEP: Verifying the service has paired with the endpoint 01/18/23 15:29:59.459
Jan 18 15:30:00.460: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Jan 18 15:30:00.467: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9671-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 15:30:00.993
STEP: Creating a custom resource that should be mutated by the webhook 01/18/23 15:30:01.026
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 15:30:03.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2287" for this suite. 01/18/23 15:30:03.69
STEP: Destroying namespace "webhook-2287-markers" for this suite. 01/18/23 15:30:03.699
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":80,"skipped":1569,"failed":0}
------------------------------
• [SLOW TEST] [6.897 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:29:56.904
    Jan 18 15:29:56.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename webhook 01/18/23 15:29:56.907
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:29:56.927
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:29:56.931
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 15:29:56.95
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 15:29:57.399
    STEP: Deploying the webhook pod 01/18/23 15:29:57.408
    STEP: Wait for the deployment to be ready 01/18/23 15:29:57.423
    Jan 18 15:29:57.430: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/18/23 15:29:59.447
    STEP: Verifying the service has paired with the endpoint 01/18/23 15:29:59.459
    Jan 18 15:30:00.460: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Jan 18 15:30:00.467: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9671-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 15:30:00.993
    STEP: Creating a custom resource that should be mutated by the webhook 01/18/23 15:30:01.026
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 15:30:03.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2287" for this suite. 01/18/23 15:30:03.69
    STEP: Destroying namespace "webhook-2287-markers" for this suite. 01/18/23 15:30:03.699
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:30:03.852
Jan 18 15:30:03.852: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/18/23 15:30:03.855
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:30:03.88
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:30:03.885
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 01/18/23 15:30:03.891
STEP: Creating hostNetwork=false pod 01/18/23 15:30:03.892
Jan 18 15:30:03.916: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-7785" to be "running and ready"
Jan 18 15:30:03.927: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.37501ms
Jan 18 15:30:03.927: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:30:05.939: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022777855s
Jan 18 15:30:05.939: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:30:07.932: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.015323221s
Jan 18 15:30:07.932: INFO: The phase of Pod test-pod is Running (Ready = true)
Jan 18 15:30:07.932: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 01/18/23 15:30:07.936
Jan 18 15:30:07.945: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-7785" to be "running and ready"
Jan 18 15:30:07.949: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.107524ms
Jan 18 15:30:07.949: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:30:09.955: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009856511s
Jan 18 15:30:09.955: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jan 18 15:30:09.955: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 01/18/23 15:30:09.959
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/18/23 15:30:09.959
Jan 18 15:30:09.959: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7785 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 15:30:09.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 15:30:09.960: INFO: ExecWithOptions: Clientset creation
Jan 18 15:30:09.960: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7785/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 18 15:30:10.047: INFO: Exec stderr: ""
Jan 18 15:30:10.047: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7785 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 15:30:10.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 15:30:10.048: INFO: ExecWithOptions: Clientset creation
Jan 18 15:30:10.048: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7785/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 18 15:30:10.145: INFO: Exec stderr: ""
Jan 18 15:30:10.145: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7785 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 15:30:10.145: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 15:30:10.146: INFO: ExecWithOptions: Clientset creation
Jan 18 15:30:10.146: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7785/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 18 15:30:10.230: INFO: Exec stderr: ""
Jan 18 15:30:10.230: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7785 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 15:30:10.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 15:30:10.231: INFO: ExecWithOptions: Clientset creation
Jan 18 15:30:10.231: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7785/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 18 15:30:10.314: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/18/23 15:30:10.314
Jan 18 15:30:10.314: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7785 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 15:30:10.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 15:30:10.315: INFO: ExecWithOptions: Clientset creation
Jan 18 15:30:10.315: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7785/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 18 15:30:10.410: INFO: Exec stderr: ""
Jan 18 15:30:10.410: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7785 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 15:30:10.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 15:30:10.411: INFO: ExecWithOptions: Clientset creation
Jan 18 15:30:10.411: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7785/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 18 15:30:10.504: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/18/23 15:30:10.504
Jan 18 15:30:10.504: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7785 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 15:30:10.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 15:30:10.505: INFO: ExecWithOptions: Clientset creation
Jan 18 15:30:10.505: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7785/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 18 15:30:10.628: INFO: Exec stderr: ""
Jan 18 15:30:10.628: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7785 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 15:30:10.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 15:30:10.629: INFO: ExecWithOptions: Clientset creation
Jan 18 15:30:10.630: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7785/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 18 15:30:10.721: INFO: Exec stderr: ""
Jan 18 15:30:10.721: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7785 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 15:30:10.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 15:30:10.723: INFO: ExecWithOptions: Clientset creation
Jan 18 15:30:10.723: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7785/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 18 15:30:10.847: INFO: Exec stderr: ""
Jan 18 15:30:10.847: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7785 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 15:30:10.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 15:30:10.848: INFO: ExecWithOptions: Clientset creation
Jan 18 15:30:10.848: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7785/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 18 15:30:10.964: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Jan 18 15:30:10.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-7785" for this suite. 01/18/23 15:30:10.971
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":81,"skipped":1587,"failed":0}
------------------------------
• [SLOW TEST] [7.148 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:30:03.852
    Jan 18 15:30:03.852: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/18/23 15:30:03.855
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:30:03.88
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:30:03.885
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 01/18/23 15:30:03.891
    STEP: Creating hostNetwork=false pod 01/18/23 15:30:03.892
    Jan 18 15:30:03.916: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-7785" to be "running and ready"
    Jan 18 15:30:03.927: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.37501ms
    Jan 18 15:30:03.927: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:30:05.939: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022777855s
    Jan 18 15:30:05.939: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:30:07.932: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.015323221s
    Jan 18 15:30:07.932: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jan 18 15:30:07.932: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 01/18/23 15:30:07.936
    Jan 18 15:30:07.945: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-7785" to be "running and ready"
    Jan 18 15:30:07.949: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.107524ms
    Jan 18 15:30:07.949: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:30:09.955: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009856511s
    Jan 18 15:30:09.955: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jan 18 15:30:09.955: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 01/18/23 15:30:09.959
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/18/23 15:30:09.959
    Jan 18 15:30:09.959: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7785 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 15:30:09.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 15:30:09.960: INFO: ExecWithOptions: Clientset creation
    Jan 18 15:30:09.960: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7785/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 18 15:30:10.047: INFO: Exec stderr: ""
    Jan 18 15:30:10.047: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7785 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 15:30:10.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 15:30:10.048: INFO: ExecWithOptions: Clientset creation
    Jan 18 15:30:10.048: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7785/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 18 15:30:10.145: INFO: Exec stderr: ""
    Jan 18 15:30:10.145: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7785 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 15:30:10.145: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 15:30:10.146: INFO: ExecWithOptions: Clientset creation
    Jan 18 15:30:10.146: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7785/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 18 15:30:10.230: INFO: Exec stderr: ""
    Jan 18 15:30:10.230: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7785 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 15:30:10.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 15:30:10.231: INFO: ExecWithOptions: Clientset creation
    Jan 18 15:30:10.231: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7785/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 18 15:30:10.314: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/18/23 15:30:10.314
    Jan 18 15:30:10.314: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7785 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 15:30:10.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 15:30:10.315: INFO: ExecWithOptions: Clientset creation
    Jan 18 15:30:10.315: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7785/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 18 15:30:10.410: INFO: Exec stderr: ""
    Jan 18 15:30:10.410: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7785 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 15:30:10.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 15:30:10.411: INFO: ExecWithOptions: Clientset creation
    Jan 18 15:30:10.411: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7785/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 18 15:30:10.504: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/18/23 15:30:10.504
    Jan 18 15:30:10.504: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7785 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 15:30:10.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 15:30:10.505: INFO: ExecWithOptions: Clientset creation
    Jan 18 15:30:10.505: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7785/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 18 15:30:10.628: INFO: Exec stderr: ""
    Jan 18 15:30:10.628: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7785 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 15:30:10.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 15:30:10.629: INFO: ExecWithOptions: Clientset creation
    Jan 18 15:30:10.630: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7785/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 18 15:30:10.721: INFO: Exec stderr: ""
    Jan 18 15:30:10.721: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7785 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 15:30:10.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 15:30:10.723: INFO: ExecWithOptions: Clientset creation
    Jan 18 15:30:10.723: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7785/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 18 15:30:10.847: INFO: Exec stderr: ""
    Jan 18 15:30:10.847: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7785 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 15:30:10.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 15:30:10.848: INFO: ExecWithOptions: Clientset creation
    Jan 18 15:30:10.848: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7785/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 18 15:30:10.964: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Jan 18 15:30:10.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-7785" for this suite. 01/18/23 15:30:10.971
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:30:10.981
Jan 18 15:30:10.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename job 01/18/23 15:30:10.982
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:30:11.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:30:11.009
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 01/18/23 15:30:11.012
STEP: Ensuring active pods == parallelism 01/18/23 15:30:11.018
STEP: Orphaning one of the Job's Pods 01/18/23 15:30:15.024
Jan 18 15:30:15.547: INFO: Successfully updated pod "adopt-release-q5md8"
STEP: Checking that the Job readopts the Pod 01/18/23 15:30:15.547
Jan 18 15:30:15.547: INFO: Waiting up to 15m0s for pod "adopt-release-q5md8" in namespace "job-3238" to be "adopted"
Jan 18 15:30:15.554: INFO: Pod "adopt-release-q5md8": Phase="Running", Reason="", readiness=true. Elapsed: 7.014466ms
Jan 18 15:30:17.561: INFO: Pod "adopt-release-q5md8": Phase="Running", Reason="", readiness=true. Elapsed: 2.013509341s
Jan 18 15:30:17.561: INFO: Pod "adopt-release-q5md8" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 01/18/23 15:30:17.561
Jan 18 15:30:18.075: INFO: Successfully updated pod "adopt-release-q5md8"
STEP: Checking that the Job releases the Pod 01/18/23 15:30:18.075
Jan 18 15:30:18.077: INFO: Waiting up to 15m0s for pod "adopt-release-q5md8" in namespace "job-3238" to be "released"
Jan 18 15:30:18.086: INFO: Pod "adopt-release-q5md8": Phase="Running", Reason="", readiness=true. Elapsed: 8.839055ms
Jan 18 15:30:20.091: INFO: Pod "adopt-release-q5md8": Phase="Running", Reason="", readiness=true. Elapsed: 2.014283185s
Jan 18 15:30:20.091: INFO: Pod "adopt-release-q5md8" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 18 15:30:20.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3238" for this suite. 01/18/23 15:30:20.099
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":82,"skipped":1588,"failed":0}
------------------------------
• [SLOW TEST] [9.129 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:30:10.981
    Jan 18 15:30:10.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename job 01/18/23 15:30:10.982
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:30:11.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:30:11.009
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 01/18/23 15:30:11.012
    STEP: Ensuring active pods == parallelism 01/18/23 15:30:11.018
    STEP: Orphaning one of the Job's Pods 01/18/23 15:30:15.024
    Jan 18 15:30:15.547: INFO: Successfully updated pod "adopt-release-q5md8"
    STEP: Checking that the Job readopts the Pod 01/18/23 15:30:15.547
    Jan 18 15:30:15.547: INFO: Waiting up to 15m0s for pod "adopt-release-q5md8" in namespace "job-3238" to be "adopted"
    Jan 18 15:30:15.554: INFO: Pod "adopt-release-q5md8": Phase="Running", Reason="", readiness=true. Elapsed: 7.014466ms
    Jan 18 15:30:17.561: INFO: Pod "adopt-release-q5md8": Phase="Running", Reason="", readiness=true. Elapsed: 2.013509341s
    Jan 18 15:30:17.561: INFO: Pod "adopt-release-q5md8" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 01/18/23 15:30:17.561
    Jan 18 15:30:18.075: INFO: Successfully updated pod "adopt-release-q5md8"
    STEP: Checking that the Job releases the Pod 01/18/23 15:30:18.075
    Jan 18 15:30:18.077: INFO: Waiting up to 15m0s for pod "adopt-release-q5md8" in namespace "job-3238" to be "released"
    Jan 18 15:30:18.086: INFO: Pod "adopt-release-q5md8": Phase="Running", Reason="", readiness=true. Elapsed: 8.839055ms
    Jan 18 15:30:20.091: INFO: Pod "adopt-release-q5md8": Phase="Running", Reason="", readiness=true. Elapsed: 2.014283185s
    Jan 18 15:30:20.091: INFO: Pod "adopt-release-q5md8" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 18 15:30:20.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-3238" for this suite. 01/18/23 15:30:20.099
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:30:20.112
Jan 18 15:30:20.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename server-version 01/18/23 15:30:20.118
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:30:20.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:30:20.15
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 01/18/23 15:30:20.158
STEP: Confirm major version 01/18/23 15:30:20.161
Jan 18 15:30:20.161: INFO: Major version: 1
STEP: Confirm minor version 01/18/23 15:30:20.161
Jan 18 15:30:20.161: INFO: cleanMinorVersion: 25
Jan 18 15:30:20.161: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Jan 18 15:30:20.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-200" for this suite. 01/18/23 15:30:20.167
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":83,"skipped":1591,"failed":0}
------------------------------
• [0.066 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:30:20.112
    Jan 18 15:30:20.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename server-version 01/18/23 15:30:20.118
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:30:20.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:30:20.15
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 01/18/23 15:30:20.158
    STEP: Confirm major version 01/18/23 15:30:20.161
    Jan 18 15:30:20.161: INFO: Major version: 1
    STEP: Confirm minor version 01/18/23 15:30:20.161
    Jan 18 15:30:20.161: INFO: cleanMinorVersion: 25
    Jan 18 15:30:20.161: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Jan 18 15:30:20.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-200" for this suite. 01/18/23 15:30:20.167
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:30:20.183
Jan 18 15:30:20.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename var-expansion 01/18/23 15:30:20.185
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:30:20.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:30:20.21
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 01/18/23 15:30:20.215
Jan 18 15:30:20.234: INFO: Waiting up to 2m0s for pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9" in namespace "var-expansion-2184" to be "running"
Jan 18 15:30:20.250: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.794688ms
Jan 18 15:30:22.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020267802s
Jan 18 15:30:24.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020090622s
Jan 18 15:30:26.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019011378s
Jan 18 15:30:28.273: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.037971322s
Jan 18 15:30:30.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.020215157s
Jan 18 15:30:32.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.020098422s
Jan 18 15:30:34.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.02070254s
Jan 18 15:30:36.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 16.018854735s
Jan 18 15:30:38.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 18.020389378s
Jan 18 15:30:40.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 20.020430527s
Jan 18 15:30:42.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 22.019378538s
Jan 18 15:30:44.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 24.019150667s
Jan 18 15:30:46.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 26.020558423s
Jan 18 15:30:48.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 28.018850082s
Jan 18 15:30:50.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 30.021227586s
Jan 18 15:30:52.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 32.020779058s
Jan 18 15:30:54.259: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 34.024588291s
Jan 18 15:30:56.262: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 36.027181349s
Jan 18 15:30:58.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 38.021045988s
Jan 18 15:31:00.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 40.021626679s
Jan 18 15:31:02.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 42.021049274s
Jan 18 15:31:04.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 44.019554301s
Jan 18 15:31:06.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 46.020493864s
Jan 18 15:31:08.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 48.021357776s
Jan 18 15:31:10.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 50.019584335s
Jan 18 15:31:12.257: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 52.021787695s
Jan 18 15:31:14.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 54.020324297s
Jan 18 15:31:16.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 56.020117513s
Jan 18 15:31:18.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 58.0212706s
Jan 18 15:31:20.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.020447651s
Jan 18 15:31:22.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.020795997s
Jan 18 15:31:24.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.020271424s
Jan 18 15:31:26.258: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.022688575s
Jan 18 15:31:28.260: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.025045259s
Jan 18 15:31:30.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.019196147s
Jan 18 15:31:32.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.020774228s
Jan 18 15:31:34.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.020221249s
Jan 18 15:31:36.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.021429288s
Jan 18 15:31:38.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.020470997s
Jan 18 15:31:40.257: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.022336092s
Jan 18 15:31:42.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.01950626s
Jan 18 15:31:44.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.019118447s
Jan 18 15:31:46.257: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.022540488s
Jan 18 15:31:48.258: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.023213064s
Jan 18 15:31:50.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.018715169s
Jan 18 15:31:52.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.020790236s
Jan 18 15:31:54.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.020360736s
Jan 18 15:31:56.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.020164505s
Jan 18 15:31:58.260: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.025330938s
Jan 18 15:32:00.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.018957094s
Jan 18 15:32:02.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.019804638s
Jan 18 15:32:04.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.018799962s
Jan 18 15:32:06.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.021386939s
Jan 18 15:32:08.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.021140798s
Jan 18 15:32:10.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.021328185s
Jan 18 15:32:12.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.019575172s
Jan 18 15:32:14.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.019717635s
Jan 18 15:32:16.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.019768621s
Jan 18 15:32:18.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.020065885s
Jan 18 15:32:20.257: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.022183701s
Jan 18 15:32:20.263: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.028332077s
STEP: updating the pod 01/18/23 15:32:20.263
Jan 18 15:32:20.804: INFO: Successfully updated pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9"
STEP: waiting for pod running 01/18/23 15:32:20.804
Jan 18 15:32:20.813: INFO: Waiting up to 2m0s for pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9" in namespace "var-expansion-2184" to be "running"
Jan 18 15:32:20.817: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.14549ms
Jan 18 15:32:22.823: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Running", Reason="", readiness=true. Elapsed: 2.009892278s
Jan 18 15:32:22.823: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9" satisfied condition "running"
STEP: deleting the pod gracefully 01/18/23 15:32:22.823
Jan 18 15:32:22.824: INFO: Deleting pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9" in namespace "var-expansion-2184"
Jan 18 15:32:22.832: INFO: Wait up to 5m0s for pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 18 15:32:54.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2184" for this suite. 01/18/23 15:32:54.868
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":84,"skipped":1595,"failed":0}
------------------------------
• [SLOW TEST] [154.691 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:30:20.183
    Jan 18 15:30:20.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename var-expansion 01/18/23 15:30:20.185
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:30:20.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:30:20.21
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 01/18/23 15:30:20.215
    Jan 18 15:30:20.234: INFO: Waiting up to 2m0s for pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9" in namespace "var-expansion-2184" to be "running"
    Jan 18 15:30:20.250: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.794688ms
    Jan 18 15:30:22.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020267802s
    Jan 18 15:30:24.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020090622s
    Jan 18 15:30:26.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019011378s
    Jan 18 15:30:28.273: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.037971322s
    Jan 18 15:30:30.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.020215157s
    Jan 18 15:30:32.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.020098422s
    Jan 18 15:30:34.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.02070254s
    Jan 18 15:30:36.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 16.018854735s
    Jan 18 15:30:38.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 18.020389378s
    Jan 18 15:30:40.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 20.020430527s
    Jan 18 15:30:42.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 22.019378538s
    Jan 18 15:30:44.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 24.019150667s
    Jan 18 15:30:46.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 26.020558423s
    Jan 18 15:30:48.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 28.018850082s
    Jan 18 15:30:50.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 30.021227586s
    Jan 18 15:30:52.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 32.020779058s
    Jan 18 15:30:54.259: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 34.024588291s
    Jan 18 15:30:56.262: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 36.027181349s
    Jan 18 15:30:58.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 38.021045988s
    Jan 18 15:31:00.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 40.021626679s
    Jan 18 15:31:02.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 42.021049274s
    Jan 18 15:31:04.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 44.019554301s
    Jan 18 15:31:06.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 46.020493864s
    Jan 18 15:31:08.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 48.021357776s
    Jan 18 15:31:10.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 50.019584335s
    Jan 18 15:31:12.257: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 52.021787695s
    Jan 18 15:31:14.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 54.020324297s
    Jan 18 15:31:16.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 56.020117513s
    Jan 18 15:31:18.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 58.0212706s
    Jan 18 15:31:20.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.020447651s
    Jan 18 15:31:22.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.020795997s
    Jan 18 15:31:24.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.020271424s
    Jan 18 15:31:26.258: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.022688575s
    Jan 18 15:31:28.260: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.025045259s
    Jan 18 15:31:30.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.019196147s
    Jan 18 15:31:32.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.020774228s
    Jan 18 15:31:34.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.020221249s
    Jan 18 15:31:36.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.021429288s
    Jan 18 15:31:38.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.020470997s
    Jan 18 15:31:40.257: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.022336092s
    Jan 18 15:31:42.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.01950626s
    Jan 18 15:31:44.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.019118447s
    Jan 18 15:31:46.257: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.022540488s
    Jan 18 15:31:48.258: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.023213064s
    Jan 18 15:31:50.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.018715169s
    Jan 18 15:31:52.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.020790236s
    Jan 18 15:31:54.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.020360736s
    Jan 18 15:31:56.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.020164505s
    Jan 18 15:31:58.260: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.025330938s
    Jan 18 15:32:00.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.018957094s
    Jan 18 15:32:02.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.019804638s
    Jan 18 15:32:04.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.018799962s
    Jan 18 15:32:06.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.021386939s
    Jan 18 15:32:08.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.021140798s
    Jan 18 15:32:10.256: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.021328185s
    Jan 18 15:32:12.254: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.019575172s
    Jan 18 15:32:14.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.019717635s
    Jan 18 15:32:16.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.019768621s
    Jan 18 15:32:18.255: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.020065885s
    Jan 18 15:32:20.257: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.022183701s
    Jan 18 15:32:20.263: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.028332077s
    STEP: updating the pod 01/18/23 15:32:20.263
    Jan 18 15:32:20.804: INFO: Successfully updated pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9"
    STEP: waiting for pod running 01/18/23 15:32:20.804
    Jan 18 15:32:20.813: INFO: Waiting up to 2m0s for pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9" in namespace "var-expansion-2184" to be "running"
    Jan 18 15:32:20.817: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.14549ms
    Jan 18 15:32:22.823: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9": Phase="Running", Reason="", readiness=true. Elapsed: 2.009892278s
    Jan 18 15:32:22.823: INFO: Pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9" satisfied condition "running"
    STEP: deleting the pod gracefully 01/18/23 15:32:22.823
    Jan 18 15:32:22.824: INFO: Deleting pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9" in namespace "var-expansion-2184"
    Jan 18 15:32:22.832: INFO: Wait up to 5m0s for pod "var-expansion-e34b421a-88b3-452e-b36f-b312948246a9" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 18 15:32:54.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2184" for this suite. 01/18/23 15:32:54.868
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:32:54.88
Jan 18 15:32:54.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename pods 01/18/23 15:32:54.883
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:32:54.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:32:54.925
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 01/18/23 15:32:54.93
STEP: setting up watch 01/18/23 15:32:54.931
STEP: submitting the pod to kubernetes 01/18/23 15:32:55.038
STEP: verifying the pod is in kubernetes 01/18/23 15:32:55.051
STEP: verifying pod creation was observed 01/18/23 15:32:55.056
Jan 18 15:32:55.057: INFO: Waiting up to 5m0s for pod "pod-submit-remove-f62534e9-aeae-4a9b-b0a3-d8dd8da5f1dd" in namespace "pods-3360" to be "running"
Jan 18 15:32:55.074: INFO: Pod "pod-submit-remove-f62534e9-aeae-4a9b-b0a3-d8dd8da5f1dd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.946161ms
Jan 18 15:32:57.079: INFO: Pod "pod-submit-remove-f62534e9-aeae-4a9b-b0a3-d8dd8da5f1dd": Phase="Running", Reason="", readiness=true. Elapsed: 2.022148915s
Jan 18 15:32:57.079: INFO: Pod "pod-submit-remove-f62534e9-aeae-4a9b-b0a3-d8dd8da5f1dd" satisfied condition "running"
STEP: deleting the pod gracefully 01/18/23 15:32:57.084
STEP: verifying pod deletion was observed 01/18/23 15:32:57.095
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 15:32:59.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3360" for this suite. 01/18/23 15:32:59.756
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":85,"skipped":1613,"failed":0}
------------------------------
• [4.885 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:32:54.88
    Jan 18 15:32:54.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename pods 01/18/23 15:32:54.883
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:32:54.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:32:54.925
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 01/18/23 15:32:54.93
    STEP: setting up watch 01/18/23 15:32:54.931
    STEP: submitting the pod to kubernetes 01/18/23 15:32:55.038
    STEP: verifying the pod is in kubernetes 01/18/23 15:32:55.051
    STEP: verifying pod creation was observed 01/18/23 15:32:55.056
    Jan 18 15:32:55.057: INFO: Waiting up to 5m0s for pod "pod-submit-remove-f62534e9-aeae-4a9b-b0a3-d8dd8da5f1dd" in namespace "pods-3360" to be "running"
    Jan 18 15:32:55.074: INFO: Pod "pod-submit-remove-f62534e9-aeae-4a9b-b0a3-d8dd8da5f1dd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.946161ms
    Jan 18 15:32:57.079: INFO: Pod "pod-submit-remove-f62534e9-aeae-4a9b-b0a3-d8dd8da5f1dd": Phase="Running", Reason="", readiness=true. Elapsed: 2.022148915s
    Jan 18 15:32:57.079: INFO: Pod "pod-submit-remove-f62534e9-aeae-4a9b-b0a3-d8dd8da5f1dd" satisfied condition "running"
    STEP: deleting the pod gracefully 01/18/23 15:32:57.084
    STEP: verifying pod deletion was observed 01/18/23 15:32:57.095
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 15:32:59.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3360" for this suite. 01/18/23 15:32:59.756
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:32:59.765
Jan 18 15:32:59.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename daemonsets 01/18/23 15:32:59.766
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:32:59.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:32:59.806
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 01/18/23 15:32:59.864
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 15:32:59.871
Jan 18 15:32:59.885: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:32:59.889: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 15:32:59.890: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 15:33:00.898: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:33:00.906: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 15:33:00.906: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 15:33:01.899: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:33:01.905: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 15:33:01.905: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 15:33:02.899: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:33:02.904: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 15:33:02.904: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 01/18/23 15:33:02.908
Jan 18 15:33:02.914: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 01/18/23 15:33:02.914
Jan 18 15:33:02.926: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 01/18/23 15:33:02.927
Jan 18 15:33:02.939: INFO: Observed &DaemonSet event: ADDED
Jan 18 15:33:02.940: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 15:33:02.940: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 15:33:02.940: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 15:33:02.941: INFO: Found daemon set daemon-set in namespace daemonsets-447 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 18 15:33:02.941: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 01/18/23 15:33:02.941
STEP: watching for the daemon set status to be patched 01/18/23 15:33:02.953
Jan 18 15:33:02.960: INFO: Observed &DaemonSet event: ADDED
Jan 18 15:33:02.960: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 15:33:02.961: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 15:33:02.961: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 15:33:02.961: INFO: Observed daemon set daemon-set in namespace daemonsets-447 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 18 15:33:02.962: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 15:33:02.962: INFO: Found daemon set daemon-set in namespace daemonsets-447 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan 18 15:33:02.962: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/18/23 15:33:02.965
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-447, will wait for the garbage collector to delete the pods 01/18/23 15:33:02.965
Jan 18 15:33:03.026: INFO: Deleting DaemonSet.extensions daemon-set took: 6.479038ms
Jan 18 15:33:03.127: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.723869ms
Jan 18 15:33:05.131: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 15:33:05.132: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 15:33:05.135: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"39156"},"items":null}

Jan 18 15:33:05.145: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"39156"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 18 15:33:05.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-447" for this suite. 01/18/23 15:33:05.173
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":86,"skipped":1613,"failed":0}
------------------------------
• [SLOW TEST] [5.417 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:32:59.765
    Jan 18 15:32:59.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename daemonsets 01/18/23 15:32:59.766
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:32:59.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:32:59.806
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 01/18/23 15:32:59.864
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 15:32:59.871
    Jan 18 15:32:59.885: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:32:59.889: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 15:32:59.890: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 15:33:00.898: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:33:00.906: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 15:33:00.906: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 15:33:01.899: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:33:01.905: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 15:33:01.905: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 15:33:02.899: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:33:02.904: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 15:33:02.904: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 01/18/23 15:33:02.908
    Jan 18 15:33:02.914: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 01/18/23 15:33:02.914
    Jan 18 15:33:02.926: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 01/18/23 15:33:02.927
    Jan 18 15:33:02.939: INFO: Observed &DaemonSet event: ADDED
    Jan 18 15:33:02.940: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 15:33:02.940: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 15:33:02.940: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 15:33:02.941: INFO: Found daemon set daemon-set in namespace daemonsets-447 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 18 15:33:02.941: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 01/18/23 15:33:02.941
    STEP: watching for the daemon set status to be patched 01/18/23 15:33:02.953
    Jan 18 15:33:02.960: INFO: Observed &DaemonSet event: ADDED
    Jan 18 15:33:02.960: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 15:33:02.961: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 15:33:02.961: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 15:33:02.961: INFO: Observed daemon set daemon-set in namespace daemonsets-447 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 18 15:33:02.962: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 15:33:02.962: INFO: Found daemon set daemon-set in namespace daemonsets-447 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jan 18 15:33:02.962: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 15:33:02.965
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-447, will wait for the garbage collector to delete the pods 01/18/23 15:33:02.965
    Jan 18 15:33:03.026: INFO: Deleting DaemonSet.extensions daemon-set took: 6.479038ms
    Jan 18 15:33:03.127: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.723869ms
    Jan 18 15:33:05.131: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 15:33:05.132: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 15:33:05.135: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"39156"},"items":null}

    Jan 18 15:33:05.145: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"39156"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 15:33:05.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-447" for this suite. 01/18/23 15:33:05.173
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:33:05.189
Jan 18 15:33:05.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename pod-network-test 01/18/23 15:33:05.192
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:33:05.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:33:05.246
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-3018 01/18/23 15:33:05.253
STEP: creating a selector 01/18/23 15:33:05.253
STEP: Creating the service pods in kubernetes 01/18/23 15:33:05.254
Jan 18 15:33:05.254: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 18 15:33:05.294: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3018" to be "running and ready"
Jan 18 15:33:05.320: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 25.715378ms
Jan 18 15:33:05.320: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:33:07.325: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.030304342s
Jan 18 15:33:07.325: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:33:09.328: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.033629145s
Jan 18 15:33:09.328: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:33:11.326: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.031033904s
Jan 18 15:33:11.326: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:33:13.326: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.031845194s
Jan 18 15:33:13.327: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:33:15.326: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.031139661s
Jan 18 15:33:15.326: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:33:17.326: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.031118632s
Jan 18 15:33:17.326: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:33:19.325: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.030825716s
Jan 18 15:33:19.326: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:33:21.329: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.034170017s
Jan 18 15:33:21.329: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:33:23.326: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.032005466s
Jan 18 15:33:23.327: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:33:25.328: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.033717531s
Jan 18 15:33:25.328: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:33:27.326: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.031292079s
Jan 18 15:33:27.326: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 18 15:33:27.326: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 18 15:33:27.331: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3018" to be "running and ready"
Jan 18 15:33:27.336: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.479249ms
Jan 18 15:33:27.336: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 18 15:33:27.336: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/18/23 15:33:27.341
Jan 18 15:33:27.355: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3018" to be "running"
Jan 18 15:33:27.392: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 36.573805ms
Jan 18 15:33:29.398: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.042459497s
Jan 18 15:33:29.398: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 18 15:33:29.402: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3018" to be "running"
Jan 18 15:33:29.408: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.975888ms
Jan 18 15:33:29.408: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 18 15:33:29.413: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 18 15:33:29.414: INFO: Going to poll 10.233.78.69 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 18 15:33:29.418: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.78.69 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3018 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 15:33:29.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 15:33:29.419: INFO: ExecWithOptions: Clientset creation
Jan 18 15:33:29.419: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3018/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.78.69+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 15:33:30.516: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 18 15:33:30.516: INFO: Going to poll 10.233.68.145 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 18 15:33:30.520: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.68.145 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3018 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 15:33:30.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 15:33:30.521: INFO: ExecWithOptions: Clientset creation
Jan 18 15:33:30.521: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3018/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.68.145+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 15:33:31.620: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 18 15:33:31.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3018" for this suite. 01/18/23 15:33:31.631
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":87,"skipped":1636,"failed":0}
------------------------------
• [SLOW TEST] [26.450 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:33:05.189
    Jan 18 15:33:05.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename pod-network-test 01/18/23 15:33:05.192
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:33:05.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:33:05.246
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-3018 01/18/23 15:33:05.253
    STEP: creating a selector 01/18/23 15:33:05.253
    STEP: Creating the service pods in kubernetes 01/18/23 15:33:05.254
    Jan 18 15:33:05.254: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 18 15:33:05.294: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3018" to be "running and ready"
    Jan 18 15:33:05.320: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 25.715378ms
    Jan 18 15:33:05.320: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:33:07.325: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.030304342s
    Jan 18 15:33:07.325: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:33:09.328: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.033629145s
    Jan 18 15:33:09.328: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:33:11.326: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.031033904s
    Jan 18 15:33:11.326: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:33:13.326: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.031845194s
    Jan 18 15:33:13.327: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:33:15.326: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.031139661s
    Jan 18 15:33:15.326: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:33:17.326: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.031118632s
    Jan 18 15:33:17.326: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:33:19.325: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.030825716s
    Jan 18 15:33:19.326: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:33:21.329: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.034170017s
    Jan 18 15:33:21.329: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:33:23.326: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.032005466s
    Jan 18 15:33:23.327: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:33:25.328: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.033717531s
    Jan 18 15:33:25.328: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:33:27.326: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.031292079s
    Jan 18 15:33:27.326: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 18 15:33:27.326: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 18 15:33:27.331: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3018" to be "running and ready"
    Jan 18 15:33:27.336: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.479249ms
    Jan 18 15:33:27.336: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 18 15:33:27.336: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/18/23 15:33:27.341
    Jan 18 15:33:27.355: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3018" to be "running"
    Jan 18 15:33:27.392: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 36.573805ms
    Jan 18 15:33:29.398: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.042459497s
    Jan 18 15:33:29.398: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 18 15:33:29.402: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3018" to be "running"
    Jan 18 15:33:29.408: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.975888ms
    Jan 18 15:33:29.408: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 18 15:33:29.413: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 18 15:33:29.414: INFO: Going to poll 10.233.78.69 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Jan 18 15:33:29.418: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.78.69 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3018 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 15:33:29.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 15:33:29.419: INFO: ExecWithOptions: Clientset creation
    Jan 18 15:33:29.419: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3018/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.78.69+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 18 15:33:30.516: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 18 15:33:30.516: INFO: Going to poll 10.233.68.145 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Jan 18 15:33:30.520: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.68.145 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3018 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 15:33:30.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 15:33:30.521: INFO: ExecWithOptions: Clientset creation
    Jan 18 15:33:30.521: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3018/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.68.145+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 18 15:33:31.620: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 18 15:33:31.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-3018" for this suite. 01/18/23 15:33:31.631
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:33:31.646
Jan 18 15:33:31.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename pods 01/18/23 15:33:31.648
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:33:31.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:33:31.679
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 01/18/23 15:33:31.683
STEP: submitting the pod to kubernetes 01/18/23 15:33:31.683
Jan 18 15:33:31.691: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-3af6cbf3-6903-4420-9ab2-6b393a990f5b" in namespace "pods-4203" to be "running and ready"
Jan 18 15:33:31.702: INFO: Pod "pod-update-activedeadlineseconds-3af6cbf3-6903-4420-9ab2-6b393a990f5b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.940434ms
Jan 18 15:33:31.702: INFO: The phase of Pod pod-update-activedeadlineseconds-3af6cbf3-6903-4420-9ab2-6b393a990f5b is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:33:33.707: INFO: Pod "pod-update-activedeadlineseconds-3af6cbf3-6903-4420-9ab2-6b393a990f5b": Phase="Running", Reason="", readiness=true. Elapsed: 2.014915811s
Jan 18 15:33:33.707: INFO: The phase of Pod pod-update-activedeadlineseconds-3af6cbf3-6903-4420-9ab2-6b393a990f5b is Running (Ready = true)
Jan 18 15:33:33.707: INFO: Pod "pod-update-activedeadlineseconds-3af6cbf3-6903-4420-9ab2-6b393a990f5b" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/18/23 15:33:33.712
STEP: updating the pod 01/18/23 15:33:33.725
Jan 18 15:33:34.244: INFO: Successfully updated pod "pod-update-activedeadlineseconds-3af6cbf3-6903-4420-9ab2-6b393a990f5b"
Jan 18 15:33:34.244: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-3af6cbf3-6903-4420-9ab2-6b393a990f5b" in namespace "pods-4203" to be "terminated with reason DeadlineExceeded"
Jan 18 15:33:34.255: INFO: Pod "pod-update-activedeadlineseconds-3af6cbf3-6903-4420-9ab2-6b393a990f5b": Phase="Running", Reason="", readiness=true. Elapsed: 10.270254ms
Jan 18 15:33:36.261: INFO: Pod "pod-update-activedeadlineseconds-3af6cbf3-6903-4420-9ab2-6b393a990f5b": Phase="Running", Reason="", readiness=true. Elapsed: 2.016569487s
Jan 18 15:33:38.281: INFO: Pod "pod-update-activedeadlineseconds-3af6cbf3-6903-4420-9ab2-6b393a990f5b": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.036813261s
Jan 18 15:33:38.281: INFO: Pod "pod-update-activedeadlineseconds-3af6cbf3-6903-4420-9ab2-6b393a990f5b" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 15:33:38.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4203" for this suite. 01/18/23 15:33:38.331
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":88,"skipped":1640,"failed":0}
------------------------------
• [SLOW TEST] [6.743 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:33:31.646
    Jan 18 15:33:31.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename pods 01/18/23 15:33:31.648
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:33:31.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:33:31.679
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 01/18/23 15:33:31.683
    STEP: submitting the pod to kubernetes 01/18/23 15:33:31.683
    Jan 18 15:33:31.691: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-3af6cbf3-6903-4420-9ab2-6b393a990f5b" in namespace "pods-4203" to be "running and ready"
    Jan 18 15:33:31.702: INFO: Pod "pod-update-activedeadlineseconds-3af6cbf3-6903-4420-9ab2-6b393a990f5b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.940434ms
    Jan 18 15:33:31.702: INFO: The phase of Pod pod-update-activedeadlineseconds-3af6cbf3-6903-4420-9ab2-6b393a990f5b is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:33:33.707: INFO: Pod "pod-update-activedeadlineseconds-3af6cbf3-6903-4420-9ab2-6b393a990f5b": Phase="Running", Reason="", readiness=true. Elapsed: 2.014915811s
    Jan 18 15:33:33.707: INFO: The phase of Pod pod-update-activedeadlineseconds-3af6cbf3-6903-4420-9ab2-6b393a990f5b is Running (Ready = true)
    Jan 18 15:33:33.707: INFO: Pod "pod-update-activedeadlineseconds-3af6cbf3-6903-4420-9ab2-6b393a990f5b" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/18/23 15:33:33.712
    STEP: updating the pod 01/18/23 15:33:33.725
    Jan 18 15:33:34.244: INFO: Successfully updated pod "pod-update-activedeadlineseconds-3af6cbf3-6903-4420-9ab2-6b393a990f5b"
    Jan 18 15:33:34.244: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-3af6cbf3-6903-4420-9ab2-6b393a990f5b" in namespace "pods-4203" to be "terminated with reason DeadlineExceeded"
    Jan 18 15:33:34.255: INFO: Pod "pod-update-activedeadlineseconds-3af6cbf3-6903-4420-9ab2-6b393a990f5b": Phase="Running", Reason="", readiness=true. Elapsed: 10.270254ms
    Jan 18 15:33:36.261: INFO: Pod "pod-update-activedeadlineseconds-3af6cbf3-6903-4420-9ab2-6b393a990f5b": Phase="Running", Reason="", readiness=true. Elapsed: 2.016569487s
    Jan 18 15:33:38.281: INFO: Pod "pod-update-activedeadlineseconds-3af6cbf3-6903-4420-9ab2-6b393a990f5b": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.036813261s
    Jan 18 15:33:38.281: INFO: Pod "pod-update-activedeadlineseconds-3af6cbf3-6903-4420-9ab2-6b393a990f5b" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 15:33:38.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4203" for this suite. 01/18/23 15:33:38.331
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:33:38.393
Jan 18 15:33:38.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename crd-webhook 01/18/23 15:33:38.394
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:33:38.436
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:33:38.445
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/18/23 15:33:38.451
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/18/23 15:33:39.3
STEP: Deploying the custom resource conversion webhook pod 01/18/23 15:33:39.309
STEP: Wait for the deployment to be ready 01/18/23 15:33:39.322
Jan 18 15:33:39.339: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jan 18 15:33:41.361: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 33, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 33, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 33, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 33, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/18/23 15:33:43.367
STEP: Verifying the service has paired with the endpoint 01/18/23 15:33:43.38
Jan 18 15:33:44.381: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jan 18 15:33:44.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Creating a v1 custom resource 01/18/23 15:33:47.008
STEP: v2 custom resource should be converted 01/18/23 15:33:47.016
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 15:33:47.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7474" for this suite. 01/18/23 15:33:47.556
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":89,"skipped":1663,"failed":0}
------------------------------
• [SLOW TEST] [9.509 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:33:38.393
    Jan 18 15:33:38.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename crd-webhook 01/18/23 15:33:38.394
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:33:38.436
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:33:38.445
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/18/23 15:33:38.451
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/18/23 15:33:39.3
    STEP: Deploying the custom resource conversion webhook pod 01/18/23 15:33:39.309
    STEP: Wait for the deployment to be ready 01/18/23 15:33:39.322
    Jan 18 15:33:39.339: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Jan 18 15:33:41.361: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 15, 33, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 33, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 15, 33, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 15, 33, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/18/23 15:33:43.367
    STEP: Verifying the service has paired with the endpoint 01/18/23 15:33:43.38
    Jan 18 15:33:44.381: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jan 18 15:33:44.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Creating a v1 custom resource 01/18/23 15:33:47.008
    STEP: v2 custom resource should be converted 01/18/23 15:33:47.016
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 15:33:47.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-7474" for this suite. 01/18/23 15:33:47.556
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:33:47.904
Jan 18 15:33:47.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename secrets 01/18/23 15:33:47.908
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:33:47.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:33:48.01
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-6aca89d0-091c-4a7c-8d53-9c0264dbdc5c 01/18/23 15:33:48.076
STEP: Creating a pod to test consume secrets 01/18/23 15:33:48.081
Jan 18 15:33:48.091: INFO: Waiting up to 5m0s for pod "pod-secrets-4b455ac9-04cb-4573-9b71-1c3f713ccbbd" in namespace "secrets-9530" to be "Succeeded or Failed"
Jan 18 15:33:48.117: INFO: Pod "pod-secrets-4b455ac9-04cb-4573-9b71-1c3f713ccbbd": Phase="Pending", Reason="", readiness=false. Elapsed: 26.181463ms
Jan 18 15:33:50.123: INFO: Pod "pod-secrets-4b455ac9-04cb-4573-9b71-1c3f713ccbbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032482785s
Jan 18 15:33:52.125: INFO: Pod "pod-secrets-4b455ac9-04cb-4573-9b71-1c3f713ccbbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033856057s
STEP: Saw pod success 01/18/23 15:33:52.125
Jan 18 15:33:52.125: INFO: Pod "pod-secrets-4b455ac9-04cb-4573-9b71-1c3f713ccbbd" satisfied condition "Succeeded or Failed"
Jan 18 15:33:52.131: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-4b455ac9-04cb-4573-9b71-1c3f713ccbbd container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 15:33:52.159
Jan 18 15:33:52.179: INFO: Waiting for pod pod-secrets-4b455ac9-04cb-4573-9b71-1c3f713ccbbd to disappear
Jan 18 15:33:52.186: INFO: Pod pod-secrets-4b455ac9-04cb-4573-9b71-1c3f713ccbbd no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 18 15:33:52.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9530" for this suite. 01/18/23 15:33:52.192
STEP: Destroying namespace "secret-namespace-5806" for this suite. 01/18/23 15:33:52.202
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":90,"skipped":1676,"failed":0}
------------------------------
• [4.306 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:33:47.904
    Jan 18 15:33:47.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename secrets 01/18/23 15:33:47.908
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:33:47.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:33:48.01
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-6aca89d0-091c-4a7c-8d53-9c0264dbdc5c 01/18/23 15:33:48.076
    STEP: Creating a pod to test consume secrets 01/18/23 15:33:48.081
    Jan 18 15:33:48.091: INFO: Waiting up to 5m0s for pod "pod-secrets-4b455ac9-04cb-4573-9b71-1c3f713ccbbd" in namespace "secrets-9530" to be "Succeeded or Failed"
    Jan 18 15:33:48.117: INFO: Pod "pod-secrets-4b455ac9-04cb-4573-9b71-1c3f713ccbbd": Phase="Pending", Reason="", readiness=false. Elapsed: 26.181463ms
    Jan 18 15:33:50.123: INFO: Pod "pod-secrets-4b455ac9-04cb-4573-9b71-1c3f713ccbbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032482785s
    Jan 18 15:33:52.125: INFO: Pod "pod-secrets-4b455ac9-04cb-4573-9b71-1c3f713ccbbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033856057s
    STEP: Saw pod success 01/18/23 15:33:52.125
    Jan 18 15:33:52.125: INFO: Pod "pod-secrets-4b455ac9-04cb-4573-9b71-1c3f713ccbbd" satisfied condition "Succeeded or Failed"
    Jan 18 15:33:52.131: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-4b455ac9-04cb-4573-9b71-1c3f713ccbbd container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 15:33:52.159
    Jan 18 15:33:52.179: INFO: Waiting for pod pod-secrets-4b455ac9-04cb-4573-9b71-1c3f713ccbbd to disappear
    Jan 18 15:33:52.186: INFO: Pod pod-secrets-4b455ac9-04cb-4573-9b71-1c3f713ccbbd no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 15:33:52.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9530" for this suite. 01/18/23 15:33:52.192
    STEP: Destroying namespace "secret-namespace-5806" for this suite. 01/18/23 15:33:52.202
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:33:52.216
Jan 18 15:33:52.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename configmap 01/18/23 15:33:52.218
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:33:52.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:33:52.244
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-68255547-89fa-45ac-91a1-116969dad582 01/18/23 15:33:52.248
STEP: Creating a pod to test consume configMaps 01/18/23 15:33:52.253
Jan 18 15:33:52.279: INFO: Waiting up to 5m0s for pod "pod-configmaps-0f0c65f7-bde3-4945-a52f-8f04c6b7316d" in namespace "configmap-6991" to be "Succeeded or Failed"
Jan 18 15:33:52.286: INFO: Pod "pod-configmaps-0f0c65f7-bde3-4945-a52f-8f04c6b7316d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.124259ms
Jan 18 15:33:54.292: INFO: Pod "pod-configmaps-0f0c65f7-bde3-4945-a52f-8f04c6b7316d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012729889s
Jan 18 15:33:56.291: INFO: Pod "pod-configmaps-0f0c65f7-bde3-4945-a52f-8f04c6b7316d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011905007s
STEP: Saw pod success 01/18/23 15:33:56.292
Jan 18 15:33:56.292: INFO: Pod "pod-configmaps-0f0c65f7-bde3-4945-a52f-8f04c6b7316d" satisfied condition "Succeeded or Failed"
Jan 18 15:33:56.296: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-0f0c65f7-bde3-4945-a52f-8f04c6b7316d container agnhost-container: <nil>
STEP: delete the pod 01/18/23 15:33:56.305
Jan 18 15:33:56.324: INFO: Waiting for pod pod-configmaps-0f0c65f7-bde3-4945-a52f-8f04c6b7316d to disappear
Jan 18 15:33:56.334: INFO: Pod pod-configmaps-0f0c65f7-bde3-4945-a52f-8f04c6b7316d no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 15:33:56.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6991" for this suite. 01/18/23 15:33:56.348
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":91,"skipped":1680,"failed":0}
------------------------------
• [4.145 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:33:52.216
    Jan 18 15:33:52.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename configmap 01/18/23 15:33:52.218
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:33:52.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:33:52.244
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-68255547-89fa-45ac-91a1-116969dad582 01/18/23 15:33:52.248
    STEP: Creating a pod to test consume configMaps 01/18/23 15:33:52.253
    Jan 18 15:33:52.279: INFO: Waiting up to 5m0s for pod "pod-configmaps-0f0c65f7-bde3-4945-a52f-8f04c6b7316d" in namespace "configmap-6991" to be "Succeeded or Failed"
    Jan 18 15:33:52.286: INFO: Pod "pod-configmaps-0f0c65f7-bde3-4945-a52f-8f04c6b7316d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.124259ms
    Jan 18 15:33:54.292: INFO: Pod "pod-configmaps-0f0c65f7-bde3-4945-a52f-8f04c6b7316d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012729889s
    Jan 18 15:33:56.291: INFO: Pod "pod-configmaps-0f0c65f7-bde3-4945-a52f-8f04c6b7316d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011905007s
    STEP: Saw pod success 01/18/23 15:33:56.292
    Jan 18 15:33:56.292: INFO: Pod "pod-configmaps-0f0c65f7-bde3-4945-a52f-8f04c6b7316d" satisfied condition "Succeeded or Failed"
    Jan 18 15:33:56.296: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-0f0c65f7-bde3-4945-a52f-8f04c6b7316d container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 15:33:56.305
    Jan 18 15:33:56.324: INFO: Waiting for pod pod-configmaps-0f0c65f7-bde3-4945-a52f-8f04c6b7316d to disappear
    Jan 18 15:33:56.334: INFO: Pod pod-configmaps-0f0c65f7-bde3-4945-a52f-8f04c6b7316d no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 15:33:56.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6991" for this suite. 01/18/23 15:33:56.348
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:33:56.373
Jan 18 15:33:56.373: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename namespaces 01/18/23 15:33:56.374
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:33:56.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:33:56.414
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 01/18/23 15:33:56.422
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:33:56.445
STEP: Creating a pod in the namespace 01/18/23 15:33:56.451
STEP: Waiting for the pod to have running status 01/18/23 15:33:56.464
Jan 18 15:33:56.465: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-2713" to be "running"
Jan 18 15:33:56.471: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.250783ms
Jan 18 15:33:58.476: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011815052s
Jan 18 15:33:58.476: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 01/18/23 15:33:58.476
STEP: Waiting for the namespace to be removed. 01/18/23 15:33:58.486
STEP: Recreating the namespace 01/18/23 15:34:09.493
STEP: Verifying there are no pods in the namespace 01/18/23 15:34:09.518
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 18 15:34:09.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9239" for this suite. 01/18/23 15:34:09.531
STEP: Destroying namespace "nsdeletetest-2713" for this suite. 01/18/23 15:34:09.537
Jan 18 15:34:09.547: INFO: Namespace nsdeletetest-2713 was already deleted
STEP: Destroying namespace "nsdeletetest-8492" for this suite. 01/18/23 15:34:09.548
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":92,"skipped":1728,"failed":0}
------------------------------
• [SLOW TEST] [13.184 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:33:56.373
    Jan 18 15:33:56.373: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename namespaces 01/18/23 15:33:56.374
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:33:56.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:33:56.414
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 01/18/23 15:33:56.422
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:33:56.445
    STEP: Creating a pod in the namespace 01/18/23 15:33:56.451
    STEP: Waiting for the pod to have running status 01/18/23 15:33:56.464
    Jan 18 15:33:56.465: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-2713" to be "running"
    Jan 18 15:33:56.471: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.250783ms
    Jan 18 15:33:58.476: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011815052s
    Jan 18 15:33:58.476: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 01/18/23 15:33:58.476
    STEP: Waiting for the namespace to be removed. 01/18/23 15:33:58.486
    STEP: Recreating the namespace 01/18/23 15:34:09.493
    STEP: Verifying there are no pods in the namespace 01/18/23 15:34:09.518
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 15:34:09.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-9239" for this suite. 01/18/23 15:34:09.531
    STEP: Destroying namespace "nsdeletetest-2713" for this suite. 01/18/23 15:34:09.537
    Jan 18 15:34:09.547: INFO: Namespace nsdeletetest-2713 was already deleted
    STEP: Destroying namespace "nsdeletetest-8492" for this suite. 01/18/23 15:34:09.548
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:34:09.561
Jan 18 15:34:09.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename webhook 01/18/23 15:34:09.563
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:34:09.583
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:34:09.588
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 15:34:09.605
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 15:34:10.294
STEP: Deploying the webhook pod 01/18/23 15:34:10.305
STEP: Wait for the deployment to be ready 01/18/23 15:34:10.318
Jan 18 15:34:10.333: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 15:34:12.355
STEP: Verifying the service has paired with the endpoint 01/18/23 15:34:12.374
Jan 18 15:34:13.375: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Jan 18 15:34:13.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2588-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 15:34:13.926
STEP: Creating a custom resource while v1 is storage version 01/18/23 15:34:13.981
STEP: Patching Custom Resource Definition to set v2 as storage 01/18/23 15:34:16.103
STEP: Patching the custom resource while v2 is storage version 01/18/23 15:34:16.13
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 15:34:16.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-351" for this suite. 01/18/23 15:34:16.765
STEP: Destroying namespace "webhook-351-markers" for this suite. 01/18/23 15:34:16.776
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":93,"skipped":1737,"failed":0}
------------------------------
• [SLOW TEST] [7.335 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:34:09.561
    Jan 18 15:34:09.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename webhook 01/18/23 15:34:09.563
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:34:09.583
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:34:09.588
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 15:34:09.605
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 15:34:10.294
    STEP: Deploying the webhook pod 01/18/23 15:34:10.305
    STEP: Wait for the deployment to be ready 01/18/23 15:34:10.318
    Jan 18 15:34:10.333: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 15:34:12.355
    STEP: Verifying the service has paired with the endpoint 01/18/23 15:34:12.374
    Jan 18 15:34:13.375: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Jan 18 15:34:13.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2588-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 15:34:13.926
    STEP: Creating a custom resource while v1 is storage version 01/18/23 15:34:13.981
    STEP: Patching Custom Resource Definition to set v2 as storage 01/18/23 15:34:16.103
    STEP: Patching the custom resource while v2 is storage version 01/18/23 15:34:16.13
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 15:34:16.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-351" for this suite. 01/18/23 15:34:16.765
    STEP: Destroying namespace "webhook-351-markers" for this suite. 01/18/23 15:34:16.776
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:34:16.902
Jan 18 15:34:16.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename deployment 01/18/23 15:34:16.911
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:34:16.963
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:34:16.976
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jan 18 15:34:17.004: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 18 15:34:22.019: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 15:34:22.019
Jan 18 15:34:22.020: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/18/23 15:34:22.032
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 15:34:22.059: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8485  3c49eb0a-507d-473c-b233-9c1e6c025090 39873 1 2023-01-18 15:34:22 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-18 15:34:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009cd2828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jan 18 15:34:22.069: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jan 18 15:34:22.069: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan 18 15:34:22.069: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-8485  df73d37a-7f56-4053-81e3-265946fe5c0e 39874 1 2023-01-18 15:34:16 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 3c49eb0a-507d-473c-b233-9c1e6c025090 0xc002eb8467 0xc002eb8468}] [] [{e2e.test Update apps/v1 2023-01-18 15:34:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 15:34:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-18 15:34:22 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"3c49eb0a-507d-473c-b233-9c1e6c025090\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002eb8528 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 15:34:22.078: INFO: Pod "test-cleanup-controller-r99s5" is available:
&Pod{ObjectMeta:{test-cleanup-controller-r99s5 test-cleanup-controller- deployment-8485  44fb0093-70ac-4ddf-80fd-18863751cbf7 39856 0 2023-01-18 15:34:16 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:c16f99e97acd1c8558ce4622e2ffd4599c5d0864aebe726d03b69f7c6fc10b4d cni.projectcalico.org/podIP:10.233.68.153/32 cni.projectcalico.org/podIPs:10.233.68.153/32] [{apps/v1 ReplicaSet test-cleanup-controller df73d37a-7f56-4053-81e3-265946fe5c0e 0xc002eb8867 0xc002eb8868}] [] [{kube-controller-manager Update v1 2023-01-18 15:34:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df73d37a-7f56-4053-81e3-265946fe5c0e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 15:34:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 15:34:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xlsm4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xlsm4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:34:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:34:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:34:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:34:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.153,StartTime:2023-01-18 15:34:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 15:34:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://8ac76f2aad288e517f4a0d1c8292ff5054bbe0d9ae3e345cc80604e8bbf4ada0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 18 15:34:22.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8485" for this suite. 01/18/23 15:34:22.1
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":94,"skipped":1748,"failed":0}
------------------------------
• [SLOW TEST] [5.220 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:34:16.902
    Jan 18 15:34:16.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename deployment 01/18/23 15:34:16.911
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:34:16.963
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:34:16.976
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jan 18 15:34:17.004: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jan 18 15:34:22.019: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 15:34:22.019
    Jan 18 15:34:22.020: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/18/23 15:34:22.032
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 15:34:22.059: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8485  3c49eb0a-507d-473c-b233-9c1e6c025090 39873 1 2023-01-18 15:34:22 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-18 15:34:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009cd2828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 18 15:34:22.069: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Jan 18 15:34:22.069: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Jan 18 15:34:22.069: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-8485  df73d37a-7f56-4053-81e3-265946fe5c0e 39874 1 2023-01-18 15:34:16 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 3c49eb0a-507d-473c-b233-9c1e6c025090 0xc002eb8467 0xc002eb8468}] [] [{e2e.test Update apps/v1 2023-01-18 15:34:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 15:34:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-18 15:34:22 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"3c49eb0a-507d-473c-b233-9c1e6c025090\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002eb8528 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 15:34:22.078: INFO: Pod "test-cleanup-controller-r99s5" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-r99s5 test-cleanup-controller- deployment-8485  44fb0093-70ac-4ddf-80fd-18863751cbf7 39856 0 2023-01-18 15:34:16 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:c16f99e97acd1c8558ce4622e2ffd4599c5d0864aebe726d03b69f7c6fc10b4d cni.projectcalico.org/podIP:10.233.68.153/32 cni.projectcalico.org/podIPs:10.233.68.153/32] [{apps/v1 ReplicaSet test-cleanup-controller df73d37a-7f56-4053-81e3-265946fe5c0e 0xc002eb8867 0xc002eb8868}] [] [{kube-controller-manager Update v1 2023-01-18 15:34:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df73d37a-7f56-4053-81e3-265946fe5c0e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 15:34:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 15:34:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xlsm4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xlsm4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:34:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:34:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:34:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:34:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.153,StartTime:2023-01-18 15:34:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 15:34:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://8ac76f2aad288e517f4a0d1c8292ff5054bbe0d9ae3e345cc80604e8bbf4ada0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 18 15:34:22.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-8485" for this suite. 01/18/23 15:34:22.1
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:34:22.124
Jan 18 15:34:22.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename kubelet-test 01/18/23 15:34:22.127
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:34:22.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:34:22.198
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jan 18 15:34:22.212: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs1a17fa40-dbc8-41f8-ac0e-0582f08146ae" in namespace "kubelet-test-8865" to be "running and ready"
Jan 18 15:34:22.229: INFO: Pod "busybox-readonly-fs1a17fa40-dbc8-41f8-ac0e-0582f08146ae": Phase="Pending", Reason="", readiness=false. Elapsed: 16.470072ms
Jan 18 15:34:22.229: INFO: The phase of Pod busybox-readonly-fs1a17fa40-dbc8-41f8-ac0e-0582f08146ae is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:34:24.238: INFO: Pod "busybox-readonly-fs1a17fa40-dbc8-41f8-ac0e-0582f08146ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026131553s
Jan 18 15:34:24.239: INFO: The phase of Pod busybox-readonly-fs1a17fa40-dbc8-41f8-ac0e-0582f08146ae is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:34:26.235: INFO: Pod "busybox-readonly-fs1a17fa40-dbc8-41f8-ac0e-0582f08146ae": Phase="Running", Reason="", readiness=true. Elapsed: 4.022751799s
Jan 18 15:34:26.235: INFO: The phase of Pod busybox-readonly-fs1a17fa40-dbc8-41f8-ac0e-0582f08146ae is Running (Ready = true)
Jan 18 15:34:26.235: INFO: Pod "busybox-readonly-fs1a17fa40-dbc8-41f8-ac0e-0582f08146ae" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 18 15:34:26.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8865" for this suite. 01/18/23 15:34:26.255
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":95,"skipped":1762,"failed":0}
------------------------------
• [4.144 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:34:22.124
    Jan 18 15:34:22.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename kubelet-test 01/18/23 15:34:22.127
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:34:22.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:34:22.198
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jan 18 15:34:22.212: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs1a17fa40-dbc8-41f8-ac0e-0582f08146ae" in namespace "kubelet-test-8865" to be "running and ready"
    Jan 18 15:34:22.229: INFO: Pod "busybox-readonly-fs1a17fa40-dbc8-41f8-ac0e-0582f08146ae": Phase="Pending", Reason="", readiness=false. Elapsed: 16.470072ms
    Jan 18 15:34:22.229: INFO: The phase of Pod busybox-readonly-fs1a17fa40-dbc8-41f8-ac0e-0582f08146ae is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:34:24.238: INFO: Pod "busybox-readonly-fs1a17fa40-dbc8-41f8-ac0e-0582f08146ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026131553s
    Jan 18 15:34:24.239: INFO: The phase of Pod busybox-readonly-fs1a17fa40-dbc8-41f8-ac0e-0582f08146ae is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:34:26.235: INFO: Pod "busybox-readonly-fs1a17fa40-dbc8-41f8-ac0e-0582f08146ae": Phase="Running", Reason="", readiness=true. Elapsed: 4.022751799s
    Jan 18 15:34:26.235: INFO: The phase of Pod busybox-readonly-fs1a17fa40-dbc8-41f8-ac0e-0582f08146ae is Running (Ready = true)
    Jan 18 15:34:26.235: INFO: Pod "busybox-readonly-fs1a17fa40-dbc8-41f8-ac0e-0582f08146ae" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 18 15:34:26.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-8865" for this suite. 01/18/23 15:34:26.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:34:26.27
Jan 18 15:34:26.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 15:34:26.273
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:34:26.306
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:34:26.31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 01/18/23 15:34:26.317
Jan 18 15:34:26.329: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ee286b57-a319-4774-862f-a52adb1c744b" in namespace "projected-1658" to be "Succeeded or Failed"
Jan 18 15:34:26.343: INFO: Pod "downwardapi-volume-ee286b57-a319-4774-862f-a52adb1c744b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.814714ms
Jan 18 15:34:28.351: INFO: Pod "downwardapi-volume-ee286b57-a319-4774-862f-a52adb1c744b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021310357s
Jan 18 15:34:30.350: INFO: Pod "downwardapi-volume-ee286b57-a319-4774-862f-a52adb1c744b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020067297s
STEP: Saw pod success 01/18/23 15:34:30.35
Jan 18 15:34:30.350: INFO: Pod "downwardapi-volume-ee286b57-a319-4774-862f-a52adb1c744b" satisfied condition "Succeeded or Failed"
Jan 18 15:34:30.356: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-ee286b57-a319-4774-862f-a52adb1c744b container client-container: <nil>
STEP: delete the pod 01/18/23 15:34:30.365
Jan 18 15:34:30.395: INFO: Waiting for pod downwardapi-volume-ee286b57-a319-4774-862f-a52adb1c744b to disappear
Jan 18 15:34:30.399: INFO: Pod downwardapi-volume-ee286b57-a319-4774-862f-a52adb1c744b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 15:34:30.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1658" for this suite. 01/18/23 15:34:30.406
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":96,"skipped":1775,"failed":0}
------------------------------
• [4.144 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:34:26.27
    Jan 18 15:34:26.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 15:34:26.273
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:34:26.306
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:34:26.31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 01/18/23 15:34:26.317
    Jan 18 15:34:26.329: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ee286b57-a319-4774-862f-a52adb1c744b" in namespace "projected-1658" to be "Succeeded or Failed"
    Jan 18 15:34:26.343: INFO: Pod "downwardapi-volume-ee286b57-a319-4774-862f-a52adb1c744b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.814714ms
    Jan 18 15:34:28.351: INFO: Pod "downwardapi-volume-ee286b57-a319-4774-862f-a52adb1c744b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021310357s
    Jan 18 15:34:30.350: INFO: Pod "downwardapi-volume-ee286b57-a319-4774-862f-a52adb1c744b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020067297s
    STEP: Saw pod success 01/18/23 15:34:30.35
    Jan 18 15:34:30.350: INFO: Pod "downwardapi-volume-ee286b57-a319-4774-862f-a52adb1c744b" satisfied condition "Succeeded or Failed"
    Jan 18 15:34:30.356: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-ee286b57-a319-4774-862f-a52adb1c744b container client-container: <nil>
    STEP: delete the pod 01/18/23 15:34:30.365
    Jan 18 15:34:30.395: INFO: Waiting for pod downwardapi-volume-ee286b57-a319-4774-862f-a52adb1c744b to disappear
    Jan 18 15:34:30.399: INFO: Pod downwardapi-volume-ee286b57-a319-4774-862f-a52adb1c744b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 15:34:30.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1658" for this suite. 01/18/23 15:34:30.406
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:34:30.415
Jan 18 15:34:30.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename pod-network-test 01/18/23 15:34:30.418
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:34:30.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:34:30.443
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-8700 01/18/23 15:34:30.448
STEP: creating a selector 01/18/23 15:34:30.449
STEP: Creating the service pods in kubernetes 01/18/23 15:34:30.449
Jan 18 15:34:30.449: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 18 15:34:30.485: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8700" to be "running and ready"
Jan 18 15:34:30.492: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.838484ms
Jan 18 15:34:30.492: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:34:32.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.01475465s
Jan 18 15:34:32.500: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:34:34.497: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011776824s
Jan 18 15:34:34.497: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:34:36.501: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.016384855s
Jan 18 15:34:36.501: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:34:38.499: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.014142307s
Jan 18 15:34:38.499: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:34:40.499: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013680589s
Jan 18 15:34:40.499: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:34:42.496: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.011574105s
Jan 18 15:34:42.496: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:34:44.499: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.014100332s
Jan 18 15:34:44.499: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:34:46.498: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.013370999s
Jan 18 15:34:46.498: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:34:48.496: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.0116075s
Jan 18 15:34:48.497: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:34:50.498: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.013423554s
Jan 18 15:34:50.498: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:34:52.497: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.012351529s
Jan 18 15:34:52.497: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 18 15:34:52.498: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 18 15:34:52.501: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8700" to be "running and ready"
Jan 18 15:34:52.506: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.745955ms
Jan 18 15:34:52.506: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 18 15:34:52.507: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/18/23 15:34:52.51
Jan 18 15:34:52.531: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8700" to be "running"
Jan 18 15:34:52.547: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 15.916123ms
Jan 18 15:34:54.555: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.024150404s
Jan 18 15:34:54.555: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 18 15:34:54.562: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8700" to be "running"
Jan 18 15:34:54.566: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.71452ms
Jan 18 15:34:54.566: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 18 15:34:54.570: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 18 15:34:54.570: INFO: Going to poll 10.233.78.70 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 18 15:34:54.575: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.78.70:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8700 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 15:34:54.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 15:34:54.577: INFO: ExecWithOptions: Clientset creation
Jan 18 15:34:54.577: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8700/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.78.70%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 15:34:54.690: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 18 15:34:54.690: INFO: Going to poll 10.233.68.156 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 18 15:34:54.695: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.68.156:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8700 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 15:34:54.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 15:34:54.696: INFO: ExecWithOptions: Clientset creation
Jan 18 15:34:54.696: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8700/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.68.156%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 15:34:54.816: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 18 15:34:54.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8700" for this suite. 01/18/23 15:34:54.823
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":97,"skipped":1776,"failed":0}
------------------------------
• [SLOW TEST] [24.422 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:34:30.415
    Jan 18 15:34:30.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename pod-network-test 01/18/23 15:34:30.418
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:34:30.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:34:30.443
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-8700 01/18/23 15:34:30.448
    STEP: creating a selector 01/18/23 15:34:30.449
    STEP: Creating the service pods in kubernetes 01/18/23 15:34:30.449
    Jan 18 15:34:30.449: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 18 15:34:30.485: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8700" to be "running and ready"
    Jan 18 15:34:30.492: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.838484ms
    Jan 18 15:34:30.492: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:34:32.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.01475465s
    Jan 18 15:34:32.500: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:34:34.497: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011776824s
    Jan 18 15:34:34.497: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:34:36.501: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.016384855s
    Jan 18 15:34:36.501: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:34:38.499: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.014142307s
    Jan 18 15:34:38.499: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:34:40.499: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013680589s
    Jan 18 15:34:40.499: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:34:42.496: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.011574105s
    Jan 18 15:34:42.496: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:34:44.499: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.014100332s
    Jan 18 15:34:44.499: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:34:46.498: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.013370999s
    Jan 18 15:34:46.498: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:34:48.496: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.0116075s
    Jan 18 15:34:48.497: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:34:50.498: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.013423554s
    Jan 18 15:34:50.498: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:34:52.497: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.012351529s
    Jan 18 15:34:52.497: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 18 15:34:52.498: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 18 15:34:52.501: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8700" to be "running and ready"
    Jan 18 15:34:52.506: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.745955ms
    Jan 18 15:34:52.506: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 18 15:34:52.507: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/18/23 15:34:52.51
    Jan 18 15:34:52.531: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8700" to be "running"
    Jan 18 15:34:52.547: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 15.916123ms
    Jan 18 15:34:54.555: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.024150404s
    Jan 18 15:34:54.555: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 18 15:34:54.562: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8700" to be "running"
    Jan 18 15:34:54.566: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.71452ms
    Jan 18 15:34:54.566: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 18 15:34:54.570: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 18 15:34:54.570: INFO: Going to poll 10.233.78.70 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Jan 18 15:34:54.575: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.78.70:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8700 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 15:34:54.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 15:34:54.577: INFO: ExecWithOptions: Clientset creation
    Jan 18 15:34:54.577: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8700/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.78.70%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 18 15:34:54.690: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 18 15:34:54.690: INFO: Going to poll 10.233.68.156 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Jan 18 15:34:54.695: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.68.156:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8700 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 15:34:54.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 15:34:54.696: INFO: ExecWithOptions: Clientset creation
    Jan 18 15:34:54.696: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8700/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.68.156%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 18 15:34:54.816: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 18 15:34:54.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-8700" for this suite. 01/18/23 15:34:54.823
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:34:54.841
Jan 18 15:34:54.842: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename disruption 01/18/23 15:34:54.843
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:34:54.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:34:54.877
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 01/18/23 15:34:54.883
STEP: Waiting for the pdb to be processed 01/18/23 15:34:54.897
STEP: First trying to evict a pod which shouldn't be evictable 01/18/23 15:34:54.92
STEP: Waiting for all pods to be running 01/18/23 15:34:54.921
Jan 18 15:34:54.926: INFO: pods: 0 < 3
STEP: locating a running pod 01/18/23 15:34:56.933
STEP: Updating the pdb to allow a pod to be evicted 01/18/23 15:34:56.946
STEP: Waiting for the pdb to be processed 01/18/23 15:34:56.955
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/18/23 15:34:58.965
STEP: Waiting for all pods to be running 01/18/23 15:34:58.966
STEP: Waiting for the pdb to observed all healthy pods 01/18/23 15:34:58.971
STEP: Patching the pdb to disallow a pod to be evicted 01/18/23 15:34:59.013
STEP: Waiting for the pdb to be processed 01/18/23 15:34:59.051
STEP: Waiting for all pods to be running 01/18/23 15:34:59.062
Jan 18 15:34:59.067: INFO: running pods: 2 < 3
STEP: locating a running pod 01/18/23 15:35:01.077
STEP: Deleting the pdb to allow a pod to be evicted 01/18/23 15:35:01.089
STEP: Waiting for the pdb to be deleted 01/18/23 15:35:01.095
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/18/23 15:35:01.1
STEP: Waiting for all pods to be running 01/18/23 15:35:01.101
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 18 15:35:01.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2013" for this suite. 01/18/23 15:35:01.154
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":98,"skipped":1778,"failed":0}
------------------------------
• [SLOW TEST] [6.348 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:34:54.841
    Jan 18 15:34:54.842: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename disruption 01/18/23 15:34:54.843
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:34:54.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:34:54.877
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 01/18/23 15:34:54.883
    STEP: Waiting for the pdb to be processed 01/18/23 15:34:54.897
    STEP: First trying to evict a pod which shouldn't be evictable 01/18/23 15:34:54.92
    STEP: Waiting for all pods to be running 01/18/23 15:34:54.921
    Jan 18 15:34:54.926: INFO: pods: 0 < 3
    STEP: locating a running pod 01/18/23 15:34:56.933
    STEP: Updating the pdb to allow a pod to be evicted 01/18/23 15:34:56.946
    STEP: Waiting for the pdb to be processed 01/18/23 15:34:56.955
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/18/23 15:34:58.965
    STEP: Waiting for all pods to be running 01/18/23 15:34:58.966
    STEP: Waiting for the pdb to observed all healthy pods 01/18/23 15:34:58.971
    STEP: Patching the pdb to disallow a pod to be evicted 01/18/23 15:34:59.013
    STEP: Waiting for the pdb to be processed 01/18/23 15:34:59.051
    STEP: Waiting for all pods to be running 01/18/23 15:34:59.062
    Jan 18 15:34:59.067: INFO: running pods: 2 < 3
    STEP: locating a running pod 01/18/23 15:35:01.077
    STEP: Deleting the pdb to allow a pod to be evicted 01/18/23 15:35:01.089
    STEP: Waiting for the pdb to be deleted 01/18/23 15:35:01.095
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/18/23 15:35:01.1
    STEP: Waiting for all pods to be running 01/18/23 15:35:01.101
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 18 15:35:01.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2013" for this suite. 01/18/23 15:35:01.154
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:35:01.191
Jan 18 15:35:01.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename kubectl 01/18/23 15:35:01.194
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:35:01.37
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:35:01.378
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 01/18/23 15:35:01.383
Jan 18 15:35:01.384: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-6287 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 01/18/23 15:35:01.541
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 15:35:01.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6287" for this suite. 01/18/23 15:35:01.562
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":99,"skipped":1778,"failed":0}
------------------------------
• [0.394 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:35:01.191
    Jan 18 15:35:01.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename kubectl 01/18/23 15:35:01.194
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:35:01.37
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:35:01.378
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 01/18/23 15:35:01.383
    Jan 18 15:35:01.384: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-6287 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 01/18/23 15:35:01.541
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 15:35:01.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6287" for this suite. 01/18/23 15:35:01.562
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:35:01.586
Jan 18 15:35:01.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 15:35:01.591
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:35:01.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:35:01.614
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 01/18/23 15:35:01.618
Jan 18 15:35:01.629: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f762d743-24cf-4ac1-b3e1-a9047af87bd8" in namespace "projected-3679" to be "Succeeded or Failed"
Jan 18 15:35:01.633: INFO: Pod "downwardapi-volume-f762d743-24cf-4ac1-b3e1-a9047af87bd8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.706437ms
Jan 18 15:35:03.649: INFO: Pod "downwardapi-volume-f762d743-24cf-4ac1-b3e1-a9047af87bd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019686878s
Jan 18 15:35:05.639: INFO: Pod "downwardapi-volume-f762d743-24cf-4ac1-b3e1-a9047af87bd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009678644s
STEP: Saw pod success 01/18/23 15:35:05.639
Jan 18 15:35:05.640: INFO: Pod "downwardapi-volume-f762d743-24cf-4ac1-b3e1-a9047af87bd8" satisfied condition "Succeeded or Failed"
Jan 18 15:35:05.645: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-f762d743-24cf-4ac1-b3e1-a9047af87bd8 container client-container: <nil>
STEP: delete the pod 01/18/23 15:35:05.652
Jan 18 15:35:05.669: INFO: Waiting for pod downwardapi-volume-f762d743-24cf-4ac1-b3e1-a9047af87bd8 to disappear
Jan 18 15:35:05.687: INFO: Pod downwardapi-volume-f762d743-24cf-4ac1-b3e1-a9047af87bd8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 15:35:05.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3679" for this suite. 01/18/23 15:35:05.697
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":100,"skipped":1800,"failed":0}
------------------------------
• [4.119 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:35:01.586
    Jan 18 15:35:01.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 15:35:01.591
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:35:01.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:35:01.614
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 01/18/23 15:35:01.618
    Jan 18 15:35:01.629: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f762d743-24cf-4ac1-b3e1-a9047af87bd8" in namespace "projected-3679" to be "Succeeded or Failed"
    Jan 18 15:35:01.633: INFO: Pod "downwardapi-volume-f762d743-24cf-4ac1-b3e1-a9047af87bd8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.706437ms
    Jan 18 15:35:03.649: INFO: Pod "downwardapi-volume-f762d743-24cf-4ac1-b3e1-a9047af87bd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019686878s
    Jan 18 15:35:05.639: INFO: Pod "downwardapi-volume-f762d743-24cf-4ac1-b3e1-a9047af87bd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009678644s
    STEP: Saw pod success 01/18/23 15:35:05.639
    Jan 18 15:35:05.640: INFO: Pod "downwardapi-volume-f762d743-24cf-4ac1-b3e1-a9047af87bd8" satisfied condition "Succeeded or Failed"
    Jan 18 15:35:05.645: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-f762d743-24cf-4ac1-b3e1-a9047af87bd8 container client-container: <nil>
    STEP: delete the pod 01/18/23 15:35:05.652
    Jan 18 15:35:05.669: INFO: Waiting for pod downwardapi-volume-f762d743-24cf-4ac1-b3e1-a9047af87bd8 to disappear
    Jan 18 15:35:05.687: INFO: Pod downwardapi-volume-f762d743-24cf-4ac1-b3e1-a9047af87bd8 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 15:35:05.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3679" for this suite. 01/18/23 15:35:05.697
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:35:05.728
Jan 18 15:35:05.728: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename configmap 01/18/23 15:35:05.729
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:35:05.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:35:05.806
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-01c1999d-50a8-4757-a5b9-474ecbac40f1 01/18/23 15:35:05.81
STEP: Creating a pod to test consume configMaps 01/18/23 15:35:05.815
Jan 18 15:35:05.832: INFO: Waiting up to 5m0s for pod "pod-configmaps-ec0ba787-0291-49ad-b49e-1f6b939023a3" in namespace "configmap-9711" to be "Succeeded or Failed"
Jan 18 15:35:05.844: INFO: Pod "pod-configmaps-ec0ba787-0291-49ad-b49e-1f6b939023a3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.053261ms
Jan 18 15:35:07.849: INFO: Pod "pod-configmaps-ec0ba787-0291-49ad-b49e-1f6b939023a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017585819s
Jan 18 15:35:09.852: INFO: Pod "pod-configmaps-ec0ba787-0291-49ad-b49e-1f6b939023a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020092294s
STEP: Saw pod success 01/18/23 15:35:09.852
Jan 18 15:35:09.853: INFO: Pod "pod-configmaps-ec0ba787-0291-49ad-b49e-1f6b939023a3" satisfied condition "Succeeded or Failed"
Jan 18 15:35:09.860: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-ec0ba787-0291-49ad-b49e-1f6b939023a3 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 15:35:09.868
Jan 18 15:35:09.892: INFO: Waiting for pod pod-configmaps-ec0ba787-0291-49ad-b49e-1f6b939023a3 to disappear
Jan 18 15:35:09.900: INFO: Pod pod-configmaps-ec0ba787-0291-49ad-b49e-1f6b939023a3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 15:35:09.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9711" for this suite. 01/18/23 15:35:09.908
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":101,"skipped":1868,"failed":0}
------------------------------
• [4.187 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:35:05.728
    Jan 18 15:35:05.728: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename configmap 01/18/23 15:35:05.729
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:35:05.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:35:05.806
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-01c1999d-50a8-4757-a5b9-474ecbac40f1 01/18/23 15:35:05.81
    STEP: Creating a pod to test consume configMaps 01/18/23 15:35:05.815
    Jan 18 15:35:05.832: INFO: Waiting up to 5m0s for pod "pod-configmaps-ec0ba787-0291-49ad-b49e-1f6b939023a3" in namespace "configmap-9711" to be "Succeeded or Failed"
    Jan 18 15:35:05.844: INFO: Pod "pod-configmaps-ec0ba787-0291-49ad-b49e-1f6b939023a3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.053261ms
    Jan 18 15:35:07.849: INFO: Pod "pod-configmaps-ec0ba787-0291-49ad-b49e-1f6b939023a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017585819s
    Jan 18 15:35:09.852: INFO: Pod "pod-configmaps-ec0ba787-0291-49ad-b49e-1f6b939023a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020092294s
    STEP: Saw pod success 01/18/23 15:35:09.852
    Jan 18 15:35:09.853: INFO: Pod "pod-configmaps-ec0ba787-0291-49ad-b49e-1f6b939023a3" satisfied condition "Succeeded or Failed"
    Jan 18 15:35:09.860: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-ec0ba787-0291-49ad-b49e-1f6b939023a3 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 15:35:09.868
    Jan 18 15:35:09.892: INFO: Waiting for pod pod-configmaps-ec0ba787-0291-49ad-b49e-1f6b939023a3 to disappear
    Jan 18 15:35:09.900: INFO: Pod pod-configmaps-ec0ba787-0291-49ad-b49e-1f6b939023a3 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 15:35:09.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9711" for this suite. 01/18/23 15:35:09.908
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:35:09.928
Jan 18 15:35:09.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename svcaccounts 01/18/23 15:35:09.929
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:35:09.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:35:09.95
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Jan 18 15:35:09.981: INFO: created pod
Jan 18 15:35:09.981: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-7148" to be "Succeeded or Failed"
Jan 18 15:35:09.987: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.986579ms
Jan 18 15:35:11.995: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013799772s
Jan 18 15:35:13.993: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011321281s
STEP: Saw pod success 01/18/23 15:35:13.993
Jan 18 15:35:13.993: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan 18 15:35:43.994: INFO: polling logs
Jan 18 15:35:44.004: INFO: Pod logs: 
I0118 15:35:11.066460       1 log.go:195] OK: Got token
I0118 15:35:11.066541       1 log.go:195] validating with in-cluster discovery
I0118 15:35:11.067373       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0118 15:35:11.067426       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7148:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674056710, NotBefore:1674056110, IssuedAt:1674056110, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7148", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"03a20bfa-a393-4805-91c0-0fb74706f19d"}}}
I0118 15:35:11.105413       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0118 15:35:11.114659       1 log.go:195] OK: Validated signature on JWT
I0118 15:35:11.114996       1 log.go:195] OK: Got valid claims from token!
I0118 15:35:11.115129       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7148:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674056710, NotBefore:1674056110, IssuedAt:1674056110, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7148", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"03a20bfa-a393-4805-91c0-0fb74706f19d"}}}

Jan 18 15:35:44.004: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 18 15:35:44.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7148" for this suite. 01/18/23 15:35:44.018
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":102,"skipped":1908,"failed":0}
------------------------------
• [SLOW TEST] [34.100 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:35:09.928
    Jan 18 15:35:09.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 15:35:09.929
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:35:09.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:35:09.95
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Jan 18 15:35:09.981: INFO: created pod
    Jan 18 15:35:09.981: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-7148" to be "Succeeded or Failed"
    Jan 18 15:35:09.987: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.986579ms
    Jan 18 15:35:11.995: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013799772s
    Jan 18 15:35:13.993: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011321281s
    STEP: Saw pod success 01/18/23 15:35:13.993
    Jan 18 15:35:13.993: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jan 18 15:35:43.994: INFO: polling logs
    Jan 18 15:35:44.004: INFO: Pod logs: 
    I0118 15:35:11.066460       1 log.go:195] OK: Got token
    I0118 15:35:11.066541       1 log.go:195] validating with in-cluster discovery
    I0118 15:35:11.067373       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0118 15:35:11.067426       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7148:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674056710, NotBefore:1674056110, IssuedAt:1674056110, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7148", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"03a20bfa-a393-4805-91c0-0fb74706f19d"}}}
    I0118 15:35:11.105413       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0118 15:35:11.114659       1 log.go:195] OK: Validated signature on JWT
    I0118 15:35:11.114996       1 log.go:195] OK: Got valid claims from token!
    I0118 15:35:11.115129       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7148:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674056710, NotBefore:1674056110, IssuedAt:1674056110, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7148", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"03a20bfa-a393-4805-91c0-0fb74706f19d"}}}

    Jan 18 15:35:44.004: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 18 15:35:44.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-7148" for this suite. 01/18/23 15:35:44.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:35:44.035
Jan 18 15:35:44.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 15:35:44.041
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:35:44.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:35:44.092
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-cee9c5d5-2451-4222-8b6a-96e0e9efdd85 01/18/23 15:35:44.097
STEP: Creating a pod to test consume secrets 01/18/23 15:35:44.102
Jan 18 15:35:44.116: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-834c0c7e-e159-4585-ad2d-9880b178917f" in namespace "projected-4170" to be "Succeeded or Failed"
Jan 18 15:35:44.131: INFO: Pod "pod-projected-secrets-834c0c7e-e159-4585-ad2d-9880b178917f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.206697ms
Jan 18 15:35:46.137: INFO: Pod "pod-projected-secrets-834c0c7e-e159-4585-ad2d-9880b178917f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020271159s
Jan 18 15:35:48.136: INFO: Pod "pod-projected-secrets-834c0c7e-e159-4585-ad2d-9880b178917f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019382733s
STEP: Saw pod success 01/18/23 15:35:48.136
Jan 18 15:35:48.137: INFO: Pod "pod-projected-secrets-834c0c7e-e159-4585-ad2d-9880b178917f" satisfied condition "Succeeded or Failed"
Jan 18 15:35:48.142: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-secrets-834c0c7e-e159-4585-ad2d-9880b178917f container projected-secret-volume-test: <nil>
STEP: delete the pod 01/18/23 15:35:48.151
Jan 18 15:35:48.166: INFO: Waiting for pod pod-projected-secrets-834c0c7e-e159-4585-ad2d-9880b178917f to disappear
Jan 18 15:35:48.169: INFO: Pod pod-projected-secrets-834c0c7e-e159-4585-ad2d-9880b178917f no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 18 15:35:48.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4170" for this suite. 01/18/23 15:35:48.174
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":103,"skipped":1931,"failed":0}
------------------------------
• [4.146 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:35:44.035
    Jan 18 15:35:44.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 15:35:44.041
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:35:44.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:35:44.092
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-cee9c5d5-2451-4222-8b6a-96e0e9efdd85 01/18/23 15:35:44.097
    STEP: Creating a pod to test consume secrets 01/18/23 15:35:44.102
    Jan 18 15:35:44.116: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-834c0c7e-e159-4585-ad2d-9880b178917f" in namespace "projected-4170" to be "Succeeded or Failed"
    Jan 18 15:35:44.131: INFO: Pod "pod-projected-secrets-834c0c7e-e159-4585-ad2d-9880b178917f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.206697ms
    Jan 18 15:35:46.137: INFO: Pod "pod-projected-secrets-834c0c7e-e159-4585-ad2d-9880b178917f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020271159s
    Jan 18 15:35:48.136: INFO: Pod "pod-projected-secrets-834c0c7e-e159-4585-ad2d-9880b178917f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019382733s
    STEP: Saw pod success 01/18/23 15:35:48.136
    Jan 18 15:35:48.137: INFO: Pod "pod-projected-secrets-834c0c7e-e159-4585-ad2d-9880b178917f" satisfied condition "Succeeded or Failed"
    Jan 18 15:35:48.142: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-secrets-834c0c7e-e159-4585-ad2d-9880b178917f container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 15:35:48.151
    Jan 18 15:35:48.166: INFO: Waiting for pod pod-projected-secrets-834c0c7e-e159-4585-ad2d-9880b178917f to disappear
    Jan 18 15:35:48.169: INFO: Pod pod-projected-secrets-834c0c7e-e159-4585-ad2d-9880b178917f no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 18 15:35:48.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4170" for this suite. 01/18/23 15:35:48.174
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:35:48.184
Jan 18 15:35:48.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename services 01/18/23 15:35:48.186
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:35:48.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:35:48.209
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-9835 01/18/23 15:35:48.214
STEP: creating service affinity-clusterip-transition in namespace services-9835 01/18/23 15:35:48.214
STEP: creating replication controller affinity-clusterip-transition in namespace services-9835 01/18/23 15:35:48.242
I0118 15:35:48.291261      19 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-9835, replica count: 3
I0118 15:35:51.345389      19 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 15:35:51.353: INFO: Creating new exec pod
Jan 18 15:35:51.363: INFO: Waiting up to 5m0s for pod "execpod-affinity9zmt2" in namespace "services-9835" to be "running"
Jan 18 15:35:51.378: INFO: Pod "execpod-affinity9zmt2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.920099ms
Jan 18 15:35:53.385: INFO: Pod "execpod-affinity9zmt2": Phase="Running", Reason="", readiness=true. Elapsed: 2.021565514s
Jan 18 15:35:53.385: INFO: Pod "execpod-affinity9zmt2" satisfied condition "running"
Jan 18 15:35:54.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-9835 exec execpod-affinity9zmt2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jan 18 15:35:54.593: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan 18 15:35:54.593: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 15:35:54.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-9835 exec execpod-affinity9zmt2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.36.163 80'
Jan 18 15:35:54.858: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.36.163 80\nConnection to 10.233.36.163 80 port [tcp/http] succeeded!\n"
Jan 18 15:35:54.858: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 15:35:54.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-9835 exec execpod-affinity9zmt2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.36.163:80/ ; done'
Jan 18 15:35:55.320: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n"
Jan 18 15:35:55.320: INFO: stdout: "\naffinity-clusterip-transition-vmfwz\naffinity-clusterip-transition-vdw48\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-vmfwz\naffinity-clusterip-transition-vdw48\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-vmfwz\naffinity-clusterip-transition-vdw48\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-vmfwz\naffinity-clusterip-transition-vdw48\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-vmfwz\naffinity-clusterip-transition-vdw48\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-vmfwz"
Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-vmfwz
Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-vdw48
Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-bkzgc
Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-vmfwz
Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-vdw48
Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-bkzgc
Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-vmfwz
Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-vdw48
Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-bkzgc
Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-vmfwz
Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-vdw48
Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-bkzgc
Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-vmfwz
Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-vdw48
Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-bkzgc
Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-vmfwz
Jan 18 15:35:55.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-9835 exec execpod-affinity9zmt2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.36.163:80/ ; done'
Jan 18 15:35:55.661: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n"
Jan 18 15:35:55.662: INFO: stdout: "\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc"
Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
Jan 18 15:35:55.662: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-9835, will wait for the garbage collector to delete the pods 01/18/23 15:35:55.696
Jan 18 15:35:55.784: INFO: Deleting ReplicationController affinity-clusterip-transition took: 14.814187ms
Jan 18 15:35:55.885: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.782964ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 15:35:59.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9835" for this suite. 01/18/23 15:35:59.017
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":104,"skipped":1933,"failed":0}
------------------------------
• [SLOW TEST] [10.842 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:35:48.184
    Jan 18 15:35:48.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename services 01/18/23 15:35:48.186
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:35:48.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:35:48.209
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-9835 01/18/23 15:35:48.214
    STEP: creating service affinity-clusterip-transition in namespace services-9835 01/18/23 15:35:48.214
    STEP: creating replication controller affinity-clusterip-transition in namespace services-9835 01/18/23 15:35:48.242
    I0118 15:35:48.291261      19 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-9835, replica count: 3
    I0118 15:35:51.345389      19 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 15:35:51.353: INFO: Creating new exec pod
    Jan 18 15:35:51.363: INFO: Waiting up to 5m0s for pod "execpod-affinity9zmt2" in namespace "services-9835" to be "running"
    Jan 18 15:35:51.378: INFO: Pod "execpod-affinity9zmt2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.920099ms
    Jan 18 15:35:53.385: INFO: Pod "execpod-affinity9zmt2": Phase="Running", Reason="", readiness=true. Elapsed: 2.021565514s
    Jan 18 15:35:53.385: INFO: Pod "execpod-affinity9zmt2" satisfied condition "running"
    Jan 18 15:35:54.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-9835 exec execpod-affinity9zmt2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Jan 18 15:35:54.593: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jan 18 15:35:54.593: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 15:35:54.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-9835 exec execpod-affinity9zmt2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.36.163 80'
    Jan 18 15:35:54.858: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.36.163 80\nConnection to 10.233.36.163 80 port [tcp/http] succeeded!\n"
    Jan 18 15:35:54.858: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 15:35:54.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-9835 exec execpod-affinity9zmt2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.36.163:80/ ; done'
    Jan 18 15:35:55.320: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n"
    Jan 18 15:35:55.320: INFO: stdout: "\naffinity-clusterip-transition-vmfwz\naffinity-clusterip-transition-vdw48\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-vmfwz\naffinity-clusterip-transition-vdw48\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-vmfwz\naffinity-clusterip-transition-vdw48\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-vmfwz\naffinity-clusterip-transition-vdw48\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-vmfwz\naffinity-clusterip-transition-vdw48\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-vmfwz"
    Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-vmfwz
    Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-vdw48
    Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-bkzgc
    Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-vmfwz
    Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-vdw48
    Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-bkzgc
    Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-vmfwz
    Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-vdw48
    Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-bkzgc
    Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-vmfwz
    Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-vdw48
    Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-bkzgc
    Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-vmfwz
    Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-vdw48
    Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-bkzgc
    Jan 18 15:35:55.320: INFO: Received response from host: affinity-clusterip-transition-vmfwz
    Jan 18 15:35:55.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-9835 exec execpod-affinity9zmt2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.36.163:80/ ; done'
    Jan 18 15:35:55.661: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.163:80/\n"
    Jan 18 15:35:55.662: INFO: stdout: "\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc\naffinity-clusterip-transition-bkzgc"
    Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
    Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
    Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
    Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
    Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
    Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
    Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
    Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
    Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
    Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
    Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
    Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
    Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
    Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
    Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
    Jan 18 15:35:55.662: INFO: Received response from host: affinity-clusterip-transition-bkzgc
    Jan 18 15:35:55.662: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-9835, will wait for the garbage collector to delete the pods 01/18/23 15:35:55.696
    Jan 18 15:35:55.784: INFO: Deleting ReplicationController affinity-clusterip-transition took: 14.814187ms
    Jan 18 15:35:55.885: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.782964ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 15:35:59.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9835" for this suite. 01/18/23 15:35:59.017
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:35:59.027
Jan 18 15:35:59.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename secrets 01/18/23 15:35:59.03
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:35:59.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:35:59.074
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-3ed45ce8-dc8a-4c50-9e27-5b8b76625c01 01/18/23 15:35:59.085
STEP: Creating secret with name s-test-opt-upd-f9fbfade-a8fa-477f-947c-179b4f88babf 01/18/23 15:35:59.096
STEP: Creating the pod 01/18/23 15:35:59.111
Jan 18 15:35:59.129: INFO: Waiting up to 5m0s for pod "pod-secrets-290a6fb7-e5dc-40f1-b3da-f6eabdf17af0" in namespace "secrets-6768" to be "running and ready"
Jan 18 15:35:59.163: INFO: Pod "pod-secrets-290a6fb7-e5dc-40f1-b3da-f6eabdf17af0": Phase="Pending", Reason="", readiness=false. Elapsed: 33.97474ms
Jan 18 15:35:59.164: INFO: The phase of Pod pod-secrets-290a6fb7-e5dc-40f1-b3da-f6eabdf17af0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:36:01.171: INFO: Pod "pod-secrets-290a6fb7-e5dc-40f1-b3da-f6eabdf17af0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041270399s
Jan 18 15:36:01.171: INFO: The phase of Pod pod-secrets-290a6fb7-e5dc-40f1-b3da-f6eabdf17af0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:36:03.169: INFO: Pod "pod-secrets-290a6fb7-e5dc-40f1-b3da-f6eabdf17af0": Phase="Running", Reason="", readiness=true. Elapsed: 4.039660971s
Jan 18 15:36:03.169: INFO: The phase of Pod pod-secrets-290a6fb7-e5dc-40f1-b3da-f6eabdf17af0 is Running (Ready = true)
Jan 18 15:36:03.170: INFO: Pod "pod-secrets-290a6fb7-e5dc-40f1-b3da-f6eabdf17af0" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-3ed45ce8-dc8a-4c50-9e27-5b8b76625c01 01/18/23 15:36:03.21
STEP: Updating secret s-test-opt-upd-f9fbfade-a8fa-477f-947c-179b4f88babf 01/18/23 15:36:03.217
STEP: Creating secret with name s-test-opt-create-16cd5e51-28d0-4e35-afe8-d9ccc8b49320 01/18/23 15:36:03.222
STEP: waiting to observe update in volume 01/18/23 15:36:03.227
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 18 15:36:05.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6768" for this suite. 01/18/23 15:36:05.272
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":105,"skipped":1942,"failed":0}
------------------------------
• [SLOW TEST] [6.252 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:35:59.027
    Jan 18 15:35:59.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename secrets 01/18/23 15:35:59.03
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:35:59.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:35:59.074
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-3ed45ce8-dc8a-4c50-9e27-5b8b76625c01 01/18/23 15:35:59.085
    STEP: Creating secret with name s-test-opt-upd-f9fbfade-a8fa-477f-947c-179b4f88babf 01/18/23 15:35:59.096
    STEP: Creating the pod 01/18/23 15:35:59.111
    Jan 18 15:35:59.129: INFO: Waiting up to 5m0s for pod "pod-secrets-290a6fb7-e5dc-40f1-b3da-f6eabdf17af0" in namespace "secrets-6768" to be "running and ready"
    Jan 18 15:35:59.163: INFO: Pod "pod-secrets-290a6fb7-e5dc-40f1-b3da-f6eabdf17af0": Phase="Pending", Reason="", readiness=false. Elapsed: 33.97474ms
    Jan 18 15:35:59.164: INFO: The phase of Pod pod-secrets-290a6fb7-e5dc-40f1-b3da-f6eabdf17af0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:36:01.171: INFO: Pod "pod-secrets-290a6fb7-e5dc-40f1-b3da-f6eabdf17af0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041270399s
    Jan 18 15:36:01.171: INFO: The phase of Pod pod-secrets-290a6fb7-e5dc-40f1-b3da-f6eabdf17af0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:36:03.169: INFO: Pod "pod-secrets-290a6fb7-e5dc-40f1-b3da-f6eabdf17af0": Phase="Running", Reason="", readiness=true. Elapsed: 4.039660971s
    Jan 18 15:36:03.169: INFO: The phase of Pod pod-secrets-290a6fb7-e5dc-40f1-b3da-f6eabdf17af0 is Running (Ready = true)
    Jan 18 15:36:03.170: INFO: Pod "pod-secrets-290a6fb7-e5dc-40f1-b3da-f6eabdf17af0" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-3ed45ce8-dc8a-4c50-9e27-5b8b76625c01 01/18/23 15:36:03.21
    STEP: Updating secret s-test-opt-upd-f9fbfade-a8fa-477f-947c-179b4f88babf 01/18/23 15:36:03.217
    STEP: Creating secret with name s-test-opt-create-16cd5e51-28d0-4e35-afe8-d9ccc8b49320 01/18/23 15:36:03.222
    STEP: waiting to observe update in volume 01/18/23 15:36:03.227
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 15:36:05.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6768" for this suite. 01/18/23 15:36:05.272
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:36:05.282
Jan 18 15:36:05.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename downward-api 01/18/23 15:36:05.285
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:36:05.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:36:05.304
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 01/18/23 15:36:05.31
Jan 18 15:36:05.325: INFO: Waiting up to 5m0s for pod "downward-api-c416c2dd-a94a-42ba-a459-daabd0be0cda" in namespace "downward-api-1247" to be "Succeeded or Failed"
Jan 18 15:36:05.344: INFO: Pod "downward-api-c416c2dd-a94a-42ba-a459-daabd0be0cda": Phase="Pending", Reason="", readiness=false. Elapsed: 14.661254ms
Jan 18 15:36:07.349: INFO: Pod "downward-api-c416c2dd-a94a-42ba-a459-daabd0be0cda": Phase="Running", Reason="", readiness=true. Elapsed: 2.019278327s
Jan 18 15:36:09.353: INFO: Pod "downward-api-c416c2dd-a94a-42ba-a459-daabd0be0cda": Phase="Running", Reason="", readiness=false. Elapsed: 4.023186084s
Jan 18 15:36:11.351: INFO: Pod "downward-api-c416c2dd-a94a-42ba-a459-daabd0be0cda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021469519s
STEP: Saw pod success 01/18/23 15:36:11.351
Jan 18 15:36:11.352: INFO: Pod "downward-api-c416c2dd-a94a-42ba-a459-daabd0be0cda" satisfied condition "Succeeded or Failed"
Jan 18 15:36:11.356: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downward-api-c416c2dd-a94a-42ba-a459-daabd0be0cda container dapi-container: <nil>
STEP: delete the pod 01/18/23 15:36:11.366
Jan 18 15:36:11.377: INFO: Waiting for pod downward-api-c416c2dd-a94a-42ba-a459-daabd0be0cda to disappear
Jan 18 15:36:11.382: INFO: Pod downward-api-c416c2dd-a94a-42ba-a459-daabd0be0cda no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 18 15:36:11.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1247" for this suite. 01/18/23 15:36:11.389
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":106,"skipped":1950,"failed":0}
------------------------------
• [SLOW TEST] [6.116 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:36:05.282
    Jan 18 15:36:05.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename downward-api 01/18/23 15:36:05.285
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:36:05.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:36:05.304
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 01/18/23 15:36:05.31
    Jan 18 15:36:05.325: INFO: Waiting up to 5m0s for pod "downward-api-c416c2dd-a94a-42ba-a459-daabd0be0cda" in namespace "downward-api-1247" to be "Succeeded or Failed"
    Jan 18 15:36:05.344: INFO: Pod "downward-api-c416c2dd-a94a-42ba-a459-daabd0be0cda": Phase="Pending", Reason="", readiness=false. Elapsed: 14.661254ms
    Jan 18 15:36:07.349: INFO: Pod "downward-api-c416c2dd-a94a-42ba-a459-daabd0be0cda": Phase="Running", Reason="", readiness=true. Elapsed: 2.019278327s
    Jan 18 15:36:09.353: INFO: Pod "downward-api-c416c2dd-a94a-42ba-a459-daabd0be0cda": Phase="Running", Reason="", readiness=false. Elapsed: 4.023186084s
    Jan 18 15:36:11.351: INFO: Pod "downward-api-c416c2dd-a94a-42ba-a459-daabd0be0cda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021469519s
    STEP: Saw pod success 01/18/23 15:36:11.351
    Jan 18 15:36:11.352: INFO: Pod "downward-api-c416c2dd-a94a-42ba-a459-daabd0be0cda" satisfied condition "Succeeded or Failed"
    Jan 18 15:36:11.356: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downward-api-c416c2dd-a94a-42ba-a459-daabd0be0cda container dapi-container: <nil>
    STEP: delete the pod 01/18/23 15:36:11.366
    Jan 18 15:36:11.377: INFO: Waiting for pod downward-api-c416c2dd-a94a-42ba-a459-daabd0be0cda to disappear
    Jan 18 15:36:11.382: INFO: Pod downward-api-c416c2dd-a94a-42ba-a459-daabd0be0cda no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 18 15:36:11.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1247" for this suite. 01/18/23 15:36:11.389
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:36:11.399
Jan 18 15:36:11.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename secrets 01/18/23 15:36:11.402
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:36:11.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:36:11.423
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-f80b3769-c7df-4bfa-9d94-c4bd67e1de28 01/18/23 15:36:11.427
STEP: Creating a pod to test consume secrets 01/18/23 15:36:11.431
Jan 18 15:36:11.439: INFO: Waiting up to 5m0s for pod "pod-secrets-341b0bc3-9b8f-416e-9f86-84a98f6e3207" in namespace "secrets-9448" to be "Succeeded or Failed"
Jan 18 15:36:11.443: INFO: Pod "pod-secrets-341b0bc3-9b8f-416e-9f86-84a98f6e3207": Phase="Pending", Reason="", readiness=false. Elapsed: 3.355977ms
Jan 18 15:36:13.450: INFO: Pod "pod-secrets-341b0bc3-9b8f-416e-9f86-84a98f6e3207": Phase="Running", Reason="", readiness=true. Elapsed: 2.010529934s
Jan 18 15:36:15.447: INFO: Pod "pod-secrets-341b0bc3-9b8f-416e-9f86-84a98f6e3207": Phase="Running", Reason="", readiness=false. Elapsed: 4.00768655s
Jan 18 15:36:17.450: INFO: Pod "pod-secrets-341b0bc3-9b8f-416e-9f86-84a98f6e3207": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010079088s
STEP: Saw pod success 01/18/23 15:36:17.45
Jan 18 15:36:17.450: INFO: Pod "pod-secrets-341b0bc3-9b8f-416e-9f86-84a98f6e3207" satisfied condition "Succeeded or Failed"
Jan 18 15:36:17.454: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-341b0bc3-9b8f-416e-9f86-84a98f6e3207 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 15:36:17.463
Jan 18 15:36:17.488: INFO: Waiting for pod pod-secrets-341b0bc3-9b8f-416e-9f86-84a98f6e3207 to disappear
Jan 18 15:36:17.494: INFO: Pod pod-secrets-341b0bc3-9b8f-416e-9f86-84a98f6e3207 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 18 15:36:17.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9448" for this suite. 01/18/23 15:36:17.499
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":107,"skipped":1951,"failed":0}
------------------------------
• [SLOW TEST] [6.112 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:36:11.399
    Jan 18 15:36:11.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename secrets 01/18/23 15:36:11.402
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:36:11.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:36:11.423
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-f80b3769-c7df-4bfa-9d94-c4bd67e1de28 01/18/23 15:36:11.427
    STEP: Creating a pod to test consume secrets 01/18/23 15:36:11.431
    Jan 18 15:36:11.439: INFO: Waiting up to 5m0s for pod "pod-secrets-341b0bc3-9b8f-416e-9f86-84a98f6e3207" in namespace "secrets-9448" to be "Succeeded or Failed"
    Jan 18 15:36:11.443: INFO: Pod "pod-secrets-341b0bc3-9b8f-416e-9f86-84a98f6e3207": Phase="Pending", Reason="", readiness=false. Elapsed: 3.355977ms
    Jan 18 15:36:13.450: INFO: Pod "pod-secrets-341b0bc3-9b8f-416e-9f86-84a98f6e3207": Phase="Running", Reason="", readiness=true. Elapsed: 2.010529934s
    Jan 18 15:36:15.447: INFO: Pod "pod-secrets-341b0bc3-9b8f-416e-9f86-84a98f6e3207": Phase="Running", Reason="", readiness=false. Elapsed: 4.00768655s
    Jan 18 15:36:17.450: INFO: Pod "pod-secrets-341b0bc3-9b8f-416e-9f86-84a98f6e3207": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010079088s
    STEP: Saw pod success 01/18/23 15:36:17.45
    Jan 18 15:36:17.450: INFO: Pod "pod-secrets-341b0bc3-9b8f-416e-9f86-84a98f6e3207" satisfied condition "Succeeded or Failed"
    Jan 18 15:36:17.454: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-341b0bc3-9b8f-416e-9f86-84a98f6e3207 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 15:36:17.463
    Jan 18 15:36:17.488: INFO: Waiting for pod pod-secrets-341b0bc3-9b8f-416e-9f86-84a98f6e3207 to disappear
    Jan 18 15:36:17.494: INFO: Pod pod-secrets-341b0bc3-9b8f-416e-9f86-84a98f6e3207 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 15:36:17.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9448" for this suite. 01/18/23 15:36:17.499
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:36:17.519
Jan 18 15:36:17.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename dns 01/18/23 15:36:17.521
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:36:17.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:36:17.597
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/18/23 15:36:17.603
Jan 18 15:36:17.615: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3315  edd78822-3cbb-479a-8cbe-ab3651985d53 41032 0 2023-01-18 15:36:17 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-18 15:36:17 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8v9r9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8v9r9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:36:17.616: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-3315" to be "running and ready"
Jan 18 15:36:17.620: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 3.509467ms
Jan 18 15:36:17.620: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:36:19.628: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.011834162s
Jan 18 15:36:19.629: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jan 18 15:36:19.629: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 01/18/23 15:36:19.629
Jan 18 15:36:19.629: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3315 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 15:36:19.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 15:36:19.631: INFO: ExecWithOptions: Clientset creation
Jan 18 15:36:19.632: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-3315/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 01/18/23 15:36:19.753
Jan 18 15:36:19.753: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3315 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 15:36:19.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 15:36:19.754: INFO: ExecWithOptions: Clientset creation
Jan 18 15:36:19.754: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-3315/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 15:36:19.863: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 18 15:36:19.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3315" for this suite. 01/18/23 15:36:19.9
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":108,"skipped":1954,"failed":0}
------------------------------
• [2.393 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:36:17.519
    Jan 18 15:36:17.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename dns 01/18/23 15:36:17.521
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:36:17.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:36:17.597
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/18/23 15:36:17.603
    Jan 18 15:36:17.615: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3315  edd78822-3cbb-479a-8cbe-ab3651985d53 41032 0 2023-01-18 15:36:17 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-18 15:36:17 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8v9r9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8v9r9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:36:17.616: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-3315" to be "running and ready"
    Jan 18 15:36:17.620: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 3.509467ms
    Jan 18 15:36:17.620: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:36:19.628: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.011834162s
    Jan 18 15:36:19.629: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jan 18 15:36:19.629: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 01/18/23 15:36:19.629
    Jan 18 15:36:19.629: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3315 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 15:36:19.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 15:36:19.631: INFO: ExecWithOptions: Clientset creation
    Jan 18 15:36:19.632: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-3315/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 01/18/23 15:36:19.753
    Jan 18 15:36:19.753: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3315 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 15:36:19.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 15:36:19.754: INFO: ExecWithOptions: Clientset creation
    Jan 18 15:36:19.754: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-3315/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 18 15:36:19.863: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 18 15:36:19.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3315" for this suite. 01/18/23 15:36:19.9
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:36:19.926
Jan 18 15:36:19.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 15:36:19.929
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:36:19.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:36:19.958
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Jan 18 15:36:19.963: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 15:36:25.959
Jan 18 15:36:25.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-3307 --namespace=crd-publish-openapi-3307 create -f -'
Jan 18 15:36:27.554: INFO: stderr: ""
Jan 18 15:36:27.554: INFO: stdout: "e2e-test-crd-publish-openapi-4018-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 18 15:36:27.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-3307 --namespace=crd-publish-openapi-3307 delete e2e-test-crd-publish-openapi-4018-crds test-cr'
Jan 18 15:36:27.650: INFO: stderr: ""
Jan 18 15:36:27.650: INFO: stdout: "e2e-test-crd-publish-openapi-4018-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 18 15:36:27.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-3307 --namespace=crd-publish-openapi-3307 apply -f -'
Jan 18 15:36:27.998: INFO: stderr: ""
Jan 18 15:36:27.998: INFO: stdout: "e2e-test-crd-publish-openapi-4018-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 18 15:36:27.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-3307 --namespace=crd-publish-openapi-3307 delete e2e-test-crd-publish-openapi-4018-crds test-cr'
Jan 18 15:36:28.109: INFO: stderr: ""
Jan 18 15:36:28.109: INFO: stdout: "e2e-test-crd-publish-openapi-4018-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/18/23 15:36:28.109
Jan 18 15:36:28.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-3307 explain e2e-test-crd-publish-openapi-4018-crds'
Jan 18 15:36:28.450: INFO: stderr: ""
Jan 18 15:36:28.450: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4018-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 15:36:31.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3307" for this suite. 01/18/23 15:36:31.601
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":109,"skipped":1982,"failed":0}
------------------------------
• [SLOW TEST] [11.683 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:36:19.926
    Jan 18 15:36:19.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 15:36:19.929
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:36:19.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:36:19.958
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Jan 18 15:36:19.963: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 15:36:25.959
    Jan 18 15:36:25.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-3307 --namespace=crd-publish-openapi-3307 create -f -'
    Jan 18 15:36:27.554: INFO: stderr: ""
    Jan 18 15:36:27.554: INFO: stdout: "e2e-test-crd-publish-openapi-4018-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 18 15:36:27.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-3307 --namespace=crd-publish-openapi-3307 delete e2e-test-crd-publish-openapi-4018-crds test-cr'
    Jan 18 15:36:27.650: INFO: stderr: ""
    Jan 18 15:36:27.650: INFO: stdout: "e2e-test-crd-publish-openapi-4018-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jan 18 15:36:27.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-3307 --namespace=crd-publish-openapi-3307 apply -f -'
    Jan 18 15:36:27.998: INFO: stderr: ""
    Jan 18 15:36:27.998: INFO: stdout: "e2e-test-crd-publish-openapi-4018-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 18 15:36:27.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-3307 --namespace=crd-publish-openapi-3307 delete e2e-test-crd-publish-openapi-4018-crds test-cr'
    Jan 18 15:36:28.109: INFO: stderr: ""
    Jan 18 15:36:28.109: INFO: stdout: "e2e-test-crd-publish-openapi-4018-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/18/23 15:36:28.109
    Jan 18 15:36:28.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=crd-publish-openapi-3307 explain e2e-test-crd-publish-openapi-4018-crds'
    Jan 18 15:36:28.450: INFO: stderr: ""
    Jan 18 15:36:28.450: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4018-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 15:36:31.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3307" for this suite. 01/18/23 15:36:31.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:36:31.612
Jan 18 15:36:31.613: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename dns 01/18/23 15:36:31.614
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:36:31.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:36:31.637
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 01/18/23 15:36:31.641
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4625 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4625;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4625 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4625;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4625.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4625.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4625.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4625.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4625.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4625.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4625.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4625.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4625.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4625.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4625.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4625.svc;check="$$(dig +notcp +noall +answer +search 156.61.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.61.156_udp@PTR;check="$$(dig +tcp +noall +answer +search 156.61.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.61.156_tcp@PTR;sleep 1; done
 01/18/23 15:36:31.686
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4625 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4625;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4625 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4625;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4625.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4625.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4625.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4625.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4625.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4625.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4625.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4625.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4625.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4625.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4625.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4625.svc;check="$$(dig +notcp +noall +answer +search 156.61.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.61.156_udp@PTR;check="$$(dig +tcp +noall +answer +search 156.61.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.61.156_tcp@PTR;sleep 1; done
 01/18/23 15:36:31.686
STEP: creating a pod to probe DNS 01/18/23 15:36:31.686
STEP: submitting the pod to kubernetes 01/18/23 15:36:31.687
Jan 18 15:36:31.709: INFO: Waiting up to 15m0s for pod "dns-test-db171a53-9160-4572-9671-bec91af08e24" in namespace "dns-4625" to be "running"
Jan 18 15:36:31.718: INFO: Pod "dns-test-db171a53-9160-4572-9671-bec91af08e24": Phase="Pending", Reason="", readiness=false. Elapsed: 9.013713ms
Jan 18 15:36:33.740: INFO: Pod "dns-test-db171a53-9160-4572-9671-bec91af08e24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03032399s
Jan 18 15:36:35.727: INFO: Pod "dns-test-db171a53-9160-4572-9671-bec91af08e24": Phase="Running", Reason="", readiness=true. Elapsed: 4.017409457s
Jan 18 15:36:35.727: INFO: Pod "dns-test-db171a53-9160-4572-9671-bec91af08e24" satisfied condition "running"
STEP: retrieving the pod 01/18/23 15:36:35.727
STEP: looking for the results for each expected name from probers 01/18/23 15:36:35.733
Jan 18 15:36:35.738: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:35.743: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:35.748: INFO: Unable to read wheezy_udp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:35.753: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:35.759: INFO: Unable to read wheezy_udp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:35.766: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:35.774: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:35.782: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:35.819: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:35.825: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:35.835: INFO: Unable to read jessie_udp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:35.840: INFO: Unable to read jessie_tcp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:35.847: INFO: Unable to read jessie_udp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:35.854: INFO: Unable to read jessie_tcp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:35.860: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:35.865: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:35.886: INFO: Lookups using dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4625 wheezy_tcp@dns-test-service.dns-4625 wheezy_udp@dns-test-service.dns-4625.svc wheezy_tcp@dns-test-service.dns-4625.svc wheezy_udp@_http._tcp.dns-test-service.dns-4625.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4625.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4625 jessie_tcp@dns-test-service.dns-4625 jessie_udp@dns-test-service.dns-4625.svc jessie_tcp@dns-test-service.dns-4625.svc jessie_udp@_http._tcp.dns-test-service.dns-4625.svc jessie_tcp@_http._tcp.dns-test-service.dns-4625.svc]

Jan 18 15:36:40.893: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:40.898: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:40.903: INFO: Unable to read wheezy_udp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:40.908: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:40.913: INFO: Unable to read wheezy_udp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:40.918: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:40.922: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:40.926: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:40.956: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:40.962: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:40.967: INFO: Unable to read jessie_udp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:40.972: INFO: Unable to read jessie_tcp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:40.977: INFO: Unable to read jessie_udp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:40.982: INFO: Unable to read jessie_tcp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:40.988: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:40.993: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:41.013: INFO: Lookups using dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4625 wheezy_tcp@dns-test-service.dns-4625 wheezy_udp@dns-test-service.dns-4625.svc wheezy_tcp@dns-test-service.dns-4625.svc wheezy_udp@_http._tcp.dns-test-service.dns-4625.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4625.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4625 jessie_tcp@dns-test-service.dns-4625 jessie_udp@dns-test-service.dns-4625.svc jessie_tcp@dns-test-service.dns-4625.svc jessie_udp@_http._tcp.dns-test-service.dns-4625.svc jessie_tcp@_http._tcp.dns-test-service.dns-4625.svc]

Jan 18 15:36:45.894: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:45.899: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:45.904: INFO: Unable to read wheezy_udp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:45.909: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:45.914: INFO: Unable to read wheezy_udp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:45.918: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:45.923: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:45.927: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:45.953: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:45.958: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:45.963: INFO: Unable to read jessie_udp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:45.968: INFO: Unable to read jessie_tcp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:45.973: INFO: Unable to read jessie_udp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:45.977: INFO: Unable to read jessie_tcp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:45.982: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:45.986: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:46.007: INFO: Lookups using dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4625 wheezy_tcp@dns-test-service.dns-4625 wheezy_udp@dns-test-service.dns-4625.svc wheezy_tcp@dns-test-service.dns-4625.svc wheezy_udp@_http._tcp.dns-test-service.dns-4625.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4625.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4625 jessie_tcp@dns-test-service.dns-4625 jessie_udp@dns-test-service.dns-4625.svc jessie_tcp@dns-test-service.dns-4625.svc jessie_udp@_http._tcp.dns-test-service.dns-4625.svc jessie_tcp@_http._tcp.dns-test-service.dns-4625.svc]

Jan 18 15:36:50.894: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:50.901: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:50.906: INFO: Unable to read wheezy_udp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:50.913: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:50.917: INFO: Unable to read wheezy_udp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:50.922: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:50.926: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:50.931: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:50.959: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:50.965: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:50.975: INFO: Unable to read jessie_udp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:50.980: INFO: Unable to read jessie_tcp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:50.987: INFO: Unable to read jessie_udp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:50.997: INFO: Unable to read jessie_tcp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:51.004: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:51.015: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:51.044: INFO: Lookups using dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4625 wheezy_tcp@dns-test-service.dns-4625 wheezy_udp@dns-test-service.dns-4625.svc wheezy_tcp@dns-test-service.dns-4625.svc wheezy_udp@_http._tcp.dns-test-service.dns-4625.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4625.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4625 jessie_tcp@dns-test-service.dns-4625 jessie_udp@dns-test-service.dns-4625.svc jessie_tcp@dns-test-service.dns-4625.svc jessie_udp@_http._tcp.dns-test-service.dns-4625.svc jessie_tcp@_http._tcp.dns-test-service.dns-4625.svc]

Jan 18 15:36:55.896: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:55.901: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:55.906: INFO: Unable to read wheezy_udp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:55.913: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:55.917: INFO: Unable to read wheezy_udp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:55.926: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:55.932: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:55.939: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:55.968: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:55.980: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:55.990: INFO: Unable to read jessie_udp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:55.995: INFO: Unable to read jessie_tcp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:56.004: INFO: Unable to read jessie_udp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:56.010: INFO: Unable to read jessie_tcp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:56.016: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:56.022: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:36:56.052: INFO: Lookups using dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4625 wheezy_tcp@dns-test-service.dns-4625 wheezy_udp@dns-test-service.dns-4625.svc wheezy_tcp@dns-test-service.dns-4625.svc wheezy_udp@_http._tcp.dns-test-service.dns-4625.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4625.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4625 jessie_tcp@dns-test-service.dns-4625 jessie_udp@dns-test-service.dns-4625.svc jessie_tcp@dns-test-service.dns-4625.svc jessie_udp@_http._tcp.dns-test-service.dns-4625.svc jessie_tcp@_http._tcp.dns-test-service.dns-4625.svc]

Jan 18 15:37:00.896: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:37:00.903: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:37:00.910: INFO: Unable to read wheezy_udp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:37:00.915: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:37:00.941: INFO: Unable to read wheezy_udp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:37:00.950: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:37:00.956: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:37:00.961: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:37:00.988: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:37:00.993: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:37:00.998: INFO: Unable to read jessie_udp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:37:01.003: INFO: Unable to read jessie_tcp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:37:01.009: INFO: Unable to read jessie_udp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:37:01.014: INFO: Unable to read jessie_tcp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:37:01.019: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:37:01.023: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
Jan 18 15:37:01.041: INFO: Lookups using dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4625 wheezy_tcp@dns-test-service.dns-4625 wheezy_udp@dns-test-service.dns-4625.svc wheezy_tcp@dns-test-service.dns-4625.svc wheezy_udp@_http._tcp.dns-test-service.dns-4625.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4625.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4625 jessie_tcp@dns-test-service.dns-4625 jessie_udp@dns-test-service.dns-4625.svc jessie_tcp@dns-test-service.dns-4625.svc jessie_udp@_http._tcp.dns-test-service.dns-4625.svc jessie_tcp@_http._tcp.dns-test-service.dns-4625.svc]

Jan 18 15:37:06.067: INFO: DNS probes using dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24 succeeded

STEP: deleting the pod 01/18/23 15:37:06.067
STEP: deleting the test service 01/18/23 15:37:06.136
STEP: deleting the test headless service 01/18/23 15:37:06.235
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 18 15:37:06.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4625" for this suite. 01/18/23 15:37:06.263
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":110,"skipped":2001,"failed":0}
------------------------------
• [SLOW TEST] [34.660 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:36:31.612
    Jan 18 15:36:31.613: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename dns 01/18/23 15:36:31.614
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:36:31.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:36:31.637
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 01/18/23 15:36:31.641
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4625 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4625;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4625 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4625;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4625.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4625.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4625.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4625.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4625.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4625.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4625.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4625.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4625.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4625.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4625.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4625.svc;check="$$(dig +notcp +noall +answer +search 156.61.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.61.156_udp@PTR;check="$$(dig +tcp +noall +answer +search 156.61.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.61.156_tcp@PTR;sleep 1; done
     01/18/23 15:36:31.686
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4625 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4625;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4625 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4625;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4625.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4625.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4625.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4625.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4625.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4625.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4625.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4625.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4625.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4625.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4625.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4625.svc;check="$$(dig +notcp +noall +answer +search 156.61.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.61.156_udp@PTR;check="$$(dig +tcp +noall +answer +search 156.61.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.61.156_tcp@PTR;sleep 1; done
     01/18/23 15:36:31.686
    STEP: creating a pod to probe DNS 01/18/23 15:36:31.686
    STEP: submitting the pod to kubernetes 01/18/23 15:36:31.687
    Jan 18 15:36:31.709: INFO: Waiting up to 15m0s for pod "dns-test-db171a53-9160-4572-9671-bec91af08e24" in namespace "dns-4625" to be "running"
    Jan 18 15:36:31.718: INFO: Pod "dns-test-db171a53-9160-4572-9671-bec91af08e24": Phase="Pending", Reason="", readiness=false. Elapsed: 9.013713ms
    Jan 18 15:36:33.740: INFO: Pod "dns-test-db171a53-9160-4572-9671-bec91af08e24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03032399s
    Jan 18 15:36:35.727: INFO: Pod "dns-test-db171a53-9160-4572-9671-bec91af08e24": Phase="Running", Reason="", readiness=true. Elapsed: 4.017409457s
    Jan 18 15:36:35.727: INFO: Pod "dns-test-db171a53-9160-4572-9671-bec91af08e24" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 15:36:35.727
    STEP: looking for the results for each expected name from probers 01/18/23 15:36:35.733
    Jan 18 15:36:35.738: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:35.743: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:35.748: INFO: Unable to read wheezy_udp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:35.753: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:35.759: INFO: Unable to read wheezy_udp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:35.766: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:35.774: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:35.782: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:35.819: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:35.825: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:35.835: INFO: Unable to read jessie_udp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:35.840: INFO: Unable to read jessie_tcp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:35.847: INFO: Unable to read jessie_udp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:35.854: INFO: Unable to read jessie_tcp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:35.860: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:35.865: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:35.886: INFO: Lookups using dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4625 wheezy_tcp@dns-test-service.dns-4625 wheezy_udp@dns-test-service.dns-4625.svc wheezy_tcp@dns-test-service.dns-4625.svc wheezy_udp@_http._tcp.dns-test-service.dns-4625.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4625.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4625 jessie_tcp@dns-test-service.dns-4625 jessie_udp@dns-test-service.dns-4625.svc jessie_tcp@dns-test-service.dns-4625.svc jessie_udp@_http._tcp.dns-test-service.dns-4625.svc jessie_tcp@_http._tcp.dns-test-service.dns-4625.svc]

    Jan 18 15:36:40.893: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:40.898: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:40.903: INFO: Unable to read wheezy_udp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:40.908: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:40.913: INFO: Unable to read wheezy_udp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:40.918: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:40.922: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:40.926: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:40.956: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:40.962: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:40.967: INFO: Unable to read jessie_udp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:40.972: INFO: Unable to read jessie_tcp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:40.977: INFO: Unable to read jessie_udp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:40.982: INFO: Unable to read jessie_tcp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:40.988: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:40.993: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:41.013: INFO: Lookups using dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4625 wheezy_tcp@dns-test-service.dns-4625 wheezy_udp@dns-test-service.dns-4625.svc wheezy_tcp@dns-test-service.dns-4625.svc wheezy_udp@_http._tcp.dns-test-service.dns-4625.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4625.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4625 jessie_tcp@dns-test-service.dns-4625 jessie_udp@dns-test-service.dns-4625.svc jessie_tcp@dns-test-service.dns-4625.svc jessie_udp@_http._tcp.dns-test-service.dns-4625.svc jessie_tcp@_http._tcp.dns-test-service.dns-4625.svc]

    Jan 18 15:36:45.894: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:45.899: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:45.904: INFO: Unable to read wheezy_udp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:45.909: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:45.914: INFO: Unable to read wheezy_udp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:45.918: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:45.923: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:45.927: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:45.953: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:45.958: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:45.963: INFO: Unable to read jessie_udp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:45.968: INFO: Unable to read jessie_tcp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:45.973: INFO: Unable to read jessie_udp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:45.977: INFO: Unable to read jessie_tcp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:45.982: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:45.986: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:46.007: INFO: Lookups using dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4625 wheezy_tcp@dns-test-service.dns-4625 wheezy_udp@dns-test-service.dns-4625.svc wheezy_tcp@dns-test-service.dns-4625.svc wheezy_udp@_http._tcp.dns-test-service.dns-4625.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4625.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4625 jessie_tcp@dns-test-service.dns-4625 jessie_udp@dns-test-service.dns-4625.svc jessie_tcp@dns-test-service.dns-4625.svc jessie_udp@_http._tcp.dns-test-service.dns-4625.svc jessie_tcp@_http._tcp.dns-test-service.dns-4625.svc]

    Jan 18 15:36:50.894: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:50.901: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:50.906: INFO: Unable to read wheezy_udp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:50.913: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:50.917: INFO: Unable to read wheezy_udp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:50.922: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:50.926: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:50.931: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:50.959: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:50.965: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:50.975: INFO: Unable to read jessie_udp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:50.980: INFO: Unable to read jessie_tcp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:50.987: INFO: Unable to read jessie_udp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:50.997: INFO: Unable to read jessie_tcp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:51.004: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:51.015: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:51.044: INFO: Lookups using dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4625 wheezy_tcp@dns-test-service.dns-4625 wheezy_udp@dns-test-service.dns-4625.svc wheezy_tcp@dns-test-service.dns-4625.svc wheezy_udp@_http._tcp.dns-test-service.dns-4625.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4625.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4625 jessie_tcp@dns-test-service.dns-4625 jessie_udp@dns-test-service.dns-4625.svc jessie_tcp@dns-test-service.dns-4625.svc jessie_udp@_http._tcp.dns-test-service.dns-4625.svc jessie_tcp@_http._tcp.dns-test-service.dns-4625.svc]

    Jan 18 15:36:55.896: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:55.901: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:55.906: INFO: Unable to read wheezy_udp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:55.913: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:55.917: INFO: Unable to read wheezy_udp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:55.926: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:55.932: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:55.939: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:55.968: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:55.980: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:55.990: INFO: Unable to read jessie_udp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:55.995: INFO: Unable to read jessie_tcp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:56.004: INFO: Unable to read jessie_udp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:56.010: INFO: Unable to read jessie_tcp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:56.016: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:56.022: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:36:56.052: INFO: Lookups using dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4625 wheezy_tcp@dns-test-service.dns-4625 wheezy_udp@dns-test-service.dns-4625.svc wheezy_tcp@dns-test-service.dns-4625.svc wheezy_udp@_http._tcp.dns-test-service.dns-4625.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4625.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4625 jessie_tcp@dns-test-service.dns-4625 jessie_udp@dns-test-service.dns-4625.svc jessie_tcp@dns-test-service.dns-4625.svc jessie_udp@_http._tcp.dns-test-service.dns-4625.svc jessie_tcp@_http._tcp.dns-test-service.dns-4625.svc]

    Jan 18 15:37:00.896: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:37:00.903: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:37:00.910: INFO: Unable to read wheezy_udp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:37:00.915: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:37:00.941: INFO: Unable to read wheezy_udp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:37:00.950: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:37:00.956: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:37:00.961: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:37:00.988: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:37:00.993: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:37:00.998: INFO: Unable to read jessie_udp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:37:01.003: INFO: Unable to read jessie_tcp@dns-test-service.dns-4625 from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:37:01.009: INFO: Unable to read jessie_udp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:37:01.014: INFO: Unable to read jessie_tcp@dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:37:01.019: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:37:01.023: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4625.svc from pod dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24: the server could not find the requested resource (get pods dns-test-db171a53-9160-4572-9671-bec91af08e24)
    Jan 18 15:37:01.041: INFO: Lookups using dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4625 wheezy_tcp@dns-test-service.dns-4625 wheezy_udp@dns-test-service.dns-4625.svc wheezy_tcp@dns-test-service.dns-4625.svc wheezy_udp@_http._tcp.dns-test-service.dns-4625.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4625.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4625 jessie_tcp@dns-test-service.dns-4625 jessie_udp@dns-test-service.dns-4625.svc jessie_tcp@dns-test-service.dns-4625.svc jessie_udp@_http._tcp.dns-test-service.dns-4625.svc jessie_tcp@_http._tcp.dns-test-service.dns-4625.svc]

    Jan 18 15:37:06.067: INFO: DNS probes using dns-4625/dns-test-db171a53-9160-4572-9671-bec91af08e24 succeeded

    STEP: deleting the pod 01/18/23 15:37:06.067
    STEP: deleting the test service 01/18/23 15:37:06.136
    STEP: deleting the test headless service 01/18/23 15:37:06.235
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 18 15:37:06.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4625" for this suite. 01/18/23 15:37:06.263
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:37:06.277
Jan 18 15:37:06.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename gc 01/18/23 15:37:06.283
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:37:06.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:37:06.329
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 01/18/23 15:37:06.346
STEP: Wait for the Deployment to create new ReplicaSet 01/18/23 15:37:06.354
STEP: delete the deployment 01/18/23 15:37:06.378
STEP: wait for all rs to be garbage collected 01/18/23 15:37:06.407
STEP: expected 0 rs, got 1 rs 01/18/23 15:37:06.45
STEP: expected 0 pods, got 2 pods 01/18/23 15:37:06.484
STEP: Gathering metrics 01/18/23 15:37:07.032
Jan 18 15:37:07.085: INFO: Waiting up to 5m0s for pod "kube-controller-manager-v1-25-1-18760-m" in namespace "kube-system" to be "running and ready"
Jan 18 15:37:07.089: INFO: Pod "kube-controller-manager-v1-25-1-18760-m": Phase="Running", Reason="", readiness=true. Elapsed: 3.822751ms
Jan 18 15:37:07.089: INFO: The phase of Pod kube-controller-manager-v1-25-1-18760-m is Running (Ready = true)
Jan 18 15:37:07.089: INFO: Pod "kube-controller-manager-v1-25-1-18760-m" satisfied condition "running and ready"
Jan 18 15:37:07.203: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 18 15:37:07.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5195" for this suite. 01/18/23 15:37:07.21
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":111,"skipped":2018,"failed":0}
------------------------------
• [0.946 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:37:06.277
    Jan 18 15:37:06.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename gc 01/18/23 15:37:06.283
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:37:06.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:37:06.329
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 01/18/23 15:37:06.346
    STEP: Wait for the Deployment to create new ReplicaSet 01/18/23 15:37:06.354
    STEP: delete the deployment 01/18/23 15:37:06.378
    STEP: wait for all rs to be garbage collected 01/18/23 15:37:06.407
    STEP: expected 0 rs, got 1 rs 01/18/23 15:37:06.45
    STEP: expected 0 pods, got 2 pods 01/18/23 15:37:06.484
    STEP: Gathering metrics 01/18/23 15:37:07.032
    Jan 18 15:37:07.085: INFO: Waiting up to 5m0s for pod "kube-controller-manager-v1-25-1-18760-m" in namespace "kube-system" to be "running and ready"
    Jan 18 15:37:07.089: INFO: Pod "kube-controller-manager-v1-25-1-18760-m": Phase="Running", Reason="", readiness=true. Elapsed: 3.822751ms
    Jan 18 15:37:07.089: INFO: The phase of Pod kube-controller-manager-v1-25-1-18760-m is Running (Ready = true)
    Jan 18 15:37:07.089: INFO: Pod "kube-controller-manager-v1-25-1-18760-m" satisfied condition "running and ready"
    Jan 18 15:37:07.203: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 18 15:37:07.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5195" for this suite. 01/18/23 15:37:07.21
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:37:07.223
Jan 18 15:37:07.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename containers 01/18/23 15:37:07.225
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:37:07.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:37:07.248
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Jan 18 15:37:07.265: INFO: Waiting up to 5m0s for pod "client-containers-b5d679c6-ef32-4661-8740-d33db856eba7" in namespace "containers-8332" to be "running"
Jan 18 15:37:07.272: INFO: Pod "client-containers-b5d679c6-ef32-4661-8740-d33db856eba7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.850298ms
Jan 18 15:37:09.279: INFO: Pod "client-containers-b5d679c6-ef32-4661-8740-d33db856eba7": Phase="Running", Reason="", readiness=true. Elapsed: 2.013214964s
Jan 18 15:37:09.279: INFO: Pod "client-containers-b5d679c6-ef32-4661-8740-d33db856eba7" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 18 15:37:09.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8332" for this suite. 01/18/23 15:37:09.295
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":112,"skipped":2028,"failed":0}
------------------------------
• [2.099 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:37:07.223
    Jan 18 15:37:07.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename containers 01/18/23 15:37:07.225
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:37:07.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:37:07.248
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Jan 18 15:37:07.265: INFO: Waiting up to 5m0s for pod "client-containers-b5d679c6-ef32-4661-8740-d33db856eba7" in namespace "containers-8332" to be "running"
    Jan 18 15:37:07.272: INFO: Pod "client-containers-b5d679c6-ef32-4661-8740-d33db856eba7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.850298ms
    Jan 18 15:37:09.279: INFO: Pod "client-containers-b5d679c6-ef32-4661-8740-d33db856eba7": Phase="Running", Reason="", readiness=true. Elapsed: 2.013214964s
    Jan 18 15:37:09.279: INFO: Pod "client-containers-b5d679c6-ef32-4661-8740-d33db856eba7" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 18 15:37:09.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-8332" for this suite. 01/18/23 15:37:09.295
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:37:09.323
Jan 18 15:37:09.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename ephemeral-containers-test 01/18/23 15:37:09.324
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:37:09.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:37:09.351
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 01/18/23 15:37:09.359
Jan 18 15:37:09.375: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4544" to be "running and ready"
Jan 18 15:37:09.393: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.25992ms
Jan 18 15:37:09.393: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:37:11.397: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.021475329s
Jan 18 15:37:11.397: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jan 18 15:37:11.397: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 01/18/23 15:37:11.402
Jan 18 15:37:11.432: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4544" to be "container debugger running"
Jan 18 15:37:11.440: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.415182ms
Jan 18 15:37:13.447: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015269504s
Jan 18 15:37:15.453: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.021069476s
Jan 18 15:37:15.453: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 01/18/23 15:37:15.453
Jan 18 15:37:15.453: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4544 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 15:37:15.454: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 15:37:15.455: INFO: ExecWithOptions: Clientset creation
Jan 18 15:37:15.455: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-4544/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jan 18 15:37:15.549: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 18 15:37:15.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-4544" for this suite. 01/18/23 15:37:15.57
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":113,"skipped":2034,"failed":0}
------------------------------
• [SLOW TEST] [6.270 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:37:09.323
    Jan 18 15:37:09.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename ephemeral-containers-test 01/18/23 15:37:09.324
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:37:09.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:37:09.351
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 01/18/23 15:37:09.359
    Jan 18 15:37:09.375: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4544" to be "running and ready"
    Jan 18 15:37:09.393: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.25992ms
    Jan 18 15:37:09.393: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:37:11.397: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.021475329s
    Jan 18 15:37:11.397: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jan 18 15:37:11.397: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 01/18/23 15:37:11.402
    Jan 18 15:37:11.432: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4544" to be "container debugger running"
    Jan 18 15:37:11.440: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.415182ms
    Jan 18 15:37:13.447: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015269504s
    Jan 18 15:37:15.453: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.021069476s
    Jan 18 15:37:15.453: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 01/18/23 15:37:15.453
    Jan 18 15:37:15.453: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4544 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 15:37:15.454: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 15:37:15.455: INFO: ExecWithOptions: Clientset creation
    Jan 18 15:37:15.455: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-4544/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jan 18 15:37:15.549: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 18 15:37:15.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-4544" for this suite. 01/18/23 15:37:15.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:37:15.595
Jan 18 15:37:15.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename container-probe 01/18/23 15:37:15.597
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:37:15.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:37:15.633
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Jan 18 15:37:15.656: INFO: Waiting up to 5m0s for pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c" in namespace "container-probe-9027" to be "running and ready"
Jan 18 15:37:15.674: INFO: Pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.825742ms
Jan 18 15:37:15.675: INFO: The phase of Pod test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:37:17.704: INFO: Pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c": Phase="Running", Reason="", readiness=false. Elapsed: 2.048301997s
Jan 18 15:37:17.705: INFO: The phase of Pod test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c is Running (Ready = false)
Jan 18 15:37:19.680: INFO: Pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c": Phase="Running", Reason="", readiness=false. Elapsed: 4.02371011s
Jan 18 15:37:19.680: INFO: The phase of Pod test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c is Running (Ready = false)
Jan 18 15:37:21.680: INFO: Pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c": Phase="Running", Reason="", readiness=false. Elapsed: 6.023657734s
Jan 18 15:37:21.680: INFO: The phase of Pod test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c is Running (Ready = false)
Jan 18 15:37:23.683: INFO: Pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c": Phase="Running", Reason="", readiness=false. Elapsed: 8.026348952s
Jan 18 15:37:23.683: INFO: The phase of Pod test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c is Running (Ready = false)
Jan 18 15:37:25.713: INFO: Pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c": Phase="Running", Reason="", readiness=false. Elapsed: 10.057250355s
Jan 18 15:37:25.714: INFO: The phase of Pod test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c is Running (Ready = false)
Jan 18 15:37:27.680: INFO: Pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c": Phase="Running", Reason="", readiness=false. Elapsed: 12.024187651s
Jan 18 15:37:27.681: INFO: The phase of Pod test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c is Running (Ready = false)
Jan 18 15:37:29.681: INFO: Pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c": Phase="Running", Reason="", readiness=false. Elapsed: 14.024519655s
Jan 18 15:37:29.681: INFO: The phase of Pod test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c is Running (Ready = false)
Jan 18 15:37:31.680: INFO: Pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c": Phase="Running", Reason="", readiness=false. Elapsed: 16.024124361s
Jan 18 15:37:31.681: INFO: The phase of Pod test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c is Running (Ready = false)
Jan 18 15:37:33.680: INFO: Pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c": Phase="Running", Reason="", readiness=false. Elapsed: 18.024108744s
Jan 18 15:37:33.681: INFO: The phase of Pod test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c is Running (Ready = false)
Jan 18 15:37:35.682: INFO: Pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c": Phase="Running", Reason="", readiness=false. Elapsed: 20.025476959s
Jan 18 15:37:35.682: INFO: The phase of Pod test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c is Running (Ready = false)
Jan 18 15:37:37.681: INFO: Pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c": Phase="Running", Reason="", readiness=true. Elapsed: 22.024747904s
Jan 18 15:37:37.681: INFO: The phase of Pod test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c is Running (Ready = true)
Jan 18 15:37:37.681: INFO: Pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c" satisfied condition "running and ready"
Jan 18 15:37:37.687: INFO: Container started at 2023-01-18 15:37:17 +0000 UTC, pod became ready at 2023-01-18 15:37:36 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 18 15:37:37.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9027" for this suite. 01/18/23 15:37:37.693
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":114,"skipped":2049,"failed":0}
------------------------------
• [SLOW TEST] [22.106 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:37:15.595
    Jan 18 15:37:15.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename container-probe 01/18/23 15:37:15.597
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:37:15.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:37:15.633
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Jan 18 15:37:15.656: INFO: Waiting up to 5m0s for pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c" in namespace "container-probe-9027" to be "running and ready"
    Jan 18 15:37:15.674: INFO: Pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.825742ms
    Jan 18 15:37:15.675: INFO: The phase of Pod test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:37:17.704: INFO: Pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c": Phase="Running", Reason="", readiness=false. Elapsed: 2.048301997s
    Jan 18 15:37:17.705: INFO: The phase of Pod test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c is Running (Ready = false)
    Jan 18 15:37:19.680: INFO: Pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c": Phase="Running", Reason="", readiness=false. Elapsed: 4.02371011s
    Jan 18 15:37:19.680: INFO: The phase of Pod test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c is Running (Ready = false)
    Jan 18 15:37:21.680: INFO: Pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c": Phase="Running", Reason="", readiness=false. Elapsed: 6.023657734s
    Jan 18 15:37:21.680: INFO: The phase of Pod test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c is Running (Ready = false)
    Jan 18 15:37:23.683: INFO: Pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c": Phase="Running", Reason="", readiness=false. Elapsed: 8.026348952s
    Jan 18 15:37:23.683: INFO: The phase of Pod test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c is Running (Ready = false)
    Jan 18 15:37:25.713: INFO: Pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c": Phase="Running", Reason="", readiness=false. Elapsed: 10.057250355s
    Jan 18 15:37:25.714: INFO: The phase of Pod test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c is Running (Ready = false)
    Jan 18 15:37:27.680: INFO: Pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c": Phase="Running", Reason="", readiness=false. Elapsed: 12.024187651s
    Jan 18 15:37:27.681: INFO: The phase of Pod test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c is Running (Ready = false)
    Jan 18 15:37:29.681: INFO: Pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c": Phase="Running", Reason="", readiness=false. Elapsed: 14.024519655s
    Jan 18 15:37:29.681: INFO: The phase of Pod test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c is Running (Ready = false)
    Jan 18 15:37:31.680: INFO: Pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c": Phase="Running", Reason="", readiness=false. Elapsed: 16.024124361s
    Jan 18 15:37:31.681: INFO: The phase of Pod test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c is Running (Ready = false)
    Jan 18 15:37:33.680: INFO: Pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c": Phase="Running", Reason="", readiness=false. Elapsed: 18.024108744s
    Jan 18 15:37:33.681: INFO: The phase of Pod test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c is Running (Ready = false)
    Jan 18 15:37:35.682: INFO: Pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c": Phase="Running", Reason="", readiness=false. Elapsed: 20.025476959s
    Jan 18 15:37:35.682: INFO: The phase of Pod test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c is Running (Ready = false)
    Jan 18 15:37:37.681: INFO: Pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c": Phase="Running", Reason="", readiness=true. Elapsed: 22.024747904s
    Jan 18 15:37:37.681: INFO: The phase of Pod test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c is Running (Ready = true)
    Jan 18 15:37:37.681: INFO: Pod "test-webserver-c80a0f42-9bb8-4cc7-9be0-4183ec552b8c" satisfied condition "running and ready"
    Jan 18 15:37:37.687: INFO: Container started at 2023-01-18 15:37:17 +0000 UTC, pod became ready at 2023-01-18 15:37:36 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 18 15:37:37.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9027" for this suite. 01/18/23 15:37:37.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:37:37.704
Jan 18 15:37:37.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename emptydir 01/18/23 15:37:37.707
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:37:37.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:37:37.734
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/18/23 15:37:37.738
Jan 18 15:37:37.747: INFO: Waiting up to 5m0s for pod "pod-514a4b43-313b-4771-b3c3-77ed782233b9" in namespace "emptydir-1396" to be "Succeeded or Failed"
Jan 18 15:37:37.757: INFO: Pod "pod-514a4b43-313b-4771-b3c3-77ed782233b9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.660677ms
Jan 18 15:37:39.765: INFO: Pod "pod-514a4b43-313b-4771-b3c3-77ed782233b9": Phase="Running", Reason="", readiness=true. Elapsed: 2.017561341s
Jan 18 15:37:41.763: INFO: Pod "pod-514a4b43-313b-4771-b3c3-77ed782233b9": Phase="Running", Reason="", readiness=false. Elapsed: 4.016167912s
Jan 18 15:37:43.763: INFO: Pod "pod-514a4b43-313b-4771-b3c3-77ed782233b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016075408s
STEP: Saw pod success 01/18/23 15:37:43.763
Jan 18 15:37:43.763: INFO: Pod "pod-514a4b43-313b-4771-b3c3-77ed782233b9" satisfied condition "Succeeded or Failed"
Jan 18 15:37:43.768: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-514a4b43-313b-4771-b3c3-77ed782233b9 container test-container: <nil>
STEP: delete the pod 01/18/23 15:37:43.778
Jan 18 15:37:43.797: INFO: Waiting for pod pod-514a4b43-313b-4771-b3c3-77ed782233b9 to disappear
Jan 18 15:37:43.804: INFO: Pod pod-514a4b43-313b-4771-b3c3-77ed782233b9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 15:37:43.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1396" for this suite. 01/18/23 15:37:43.809
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":115,"skipped":2054,"failed":0}
------------------------------
• [SLOW TEST] [6.117 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:37:37.704
    Jan 18 15:37:37.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename emptydir 01/18/23 15:37:37.707
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:37:37.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:37:37.734
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/18/23 15:37:37.738
    Jan 18 15:37:37.747: INFO: Waiting up to 5m0s for pod "pod-514a4b43-313b-4771-b3c3-77ed782233b9" in namespace "emptydir-1396" to be "Succeeded or Failed"
    Jan 18 15:37:37.757: INFO: Pod "pod-514a4b43-313b-4771-b3c3-77ed782233b9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.660677ms
    Jan 18 15:37:39.765: INFO: Pod "pod-514a4b43-313b-4771-b3c3-77ed782233b9": Phase="Running", Reason="", readiness=true. Elapsed: 2.017561341s
    Jan 18 15:37:41.763: INFO: Pod "pod-514a4b43-313b-4771-b3c3-77ed782233b9": Phase="Running", Reason="", readiness=false. Elapsed: 4.016167912s
    Jan 18 15:37:43.763: INFO: Pod "pod-514a4b43-313b-4771-b3c3-77ed782233b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016075408s
    STEP: Saw pod success 01/18/23 15:37:43.763
    Jan 18 15:37:43.763: INFO: Pod "pod-514a4b43-313b-4771-b3c3-77ed782233b9" satisfied condition "Succeeded or Failed"
    Jan 18 15:37:43.768: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-514a4b43-313b-4771-b3c3-77ed782233b9 container test-container: <nil>
    STEP: delete the pod 01/18/23 15:37:43.778
    Jan 18 15:37:43.797: INFO: Waiting for pod pod-514a4b43-313b-4771-b3c3-77ed782233b9 to disappear
    Jan 18 15:37:43.804: INFO: Pod pod-514a4b43-313b-4771-b3c3-77ed782233b9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 15:37:43.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1396" for this suite. 01/18/23 15:37:43.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:37:43.839
Jan 18 15:37:43.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename limitrange 01/18/23 15:37:43.84
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:37:43.866
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:37:43.872
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 01/18/23 15:37:43.876
STEP: Setting up watch 01/18/23 15:37:43.876
STEP: Submitting a LimitRange 01/18/23 15:37:43.98
STEP: Verifying LimitRange creation was observed 01/18/23 15:37:43.987
STEP: Fetching the LimitRange to ensure it has proper values 01/18/23 15:37:43.991
Jan 18 15:37:43.996: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 18 15:37:43.996: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 01/18/23 15:37:43.996
STEP: Ensuring Pod has resource requirements applied from LimitRange 01/18/23 15:37:44.003
Jan 18 15:37:44.008: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 18 15:37:44.008: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 01/18/23 15:37:44.009
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/18/23 15:37:44.024
Jan 18 15:37:44.031: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan 18 15:37:44.031: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 01/18/23 15:37:44.031
STEP: Failing to create a Pod with more than max resources 01/18/23 15:37:44.037
STEP: Updating a LimitRange 01/18/23 15:37:44.041
STEP: Verifying LimitRange updating is effective 01/18/23 15:37:44.051
STEP: Creating a Pod with less than former min resources 01/18/23 15:37:46.056
STEP: Failing to create a Pod with more than max resources 01/18/23 15:37:46.064
STEP: Deleting a LimitRange 01/18/23 15:37:46.069
STEP: Verifying the LimitRange was deleted 01/18/23 15:37:46.092
Jan 18 15:37:51.099: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 01/18/23 15:37:51.099
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Jan 18 15:37:51.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-6012" for this suite. 01/18/23 15:37:51.119
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":116,"skipped":2091,"failed":0}
------------------------------
• [SLOW TEST] [7.297 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:37:43.839
    Jan 18 15:37:43.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename limitrange 01/18/23 15:37:43.84
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:37:43.866
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:37:43.872
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 01/18/23 15:37:43.876
    STEP: Setting up watch 01/18/23 15:37:43.876
    STEP: Submitting a LimitRange 01/18/23 15:37:43.98
    STEP: Verifying LimitRange creation was observed 01/18/23 15:37:43.987
    STEP: Fetching the LimitRange to ensure it has proper values 01/18/23 15:37:43.991
    Jan 18 15:37:43.996: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 18 15:37:43.996: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 01/18/23 15:37:43.996
    STEP: Ensuring Pod has resource requirements applied from LimitRange 01/18/23 15:37:44.003
    Jan 18 15:37:44.008: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 18 15:37:44.008: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 01/18/23 15:37:44.009
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/18/23 15:37:44.024
    Jan 18 15:37:44.031: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jan 18 15:37:44.031: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 01/18/23 15:37:44.031
    STEP: Failing to create a Pod with more than max resources 01/18/23 15:37:44.037
    STEP: Updating a LimitRange 01/18/23 15:37:44.041
    STEP: Verifying LimitRange updating is effective 01/18/23 15:37:44.051
    STEP: Creating a Pod with less than former min resources 01/18/23 15:37:46.056
    STEP: Failing to create a Pod with more than max resources 01/18/23 15:37:46.064
    STEP: Deleting a LimitRange 01/18/23 15:37:46.069
    STEP: Verifying the LimitRange was deleted 01/18/23 15:37:46.092
    Jan 18 15:37:51.099: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 01/18/23 15:37:51.099
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Jan 18 15:37:51.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-6012" for this suite. 01/18/23 15:37:51.119
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:37:51.141
Jan 18 15:37:51.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 15:37:51.143
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:37:51.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:37:51.19
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 01/18/23 15:37:51.195
Jan 18 15:37:51.208: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3cfe543b-c057-4236-910f-137a0fb297a9" in namespace "projected-4801" to be "Succeeded or Failed"
Jan 18 15:37:51.227: INFO: Pod "downwardapi-volume-3cfe543b-c057-4236-910f-137a0fb297a9": Phase="Pending", Reason="", readiness=false. Elapsed: 18.775355ms
Jan 18 15:37:53.232: INFO: Pod "downwardapi-volume-3cfe543b-c057-4236-910f-137a0fb297a9": Phase="Running", Reason="", readiness=true. Elapsed: 2.024079312s
Jan 18 15:37:55.233: INFO: Pod "downwardapi-volume-3cfe543b-c057-4236-910f-137a0fb297a9": Phase="Running", Reason="", readiness=false. Elapsed: 4.024535943s
Jan 18 15:37:57.233: INFO: Pod "downwardapi-volume-3cfe543b-c057-4236-910f-137a0fb297a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024987343s
STEP: Saw pod success 01/18/23 15:37:57.233
Jan 18 15:37:57.234: INFO: Pod "downwardapi-volume-3cfe543b-c057-4236-910f-137a0fb297a9" satisfied condition "Succeeded or Failed"
Jan 18 15:37:57.237: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-3cfe543b-c057-4236-910f-137a0fb297a9 container client-container: <nil>
STEP: delete the pod 01/18/23 15:37:57.247
Jan 18 15:37:57.286: INFO: Waiting for pod downwardapi-volume-3cfe543b-c057-4236-910f-137a0fb297a9 to disappear
Jan 18 15:37:57.292: INFO: Pod downwardapi-volume-3cfe543b-c057-4236-910f-137a0fb297a9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 15:37:57.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4801" for this suite. 01/18/23 15:37:57.298
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":117,"skipped":2106,"failed":0}
------------------------------
• [SLOW TEST] [6.165 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:37:51.141
    Jan 18 15:37:51.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 15:37:51.143
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:37:51.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:37:51.19
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 01/18/23 15:37:51.195
    Jan 18 15:37:51.208: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3cfe543b-c057-4236-910f-137a0fb297a9" in namespace "projected-4801" to be "Succeeded or Failed"
    Jan 18 15:37:51.227: INFO: Pod "downwardapi-volume-3cfe543b-c057-4236-910f-137a0fb297a9": Phase="Pending", Reason="", readiness=false. Elapsed: 18.775355ms
    Jan 18 15:37:53.232: INFO: Pod "downwardapi-volume-3cfe543b-c057-4236-910f-137a0fb297a9": Phase="Running", Reason="", readiness=true. Elapsed: 2.024079312s
    Jan 18 15:37:55.233: INFO: Pod "downwardapi-volume-3cfe543b-c057-4236-910f-137a0fb297a9": Phase="Running", Reason="", readiness=false. Elapsed: 4.024535943s
    Jan 18 15:37:57.233: INFO: Pod "downwardapi-volume-3cfe543b-c057-4236-910f-137a0fb297a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024987343s
    STEP: Saw pod success 01/18/23 15:37:57.233
    Jan 18 15:37:57.234: INFO: Pod "downwardapi-volume-3cfe543b-c057-4236-910f-137a0fb297a9" satisfied condition "Succeeded or Failed"
    Jan 18 15:37:57.237: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-3cfe543b-c057-4236-910f-137a0fb297a9 container client-container: <nil>
    STEP: delete the pod 01/18/23 15:37:57.247
    Jan 18 15:37:57.286: INFO: Waiting for pod downwardapi-volume-3cfe543b-c057-4236-910f-137a0fb297a9 to disappear
    Jan 18 15:37:57.292: INFO: Pod downwardapi-volume-3cfe543b-c057-4236-910f-137a0fb297a9 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 15:37:57.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4801" for this suite. 01/18/23 15:37:57.298
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:37:57.31
Jan 18 15:37:57.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename gc 01/18/23 15:37:57.313
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:37:57.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:37:57.462
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 01/18/23 15:37:57.468
STEP: delete the rc 01/18/23 15:38:02.481
STEP: wait for all pods to be garbage collected 01/18/23 15:38:02.506
STEP: Gathering metrics 01/18/23 15:38:07.515
Jan 18 15:38:07.548: INFO: Waiting up to 5m0s for pod "kube-controller-manager-v1-25-1-18760-m" in namespace "kube-system" to be "running and ready"
Jan 18 15:38:07.552: INFO: Pod "kube-controller-manager-v1-25-1-18760-m": Phase="Running", Reason="", readiness=true. Elapsed: 4.194226ms
Jan 18 15:38:07.552: INFO: The phase of Pod kube-controller-manager-v1-25-1-18760-m is Running (Ready = true)
Jan 18 15:38:07.553: INFO: Pod "kube-controller-manager-v1-25-1-18760-m" satisfied condition "running and ready"
Jan 18 15:38:07.649: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 18 15:38:07.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2204" for this suite. 01/18/23 15:38:07.655
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":118,"skipped":2120,"failed":0}
------------------------------
• [SLOW TEST] [10.356 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:37:57.31
    Jan 18 15:37:57.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename gc 01/18/23 15:37:57.313
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:37:57.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:37:57.462
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 01/18/23 15:37:57.468
    STEP: delete the rc 01/18/23 15:38:02.481
    STEP: wait for all pods to be garbage collected 01/18/23 15:38:02.506
    STEP: Gathering metrics 01/18/23 15:38:07.515
    Jan 18 15:38:07.548: INFO: Waiting up to 5m0s for pod "kube-controller-manager-v1-25-1-18760-m" in namespace "kube-system" to be "running and ready"
    Jan 18 15:38:07.552: INFO: Pod "kube-controller-manager-v1-25-1-18760-m": Phase="Running", Reason="", readiness=true. Elapsed: 4.194226ms
    Jan 18 15:38:07.552: INFO: The phase of Pod kube-controller-manager-v1-25-1-18760-m is Running (Ready = true)
    Jan 18 15:38:07.553: INFO: Pod "kube-controller-manager-v1-25-1-18760-m" satisfied condition "running and ready"
    Jan 18 15:38:07.649: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 18 15:38:07.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2204" for this suite. 01/18/23 15:38:07.655
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:38:07.668
Jan 18 15:38:07.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename subpath 01/18/23 15:38:07.67
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:38:07.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:38:07.743
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/18/23 15:38:07.746
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-gcfc 01/18/23 15:38:07.755
STEP: Creating a pod to test atomic-volume-subpath 01/18/23 15:38:07.755
Jan 18 15:38:07.764: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-gcfc" in namespace "subpath-8568" to be "Succeeded or Failed"
Jan 18 15:38:07.787: INFO: Pod "pod-subpath-test-downwardapi-gcfc": Phase="Pending", Reason="", readiness=false. Elapsed: 22.366176ms
Jan 18 15:38:09.800: INFO: Pod "pod-subpath-test-downwardapi-gcfc": Phase="Running", Reason="", readiness=true. Elapsed: 2.035136531s
Jan 18 15:38:11.794: INFO: Pod "pod-subpath-test-downwardapi-gcfc": Phase="Running", Reason="", readiness=true. Elapsed: 4.029061043s
Jan 18 15:38:13.818: INFO: Pod "pod-subpath-test-downwardapi-gcfc": Phase="Running", Reason="", readiness=true. Elapsed: 6.053957044s
Jan 18 15:38:15.798: INFO: Pod "pod-subpath-test-downwardapi-gcfc": Phase="Running", Reason="", readiness=true. Elapsed: 8.033089301s
Jan 18 15:38:17.793: INFO: Pod "pod-subpath-test-downwardapi-gcfc": Phase="Running", Reason="", readiness=true. Elapsed: 10.028262486s
Jan 18 15:38:19.793: INFO: Pod "pod-subpath-test-downwardapi-gcfc": Phase="Running", Reason="", readiness=true. Elapsed: 12.028098438s
Jan 18 15:38:21.792: INFO: Pod "pod-subpath-test-downwardapi-gcfc": Phase="Running", Reason="", readiness=true. Elapsed: 14.027995854s
Jan 18 15:38:23.795: INFO: Pod "pod-subpath-test-downwardapi-gcfc": Phase="Running", Reason="", readiness=true. Elapsed: 16.030143046s
Jan 18 15:38:25.792: INFO: Pod "pod-subpath-test-downwardapi-gcfc": Phase="Running", Reason="", readiness=true. Elapsed: 18.027701673s
Jan 18 15:38:27.793: INFO: Pod "pod-subpath-test-downwardapi-gcfc": Phase="Running", Reason="", readiness=true. Elapsed: 20.028167263s
Jan 18 15:38:29.794: INFO: Pod "pod-subpath-test-downwardapi-gcfc": Phase="Running", Reason="", readiness=false. Elapsed: 22.029194186s
Jan 18 15:38:31.793: INFO: Pod "pod-subpath-test-downwardapi-gcfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.028793178s
STEP: Saw pod success 01/18/23 15:38:31.793
Jan 18 15:38:31.795: INFO: Pod "pod-subpath-test-downwardapi-gcfc" satisfied condition "Succeeded or Failed"
Jan 18 15:38:31.799: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-subpath-test-downwardapi-gcfc container test-container-subpath-downwardapi-gcfc: <nil>
STEP: delete the pod 01/18/23 15:38:31.807
Jan 18 15:38:31.864: INFO: Waiting for pod pod-subpath-test-downwardapi-gcfc to disappear
Jan 18 15:38:31.869: INFO: Pod pod-subpath-test-downwardapi-gcfc no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-gcfc 01/18/23 15:38:31.87
Jan 18 15:38:31.870: INFO: Deleting pod "pod-subpath-test-downwardapi-gcfc" in namespace "subpath-8568"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 18 15:38:31.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8568" for this suite. 01/18/23 15:38:31.881
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":119,"skipped":2130,"failed":0}
------------------------------
• [SLOW TEST] [24.226 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:38:07.668
    Jan 18 15:38:07.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename subpath 01/18/23 15:38:07.67
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:38:07.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:38:07.743
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/18/23 15:38:07.746
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-gcfc 01/18/23 15:38:07.755
    STEP: Creating a pod to test atomic-volume-subpath 01/18/23 15:38:07.755
    Jan 18 15:38:07.764: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-gcfc" in namespace "subpath-8568" to be "Succeeded or Failed"
    Jan 18 15:38:07.787: INFO: Pod "pod-subpath-test-downwardapi-gcfc": Phase="Pending", Reason="", readiness=false. Elapsed: 22.366176ms
    Jan 18 15:38:09.800: INFO: Pod "pod-subpath-test-downwardapi-gcfc": Phase="Running", Reason="", readiness=true. Elapsed: 2.035136531s
    Jan 18 15:38:11.794: INFO: Pod "pod-subpath-test-downwardapi-gcfc": Phase="Running", Reason="", readiness=true. Elapsed: 4.029061043s
    Jan 18 15:38:13.818: INFO: Pod "pod-subpath-test-downwardapi-gcfc": Phase="Running", Reason="", readiness=true. Elapsed: 6.053957044s
    Jan 18 15:38:15.798: INFO: Pod "pod-subpath-test-downwardapi-gcfc": Phase="Running", Reason="", readiness=true. Elapsed: 8.033089301s
    Jan 18 15:38:17.793: INFO: Pod "pod-subpath-test-downwardapi-gcfc": Phase="Running", Reason="", readiness=true. Elapsed: 10.028262486s
    Jan 18 15:38:19.793: INFO: Pod "pod-subpath-test-downwardapi-gcfc": Phase="Running", Reason="", readiness=true. Elapsed: 12.028098438s
    Jan 18 15:38:21.792: INFO: Pod "pod-subpath-test-downwardapi-gcfc": Phase="Running", Reason="", readiness=true. Elapsed: 14.027995854s
    Jan 18 15:38:23.795: INFO: Pod "pod-subpath-test-downwardapi-gcfc": Phase="Running", Reason="", readiness=true. Elapsed: 16.030143046s
    Jan 18 15:38:25.792: INFO: Pod "pod-subpath-test-downwardapi-gcfc": Phase="Running", Reason="", readiness=true. Elapsed: 18.027701673s
    Jan 18 15:38:27.793: INFO: Pod "pod-subpath-test-downwardapi-gcfc": Phase="Running", Reason="", readiness=true. Elapsed: 20.028167263s
    Jan 18 15:38:29.794: INFO: Pod "pod-subpath-test-downwardapi-gcfc": Phase="Running", Reason="", readiness=false. Elapsed: 22.029194186s
    Jan 18 15:38:31.793: INFO: Pod "pod-subpath-test-downwardapi-gcfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.028793178s
    STEP: Saw pod success 01/18/23 15:38:31.793
    Jan 18 15:38:31.795: INFO: Pod "pod-subpath-test-downwardapi-gcfc" satisfied condition "Succeeded or Failed"
    Jan 18 15:38:31.799: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-subpath-test-downwardapi-gcfc container test-container-subpath-downwardapi-gcfc: <nil>
    STEP: delete the pod 01/18/23 15:38:31.807
    Jan 18 15:38:31.864: INFO: Waiting for pod pod-subpath-test-downwardapi-gcfc to disappear
    Jan 18 15:38:31.869: INFO: Pod pod-subpath-test-downwardapi-gcfc no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-gcfc 01/18/23 15:38:31.87
    Jan 18 15:38:31.870: INFO: Deleting pod "pod-subpath-test-downwardapi-gcfc" in namespace "subpath-8568"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 18 15:38:31.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-8568" for this suite. 01/18/23 15:38:31.881
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:38:31.897
Jan 18 15:38:31.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 15:38:31.899
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:38:31.918
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:38:31.923
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 01/18/23 15:38:31.943
Jan 18 15:38:31.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: mark a version not serverd 01/18/23 15:38:43.477
STEP: check the unserved version gets removed 01/18/23 15:38:43.522
STEP: check the other version is not changed 01/18/23 15:38:48.156
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 15:38:57.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5005" for this suite. 01/18/23 15:38:57.735
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":120,"skipped":2136,"failed":0}
------------------------------
• [SLOW TEST] [25.850 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:38:31.897
    Jan 18 15:38:31.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 15:38:31.899
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:38:31.918
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:38:31.923
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 01/18/23 15:38:31.943
    Jan 18 15:38:31.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: mark a version not serverd 01/18/23 15:38:43.477
    STEP: check the unserved version gets removed 01/18/23 15:38:43.522
    STEP: check the other version is not changed 01/18/23 15:38:48.156
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 15:38:57.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5005" for this suite. 01/18/23 15:38:57.735
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:38:57.747
Jan 18 15:38:57.748: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename proxy 01/18/23 15:38:57.75
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:38:57.776
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:38:57.782
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jan 18 15:38:57.793: INFO: Creating pod...
Jan 18 15:38:57.803: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4667" to be "running"
Jan 18 15:38:57.915: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 112.127869ms
Jan 18 15:38:59.921: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.118462805s
Jan 18 15:38:59.921: INFO: Pod "agnhost" satisfied condition "running"
Jan 18 15:38:59.921: INFO: Creating service...
Jan 18 15:38:59.936: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/pods/agnhost/proxy/some/path/with/DELETE
Jan 18 15:38:59.996: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 18 15:38:59.996: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/pods/agnhost/proxy/some/path/with/GET
Jan 18 15:39:00.010: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 18 15:39:00.010: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/pods/agnhost/proxy/some/path/with/HEAD
Jan 18 15:39:00.014: INFO: http.Client request:HEAD | StatusCode:200
Jan 18 15:39:00.014: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/pods/agnhost/proxy/some/path/with/OPTIONS
Jan 18 15:39:00.024: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 18 15:39:00.024: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/pods/agnhost/proxy/some/path/with/PATCH
Jan 18 15:39:00.029: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 18 15:39:00.029: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/pods/agnhost/proxy/some/path/with/POST
Jan 18 15:39:00.034: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 18 15:39:00.034: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/pods/agnhost/proxy/some/path/with/PUT
Jan 18 15:39:00.039: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 18 15:39:00.039: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/services/test-service/proxy/some/path/with/DELETE
Jan 18 15:39:00.045: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 18 15:39:00.045: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/services/test-service/proxy/some/path/with/GET
Jan 18 15:39:00.051: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 18 15:39:00.052: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/services/test-service/proxy/some/path/with/HEAD
Jan 18 15:39:00.058: INFO: http.Client request:HEAD | StatusCode:200
Jan 18 15:39:00.058: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/services/test-service/proxy/some/path/with/OPTIONS
Jan 18 15:39:00.067: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 18 15:39:00.067: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/services/test-service/proxy/some/path/with/PATCH
Jan 18 15:39:00.076: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 18 15:39:00.076: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/services/test-service/proxy/some/path/with/POST
Jan 18 15:39:00.082: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 18 15:39:00.082: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/services/test-service/proxy/some/path/with/PUT
Jan 18 15:39:00.088: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan 18 15:39:00.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4667" for this suite. 01/18/23 15:39:00.117
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":121,"skipped":2137,"failed":0}
------------------------------
• [2.376 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:38:57.747
    Jan 18 15:38:57.748: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename proxy 01/18/23 15:38:57.75
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:38:57.776
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:38:57.782
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jan 18 15:38:57.793: INFO: Creating pod...
    Jan 18 15:38:57.803: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4667" to be "running"
    Jan 18 15:38:57.915: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 112.127869ms
    Jan 18 15:38:59.921: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.118462805s
    Jan 18 15:38:59.921: INFO: Pod "agnhost" satisfied condition "running"
    Jan 18 15:38:59.921: INFO: Creating service...
    Jan 18 15:38:59.936: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/pods/agnhost/proxy/some/path/with/DELETE
    Jan 18 15:38:59.996: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 18 15:38:59.996: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/pods/agnhost/proxy/some/path/with/GET
    Jan 18 15:39:00.010: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 18 15:39:00.010: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/pods/agnhost/proxy/some/path/with/HEAD
    Jan 18 15:39:00.014: INFO: http.Client request:HEAD | StatusCode:200
    Jan 18 15:39:00.014: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/pods/agnhost/proxy/some/path/with/OPTIONS
    Jan 18 15:39:00.024: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 18 15:39:00.024: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/pods/agnhost/proxy/some/path/with/PATCH
    Jan 18 15:39:00.029: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 18 15:39:00.029: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/pods/agnhost/proxy/some/path/with/POST
    Jan 18 15:39:00.034: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 18 15:39:00.034: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/pods/agnhost/proxy/some/path/with/PUT
    Jan 18 15:39:00.039: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 18 15:39:00.039: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/services/test-service/proxy/some/path/with/DELETE
    Jan 18 15:39:00.045: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 18 15:39:00.045: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/services/test-service/proxy/some/path/with/GET
    Jan 18 15:39:00.051: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 18 15:39:00.052: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/services/test-service/proxy/some/path/with/HEAD
    Jan 18 15:39:00.058: INFO: http.Client request:HEAD | StatusCode:200
    Jan 18 15:39:00.058: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/services/test-service/proxy/some/path/with/OPTIONS
    Jan 18 15:39:00.067: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 18 15:39:00.067: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/services/test-service/proxy/some/path/with/PATCH
    Jan 18 15:39:00.076: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 18 15:39:00.076: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/services/test-service/proxy/some/path/with/POST
    Jan 18 15:39:00.082: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 18 15:39:00.082: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4667/services/test-service/proxy/some/path/with/PUT
    Jan 18 15:39:00.088: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan 18 15:39:00.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-4667" for this suite. 01/18/23 15:39:00.117
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:39:00.133
Jan 18 15:39:00.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename events 01/18/23 15:39:00.136
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:39:00.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:39:00.161
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 01/18/23 15:39:00.167
STEP: listing all events in all namespaces 01/18/23 15:39:00.172
STEP: patching the test event 01/18/23 15:39:00.182
STEP: fetching the test event 01/18/23 15:39:00.188
STEP: updating the test event 01/18/23 15:39:00.191
STEP: getting the test event 01/18/23 15:39:00.2
STEP: deleting the test event 01/18/23 15:39:00.203
STEP: listing all events in all namespaces 01/18/23 15:39:00.209
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jan 18 15:39:00.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4599" for this suite. 01/18/23 15:39:00.27
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":122,"skipped":2149,"failed":0}
------------------------------
• [0.148 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:39:00.133
    Jan 18 15:39:00.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename events 01/18/23 15:39:00.136
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:39:00.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:39:00.161
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 01/18/23 15:39:00.167
    STEP: listing all events in all namespaces 01/18/23 15:39:00.172
    STEP: patching the test event 01/18/23 15:39:00.182
    STEP: fetching the test event 01/18/23 15:39:00.188
    STEP: updating the test event 01/18/23 15:39:00.191
    STEP: getting the test event 01/18/23 15:39:00.2
    STEP: deleting the test event 01/18/23 15:39:00.203
    STEP: listing all events in all namespaces 01/18/23 15:39:00.209
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jan 18 15:39:00.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-4599" for this suite. 01/18/23 15:39:00.27
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:39:00.282
Jan 18 15:39:00.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename services 01/18/23 15:39:00.284
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:39:00.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:39:00.312
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-8432 01/18/23 15:39:00.316
STEP: creating replication controller nodeport-test in namespace services-8432 01/18/23 15:39:00.35
I0118 15:39:00.417992      19 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-8432, replica count: 2
I0118 15:39:03.469274      19 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 15:39:03.469: INFO: Creating new exec pod
Jan 18 15:39:03.477: INFO: Waiting up to 5m0s for pod "execpods8k22" in namespace "services-8432" to be "running"
Jan 18 15:39:03.483: INFO: Pod "execpods8k22": Phase="Pending", Reason="", readiness=false. Elapsed: 5.955901ms
Jan 18 15:39:05.488: INFO: Pod "execpods8k22": Phase="Running", Reason="", readiness=true. Elapsed: 2.011180847s
Jan 18 15:39:05.489: INFO: Pod "execpods8k22" satisfied condition "running"
Jan 18 15:39:06.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-8432 exec execpods8k22 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 18 15:39:06.744: INFO: stderr: "+ + ncecho -v -t hostName -w\n 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 18 15:39:06.744: INFO: stdout: ""
Jan 18 15:39:07.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-8432 exec execpods8k22 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 18 15:39:07.964: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 18 15:39:07.964: INFO: stdout: "nodeport-test-dcksz"
Jan 18 15:39:07.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-8432 exec execpods8k22 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.43.181 80'
Jan 18 15:39:08.165: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.43.181 80\nConnection to 10.233.43.181 80 port [tcp/http] succeeded!\n"
Jan 18 15:39:08.165: INFO: stdout: ""
Jan 18 15:39:09.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-8432 exec execpods8k22 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.43.181 80'
Jan 18 15:39:09.378: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.43.181 80\nConnection to 10.233.43.181 80 port [tcp/http] succeeded!\n"
Jan 18 15:39:09.379: INFO: stdout: "nodeport-test-dcksz"
Jan 18 15:39:09.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-8432 exec execpods8k22 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.168 30800'
Jan 18 15:39:09.605: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.168 30800\nConnection to 192.168.101.168 30800 port [tcp/*] succeeded!\n"
Jan 18 15:39:09.605: INFO: stdout: "nodeport-test-x6gqg"
Jan 18 15:39:09.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-8432 exec execpods8k22 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 30800'
Jan 18 15:39:09.800: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.216 30800\nConnection to 192.168.101.216 30800 port [tcp/*] succeeded!\n"
Jan 18 15:39:09.800: INFO: stdout: ""
Jan 18 15:39:10.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-8432 exec execpods8k22 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 30800'
Jan 18 15:39:10.996: INFO: stderr: "+ nc -v -t -w 2 192.168.101.216 30800\nConnection to 192.168.101.216 30800 port [tcp/*] succeeded!\n+ echo hostName\n"
Jan 18 15:39:10.996: INFO: stdout: "nodeport-test-dcksz"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 15:39:10.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8432" for this suite. 01/18/23 15:39:11.001
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":123,"skipped":2151,"failed":0}
------------------------------
• [SLOW TEST] [10.725 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:39:00.282
    Jan 18 15:39:00.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename services 01/18/23 15:39:00.284
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:39:00.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:39:00.312
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-8432 01/18/23 15:39:00.316
    STEP: creating replication controller nodeport-test in namespace services-8432 01/18/23 15:39:00.35
    I0118 15:39:00.417992      19 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-8432, replica count: 2
    I0118 15:39:03.469274      19 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 15:39:03.469: INFO: Creating new exec pod
    Jan 18 15:39:03.477: INFO: Waiting up to 5m0s for pod "execpods8k22" in namespace "services-8432" to be "running"
    Jan 18 15:39:03.483: INFO: Pod "execpods8k22": Phase="Pending", Reason="", readiness=false. Elapsed: 5.955901ms
    Jan 18 15:39:05.488: INFO: Pod "execpods8k22": Phase="Running", Reason="", readiness=true. Elapsed: 2.011180847s
    Jan 18 15:39:05.489: INFO: Pod "execpods8k22" satisfied condition "running"
    Jan 18 15:39:06.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-8432 exec execpods8k22 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 18 15:39:06.744: INFO: stderr: "+ + ncecho -v -t hostName -w\n 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 18 15:39:06.744: INFO: stdout: ""
    Jan 18 15:39:07.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-8432 exec execpods8k22 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 18 15:39:07.964: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 18 15:39:07.964: INFO: stdout: "nodeport-test-dcksz"
    Jan 18 15:39:07.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-8432 exec execpods8k22 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.43.181 80'
    Jan 18 15:39:08.165: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.43.181 80\nConnection to 10.233.43.181 80 port [tcp/http] succeeded!\n"
    Jan 18 15:39:08.165: INFO: stdout: ""
    Jan 18 15:39:09.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-8432 exec execpods8k22 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.43.181 80'
    Jan 18 15:39:09.378: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.43.181 80\nConnection to 10.233.43.181 80 port [tcp/http] succeeded!\n"
    Jan 18 15:39:09.379: INFO: stdout: "nodeport-test-dcksz"
    Jan 18 15:39:09.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-8432 exec execpods8k22 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.168 30800'
    Jan 18 15:39:09.605: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.168 30800\nConnection to 192.168.101.168 30800 port [tcp/*] succeeded!\n"
    Jan 18 15:39:09.605: INFO: stdout: "nodeport-test-x6gqg"
    Jan 18 15:39:09.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-8432 exec execpods8k22 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 30800'
    Jan 18 15:39:09.800: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.216 30800\nConnection to 192.168.101.216 30800 port [tcp/*] succeeded!\n"
    Jan 18 15:39:09.800: INFO: stdout: ""
    Jan 18 15:39:10.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-8432 exec execpods8k22 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 30800'
    Jan 18 15:39:10.996: INFO: stderr: "+ nc -v -t -w 2 192.168.101.216 30800\nConnection to 192.168.101.216 30800 port [tcp/*] succeeded!\n+ echo hostName\n"
    Jan 18 15:39:10.996: INFO: stdout: "nodeport-test-dcksz"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 15:39:10.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8432" for this suite. 01/18/23 15:39:11.001
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:39:11.009
Jan 18 15:39:11.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename configmap 01/18/23 15:39:11.01
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:39:11.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:39:11.039
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-9870dd1c-db5c-47c8-bdef-4fa63b71cfd6 01/18/23 15:39:11.051
STEP: Creating the pod 01/18/23 15:39:11.056
Jan 18 15:39:11.072: INFO: Waiting up to 5m0s for pod "pod-configmaps-0117d0ae-6197-4283-adf6-ce1e20901bfe" in namespace "configmap-6285" to be "running and ready"
Jan 18 15:39:11.087: INFO: Pod "pod-configmaps-0117d0ae-6197-4283-adf6-ce1e20901bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 15.242463ms
Jan 18 15:39:11.087: INFO: The phase of Pod pod-configmaps-0117d0ae-6197-4283-adf6-ce1e20901bfe is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:39:13.094: INFO: Pod "pod-configmaps-0117d0ae-6197-4283-adf6-ce1e20901bfe": Phase="Running", Reason="", readiness=true. Elapsed: 2.021861269s
Jan 18 15:39:13.094: INFO: The phase of Pod pod-configmaps-0117d0ae-6197-4283-adf6-ce1e20901bfe is Running (Ready = true)
Jan 18 15:39:13.094: INFO: Pod "pod-configmaps-0117d0ae-6197-4283-adf6-ce1e20901bfe" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-9870dd1c-db5c-47c8-bdef-4fa63b71cfd6 01/18/23 15:39:13.109
STEP: waiting to observe update in volume 01/18/23 15:39:13.117
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 15:39:15.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6285" for this suite. 01/18/23 15:39:15.143
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":124,"skipped":2168,"failed":0}
------------------------------
• [4.141 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:39:11.009
    Jan 18 15:39:11.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename configmap 01/18/23 15:39:11.01
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:39:11.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:39:11.039
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-9870dd1c-db5c-47c8-bdef-4fa63b71cfd6 01/18/23 15:39:11.051
    STEP: Creating the pod 01/18/23 15:39:11.056
    Jan 18 15:39:11.072: INFO: Waiting up to 5m0s for pod "pod-configmaps-0117d0ae-6197-4283-adf6-ce1e20901bfe" in namespace "configmap-6285" to be "running and ready"
    Jan 18 15:39:11.087: INFO: Pod "pod-configmaps-0117d0ae-6197-4283-adf6-ce1e20901bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 15.242463ms
    Jan 18 15:39:11.087: INFO: The phase of Pod pod-configmaps-0117d0ae-6197-4283-adf6-ce1e20901bfe is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:39:13.094: INFO: Pod "pod-configmaps-0117d0ae-6197-4283-adf6-ce1e20901bfe": Phase="Running", Reason="", readiness=true. Elapsed: 2.021861269s
    Jan 18 15:39:13.094: INFO: The phase of Pod pod-configmaps-0117d0ae-6197-4283-adf6-ce1e20901bfe is Running (Ready = true)
    Jan 18 15:39:13.094: INFO: Pod "pod-configmaps-0117d0ae-6197-4283-adf6-ce1e20901bfe" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-9870dd1c-db5c-47c8-bdef-4fa63b71cfd6 01/18/23 15:39:13.109
    STEP: waiting to observe update in volume 01/18/23 15:39:13.117
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 15:39:15.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6285" for this suite. 01/18/23 15:39:15.143
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:39:15.154
Jan 18 15:39:15.154: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename secrets 01/18/23 15:39:15.156
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:39:15.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:39:15.182
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-e24b3db5-0c95-47eb-92d6-456d336d5408 01/18/23 15:39:15.187
STEP: Creating a pod to test consume secrets 01/18/23 15:39:15.193
Jan 18 15:39:15.213: INFO: Waiting up to 5m0s for pod "pod-secrets-bb86bcdb-b610-4114-bc57-daefd7642328" in namespace "secrets-5275" to be "Succeeded or Failed"
Jan 18 15:39:15.218: INFO: Pod "pod-secrets-bb86bcdb-b610-4114-bc57-daefd7642328": Phase="Pending", Reason="", readiness=false. Elapsed: 5.343925ms
Jan 18 15:39:17.224: INFO: Pod "pod-secrets-bb86bcdb-b610-4114-bc57-daefd7642328": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011716792s
Jan 18 15:39:19.227: INFO: Pod "pod-secrets-bb86bcdb-b610-4114-bc57-daefd7642328": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014541314s
Jan 18 15:39:21.457: INFO: Pod "pod-secrets-bb86bcdb-b610-4114-bc57-daefd7642328": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.244520162s
STEP: Saw pod success 01/18/23 15:39:21.457
Jan 18 15:39:21.458: INFO: Pod "pod-secrets-bb86bcdb-b610-4114-bc57-daefd7642328" satisfied condition "Succeeded or Failed"
Jan 18 15:39:21.469: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-bb86bcdb-b610-4114-bc57-daefd7642328 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 15:39:21.48
Jan 18 15:39:21.717: INFO: Waiting for pod pod-secrets-bb86bcdb-b610-4114-bc57-daefd7642328 to disappear
Jan 18 15:39:21.725: INFO: Pod pod-secrets-bb86bcdb-b610-4114-bc57-daefd7642328 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 18 15:39:21.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5275" for this suite. 01/18/23 15:39:21.74
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":125,"skipped":2200,"failed":0}
------------------------------
• [SLOW TEST] [6.594 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:39:15.154
    Jan 18 15:39:15.154: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename secrets 01/18/23 15:39:15.156
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:39:15.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:39:15.182
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-e24b3db5-0c95-47eb-92d6-456d336d5408 01/18/23 15:39:15.187
    STEP: Creating a pod to test consume secrets 01/18/23 15:39:15.193
    Jan 18 15:39:15.213: INFO: Waiting up to 5m0s for pod "pod-secrets-bb86bcdb-b610-4114-bc57-daefd7642328" in namespace "secrets-5275" to be "Succeeded or Failed"
    Jan 18 15:39:15.218: INFO: Pod "pod-secrets-bb86bcdb-b610-4114-bc57-daefd7642328": Phase="Pending", Reason="", readiness=false. Elapsed: 5.343925ms
    Jan 18 15:39:17.224: INFO: Pod "pod-secrets-bb86bcdb-b610-4114-bc57-daefd7642328": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011716792s
    Jan 18 15:39:19.227: INFO: Pod "pod-secrets-bb86bcdb-b610-4114-bc57-daefd7642328": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014541314s
    Jan 18 15:39:21.457: INFO: Pod "pod-secrets-bb86bcdb-b610-4114-bc57-daefd7642328": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.244520162s
    STEP: Saw pod success 01/18/23 15:39:21.457
    Jan 18 15:39:21.458: INFO: Pod "pod-secrets-bb86bcdb-b610-4114-bc57-daefd7642328" satisfied condition "Succeeded or Failed"
    Jan 18 15:39:21.469: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-bb86bcdb-b610-4114-bc57-daefd7642328 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 15:39:21.48
    Jan 18 15:39:21.717: INFO: Waiting for pod pod-secrets-bb86bcdb-b610-4114-bc57-daefd7642328 to disappear
    Jan 18 15:39:21.725: INFO: Pod pod-secrets-bb86bcdb-b610-4114-bc57-daefd7642328 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 15:39:21.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5275" for this suite. 01/18/23 15:39:21.74
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:39:21.749
Jan 18 15:39:21.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename secrets 01/18/23 15:39:21.751
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:39:21.773
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:39:21.778
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-3526/secret-test-929be6ce-a388-4962-9cdc-d43ee9056c86 01/18/23 15:39:21.784
STEP: Creating a pod to test consume secrets 01/18/23 15:39:21.789
Jan 18 15:39:21.799: INFO: Waiting up to 5m0s for pod "pod-configmaps-042a9fc3-cc9f-4960-9730-9272dc107dbd" in namespace "secrets-3526" to be "Succeeded or Failed"
Jan 18 15:39:21.804: INFO: Pod "pod-configmaps-042a9fc3-cc9f-4960-9730-9272dc107dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.123791ms
Jan 18 15:39:23.818: INFO: Pod "pod-configmaps-042a9fc3-cc9f-4960-9730-9272dc107dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018654747s
Jan 18 15:39:25.819: INFO: Pod "pod-configmaps-042a9fc3-cc9f-4960-9730-9272dc107dbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018763929s
STEP: Saw pod success 01/18/23 15:39:25.819
Jan 18 15:39:25.820: INFO: Pod "pod-configmaps-042a9fc3-cc9f-4960-9730-9272dc107dbd" satisfied condition "Succeeded or Failed"
Jan 18 15:39:25.823: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-042a9fc3-cc9f-4960-9730-9272dc107dbd container env-test: <nil>
STEP: delete the pod 01/18/23 15:39:25.831
Jan 18 15:39:25.850: INFO: Waiting for pod pod-configmaps-042a9fc3-cc9f-4960-9730-9272dc107dbd to disappear
Jan 18 15:39:25.854: INFO: Pod pod-configmaps-042a9fc3-cc9f-4960-9730-9272dc107dbd no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 18 15:39:25.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3526" for this suite. 01/18/23 15:39:25.86
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":126,"skipped":2219,"failed":0}
------------------------------
• [4.118 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:39:21.749
    Jan 18 15:39:21.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename secrets 01/18/23 15:39:21.751
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:39:21.773
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:39:21.778
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-3526/secret-test-929be6ce-a388-4962-9cdc-d43ee9056c86 01/18/23 15:39:21.784
    STEP: Creating a pod to test consume secrets 01/18/23 15:39:21.789
    Jan 18 15:39:21.799: INFO: Waiting up to 5m0s for pod "pod-configmaps-042a9fc3-cc9f-4960-9730-9272dc107dbd" in namespace "secrets-3526" to be "Succeeded or Failed"
    Jan 18 15:39:21.804: INFO: Pod "pod-configmaps-042a9fc3-cc9f-4960-9730-9272dc107dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.123791ms
    Jan 18 15:39:23.818: INFO: Pod "pod-configmaps-042a9fc3-cc9f-4960-9730-9272dc107dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018654747s
    Jan 18 15:39:25.819: INFO: Pod "pod-configmaps-042a9fc3-cc9f-4960-9730-9272dc107dbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018763929s
    STEP: Saw pod success 01/18/23 15:39:25.819
    Jan 18 15:39:25.820: INFO: Pod "pod-configmaps-042a9fc3-cc9f-4960-9730-9272dc107dbd" satisfied condition "Succeeded or Failed"
    Jan 18 15:39:25.823: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-042a9fc3-cc9f-4960-9730-9272dc107dbd container env-test: <nil>
    STEP: delete the pod 01/18/23 15:39:25.831
    Jan 18 15:39:25.850: INFO: Waiting for pod pod-configmaps-042a9fc3-cc9f-4960-9730-9272dc107dbd to disappear
    Jan 18 15:39:25.854: INFO: Pod pod-configmaps-042a9fc3-cc9f-4960-9730-9272dc107dbd no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 15:39:25.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3526" for this suite. 01/18/23 15:39:25.86
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:39:25.869
Jan 18 15:39:25.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename container-runtime 01/18/23 15:39:25.871
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:39:25.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:39:25.896
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 01/18/23 15:39:25.9
STEP: wait for the container to reach Succeeded 01/18/23 15:39:25.909
STEP: get the container status 01/18/23 15:39:29.947
STEP: the container should be terminated 01/18/23 15:39:29.951
STEP: the termination message should be set 01/18/23 15:39:29.951
Jan 18 15:39:29.952: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 01/18/23 15:39:29.952
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 18 15:39:29.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5457" for this suite. 01/18/23 15:39:29.979
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":127,"skipped":2239,"failed":0}
------------------------------
• [4.117 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:39:25.869
    Jan 18 15:39:25.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename container-runtime 01/18/23 15:39:25.871
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:39:25.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:39:25.896
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 01/18/23 15:39:25.9
    STEP: wait for the container to reach Succeeded 01/18/23 15:39:25.909
    STEP: get the container status 01/18/23 15:39:29.947
    STEP: the container should be terminated 01/18/23 15:39:29.951
    STEP: the termination message should be set 01/18/23 15:39:29.951
    Jan 18 15:39:29.952: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 01/18/23 15:39:29.952
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 18 15:39:29.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-5457" for this suite. 01/18/23 15:39:29.979
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:39:29.993
Jan 18 15:39:29.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename container-probe 01/18/23 15:39:29.995
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:39:30.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:39:30.026
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-3d2121b7-27af-4c86-886b-141e1299cebd in namespace container-probe-5799 01/18/23 15:39:30.033
Jan 18 15:39:30.047: INFO: Waiting up to 5m0s for pod "liveness-3d2121b7-27af-4c86-886b-141e1299cebd" in namespace "container-probe-5799" to be "not pending"
Jan 18 15:39:30.065: INFO: Pod "liveness-3d2121b7-27af-4c86-886b-141e1299cebd": Phase="Pending", Reason="", readiness=false. Elapsed: 18.063134ms
Jan 18 15:39:32.071: INFO: Pod "liveness-3d2121b7-27af-4c86-886b-141e1299cebd": Phase="Running", Reason="", readiness=true. Elapsed: 2.023975869s
Jan 18 15:39:32.071: INFO: Pod "liveness-3d2121b7-27af-4c86-886b-141e1299cebd" satisfied condition "not pending"
Jan 18 15:39:32.071: INFO: Started pod liveness-3d2121b7-27af-4c86-886b-141e1299cebd in namespace container-probe-5799
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 15:39:32.071
Jan 18 15:39:32.076: INFO: Initial restart count of pod liveness-3d2121b7-27af-4c86-886b-141e1299cebd is 0
Jan 18 15:39:52.141: INFO: Restart count of pod container-probe-5799/liveness-3d2121b7-27af-4c86-886b-141e1299cebd is now 1 (20.064777756s elapsed)
Jan 18 15:40:12.198: INFO: Restart count of pod container-probe-5799/liveness-3d2121b7-27af-4c86-886b-141e1299cebd is now 2 (40.122001415s elapsed)
Jan 18 15:40:32.275: INFO: Restart count of pod container-probe-5799/liveness-3d2121b7-27af-4c86-886b-141e1299cebd is now 3 (1m0.199043597s elapsed)
Jan 18 15:40:52.333: INFO: Restart count of pod container-probe-5799/liveness-3d2121b7-27af-4c86-886b-141e1299cebd is now 4 (1m20.257047327s elapsed)
Jan 18 15:41:52.611: INFO: Restart count of pod container-probe-5799/liveness-3d2121b7-27af-4c86-886b-141e1299cebd is now 5 (2m20.534522851s elapsed)
STEP: deleting the pod 01/18/23 15:41:52.611
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 18 15:41:52.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5799" for this suite. 01/18/23 15:41:52.649
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":128,"skipped":2275,"failed":0}
------------------------------
• [SLOW TEST] [142.674 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:39:29.993
    Jan 18 15:39:29.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename container-probe 01/18/23 15:39:29.995
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:39:30.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:39:30.026
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-3d2121b7-27af-4c86-886b-141e1299cebd in namespace container-probe-5799 01/18/23 15:39:30.033
    Jan 18 15:39:30.047: INFO: Waiting up to 5m0s for pod "liveness-3d2121b7-27af-4c86-886b-141e1299cebd" in namespace "container-probe-5799" to be "not pending"
    Jan 18 15:39:30.065: INFO: Pod "liveness-3d2121b7-27af-4c86-886b-141e1299cebd": Phase="Pending", Reason="", readiness=false. Elapsed: 18.063134ms
    Jan 18 15:39:32.071: INFO: Pod "liveness-3d2121b7-27af-4c86-886b-141e1299cebd": Phase="Running", Reason="", readiness=true. Elapsed: 2.023975869s
    Jan 18 15:39:32.071: INFO: Pod "liveness-3d2121b7-27af-4c86-886b-141e1299cebd" satisfied condition "not pending"
    Jan 18 15:39:32.071: INFO: Started pod liveness-3d2121b7-27af-4c86-886b-141e1299cebd in namespace container-probe-5799
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 15:39:32.071
    Jan 18 15:39:32.076: INFO: Initial restart count of pod liveness-3d2121b7-27af-4c86-886b-141e1299cebd is 0
    Jan 18 15:39:52.141: INFO: Restart count of pod container-probe-5799/liveness-3d2121b7-27af-4c86-886b-141e1299cebd is now 1 (20.064777756s elapsed)
    Jan 18 15:40:12.198: INFO: Restart count of pod container-probe-5799/liveness-3d2121b7-27af-4c86-886b-141e1299cebd is now 2 (40.122001415s elapsed)
    Jan 18 15:40:32.275: INFO: Restart count of pod container-probe-5799/liveness-3d2121b7-27af-4c86-886b-141e1299cebd is now 3 (1m0.199043597s elapsed)
    Jan 18 15:40:52.333: INFO: Restart count of pod container-probe-5799/liveness-3d2121b7-27af-4c86-886b-141e1299cebd is now 4 (1m20.257047327s elapsed)
    Jan 18 15:41:52.611: INFO: Restart count of pod container-probe-5799/liveness-3d2121b7-27af-4c86-886b-141e1299cebd is now 5 (2m20.534522851s elapsed)
    STEP: deleting the pod 01/18/23 15:41:52.611
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 18 15:41:52.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5799" for this suite. 01/18/23 15:41:52.649
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:41:52.67
Jan 18 15:41:52.670: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename services 01/18/23 15:41:52.674
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:41:52.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:41:52.721
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5847 01/18/23 15:41:52.726
STEP: changing the ExternalName service to type=ClusterIP 01/18/23 15:41:52.741
STEP: creating replication controller externalname-service in namespace services-5847 01/18/23 15:41:52.765
I0118 15:41:52.776130      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5847, replica count: 2
I0118 15:41:55.827099      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 15:41:55.827: INFO: Creating new exec pod
Jan 18 15:41:55.835: INFO: Waiting up to 5m0s for pod "execpodwfwzt" in namespace "services-5847" to be "running"
Jan 18 15:41:55.845: INFO: Pod "execpodwfwzt": Phase="Pending", Reason="", readiness=false. Elapsed: 9.993625ms
Jan 18 15:41:57.851: INFO: Pod "execpodwfwzt": Phase="Running", Reason="", readiness=true. Elapsed: 2.015910608s
Jan 18 15:41:57.851: INFO: Pod "execpodwfwzt" satisfied condition "running"
Jan 18 15:41:58.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-5847 exec execpodwfwzt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 18 15:41:59.115: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 18 15:41:59.115: INFO: stdout: ""
Jan 18 15:42:00.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-5847 exec execpodwfwzt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 18 15:42:00.327: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Jan 18 15:42:00.327: INFO: stdout: "externalname-service-wp4bd"
Jan 18 15:42:00.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-5847 exec execpodwfwzt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.61.242 80'
Jan 18 15:42:00.530: INFO: stderr: "+ nc -v -t -w 2 10.233.61.242 80\nConnection to 10.233.61.242 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Jan 18 15:42:00.530: INFO: stdout: ""
Jan 18 15:42:01.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-5847 exec execpodwfwzt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.61.242 80'
Jan 18 15:42:01.727: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.61.242 80\nConnection to 10.233.61.242 80 port [tcp/http] succeeded!\n"
Jan 18 15:42:01.727: INFO: stdout: ""
Jan 18 15:42:02.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-5847 exec execpodwfwzt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.61.242 80'
Jan 18 15:42:02.717: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.61.242 80\nConnection to 10.233.61.242 80 port [tcp/http] succeeded!\n"
Jan 18 15:42:02.717: INFO: stdout: "externalname-service-m595t"
Jan 18 15:42:02.717: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 15:42:02.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5847" for this suite. 01/18/23 15:42:02.779
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":129,"skipped":2288,"failed":0}
------------------------------
• [SLOW TEST] [10.118 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:41:52.67
    Jan 18 15:41:52.670: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename services 01/18/23 15:41:52.674
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:41:52.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:41:52.721
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-5847 01/18/23 15:41:52.726
    STEP: changing the ExternalName service to type=ClusterIP 01/18/23 15:41:52.741
    STEP: creating replication controller externalname-service in namespace services-5847 01/18/23 15:41:52.765
    I0118 15:41:52.776130      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5847, replica count: 2
    I0118 15:41:55.827099      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 15:41:55.827: INFO: Creating new exec pod
    Jan 18 15:41:55.835: INFO: Waiting up to 5m0s for pod "execpodwfwzt" in namespace "services-5847" to be "running"
    Jan 18 15:41:55.845: INFO: Pod "execpodwfwzt": Phase="Pending", Reason="", readiness=false. Elapsed: 9.993625ms
    Jan 18 15:41:57.851: INFO: Pod "execpodwfwzt": Phase="Running", Reason="", readiness=true. Elapsed: 2.015910608s
    Jan 18 15:41:57.851: INFO: Pod "execpodwfwzt" satisfied condition "running"
    Jan 18 15:41:58.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-5847 exec execpodwfwzt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 18 15:41:59.115: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 18 15:41:59.115: INFO: stdout: ""
    Jan 18 15:42:00.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-5847 exec execpodwfwzt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 18 15:42:00.327: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n+ echo hostName\n"
    Jan 18 15:42:00.327: INFO: stdout: "externalname-service-wp4bd"
    Jan 18 15:42:00.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-5847 exec execpodwfwzt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.61.242 80'
    Jan 18 15:42:00.530: INFO: stderr: "+ nc -v -t -w 2 10.233.61.242 80\nConnection to 10.233.61.242 80 port [tcp/http] succeeded!\n+ echo hostName\n"
    Jan 18 15:42:00.530: INFO: stdout: ""
    Jan 18 15:42:01.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-5847 exec execpodwfwzt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.61.242 80'
    Jan 18 15:42:01.727: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.61.242 80\nConnection to 10.233.61.242 80 port [tcp/http] succeeded!\n"
    Jan 18 15:42:01.727: INFO: stdout: ""
    Jan 18 15:42:02.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-5847 exec execpodwfwzt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.61.242 80'
    Jan 18 15:42:02.717: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.61.242 80\nConnection to 10.233.61.242 80 port [tcp/http] succeeded!\n"
    Jan 18 15:42:02.717: INFO: stdout: "externalname-service-m595t"
    Jan 18 15:42:02.717: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 15:42:02.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5847" for this suite. 01/18/23 15:42:02.779
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:42:02.789
Jan 18 15:42:02.789: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename disruption 01/18/23 15:42:02.791
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:42:02.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:42:02.818
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:42:02.824
Jan 18 15:42:02.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename disruption-2 01/18/23 15:42:02.826
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:42:02.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:42:02.867
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 01/18/23 15:42:02.886
STEP: Waiting for the pdb to be processed 01/18/23 15:42:02.906
STEP: Waiting for the pdb to be processed 01/18/23 15:42:04.933
STEP: listing a collection of PDBs across all namespaces 01/18/23 15:42:04.939
STEP: listing a collection of PDBs in namespace disruption-4941 01/18/23 15:42:04.945
STEP: deleting a collection of PDBs 01/18/23 15:42:04.949
STEP: Waiting for the PDB collection to be deleted 01/18/23 15:42:04.961
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Jan 18 15:42:04.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-1946" for this suite. 01/18/23 15:42:04.97
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 18 15:42:04.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4941" for this suite. 01/18/23 15:42:04.982
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":130,"skipped":2292,"failed":0}
------------------------------
• [2.201 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:42:02.789
    Jan 18 15:42:02.789: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename disruption 01/18/23 15:42:02.791
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:42:02.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:42:02.818
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:42:02.824
    Jan 18 15:42:02.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename disruption-2 01/18/23 15:42:02.826
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:42:02.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:42:02.867
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 01/18/23 15:42:02.886
    STEP: Waiting for the pdb to be processed 01/18/23 15:42:02.906
    STEP: Waiting for the pdb to be processed 01/18/23 15:42:04.933
    STEP: listing a collection of PDBs across all namespaces 01/18/23 15:42:04.939
    STEP: listing a collection of PDBs in namespace disruption-4941 01/18/23 15:42:04.945
    STEP: deleting a collection of PDBs 01/18/23 15:42:04.949
    STEP: Waiting for the PDB collection to be deleted 01/18/23 15:42:04.961
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Jan 18 15:42:04.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-1946" for this suite. 01/18/23 15:42:04.97
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 18 15:42:04.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-4941" for this suite. 01/18/23 15:42:04.982
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:42:04.999
Jan 18 15:42:05.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename pod-network-test 01/18/23 15:42:05.002
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:42:05.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:42:05.032
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-1576 01/18/23 15:42:05.037
STEP: creating a selector 01/18/23 15:42:05.037
STEP: Creating the service pods in kubernetes 01/18/23 15:42:05.038
Jan 18 15:42:05.038: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 18 15:42:05.066: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1576" to be "running and ready"
Jan 18 15:42:05.075: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.935149ms
Jan 18 15:42:05.075: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:42:07.080: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013560031s
Jan 18 15:42:07.080: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:42:09.105: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.038161986s
Jan 18 15:42:09.105: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:42:11.080: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01354439s
Jan 18 15:42:11.080: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:42:13.087: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.020685167s
Jan 18 15:42:13.087: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:42:15.080: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.01376147s
Jan 18 15:42:15.081: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:42:17.081: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.013898668s
Jan 18 15:42:17.081: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:42:19.080: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.013545292s
Jan 18 15:42:19.080: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:42:21.081: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.014473054s
Jan 18 15:42:21.081: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:42:23.082: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.015236379s
Jan 18 15:42:23.082: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:42:25.079: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.012474174s
Jan 18 15:42:25.079: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 15:42:27.081: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014294463s
Jan 18 15:42:27.081: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 18 15:42:27.082: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 18 15:42:27.087: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1576" to be "running and ready"
Jan 18 15:42:27.095: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 7.922736ms
Jan 18 15:42:27.095: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 18 15:42:27.095: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/18/23 15:42:27.1
Jan 18 15:42:27.109: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1576" to be "running"
Jan 18 15:42:27.115: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.337799ms
Jan 18 15:42:29.121: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011830444s
Jan 18 15:42:29.121: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 18 15:42:29.131: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 18 15:42:29.131: INFO: Breadth first check of 10.233.78.74 on host 192.168.101.168...
Jan 18 15:42:29.137: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.68.193:9080/dial?request=hostname&protocol=http&host=10.233.78.74&port=8083&tries=1'] Namespace:pod-network-test-1576 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 15:42:29.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 15:42:29.141: INFO: ExecWithOptions: Clientset creation
Jan 18 15:42:29.141: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1576/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.68.193%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.78.74%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 15:42:29.260: INFO: Waiting for responses: map[]
Jan 18 15:42:29.260: INFO: reached 10.233.78.74 after 0/1 tries
Jan 18 15:42:29.260: INFO: Breadth first check of 10.233.68.192 on host 192.168.101.216...
Jan 18 15:42:29.264: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.68.193:9080/dial?request=hostname&protocol=http&host=10.233.68.192&port=8083&tries=1'] Namespace:pod-network-test-1576 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 15:42:29.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 15:42:29.265: INFO: ExecWithOptions: Clientset creation
Jan 18 15:42:29.265: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1576/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.68.193%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.68.192%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 15:42:29.381: INFO: Waiting for responses: map[]
Jan 18 15:42:29.381: INFO: reached 10.233.68.192 after 0/1 tries
Jan 18 15:42:29.381: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 18 15:42:29.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1576" for this suite. 01/18/23 15:42:29.388
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":131,"skipped":2303,"failed":0}
------------------------------
• [SLOW TEST] [24.401 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:42:04.999
    Jan 18 15:42:05.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename pod-network-test 01/18/23 15:42:05.002
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:42:05.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:42:05.032
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-1576 01/18/23 15:42:05.037
    STEP: creating a selector 01/18/23 15:42:05.037
    STEP: Creating the service pods in kubernetes 01/18/23 15:42:05.038
    Jan 18 15:42:05.038: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 18 15:42:05.066: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1576" to be "running and ready"
    Jan 18 15:42:05.075: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.935149ms
    Jan 18 15:42:05.075: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:42:07.080: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013560031s
    Jan 18 15:42:07.080: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:42:09.105: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.038161986s
    Jan 18 15:42:09.105: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:42:11.080: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01354439s
    Jan 18 15:42:11.080: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:42:13.087: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.020685167s
    Jan 18 15:42:13.087: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:42:15.080: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.01376147s
    Jan 18 15:42:15.081: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:42:17.081: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.013898668s
    Jan 18 15:42:17.081: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:42:19.080: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.013545292s
    Jan 18 15:42:19.080: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:42:21.081: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.014473054s
    Jan 18 15:42:21.081: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:42:23.082: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.015236379s
    Jan 18 15:42:23.082: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:42:25.079: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.012474174s
    Jan 18 15:42:25.079: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 15:42:27.081: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014294463s
    Jan 18 15:42:27.081: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 18 15:42:27.082: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 18 15:42:27.087: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1576" to be "running and ready"
    Jan 18 15:42:27.095: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 7.922736ms
    Jan 18 15:42:27.095: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 18 15:42:27.095: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/18/23 15:42:27.1
    Jan 18 15:42:27.109: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1576" to be "running"
    Jan 18 15:42:27.115: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.337799ms
    Jan 18 15:42:29.121: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011830444s
    Jan 18 15:42:29.121: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 18 15:42:29.131: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 18 15:42:29.131: INFO: Breadth first check of 10.233.78.74 on host 192.168.101.168...
    Jan 18 15:42:29.137: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.68.193:9080/dial?request=hostname&protocol=http&host=10.233.78.74&port=8083&tries=1'] Namespace:pod-network-test-1576 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 15:42:29.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 15:42:29.141: INFO: ExecWithOptions: Clientset creation
    Jan 18 15:42:29.141: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1576/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.68.193%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.78.74%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 18 15:42:29.260: INFO: Waiting for responses: map[]
    Jan 18 15:42:29.260: INFO: reached 10.233.78.74 after 0/1 tries
    Jan 18 15:42:29.260: INFO: Breadth first check of 10.233.68.192 on host 192.168.101.216...
    Jan 18 15:42:29.264: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.68.193:9080/dial?request=hostname&protocol=http&host=10.233.68.192&port=8083&tries=1'] Namespace:pod-network-test-1576 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 15:42:29.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 15:42:29.265: INFO: ExecWithOptions: Clientset creation
    Jan 18 15:42:29.265: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1576/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.68.193%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.68.192%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 18 15:42:29.381: INFO: Waiting for responses: map[]
    Jan 18 15:42:29.381: INFO: reached 10.233.68.192 after 0/1 tries
    Jan 18 15:42:29.381: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 18 15:42:29.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-1576" for this suite. 01/18/23 15:42:29.388
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:42:29.401
Jan 18 15:42:29.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename configmap 01/18/23 15:42:29.406
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:42:29.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:42:29.436
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-19c64b40-615a-4946-8fa4-c0ba00bc9b62 01/18/23 15:42:29.447
STEP: Creating configMap with name cm-test-opt-upd-c9e948cc-9e20-497e-9a6c-6e3c99bc741a 01/18/23 15:42:29.452
STEP: Creating the pod 01/18/23 15:42:29.459
Jan 18 15:42:29.472: INFO: Waiting up to 5m0s for pod "pod-configmaps-f35ef401-d8ca-47ca-9d96-4df55a83a8ef" in namespace "configmap-7360" to be "running and ready"
Jan 18 15:42:29.495: INFO: Pod "pod-configmaps-f35ef401-d8ca-47ca-9d96-4df55a83a8ef": Phase="Pending", Reason="", readiness=false. Elapsed: 22.245479ms
Jan 18 15:42:29.495: INFO: The phase of Pod pod-configmaps-f35ef401-d8ca-47ca-9d96-4df55a83a8ef is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:42:31.502: INFO: Pod "pod-configmaps-f35ef401-d8ca-47ca-9d96-4df55a83a8ef": Phase="Running", Reason="", readiness=true. Elapsed: 2.029940157s
Jan 18 15:42:31.502: INFO: The phase of Pod pod-configmaps-f35ef401-d8ca-47ca-9d96-4df55a83a8ef is Running (Ready = true)
Jan 18 15:42:31.502: INFO: Pod "pod-configmaps-f35ef401-d8ca-47ca-9d96-4df55a83a8ef" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-19c64b40-615a-4946-8fa4-c0ba00bc9b62 01/18/23 15:42:31.541
STEP: Updating configmap cm-test-opt-upd-c9e948cc-9e20-497e-9a6c-6e3c99bc741a 01/18/23 15:42:31.548
STEP: Creating configMap with name cm-test-opt-create-3bbfecdd-1843-45bd-b991-0d165453bd35 01/18/23 15:42:31.558
STEP: waiting to observe update in volume 01/18/23 15:42:31.563
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 15:42:33.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7360" for this suite. 01/18/23 15:42:33.599
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":132,"skipped":2311,"failed":0}
------------------------------
• [4.213 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:42:29.401
    Jan 18 15:42:29.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename configmap 01/18/23 15:42:29.406
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:42:29.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:42:29.436
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-19c64b40-615a-4946-8fa4-c0ba00bc9b62 01/18/23 15:42:29.447
    STEP: Creating configMap with name cm-test-opt-upd-c9e948cc-9e20-497e-9a6c-6e3c99bc741a 01/18/23 15:42:29.452
    STEP: Creating the pod 01/18/23 15:42:29.459
    Jan 18 15:42:29.472: INFO: Waiting up to 5m0s for pod "pod-configmaps-f35ef401-d8ca-47ca-9d96-4df55a83a8ef" in namespace "configmap-7360" to be "running and ready"
    Jan 18 15:42:29.495: INFO: Pod "pod-configmaps-f35ef401-d8ca-47ca-9d96-4df55a83a8ef": Phase="Pending", Reason="", readiness=false. Elapsed: 22.245479ms
    Jan 18 15:42:29.495: INFO: The phase of Pod pod-configmaps-f35ef401-d8ca-47ca-9d96-4df55a83a8ef is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:42:31.502: INFO: Pod "pod-configmaps-f35ef401-d8ca-47ca-9d96-4df55a83a8ef": Phase="Running", Reason="", readiness=true. Elapsed: 2.029940157s
    Jan 18 15:42:31.502: INFO: The phase of Pod pod-configmaps-f35ef401-d8ca-47ca-9d96-4df55a83a8ef is Running (Ready = true)
    Jan 18 15:42:31.502: INFO: Pod "pod-configmaps-f35ef401-d8ca-47ca-9d96-4df55a83a8ef" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-19c64b40-615a-4946-8fa4-c0ba00bc9b62 01/18/23 15:42:31.541
    STEP: Updating configmap cm-test-opt-upd-c9e948cc-9e20-497e-9a6c-6e3c99bc741a 01/18/23 15:42:31.548
    STEP: Creating configMap with name cm-test-opt-create-3bbfecdd-1843-45bd-b991-0d165453bd35 01/18/23 15:42:31.558
    STEP: waiting to observe update in volume 01/18/23 15:42:31.563
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 15:42:33.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7360" for this suite. 01/18/23 15:42:33.599
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:42:33.615
Jan 18 15:42:33.615: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename replicaset 01/18/23 15:42:33.616
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:42:33.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:42:33.64
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jan 18 15:42:33.671: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 18 15:42:38.684: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 15:42:38.684
STEP: Scaling up "test-rs" replicaset  01/18/23 15:42:38.684
Jan 18 15:42:38.710: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 01/18/23 15:42:38.71
W0118 15:42:38.752839      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 18 15:42:38.787: INFO: observed ReplicaSet test-rs in namespace replicaset-1591 with ReadyReplicas 1, AvailableReplicas 1
Jan 18 15:42:38.823: INFO: observed ReplicaSet test-rs in namespace replicaset-1591 with ReadyReplicas 1, AvailableReplicas 1
Jan 18 15:42:38.858: INFO: observed ReplicaSet test-rs in namespace replicaset-1591 with ReadyReplicas 1, AvailableReplicas 1
Jan 18 15:42:38.870: INFO: observed ReplicaSet test-rs in namespace replicaset-1591 with ReadyReplicas 1, AvailableReplicas 1
Jan 18 15:42:40.243: INFO: observed ReplicaSet test-rs in namespace replicaset-1591 with ReadyReplicas 2, AvailableReplicas 2
Jan 18 15:42:40.525: INFO: observed Replicaset test-rs in namespace replicaset-1591 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 18 15:42:40.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1591" for this suite. 01/18/23 15:42:40.532
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":133,"skipped":2311,"failed":0}
------------------------------
• [SLOW TEST] [6.925 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:42:33.615
    Jan 18 15:42:33.615: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename replicaset 01/18/23 15:42:33.616
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:42:33.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:42:33.64
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jan 18 15:42:33.671: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 18 15:42:38.684: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 15:42:38.684
    STEP: Scaling up "test-rs" replicaset  01/18/23 15:42:38.684
    Jan 18 15:42:38.710: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 01/18/23 15:42:38.71
    W0118 15:42:38.752839      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 18 15:42:38.787: INFO: observed ReplicaSet test-rs in namespace replicaset-1591 with ReadyReplicas 1, AvailableReplicas 1
    Jan 18 15:42:38.823: INFO: observed ReplicaSet test-rs in namespace replicaset-1591 with ReadyReplicas 1, AvailableReplicas 1
    Jan 18 15:42:38.858: INFO: observed ReplicaSet test-rs in namespace replicaset-1591 with ReadyReplicas 1, AvailableReplicas 1
    Jan 18 15:42:38.870: INFO: observed ReplicaSet test-rs in namespace replicaset-1591 with ReadyReplicas 1, AvailableReplicas 1
    Jan 18 15:42:40.243: INFO: observed ReplicaSet test-rs in namespace replicaset-1591 with ReadyReplicas 2, AvailableReplicas 2
    Jan 18 15:42:40.525: INFO: observed Replicaset test-rs in namespace replicaset-1591 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 18 15:42:40.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-1591" for this suite. 01/18/23 15:42:40.532
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:42:40.543
Jan 18 15:42:40.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename pods 01/18/23 15:42:40.545
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:42:40.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:42:40.569
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 01/18/23 15:42:40.576
STEP: submitting the pod to kubernetes 01/18/23 15:42:40.577
Jan 18 15:42:40.587: INFO: Waiting up to 5m0s for pod "pod-update-452421ef-f1e9-4e7e-966a-977c91d371de" in namespace "pods-5743" to be "running and ready"
Jan 18 15:42:40.591: INFO: Pod "pod-update-452421ef-f1e9-4e7e-966a-977c91d371de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.653731ms
Jan 18 15:42:40.592: INFO: The phase of Pod pod-update-452421ef-f1e9-4e7e-966a-977c91d371de is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:42:42.597: INFO: Pod "pod-update-452421ef-f1e9-4e7e-966a-977c91d371de": Phase="Running", Reason="", readiness=true. Elapsed: 2.010627486s
Jan 18 15:42:42.597: INFO: The phase of Pod pod-update-452421ef-f1e9-4e7e-966a-977c91d371de is Running (Ready = true)
Jan 18 15:42:42.597: INFO: Pod "pod-update-452421ef-f1e9-4e7e-966a-977c91d371de" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/18/23 15:42:42.601
STEP: updating the pod 01/18/23 15:42:42.606
Jan 18 15:42:43.121: INFO: Successfully updated pod "pod-update-452421ef-f1e9-4e7e-966a-977c91d371de"
Jan 18 15:42:43.121: INFO: Waiting up to 5m0s for pod "pod-update-452421ef-f1e9-4e7e-966a-977c91d371de" in namespace "pods-5743" to be "running"
Jan 18 15:42:43.138: INFO: Pod "pod-update-452421ef-f1e9-4e7e-966a-977c91d371de": Phase="Running", Reason="", readiness=true. Elapsed: 17.132159ms
Jan 18 15:42:43.138: INFO: Pod "pod-update-452421ef-f1e9-4e7e-966a-977c91d371de" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 01/18/23 15:42:43.138
Jan 18 15:42:43.143: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 15:42:43.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5743" for this suite. 01/18/23 15:42:43.149
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":134,"skipped":2327,"failed":0}
------------------------------
• [2.614 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:42:40.543
    Jan 18 15:42:40.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename pods 01/18/23 15:42:40.545
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:42:40.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:42:40.569
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 01/18/23 15:42:40.576
    STEP: submitting the pod to kubernetes 01/18/23 15:42:40.577
    Jan 18 15:42:40.587: INFO: Waiting up to 5m0s for pod "pod-update-452421ef-f1e9-4e7e-966a-977c91d371de" in namespace "pods-5743" to be "running and ready"
    Jan 18 15:42:40.591: INFO: Pod "pod-update-452421ef-f1e9-4e7e-966a-977c91d371de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.653731ms
    Jan 18 15:42:40.592: INFO: The phase of Pod pod-update-452421ef-f1e9-4e7e-966a-977c91d371de is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:42:42.597: INFO: Pod "pod-update-452421ef-f1e9-4e7e-966a-977c91d371de": Phase="Running", Reason="", readiness=true. Elapsed: 2.010627486s
    Jan 18 15:42:42.597: INFO: The phase of Pod pod-update-452421ef-f1e9-4e7e-966a-977c91d371de is Running (Ready = true)
    Jan 18 15:42:42.597: INFO: Pod "pod-update-452421ef-f1e9-4e7e-966a-977c91d371de" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/18/23 15:42:42.601
    STEP: updating the pod 01/18/23 15:42:42.606
    Jan 18 15:42:43.121: INFO: Successfully updated pod "pod-update-452421ef-f1e9-4e7e-966a-977c91d371de"
    Jan 18 15:42:43.121: INFO: Waiting up to 5m0s for pod "pod-update-452421ef-f1e9-4e7e-966a-977c91d371de" in namespace "pods-5743" to be "running"
    Jan 18 15:42:43.138: INFO: Pod "pod-update-452421ef-f1e9-4e7e-966a-977c91d371de": Phase="Running", Reason="", readiness=true. Elapsed: 17.132159ms
    Jan 18 15:42:43.138: INFO: Pod "pod-update-452421ef-f1e9-4e7e-966a-977c91d371de" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 01/18/23 15:42:43.138
    Jan 18 15:42:43.143: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 15:42:43.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5743" for this suite. 01/18/23 15:42:43.149
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:42:43.166
Jan 18 15:42:43.166: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename configmap 01/18/23 15:42:43.17
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:42:43.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:42:43.198
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-5451db47-4ce5-42fb-9b8e-e1c600e946c3 01/18/23 15:42:43.206
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 15:42:43.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4533" for this suite. 01/18/23 15:42:43.22
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":135,"skipped":2355,"failed":0}
------------------------------
• [0.063 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:42:43.166
    Jan 18 15:42:43.166: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename configmap 01/18/23 15:42:43.17
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:42:43.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:42:43.198
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-5451db47-4ce5-42fb-9b8e-e1c600e946c3 01/18/23 15:42:43.206
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 15:42:43.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4533" for this suite. 01/18/23 15:42:43.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:42:43.233
Jan 18 15:42:43.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 15:42:43.235
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:42:43.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:42:43.278
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-972f76da-d1c6-4667-b672-ea34234a94c3 01/18/23 15:42:43.285
STEP: Creating a pod to test consume secrets 01/18/23 15:42:43.294
Jan 18 15:42:43.307: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-efde512f-4790-4c8a-a19e-691752282619" in namespace "projected-9471" to be "Succeeded or Failed"
Jan 18 15:42:43.325: INFO: Pod "pod-projected-secrets-efde512f-4790-4c8a-a19e-691752282619": Phase="Pending", Reason="", readiness=false. Elapsed: 16.859535ms
Jan 18 15:42:45.331: INFO: Pod "pod-projected-secrets-efde512f-4790-4c8a-a19e-691752282619": Phase="Running", Reason="", readiness=true. Elapsed: 2.022487758s
Jan 18 15:42:47.331: INFO: Pod "pod-projected-secrets-efde512f-4790-4c8a-a19e-691752282619": Phase="Running", Reason="", readiness=false. Elapsed: 4.022480343s
Jan 18 15:42:49.330: INFO: Pod "pod-projected-secrets-efde512f-4790-4c8a-a19e-691752282619": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021562176s
STEP: Saw pod success 01/18/23 15:42:49.33
Jan 18 15:42:49.330: INFO: Pod "pod-projected-secrets-efde512f-4790-4c8a-a19e-691752282619" satisfied condition "Succeeded or Failed"
Jan 18 15:42:49.334: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-secrets-efde512f-4790-4c8a-a19e-691752282619 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/18/23 15:42:49.341
Jan 18 15:42:49.362: INFO: Waiting for pod pod-projected-secrets-efde512f-4790-4c8a-a19e-691752282619 to disappear
Jan 18 15:42:49.366: INFO: Pod pod-projected-secrets-efde512f-4790-4c8a-a19e-691752282619 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 18 15:42:49.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9471" for this suite. 01/18/23 15:42:49.371
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":136,"skipped":2368,"failed":0}
------------------------------
• [SLOW TEST] [6.154 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:42:43.233
    Jan 18 15:42:43.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 15:42:43.235
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:42:43.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:42:43.278
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-972f76da-d1c6-4667-b672-ea34234a94c3 01/18/23 15:42:43.285
    STEP: Creating a pod to test consume secrets 01/18/23 15:42:43.294
    Jan 18 15:42:43.307: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-efde512f-4790-4c8a-a19e-691752282619" in namespace "projected-9471" to be "Succeeded or Failed"
    Jan 18 15:42:43.325: INFO: Pod "pod-projected-secrets-efde512f-4790-4c8a-a19e-691752282619": Phase="Pending", Reason="", readiness=false. Elapsed: 16.859535ms
    Jan 18 15:42:45.331: INFO: Pod "pod-projected-secrets-efde512f-4790-4c8a-a19e-691752282619": Phase="Running", Reason="", readiness=true. Elapsed: 2.022487758s
    Jan 18 15:42:47.331: INFO: Pod "pod-projected-secrets-efde512f-4790-4c8a-a19e-691752282619": Phase="Running", Reason="", readiness=false. Elapsed: 4.022480343s
    Jan 18 15:42:49.330: INFO: Pod "pod-projected-secrets-efde512f-4790-4c8a-a19e-691752282619": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021562176s
    STEP: Saw pod success 01/18/23 15:42:49.33
    Jan 18 15:42:49.330: INFO: Pod "pod-projected-secrets-efde512f-4790-4c8a-a19e-691752282619" satisfied condition "Succeeded or Failed"
    Jan 18 15:42:49.334: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-secrets-efde512f-4790-4c8a-a19e-691752282619 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 15:42:49.341
    Jan 18 15:42:49.362: INFO: Waiting for pod pod-projected-secrets-efde512f-4790-4c8a-a19e-691752282619 to disappear
    Jan 18 15:42:49.366: INFO: Pod pod-projected-secrets-efde512f-4790-4c8a-a19e-691752282619 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 18 15:42:49.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9471" for this suite. 01/18/23 15:42:49.371
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:42:49.392
Jan 18 15:42:49.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename services 01/18/23 15:42:49.394
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:42:49.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:42:49.416
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-5050 01/18/23 15:42:49.42
STEP: creating service affinity-nodeport-transition in namespace services-5050 01/18/23 15:42:49.422
STEP: creating replication controller affinity-nodeport-transition in namespace services-5050 01/18/23 15:42:49.444
I0118 15:42:49.453376      19 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-5050, replica count: 3
I0118 15:42:52.504094      19 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 15:42:52.517: INFO: Creating new exec pod
Jan 18 15:42:52.524: INFO: Waiting up to 5m0s for pod "execpod-affinityk5kvw" in namespace "services-5050" to be "running"
Jan 18 15:42:52.528: INFO: Pod "execpod-affinityk5kvw": Phase="Pending", Reason="", readiness=false. Elapsed: 3.964295ms
Jan 18 15:42:54.533: INFO: Pod "execpod-affinityk5kvw": Phase="Running", Reason="", readiness=true. Elapsed: 2.009551269s
Jan 18 15:42:54.533: INFO: Pod "execpod-affinityk5kvw" satisfied condition "running"
Jan 18 15:42:55.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-5050 exec execpod-affinityk5kvw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jan 18 15:42:55.791: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan 18 15:42:55.792: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 15:42:55.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-5050 exec execpod-affinityk5kvw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.40.29 80'
Jan 18 15:42:55.976: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.40.29 80\nConnection to 10.233.40.29 80 port [tcp/http] succeeded!\n"
Jan 18 15:42:55.976: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 15:42:55.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-5050 exec execpod-affinityk5kvw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.168 31416'
Jan 18 15:42:56.204: INFO: stderr: "+ nc -v -t -w 2 192.168.101.168 31416\n+ echo hostName\nConnection to 192.168.101.168 31416 port [tcp/*] succeeded!\n"
Jan 18 15:42:56.204: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 15:42:56.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-5050 exec execpod-affinityk5kvw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 31416'
Jan 18 15:42:56.451: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.216 31416\nConnection to 192.168.101.216 31416 port [tcp/*] succeeded!\n"
Jan 18 15:42:56.451: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 15:42:56.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-5050 exec execpod-affinityk5kvw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.101.168:31416/ ; done'
Jan 18 15:42:56.838: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n"
Jan 18 15:42:56.838: INFO: stdout: "\naffinity-nodeport-transition-ftxkp\naffinity-nodeport-transition-5nlc2\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-ftxkp\naffinity-nodeport-transition-5nlc2\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-ftxkp\naffinity-nodeport-transition-5nlc2\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-ftxkp\naffinity-nodeport-transition-5nlc2\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-ftxkp\naffinity-nodeport-transition-5nlc2\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-ftxkp"
Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-ftxkp
Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-5nlc2
Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-4mq2p
Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-ftxkp
Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-5nlc2
Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-4mq2p
Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-ftxkp
Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-5nlc2
Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-4mq2p
Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-ftxkp
Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-5nlc2
Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-4mq2p
Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-ftxkp
Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-5nlc2
Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-4mq2p
Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-ftxkp
Jan 18 15:42:56.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-5050 exec execpod-affinityk5kvw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.101.168:31416/ ; done'
Jan 18 15:42:57.218: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n"
Jan 18 15:42:57.218: INFO: stdout: "\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p"
Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
Jan 18 15:42:57.218: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5050, will wait for the garbage collector to delete the pods 01/18/23 15:42:57.234
Jan 18 15:42:57.302: INFO: Deleting ReplicationController affinity-nodeport-transition took: 9.823151ms
Jan 18 15:42:57.403: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.255908ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 15:42:59.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5050" for this suite. 01/18/23 15:42:59.851
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":137,"skipped":2379,"failed":0}
------------------------------
• [SLOW TEST] [10.470 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:42:49.392
    Jan 18 15:42:49.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename services 01/18/23 15:42:49.394
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:42:49.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:42:49.416
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-5050 01/18/23 15:42:49.42
    STEP: creating service affinity-nodeport-transition in namespace services-5050 01/18/23 15:42:49.422
    STEP: creating replication controller affinity-nodeport-transition in namespace services-5050 01/18/23 15:42:49.444
    I0118 15:42:49.453376      19 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-5050, replica count: 3
    I0118 15:42:52.504094      19 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 15:42:52.517: INFO: Creating new exec pod
    Jan 18 15:42:52.524: INFO: Waiting up to 5m0s for pod "execpod-affinityk5kvw" in namespace "services-5050" to be "running"
    Jan 18 15:42:52.528: INFO: Pod "execpod-affinityk5kvw": Phase="Pending", Reason="", readiness=false. Elapsed: 3.964295ms
    Jan 18 15:42:54.533: INFO: Pod "execpod-affinityk5kvw": Phase="Running", Reason="", readiness=true. Elapsed: 2.009551269s
    Jan 18 15:42:54.533: INFO: Pod "execpod-affinityk5kvw" satisfied condition "running"
    Jan 18 15:42:55.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-5050 exec execpod-affinityk5kvw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Jan 18 15:42:55.791: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jan 18 15:42:55.792: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 15:42:55.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-5050 exec execpod-affinityk5kvw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.40.29 80'
    Jan 18 15:42:55.976: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.40.29 80\nConnection to 10.233.40.29 80 port [tcp/http] succeeded!\n"
    Jan 18 15:42:55.976: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 15:42:55.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-5050 exec execpod-affinityk5kvw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.168 31416'
    Jan 18 15:42:56.204: INFO: stderr: "+ nc -v -t -w 2 192.168.101.168 31416\n+ echo hostName\nConnection to 192.168.101.168 31416 port [tcp/*] succeeded!\n"
    Jan 18 15:42:56.204: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 15:42:56.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-5050 exec execpod-affinityk5kvw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 31416'
    Jan 18 15:42:56.451: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.216 31416\nConnection to 192.168.101.216 31416 port [tcp/*] succeeded!\n"
    Jan 18 15:42:56.451: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 15:42:56.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-5050 exec execpod-affinityk5kvw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.101.168:31416/ ; done'
    Jan 18 15:42:56.838: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n"
    Jan 18 15:42:56.838: INFO: stdout: "\naffinity-nodeport-transition-ftxkp\naffinity-nodeport-transition-5nlc2\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-ftxkp\naffinity-nodeport-transition-5nlc2\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-ftxkp\naffinity-nodeport-transition-5nlc2\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-ftxkp\naffinity-nodeport-transition-5nlc2\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-ftxkp\naffinity-nodeport-transition-5nlc2\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-ftxkp"
    Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-ftxkp
    Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-5nlc2
    Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-4mq2p
    Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-ftxkp
    Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-5nlc2
    Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-4mq2p
    Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-ftxkp
    Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-5nlc2
    Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-4mq2p
    Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-ftxkp
    Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-5nlc2
    Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-4mq2p
    Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-ftxkp
    Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-5nlc2
    Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-4mq2p
    Jan 18 15:42:56.838: INFO: Received response from host: affinity-nodeport-transition-ftxkp
    Jan 18 15:42:56.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-5050 exec execpod-affinityk5kvw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.101.168:31416/ ; done'
    Jan 18 15:42:57.218: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:31416/\n"
    Jan 18 15:42:57.218: INFO: stdout: "\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p\naffinity-nodeport-transition-4mq2p"
    Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
    Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
    Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
    Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
    Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
    Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
    Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
    Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
    Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
    Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
    Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
    Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
    Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
    Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
    Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
    Jan 18 15:42:57.218: INFO: Received response from host: affinity-nodeport-transition-4mq2p
    Jan 18 15:42:57.218: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5050, will wait for the garbage collector to delete the pods 01/18/23 15:42:57.234
    Jan 18 15:42:57.302: INFO: Deleting ReplicationController affinity-nodeport-transition took: 9.823151ms
    Jan 18 15:42:57.403: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.255908ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 15:42:59.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5050" for this suite. 01/18/23 15:42:59.851
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:42:59.865
Jan 18 15:42:59.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename downward-api 01/18/23 15:42:59.867
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:42:59.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:42:59.893
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 01/18/23 15:42:59.897
Jan 18 15:42:59.909: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2408b094-5a3e-4138-b2c7-3ea6a3455fa8" in namespace "downward-api-8659" to be "Succeeded or Failed"
Jan 18 15:42:59.916: INFO: Pod "downwardapi-volume-2408b094-5a3e-4138-b2c7-3ea6a3455fa8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.111968ms
Jan 18 15:43:01.921: INFO: Pod "downwardapi-volume-2408b094-5a3e-4138-b2c7-3ea6a3455fa8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012672616s
Jan 18 15:43:03.922: INFO: Pod "downwardapi-volume-2408b094-5a3e-4138-b2c7-3ea6a3455fa8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013397044s
STEP: Saw pod success 01/18/23 15:43:03.923
Jan 18 15:43:03.923: INFO: Pod "downwardapi-volume-2408b094-5a3e-4138-b2c7-3ea6a3455fa8" satisfied condition "Succeeded or Failed"
Jan 18 15:43:03.928: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-2408b094-5a3e-4138-b2c7-3ea6a3455fa8 container client-container: <nil>
STEP: delete the pod 01/18/23 15:43:03.939
Jan 18 15:43:03.959: INFO: Waiting for pod downwardapi-volume-2408b094-5a3e-4138-b2c7-3ea6a3455fa8 to disappear
Jan 18 15:43:03.968: INFO: Pod downwardapi-volume-2408b094-5a3e-4138-b2c7-3ea6a3455fa8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 15:43:03.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8659" for this suite. 01/18/23 15:43:03.974
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":138,"skipped":2379,"failed":0}
------------------------------
• [4.118 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:42:59.865
    Jan 18 15:42:59.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename downward-api 01/18/23 15:42:59.867
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:42:59.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:42:59.893
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 01/18/23 15:42:59.897
    Jan 18 15:42:59.909: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2408b094-5a3e-4138-b2c7-3ea6a3455fa8" in namespace "downward-api-8659" to be "Succeeded or Failed"
    Jan 18 15:42:59.916: INFO: Pod "downwardapi-volume-2408b094-5a3e-4138-b2c7-3ea6a3455fa8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.111968ms
    Jan 18 15:43:01.921: INFO: Pod "downwardapi-volume-2408b094-5a3e-4138-b2c7-3ea6a3455fa8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012672616s
    Jan 18 15:43:03.922: INFO: Pod "downwardapi-volume-2408b094-5a3e-4138-b2c7-3ea6a3455fa8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013397044s
    STEP: Saw pod success 01/18/23 15:43:03.923
    Jan 18 15:43:03.923: INFO: Pod "downwardapi-volume-2408b094-5a3e-4138-b2c7-3ea6a3455fa8" satisfied condition "Succeeded or Failed"
    Jan 18 15:43:03.928: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-2408b094-5a3e-4138-b2c7-3ea6a3455fa8 container client-container: <nil>
    STEP: delete the pod 01/18/23 15:43:03.939
    Jan 18 15:43:03.959: INFO: Waiting for pod downwardapi-volume-2408b094-5a3e-4138-b2c7-3ea6a3455fa8 to disappear
    Jan 18 15:43:03.968: INFO: Pod downwardapi-volume-2408b094-5a3e-4138-b2c7-3ea6a3455fa8 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 15:43:03.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8659" for this suite. 01/18/23 15:43:03.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:43:03.989
Jan 18 15:43:03.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 15:43:03.99
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:43:04.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:43:04.021
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-7bc30136-7eba-4692-8e3a-f9f3bb3c60d9 01/18/23 15:43:04.027
STEP: Creating a pod to test consume configMaps 01/18/23 15:43:04.033
Jan 18 15:43:04.043: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6d125256-006e-4e1a-b4f9-683624ab630a" in namespace "projected-5974" to be "Succeeded or Failed"
Jan 18 15:43:04.057: INFO: Pod "pod-projected-configmaps-6d125256-006e-4e1a-b4f9-683624ab630a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.678451ms
Jan 18 15:43:06.062: INFO: Pod "pod-projected-configmaps-6d125256-006e-4e1a-b4f9-683624ab630a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018655032s
Jan 18 15:43:08.062: INFO: Pod "pod-projected-configmaps-6d125256-006e-4e1a-b4f9-683624ab630a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018833962s
STEP: Saw pod success 01/18/23 15:43:08.062
Jan 18 15:43:08.063: INFO: Pod "pod-projected-configmaps-6d125256-006e-4e1a-b4f9-683624ab630a" satisfied condition "Succeeded or Failed"
Jan 18 15:43:08.067: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-6d125256-006e-4e1a-b4f9-683624ab630a container agnhost-container: <nil>
STEP: delete the pod 01/18/23 15:43:08.074
Jan 18 15:43:08.090: INFO: Waiting for pod pod-projected-configmaps-6d125256-006e-4e1a-b4f9-683624ab630a to disappear
Jan 18 15:43:08.093: INFO: Pod pod-projected-configmaps-6d125256-006e-4e1a-b4f9-683624ab630a no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 18 15:43:08.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5974" for this suite. 01/18/23 15:43:08.098
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":139,"skipped":2414,"failed":0}
------------------------------
• [4.118 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:43:03.989
    Jan 18 15:43:03.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 15:43:03.99
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:43:04.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:43:04.021
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-7bc30136-7eba-4692-8e3a-f9f3bb3c60d9 01/18/23 15:43:04.027
    STEP: Creating a pod to test consume configMaps 01/18/23 15:43:04.033
    Jan 18 15:43:04.043: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6d125256-006e-4e1a-b4f9-683624ab630a" in namespace "projected-5974" to be "Succeeded or Failed"
    Jan 18 15:43:04.057: INFO: Pod "pod-projected-configmaps-6d125256-006e-4e1a-b4f9-683624ab630a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.678451ms
    Jan 18 15:43:06.062: INFO: Pod "pod-projected-configmaps-6d125256-006e-4e1a-b4f9-683624ab630a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018655032s
    Jan 18 15:43:08.062: INFO: Pod "pod-projected-configmaps-6d125256-006e-4e1a-b4f9-683624ab630a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018833962s
    STEP: Saw pod success 01/18/23 15:43:08.062
    Jan 18 15:43:08.063: INFO: Pod "pod-projected-configmaps-6d125256-006e-4e1a-b4f9-683624ab630a" satisfied condition "Succeeded or Failed"
    Jan 18 15:43:08.067: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-6d125256-006e-4e1a-b4f9-683624ab630a container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 15:43:08.074
    Jan 18 15:43:08.090: INFO: Waiting for pod pod-projected-configmaps-6d125256-006e-4e1a-b4f9-683624ab630a to disappear
    Jan 18 15:43:08.093: INFO: Pod pod-projected-configmaps-6d125256-006e-4e1a-b4f9-683624ab630a no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 18 15:43:08.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5974" for this suite. 01/18/23 15:43:08.098
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:43:08.111
Jan 18 15:43:08.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename podtemplate 01/18/23 15:43:08.113
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:43:08.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:43:08.138
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 01/18/23 15:43:08.143
STEP: Replace a pod template 01/18/23 15:43:08.15
Jan 18 15:43:08.164: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan 18 15:43:08.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5795" for this suite. 01/18/23 15:43:08.17
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":140,"skipped":2479,"failed":0}
------------------------------
• [0.065 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:43:08.111
    Jan 18 15:43:08.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename podtemplate 01/18/23 15:43:08.113
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:43:08.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:43:08.138
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 01/18/23 15:43:08.143
    STEP: Replace a pod template 01/18/23 15:43:08.15
    Jan 18 15:43:08.164: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan 18 15:43:08.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-5795" for this suite. 01/18/23 15:43:08.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:43:08.178
Jan 18 15:43:08.178: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename container-runtime 01/18/23 15:43:08.179
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:43:08.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:43:08.211
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 01/18/23 15:43:08.219
STEP: wait for the container to reach Failed 01/18/23 15:43:08.231
STEP: get the container status 01/18/23 15:43:12.305
STEP: the container should be terminated 01/18/23 15:43:12.313
STEP: the termination message should be set 01/18/23 15:43:12.314
Jan 18 15:43:12.314: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/18/23 15:43:12.314
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 18 15:43:12.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-279" for this suite. 01/18/23 15:43:12.344
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":141,"skipped":2489,"failed":0}
------------------------------
• [4.172 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:43:08.178
    Jan 18 15:43:08.178: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename container-runtime 01/18/23 15:43:08.179
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:43:08.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:43:08.211
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 01/18/23 15:43:08.219
    STEP: wait for the container to reach Failed 01/18/23 15:43:08.231
    STEP: get the container status 01/18/23 15:43:12.305
    STEP: the container should be terminated 01/18/23 15:43:12.313
    STEP: the termination message should be set 01/18/23 15:43:12.314
    Jan 18 15:43:12.314: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/18/23 15:43:12.314
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 18 15:43:12.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-279" for this suite. 01/18/23 15:43:12.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:43:12.352
Jan 18 15:43:12.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename gc 01/18/23 15:43:12.356
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:43:12.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:43:12.391
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 01/18/23 15:43:12.413
STEP: delete the rc 01/18/23 15:43:17.675
STEP: wait for the rc to be deleted 01/18/23 15:43:17.702
Jan 18 15:43:18.997: INFO: 80 pods remaining
Jan 18 15:43:18.998: INFO: 80 pods has nil DeletionTimestamp
Jan 18 15:43:18.998: INFO: 
Jan 18 15:43:19.857: INFO: 76 pods remaining
Jan 18 15:43:19.864: INFO: 68 pods has nil DeletionTimestamp
Jan 18 15:43:19.864: INFO: 
Jan 18 15:43:20.719: INFO: 59 pods remaining
Jan 18 15:43:20.719: INFO: 59 pods has nil DeletionTimestamp
Jan 18 15:43:20.719: INFO: 
Jan 18 15:43:22.092: INFO: 40 pods remaining
Jan 18 15:43:22.092: INFO: 40 pods has nil DeletionTimestamp
Jan 18 15:43:22.092: INFO: 
Jan 18 15:43:22.750: INFO: 32 pods remaining
Jan 18 15:43:22.750: INFO: 31 pods has nil DeletionTimestamp
Jan 18 15:43:22.750: INFO: 
Jan 18 15:43:23.760: INFO: 19 pods remaining
Jan 18 15:43:23.761: INFO: 19 pods has nil DeletionTimestamp
Jan 18 15:43:23.761: INFO: 
STEP: Gathering metrics 01/18/23 15:43:24.718
Jan 18 15:43:24.845: INFO: Waiting up to 5m0s for pod "kube-controller-manager-v1-25-1-18760-m" in namespace "kube-system" to be "running and ready"
Jan 18 15:43:24.863: INFO: Pod "kube-controller-manager-v1-25-1-18760-m": Phase="Running", Reason="", readiness=true. Elapsed: 18.040657ms
Jan 18 15:43:24.863: INFO: The phase of Pod kube-controller-manager-v1-25-1-18760-m is Running (Ready = true)
Jan 18 15:43:24.863: INFO: Pod "kube-controller-manager-v1-25-1-18760-m" satisfied condition "running and ready"
Jan 18 15:43:25.235: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 18 15:43:25.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1788" for this suite. 01/18/23 15:43:25.287
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":142,"skipped":2502,"failed":0}
------------------------------
• [SLOW TEST] [12.975 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:43:12.352
    Jan 18 15:43:12.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename gc 01/18/23 15:43:12.356
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:43:12.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:43:12.391
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 01/18/23 15:43:12.413
    STEP: delete the rc 01/18/23 15:43:17.675
    STEP: wait for the rc to be deleted 01/18/23 15:43:17.702
    Jan 18 15:43:18.997: INFO: 80 pods remaining
    Jan 18 15:43:18.998: INFO: 80 pods has nil DeletionTimestamp
    Jan 18 15:43:18.998: INFO: 
    Jan 18 15:43:19.857: INFO: 76 pods remaining
    Jan 18 15:43:19.864: INFO: 68 pods has nil DeletionTimestamp
    Jan 18 15:43:19.864: INFO: 
    Jan 18 15:43:20.719: INFO: 59 pods remaining
    Jan 18 15:43:20.719: INFO: 59 pods has nil DeletionTimestamp
    Jan 18 15:43:20.719: INFO: 
    Jan 18 15:43:22.092: INFO: 40 pods remaining
    Jan 18 15:43:22.092: INFO: 40 pods has nil DeletionTimestamp
    Jan 18 15:43:22.092: INFO: 
    Jan 18 15:43:22.750: INFO: 32 pods remaining
    Jan 18 15:43:22.750: INFO: 31 pods has nil DeletionTimestamp
    Jan 18 15:43:22.750: INFO: 
    Jan 18 15:43:23.760: INFO: 19 pods remaining
    Jan 18 15:43:23.761: INFO: 19 pods has nil DeletionTimestamp
    Jan 18 15:43:23.761: INFO: 
    STEP: Gathering metrics 01/18/23 15:43:24.718
    Jan 18 15:43:24.845: INFO: Waiting up to 5m0s for pod "kube-controller-manager-v1-25-1-18760-m" in namespace "kube-system" to be "running and ready"
    Jan 18 15:43:24.863: INFO: Pod "kube-controller-manager-v1-25-1-18760-m": Phase="Running", Reason="", readiness=true. Elapsed: 18.040657ms
    Jan 18 15:43:24.863: INFO: The phase of Pod kube-controller-manager-v1-25-1-18760-m is Running (Ready = true)
    Jan 18 15:43:24.863: INFO: Pod "kube-controller-manager-v1-25-1-18760-m" satisfied condition "running and ready"
    Jan 18 15:43:25.235: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 18 15:43:25.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1788" for this suite. 01/18/23 15:43:25.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:43:25.331
Jan 18 15:43:25.331: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename kubectl 01/18/23 15:43:25.333
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:43:25.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:43:25.442
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 01/18/23 15:43:25.487
Jan 18 15:43:25.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-4206 create -f -'
Jan 18 15:43:27.375: INFO: stderr: ""
Jan 18 15:43:27.375: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 01/18/23 15:43:27.375
Jan 18 15:43:27.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-4206 diff -f -'
Jan 18 15:43:27.986: INFO: rc: 1
Jan 18 15:43:27.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-4206 delete -f -'
Jan 18 15:43:28.237: INFO: stderr: ""
Jan 18 15:43:28.237: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 15:43:28.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4206" for this suite. 01/18/23 15:43:28.243
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":143,"skipped":2515,"failed":0}
------------------------------
• [2.941 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:43:25.331
    Jan 18 15:43:25.331: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename kubectl 01/18/23 15:43:25.333
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:43:25.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:43:25.442
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 01/18/23 15:43:25.487
    Jan 18 15:43:25.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-4206 create -f -'
    Jan 18 15:43:27.375: INFO: stderr: ""
    Jan 18 15:43:27.375: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 01/18/23 15:43:27.375
    Jan 18 15:43:27.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-4206 diff -f -'
    Jan 18 15:43:27.986: INFO: rc: 1
    Jan 18 15:43:27.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-4206 delete -f -'
    Jan 18 15:43:28.237: INFO: stderr: ""
    Jan 18 15:43:28.237: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 15:43:28.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4206" for this suite. 01/18/23 15:43:28.243
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:43:28.273
Jan 18 15:43:28.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename emptydir 01/18/23 15:43:28.274
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:43:28.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:43:28.338
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 01/18/23 15:43:28.359
Jan 18 15:43:28.378: INFO: Waiting up to 5m0s for pod "pod-6607d092-7517-4e08-81c9-86f35d202e11" in namespace "emptydir-2422" to be "Succeeded or Failed"
Jan 18 15:43:28.390: INFO: Pod "pod-6607d092-7517-4e08-81c9-86f35d202e11": Phase="Pending", Reason="", readiness=false. Elapsed: 11.528209ms
Jan 18 15:43:30.395: INFO: Pod "pod-6607d092-7517-4e08-81c9-86f35d202e11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016790534s
Jan 18 15:43:32.399: INFO: Pod "pod-6607d092-7517-4e08-81c9-86f35d202e11": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020804454s
Jan 18 15:43:34.400: INFO: Pod "pod-6607d092-7517-4e08-81c9-86f35d202e11": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021910253s
Jan 18 15:43:36.395: INFO: Pod "pod-6607d092-7517-4e08-81c9-86f35d202e11": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016758289s
Jan 18 15:43:38.395: INFO: Pod "pod-6607d092-7517-4e08-81c9-86f35d202e11": Phase="Pending", Reason="", readiness=false. Elapsed: 10.016560204s
Jan 18 15:43:40.395: INFO: Pod "pod-6607d092-7517-4e08-81c9-86f35d202e11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.016912408s
STEP: Saw pod success 01/18/23 15:43:40.395
Jan 18 15:43:40.396: INFO: Pod "pod-6607d092-7517-4e08-81c9-86f35d202e11" satisfied condition "Succeeded or Failed"
Jan 18 15:43:40.400: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-6607d092-7517-4e08-81c9-86f35d202e11 container test-container: <nil>
STEP: delete the pod 01/18/23 15:43:40.408
Jan 18 15:43:40.422: INFO: Waiting for pod pod-6607d092-7517-4e08-81c9-86f35d202e11 to disappear
Jan 18 15:43:40.426: INFO: Pod pod-6607d092-7517-4e08-81c9-86f35d202e11 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 15:43:40.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2422" for this suite. 01/18/23 15:43:40.431
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":144,"skipped":2533,"failed":0}
------------------------------
• [SLOW TEST] [12.167 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:43:28.273
    Jan 18 15:43:28.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename emptydir 01/18/23 15:43:28.274
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:43:28.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:43:28.338
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 01/18/23 15:43:28.359
    Jan 18 15:43:28.378: INFO: Waiting up to 5m0s for pod "pod-6607d092-7517-4e08-81c9-86f35d202e11" in namespace "emptydir-2422" to be "Succeeded or Failed"
    Jan 18 15:43:28.390: INFO: Pod "pod-6607d092-7517-4e08-81c9-86f35d202e11": Phase="Pending", Reason="", readiness=false. Elapsed: 11.528209ms
    Jan 18 15:43:30.395: INFO: Pod "pod-6607d092-7517-4e08-81c9-86f35d202e11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016790534s
    Jan 18 15:43:32.399: INFO: Pod "pod-6607d092-7517-4e08-81c9-86f35d202e11": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020804454s
    Jan 18 15:43:34.400: INFO: Pod "pod-6607d092-7517-4e08-81c9-86f35d202e11": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021910253s
    Jan 18 15:43:36.395: INFO: Pod "pod-6607d092-7517-4e08-81c9-86f35d202e11": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016758289s
    Jan 18 15:43:38.395: INFO: Pod "pod-6607d092-7517-4e08-81c9-86f35d202e11": Phase="Pending", Reason="", readiness=false. Elapsed: 10.016560204s
    Jan 18 15:43:40.395: INFO: Pod "pod-6607d092-7517-4e08-81c9-86f35d202e11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.016912408s
    STEP: Saw pod success 01/18/23 15:43:40.395
    Jan 18 15:43:40.396: INFO: Pod "pod-6607d092-7517-4e08-81c9-86f35d202e11" satisfied condition "Succeeded or Failed"
    Jan 18 15:43:40.400: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-6607d092-7517-4e08-81c9-86f35d202e11 container test-container: <nil>
    STEP: delete the pod 01/18/23 15:43:40.408
    Jan 18 15:43:40.422: INFO: Waiting for pod pod-6607d092-7517-4e08-81c9-86f35d202e11 to disappear
    Jan 18 15:43:40.426: INFO: Pod pod-6607d092-7517-4e08-81c9-86f35d202e11 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 15:43:40.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2422" for this suite. 01/18/23 15:43:40.431
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:43:40.448
Jan 18 15:43:40.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename security-context 01/18/23 15:43:40.449
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:43:40.481
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:43:40.49
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/18/23 15:43:40.498
Jan 18 15:43:40.513: INFO: Waiting up to 5m0s for pod "security-context-0151dfa0-7937-4de5-b53d-01e1280645a4" in namespace "security-context-8017" to be "Succeeded or Failed"
Jan 18 15:43:40.519: INFO: Pod "security-context-0151dfa0-7937-4de5-b53d-01e1280645a4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.66595ms
Jan 18 15:43:42.524: INFO: Pod "security-context-0151dfa0-7937-4de5-b53d-01e1280645a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.01016305s
Jan 18 15:43:44.523: INFO: Pod "security-context-0151dfa0-7937-4de5-b53d-01e1280645a4": Phase="Running", Reason="", readiness=false. Elapsed: 4.009728824s
Jan 18 15:43:46.526: INFO: Pod "security-context-0151dfa0-7937-4de5-b53d-01e1280645a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012265881s
STEP: Saw pod success 01/18/23 15:43:46.526
Jan 18 15:43:46.526: INFO: Pod "security-context-0151dfa0-7937-4de5-b53d-01e1280645a4" satisfied condition "Succeeded or Failed"
Jan 18 15:43:46.532: INFO: Trying to get logs from node v1-25-1-18760-w2 pod security-context-0151dfa0-7937-4de5-b53d-01e1280645a4 container test-container: <nil>
STEP: delete the pod 01/18/23 15:43:46.541
Jan 18 15:43:46.559: INFO: Waiting for pod security-context-0151dfa0-7937-4de5-b53d-01e1280645a4 to disappear
Jan 18 15:43:46.563: INFO: Pod security-context-0151dfa0-7937-4de5-b53d-01e1280645a4 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 18 15:43:46.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-8017" for this suite. 01/18/23 15:43:46.572
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":145,"skipped":2599,"failed":0}
------------------------------
• [SLOW TEST] [6.133 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:43:40.448
    Jan 18 15:43:40.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename security-context 01/18/23 15:43:40.449
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:43:40.481
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:43:40.49
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/18/23 15:43:40.498
    Jan 18 15:43:40.513: INFO: Waiting up to 5m0s for pod "security-context-0151dfa0-7937-4de5-b53d-01e1280645a4" in namespace "security-context-8017" to be "Succeeded or Failed"
    Jan 18 15:43:40.519: INFO: Pod "security-context-0151dfa0-7937-4de5-b53d-01e1280645a4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.66595ms
    Jan 18 15:43:42.524: INFO: Pod "security-context-0151dfa0-7937-4de5-b53d-01e1280645a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.01016305s
    Jan 18 15:43:44.523: INFO: Pod "security-context-0151dfa0-7937-4de5-b53d-01e1280645a4": Phase="Running", Reason="", readiness=false. Elapsed: 4.009728824s
    Jan 18 15:43:46.526: INFO: Pod "security-context-0151dfa0-7937-4de5-b53d-01e1280645a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012265881s
    STEP: Saw pod success 01/18/23 15:43:46.526
    Jan 18 15:43:46.526: INFO: Pod "security-context-0151dfa0-7937-4de5-b53d-01e1280645a4" satisfied condition "Succeeded or Failed"
    Jan 18 15:43:46.532: INFO: Trying to get logs from node v1-25-1-18760-w2 pod security-context-0151dfa0-7937-4de5-b53d-01e1280645a4 container test-container: <nil>
    STEP: delete the pod 01/18/23 15:43:46.541
    Jan 18 15:43:46.559: INFO: Waiting for pod security-context-0151dfa0-7937-4de5-b53d-01e1280645a4 to disappear
    Jan 18 15:43:46.563: INFO: Pod security-context-0151dfa0-7937-4de5-b53d-01e1280645a4 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 18 15:43:46.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-8017" for this suite. 01/18/23 15:43:46.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:43:46.585
Jan 18 15:43:46.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename events 01/18/23 15:43:46.587
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:43:46.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:43:46.616
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 01/18/23 15:43:46.627
STEP: listing events in all namespaces 01/18/23 15:43:46.632
STEP: listing events in test namespace 01/18/23 15:43:46.653
STEP: listing events with field selection filtering on source 01/18/23 15:43:46.662
STEP: listing events with field selection filtering on reportingController 01/18/23 15:43:46.667
STEP: getting the test event 01/18/23 15:43:46.672
STEP: patching the test event 01/18/23 15:43:46.676
STEP: getting the test event 01/18/23 15:43:46.691
STEP: updating the test event 01/18/23 15:43:46.696
STEP: getting the test event 01/18/23 15:43:46.706
STEP: deleting the test event 01/18/23 15:43:46.71
STEP: listing events in all namespaces 01/18/23 15:43:46.718
STEP: listing events in test namespace 01/18/23 15:43:46.725
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jan 18 15:43:46.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9062" for this suite. 01/18/23 15:43:46.739
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":146,"skipped":2616,"failed":0}
------------------------------
• [0.160 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:43:46.585
    Jan 18 15:43:46.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename events 01/18/23 15:43:46.587
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:43:46.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:43:46.616
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 01/18/23 15:43:46.627
    STEP: listing events in all namespaces 01/18/23 15:43:46.632
    STEP: listing events in test namespace 01/18/23 15:43:46.653
    STEP: listing events with field selection filtering on source 01/18/23 15:43:46.662
    STEP: listing events with field selection filtering on reportingController 01/18/23 15:43:46.667
    STEP: getting the test event 01/18/23 15:43:46.672
    STEP: patching the test event 01/18/23 15:43:46.676
    STEP: getting the test event 01/18/23 15:43:46.691
    STEP: updating the test event 01/18/23 15:43:46.696
    STEP: getting the test event 01/18/23 15:43:46.706
    STEP: deleting the test event 01/18/23 15:43:46.71
    STEP: listing events in all namespaces 01/18/23 15:43:46.718
    STEP: listing events in test namespace 01/18/23 15:43:46.725
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jan 18 15:43:46.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-9062" for this suite. 01/18/23 15:43:46.739
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:43:46.753
Jan 18 15:43:46.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 15:43:46.755
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:43:46.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:43:46.78
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-d1e8df8e-02f4-4d9b-b105-503999bdf2fd 01/18/23 15:43:46.785
STEP: Creating a pod to test consume secrets 01/18/23 15:43:46.791
Jan 18 15:43:46.804: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-aa09cb71-54cb-4430-9eb5-e7a17db69752" in namespace "projected-5397" to be "Succeeded or Failed"
Jan 18 15:43:46.812: INFO: Pod "pod-projected-secrets-aa09cb71-54cb-4430-9eb5-e7a17db69752": Phase="Pending", Reason="", readiness=false. Elapsed: 8.817141ms
Jan 18 15:43:48.821: INFO: Pod "pod-projected-secrets-aa09cb71-54cb-4430-9eb5-e7a17db69752": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01709624s
Jan 18 15:43:50.820: INFO: Pod "pod-projected-secrets-aa09cb71-54cb-4430-9eb5-e7a17db69752": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016315589s
STEP: Saw pod success 01/18/23 15:43:50.82
Jan 18 15:43:50.821: INFO: Pod "pod-projected-secrets-aa09cb71-54cb-4430-9eb5-e7a17db69752" satisfied condition "Succeeded or Failed"
Jan 18 15:43:50.824: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-secrets-aa09cb71-54cb-4430-9eb5-e7a17db69752 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/18/23 15:43:50.841
Jan 18 15:43:50.868: INFO: Waiting for pod pod-projected-secrets-aa09cb71-54cb-4430-9eb5-e7a17db69752 to disappear
Jan 18 15:43:50.872: INFO: Pod pod-projected-secrets-aa09cb71-54cb-4430-9eb5-e7a17db69752 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 18 15:43:50.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5397" for this suite. 01/18/23 15:43:50.882
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":147,"skipped":2640,"failed":0}
------------------------------
• [4.142 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:43:46.753
    Jan 18 15:43:46.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 15:43:46.755
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:43:46.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:43:46.78
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-d1e8df8e-02f4-4d9b-b105-503999bdf2fd 01/18/23 15:43:46.785
    STEP: Creating a pod to test consume secrets 01/18/23 15:43:46.791
    Jan 18 15:43:46.804: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-aa09cb71-54cb-4430-9eb5-e7a17db69752" in namespace "projected-5397" to be "Succeeded or Failed"
    Jan 18 15:43:46.812: INFO: Pod "pod-projected-secrets-aa09cb71-54cb-4430-9eb5-e7a17db69752": Phase="Pending", Reason="", readiness=false. Elapsed: 8.817141ms
    Jan 18 15:43:48.821: INFO: Pod "pod-projected-secrets-aa09cb71-54cb-4430-9eb5-e7a17db69752": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01709624s
    Jan 18 15:43:50.820: INFO: Pod "pod-projected-secrets-aa09cb71-54cb-4430-9eb5-e7a17db69752": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016315589s
    STEP: Saw pod success 01/18/23 15:43:50.82
    Jan 18 15:43:50.821: INFO: Pod "pod-projected-secrets-aa09cb71-54cb-4430-9eb5-e7a17db69752" satisfied condition "Succeeded or Failed"
    Jan 18 15:43:50.824: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-secrets-aa09cb71-54cb-4430-9eb5-e7a17db69752 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 15:43:50.841
    Jan 18 15:43:50.868: INFO: Waiting for pod pod-projected-secrets-aa09cb71-54cb-4430-9eb5-e7a17db69752 to disappear
    Jan 18 15:43:50.872: INFO: Pod pod-projected-secrets-aa09cb71-54cb-4430-9eb5-e7a17db69752 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 18 15:43:50.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5397" for this suite. 01/18/23 15:43:50.882
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:43:50.91
Jan 18 15:43:50.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename emptydir 01/18/23 15:43:50.913
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:43:50.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:43:50.948
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/18/23 15:43:50.953
Jan 18 15:43:50.964: INFO: Waiting up to 5m0s for pod "pod-02b3116d-6049-4c78-8453-efab934220dd" in namespace "emptydir-3255" to be "Succeeded or Failed"
Jan 18 15:43:50.975: INFO: Pod "pod-02b3116d-6049-4c78-8453-efab934220dd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.319111ms
Jan 18 15:43:52.983: INFO: Pod "pod-02b3116d-6049-4c78-8453-efab934220dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01912354s
Jan 18 15:43:54.981: INFO: Pod "pod-02b3116d-6049-4c78-8453-efab934220dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017013726s
STEP: Saw pod success 01/18/23 15:43:54.981
Jan 18 15:43:54.981: INFO: Pod "pod-02b3116d-6049-4c78-8453-efab934220dd" satisfied condition "Succeeded or Failed"
Jan 18 15:43:54.987: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-02b3116d-6049-4c78-8453-efab934220dd container test-container: <nil>
STEP: delete the pod 01/18/23 15:43:54.996
Jan 18 15:43:55.013: INFO: Waiting for pod pod-02b3116d-6049-4c78-8453-efab934220dd to disappear
Jan 18 15:43:55.018: INFO: Pod pod-02b3116d-6049-4c78-8453-efab934220dd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 15:43:55.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3255" for this suite. 01/18/23 15:43:55.024
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":148,"skipped":2672,"failed":0}
------------------------------
• [4.124 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:43:50.91
    Jan 18 15:43:50.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename emptydir 01/18/23 15:43:50.913
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:43:50.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:43:50.948
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/18/23 15:43:50.953
    Jan 18 15:43:50.964: INFO: Waiting up to 5m0s for pod "pod-02b3116d-6049-4c78-8453-efab934220dd" in namespace "emptydir-3255" to be "Succeeded or Failed"
    Jan 18 15:43:50.975: INFO: Pod "pod-02b3116d-6049-4c78-8453-efab934220dd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.319111ms
    Jan 18 15:43:52.983: INFO: Pod "pod-02b3116d-6049-4c78-8453-efab934220dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01912354s
    Jan 18 15:43:54.981: INFO: Pod "pod-02b3116d-6049-4c78-8453-efab934220dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017013726s
    STEP: Saw pod success 01/18/23 15:43:54.981
    Jan 18 15:43:54.981: INFO: Pod "pod-02b3116d-6049-4c78-8453-efab934220dd" satisfied condition "Succeeded or Failed"
    Jan 18 15:43:54.987: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-02b3116d-6049-4c78-8453-efab934220dd container test-container: <nil>
    STEP: delete the pod 01/18/23 15:43:54.996
    Jan 18 15:43:55.013: INFO: Waiting for pod pod-02b3116d-6049-4c78-8453-efab934220dd to disappear
    Jan 18 15:43:55.018: INFO: Pod pod-02b3116d-6049-4c78-8453-efab934220dd no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 15:43:55.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3255" for this suite. 01/18/23 15:43:55.024
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:43:55.039
Jan 18 15:43:55.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename security-context-test 01/18/23 15:43:55.041
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:43:55.059
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:43:55.066
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Jan 18 15:43:55.080: INFO: Waiting up to 5m0s for pod "busybox-user-65534-5bdca5e5-d5e1-47aa-a436-1461ba8209cb" in namespace "security-context-test-4777" to be "Succeeded or Failed"
Jan 18 15:43:55.087: INFO: Pod "busybox-user-65534-5bdca5e5-d5e1-47aa-a436-1461ba8209cb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.280153ms
Jan 18 15:43:57.092: INFO: Pod "busybox-user-65534-5bdca5e5-d5e1-47aa-a436-1461ba8209cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01217979s
Jan 18 15:43:59.095: INFO: Pod "busybox-user-65534-5bdca5e5-d5e1-47aa-a436-1461ba8209cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014476063s
Jan 18 15:43:59.095: INFO: Pod "busybox-user-65534-5bdca5e5-d5e1-47aa-a436-1461ba8209cb" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 18 15:43:59.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4777" for this suite. 01/18/23 15:43:59.104
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":149,"skipped":2672,"failed":0}
------------------------------
• [4.074 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:43:55.039
    Jan 18 15:43:55.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename security-context-test 01/18/23 15:43:55.041
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:43:55.059
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:43:55.066
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Jan 18 15:43:55.080: INFO: Waiting up to 5m0s for pod "busybox-user-65534-5bdca5e5-d5e1-47aa-a436-1461ba8209cb" in namespace "security-context-test-4777" to be "Succeeded or Failed"
    Jan 18 15:43:55.087: INFO: Pod "busybox-user-65534-5bdca5e5-d5e1-47aa-a436-1461ba8209cb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.280153ms
    Jan 18 15:43:57.092: INFO: Pod "busybox-user-65534-5bdca5e5-d5e1-47aa-a436-1461ba8209cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01217979s
    Jan 18 15:43:59.095: INFO: Pod "busybox-user-65534-5bdca5e5-d5e1-47aa-a436-1461ba8209cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014476063s
    Jan 18 15:43:59.095: INFO: Pod "busybox-user-65534-5bdca5e5-d5e1-47aa-a436-1461ba8209cb" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 18 15:43:59.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-4777" for this suite. 01/18/23 15:43:59.104
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:43:59.115
Jan 18 15:43:59.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 15:43:59.119
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:43:59.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:43:59.141
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-388b035d-f660-4fe0-9c9d-f2c5d685d485 01/18/23 15:43:59.15
STEP: Creating a pod to test consume configMaps 01/18/23 15:43:59.157
Jan 18 15:43:59.167: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1816219d-a01f-4cb8-8725-644e2384e3fc" in namespace "projected-5444" to be "Succeeded or Failed"
Jan 18 15:43:59.175: INFO: Pod "pod-projected-configmaps-1816219d-a01f-4cb8-8725-644e2384e3fc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.919744ms
Jan 18 15:44:01.180: INFO: Pod "pod-projected-configmaps-1816219d-a01f-4cb8-8725-644e2384e3fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01250314s
Jan 18 15:44:03.181: INFO: Pod "pod-projected-configmaps-1816219d-a01f-4cb8-8725-644e2384e3fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013136302s
STEP: Saw pod success 01/18/23 15:44:03.181
Jan 18 15:44:03.182: INFO: Pod "pod-projected-configmaps-1816219d-a01f-4cb8-8725-644e2384e3fc" satisfied condition "Succeeded or Failed"
Jan 18 15:44:03.187: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-1816219d-a01f-4cb8-8725-644e2384e3fc container projected-configmap-volume-test: <nil>
STEP: delete the pod 01/18/23 15:44:03.196
Jan 18 15:44:03.207: INFO: Waiting for pod pod-projected-configmaps-1816219d-a01f-4cb8-8725-644e2384e3fc to disappear
Jan 18 15:44:03.216: INFO: Pod pod-projected-configmaps-1816219d-a01f-4cb8-8725-644e2384e3fc no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 18 15:44:03.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5444" for this suite. 01/18/23 15:44:03.221
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":150,"skipped":2672,"failed":0}
------------------------------
• [4.113 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:43:59.115
    Jan 18 15:43:59.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 15:43:59.119
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:43:59.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:43:59.141
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-388b035d-f660-4fe0-9c9d-f2c5d685d485 01/18/23 15:43:59.15
    STEP: Creating a pod to test consume configMaps 01/18/23 15:43:59.157
    Jan 18 15:43:59.167: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1816219d-a01f-4cb8-8725-644e2384e3fc" in namespace "projected-5444" to be "Succeeded or Failed"
    Jan 18 15:43:59.175: INFO: Pod "pod-projected-configmaps-1816219d-a01f-4cb8-8725-644e2384e3fc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.919744ms
    Jan 18 15:44:01.180: INFO: Pod "pod-projected-configmaps-1816219d-a01f-4cb8-8725-644e2384e3fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01250314s
    Jan 18 15:44:03.181: INFO: Pod "pod-projected-configmaps-1816219d-a01f-4cb8-8725-644e2384e3fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013136302s
    STEP: Saw pod success 01/18/23 15:44:03.181
    Jan 18 15:44:03.182: INFO: Pod "pod-projected-configmaps-1816219d-a01f-4cb8-8725-644e2384e3fc" satisfied condition "Succeeded or Failed"
    Jan 18 15:44:03.187: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-1816219d-a01f-4cb8-8725-644e2384e3fc container projected-configmap-volume-test: <nil>
    STEP: delete the pod 01/18/23 15:44:03.196
    Jan 18 15:44:03.207: INFO: Waiting for pod pod-projected-configmaps-1816219d-a01f-4cb8-8725-644e2384e3fc to disappear
    Jan 18 15:44:03.216: INFO: Pod pod-projected-configmaps-1816219d-a01f-4cb8-8725-644e2384e3fc no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 18 15:44:03.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5444" for this suite. 01/18/23 15:44:03.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:44:03.236
Jan 18 15:44:03.236: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename endpointslice 01/18/23 15:44:03.238
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:44:03.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:44:03.262
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 01/18/23 15:44:03.269
STEP: getting /apis/discovery.k8s.io 01/18/23 15:44:03.272
STEP: getting /apis/discovery.k8s.iov1 01/18/23 15:44:03.274
STEP: creating 01/18/23 15:44:03.282
STEP: getting 01/18/23 15:44:03.307
STEP: listing 01/18/23 15:44:03.312
STEP: watching 01/18/23 15:44:03.317
Jan 18 15:44:03.317: INFO: starting watch
STEP: cluster-wide listing 01/18/23 15:44:03.321
STEP: cluster-wide watching 01/18/23 15:44:03.325
Jan 18 15:44:03.326: INFO: starting watch
STEP: patching 01/18/23 15:44:03.328
STEP: updating 01/18/23 15:44:03.336
Jan 18 15:44:03.354: INFO: waiting for watch events with expected annotations
Jan 18 15:44:03.354: INFO: saw patched and updated annotations
STEP: deleting 01/18/23 15:44:03.354
STEP: deleting a collection 01/18/23 15:44:03.371
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 18 15:44:03.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8773" for this suite. 01/18/23 15:44:03.392
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":151,"skipped":2698,"failed":0}
------------------------------
• [0.161 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:44:03.236
    Jan 18 15:44:03.236: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename endpointslice 01/18/23 15:44:03.238
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:44:03.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:44:03.262
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 01/18/23 15:44:03.269
    STEP: getting /apis/discovery.k8s.io 01/18/23 15:44:03.272
    STEP: getting /apis/discovery.k8s.iov1 01/18/23 15:44:03.274
    STEP: creating 01/18/23 15:44:03.282
    STEP: getting 01/18/23 15:44:03.307
    STEP: listing 01/18/23 15:44:03.312
    STEP: watching 01/18/23 15:44:03.317
    Jan 18 15:44:03.317: INFO: starting watch
    STEP: cluster-wide listing 01/18/23 15:44:03.321
    STEP: cluster-wide watching 01/18/23 15:44:03.325
    Jan 18 15:44:03.326: INFO: starting watch
    STEP: patching 01/18/23 15:44:03.328
    STEP: updating 01/18/23 15:44:03.336
    Jan 18 15:44:03.354: INFO: waiting for watch events with expected annotations
    Jan 18 15:44:03.354: INFO: saw patched and updated annotations
    STEP: deleting 01/18/23 15:44:03.354
    STEP: deleting a collection 01/18/23 15:44:03.371
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 18 15:44:03.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-8773" for this suite. 01/18/23 15:44:03.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:44:03.401
Jan 18 15:44:03.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename var-expansion 01/18/23 15:44:03.403
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:44:03.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:44:03.451
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 01/18/23 15:44:03.454
Jan 18 15:44:03.474: INFO: Waiting up to 5m0s for pod "var-expansion-d1fec840-b03b-4ef2-903a-a356a334d849" in namespace "var-expansion-5008" to be "Succeeded or Failed"
Jan 18 15:44:03.478: INFO: Pod "var-expansion-d1fec840-b03b-4ef2-903a-a356a334d849": Phase="Pending", Reason="", readiness=false. Elapsed: 3.989153ms
Jan 18 15:44:05.483: INFO: Pod "var-expansion-d1fec840-b03b-4ef2-903a-a356a334d849": Phase="Running", Reason="", readiness=true. Elapsed: 2.009285857s
Jan 18 15:44:07.483: INFO: Pod "var-expansion-d1fec840-b03b-4ef2-903a-a356a334d849": Phase="Running", Reason="", readiness=false. Elapsed: 4.009640838s
Jan 18 15:44:09.485: INFO: Pod "var-expansion-d1fec840-b03b-4ef2-903a-a356a334d849": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010970867s
STEP: Saw pod success 01/18/23 15:44:09.485
Jan 18 15:44:09.485: INFO: Pod "var-expansion-d1fec840-b03b-4ef2-903a-a356a334d849" satisfied condition "Succeeded or Failed"
Jan 18 15:44:09.490: INFO: Trying to get logs from node v1-25-1-18760-w2 pod var-expansion-d1fec840-b03b-4ef2-903a-a356a334d849 container dapi-container: <nil>
STEP: delete the pod 01/18/23 15:44:09.5
Jan 18 15:44:09.523: INFO: Waiting for pod var-expansion-d1fec840-b03b-4ef2-903a-a356a334d849 to disappear
Jan 18 15:44:09.527: INFO: Pod var-expansion-d1fec840-b03b-4ef2-903a-a356a334d849 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 18 15:44:09.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5008" for this suite. 01/18/23 15:44:09.533
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":152,"skipped":2723,"failed":0}
------------------------------
• [SLOW TEST] [6.143 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:44:03.401
    Jan 18 15:44:03.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename var-expansion 01/18/23 15:44:03.403
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:44:03.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:44:03.451
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 01/18/23 15:44:03.454
    Jan 18 15:44:03.474: INFO: Waiting up to 5m0s for pod "var-expansion-d1fec840-b03b-4ef2-903a-a356a334d849" in namespace "var-expansion-5008" to be "Succeeded or Failed"
    Jan 18 15:44:03.478: INFO: Pod "var-expansion-d1fec840-b03b-4ef2-903a-a356a334d849": Phase="Pending", Reason="", readiness=false. Elapsed: 3.989153ms
    Jan 18 15:44:05.483: INFO: Pod "var-expansion-d1fec840-b03b-4ef2-903a-a356a334d849": Phase="Running", Reason="", readiness=true. Elapsed: 2.009285857s
    Jan 18 15:44:07.483: INFO: Pod "var-expansion-d1fec840-b03b-4ef2-903a-a356a334d849": Phase="Running", Reason="", readiness=false. Elapsed: 4.009640838s
    Jan 18 15:44:09.485: INFO: Pod "var-expansion-d1fec840-b03b-4ef2-903a-a356a334d849": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010970867s
    STEP: Saw pod success 01/18/23 15:44:09.485
    Jan 18 15:44:09.485: INFO: Pod "var-expansion-d1fec840-b03b-4ef2-903a-a356a334d849" satisfied condition "Succeeded or Failed"
    Jan 18 15:44:09.490: INFO: Trying to get logs from node v1-25-1-18760-w2 pod var-expansion-d1fec840-b03b-4ef2-903a-a356a334d849 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 15:44:09.5
    Jan 18 15:44:09.523: INFO: Waiting for pod var-expansion-d1fec840-b03b-4ef2-903a-a356a334d849 to disappear
    Jan 18 15:44:09.527: INFO: Pod var-expansion-d1fec840-b03b-4ef2-903a-a356a334d849 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 18 15:44:09.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5008" for this suite. 01/18/23 15:44:09.533
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:44:09.548
Jan 18 15:44:09.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename subpath 01/18/23 15:44:09.55
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:44:09.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:44:09.578
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/18/23 15:44:09.582
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-v9kf 01/18/23 15:44:09.594
STEP: Creating a pod to test atomic-volume-subpath 01/18/23 15:44:09.594
Jan 18 15:44:09.606: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-v9kf" in namespace "subpath-7900" to be "Succeeded or Failed"
Jan 18 15:44:09.614: INFO: Pod "pod-subpath-test-configmap-v9kf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.640583ms
Jan 18 15:44:11.620: INFO: Pod "pod-subpath-test-configmap-v9kf": Phase="Running", Reason="", readiness=true. Elapsed: 2.013583494s
Jan 18 15:44:13.620: INFO: Pod "pod-subpath-test-configmap-v9kf": Phase="Running", Reason="", readiness=true. Elapsed: 4.013710468s
Jan 18 15:44:15.817: INFO: Pod "pod-subpath-test-configmap-v9kf": Phase="Running", Reason="", readiness=true. Elapsed: 6.210162769s
Jan 18 15:44:17.619: INFO: Pod "pod-subpath-test-configmap-v9kf": Phase="Running", Reason="", readiness=true. Elapsed: 8.012297499s
Jan 18 15:44:19.619: INFO: Pod "pod-subpath-test-configmap-v9kf": Phase="Running", Reason="", readiness=true. Elapsed: 10.012635704s
Jan 18 15:44:21.619: INFO: Pod "pod-subpath-test-configmap-v9kf": Phase="Running", Reason="", readiness=true. Elapsed: 12.012761336s
Jan 18 15:44:23.625: INFO: Pod "pod-subpath-test-configmap-v9kf": Phase="Running", Reason="", readiness=true. Elapsed: 14.018053497s
Jan 18 15:44:25.620: INFO: Pod "pod-subpath-test-configmap-v9kf": Phase="Running", Reason="", readiness=true. Elapsed: 16.013705206s
Jan 18 15:44:27.620: INFO: Pod "pod-subpath-test-configmap-v9kf": Phase="Running", Reason="", readiness=true. Elapsed: 18.013474931s
Jan 18 15:44:29.619: INFO: Pod "pod-subpath-test-configmap-v9kf": Phase="Running", Reason="", readiness=true. Elapsed: 20.012916987s
Jan 18 15:44:31.622: INFO: Pod "pod-subpath-test-configmap-v9kf": Phase="Running", Reason="", readiness=false. Elapsed: 22.01588718s
Jan 18 15:44:33.623: INFO: Pod "pod-subpath-test-configmap-v9kf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016439806s
STEP: Saw pod success 01/18/23 15:44:33.623
Jan 18 15:44:33.624: INFO: Pod "pod-subpath-test-configmap-v9kf" satisfied condition "Succeeded or Failed"
Jan 18 15:44:33.631: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-subpath-test-configmap-v9kf container test-container-subpath-configmap-v9kf: <nil>
STEP: delete the pod 01/18/23 15:44:33.647
Jan 18 15:44:33.666: INFO: Waiting for pod pod-subpath-test-configmap-v9kf to disappear
Jan 18 15:44:33.670: INFO: Pod pod-subpath-test-configmap-v9kf no longer exists
STEP: Deleting pod pod-subpath-test-configmap-v9kf 01/18/23 15:44:33.671
Jan 18 15:44:33.671: INFO: Deleting pod "pod-subpath-test-configmap-v9kf" in namespace "subpath-7900"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 18 15:44:33.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7900" for this suite. 01/18/23 15:44:33.682
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":153,"skipped":2729,"failed":0}
------------------------------
• [SLOW TEST] [24.142 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:44:09.548
    Jan 18 15:44:09.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename subpath 01/18/23 15:44:09.55
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:44:09.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:44:09.578
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/18/23 15:44:09.582
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-v9kf 01/18/23 15:44:09.594
    STEP: Creating a pod to test atomic-volume-subpath 01/18/23 15:44:09.594
    Jan 18 15:44:09.606: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-v9kf" in namespace "subpath-7900" to be "Succeeded or Failed"
    Jan 18 15:44:09.614: INFO: Pod "pod-subpath-test-configmap-v9kf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.640583ms
    Jan 18 15:44:11.620: INFO: Pod "pod-subpath-test-configmap-v9kf": Phase="Running", Reason="", readiness=true. Elapsed: 2.013583494s
    Jan 18 15:44:13.620: INFO: Pod "pod-subpath-test-configmap-v9kf": Phase="Running", Reason="", readiness=true. Elapsed: 4.013710468s
    Jan 18 15:44:15.817: INFO: Pod "pod-subpath-test-configmap-v9kf": Phase="Running", Reason="", readiness=true. Elapsed: 6.210162769s
    Jan 18 15:44:17.619: INFO: Pod "pod-subpath-test-configmap-v9kf": Phase="Running", Reason="", readiness=true. Elapsed: 8.012297499s
    Jan 18 15:44:19.619: INFO: Pod "pod-subpath-test-configmap-v9kf": Phase="Running", Reason="", readiness=true. Elapsed: 10.012635704s
    Jan 18 15:44:21.619: INFO: Pod "pod-subpath-test-configmap-v9kf": Phase="Running", Reason="", readiness=true. Elapsed: 12.012761336s
    Jan 18 15:44:23.625: INFO: Pod "pod-subpath-test-configmap-v9kf": Phase="Running", Reason="", readiness=true. Elapsed: 14.018053497s
    Jan 18 15:44:25.620: INFO: Pod "pod-subpath-test-configmap-v9kf": Phase="Running", Reason="", readiness=true. Elapsed: 16.013705206s
    Jan 18 15:44:27.620: INFO: Pod "pod-subpath-test-configmap-v9kf": Phase="Running", Reason="", readiness=true. Elapsed: 18.013474931s
    Jan 18 15:44:29.619: INFO: Pod "pod-subpath-test-configmap-v9kf": Phase="Running", Reason="", readiness=true. Elapsed: 20.012916987s
    Jan 18 15:44:31.622: INFO: Pod "pod-subpath-test-configmap-v9kf": Phase="Running", Reason="", readiness=false. Elapsed: 22.01588718s
    Jan 18 15:44:33.623: INFO: Pod "pod-subpath-test-configmap-v9kf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016439806s
    STEP: Saw pod success 01/18/23 15:44:33.623
    Jan 18 15:44:33.624: INFO: Pod "pod-subpath-test-configmap-v9kf" satisfied condition "Succeeded or Failed"
    Jan 18 15:44:33.631: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-subpath-test-configmap-v9kf container test-container-subpath-configmap-v9kf: <nil>
    STEP: delete the pod 01/18/23 15:44:33.647
    Jan 18 15:44:33.666: INFO: Waiting for pod pod-subpath-test-configmap-v9kf to disappear
    Jan 18 15:44:33.670: INFO: Pod pod-subpath-test-configmap-v9kf no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-v9kf 01/18/23 15:44:33.671
    Jan 18 15:44:33.671: INFO: Deleting pod "pod-subpath-test-configmap-v9kf" in namespace "subpath-7900"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 18 15:44:33.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7900" for this suite. 01/18/23 15:44:33.682
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:44:33.696
Jan 18 15:44:33.697: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename subpath 01/18/23 15:44:33.698
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:44:33.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:44:33.885
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/18/23 15:44:33.89
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-mcj4 01/18/23 15:44:33.902
STEP: Creating a pod to test atomic-volume-subpath 01/18/23 15:44:33.902
Jan 18 15:44:33.912: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-mcj4" in namespace "subpath-2016" to be "Succeeded or Failed"
Jan 18 15:44:33.918: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.665345ms
Jan 18 15:44:35.924: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011535358s
Jan 18 15:44:37.922: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Running", Reason="", readiness=true. Elapsed: 4.010396814s
Jan 18 15:44:39.925: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Running", Reason="", readiness=true. Elapsed: 6.012511536s
Jan 18 15:44:41.925: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Running", Reason="", readiness=true. Elapsed: 8.012760739s
Jan 18 15:44:43.923: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Running", Reason="", readiness=true. Elapsed: 10.010870575s
Jan 18 15:44:45.924: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Running", Reason="", readiness=true. Elapsed: 12.012283654s
Jan 18 15:44:47.924: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Running", Reason="", readiness=true. Elapsed: 14.012310653s
Jan 18 15:44:49.923: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Running", Reason="", readiness=true. Elapsed: 16.011221925s
Jan 18 15:44:51.924: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Running", Reason="", readiness=true. Elapsed: 18.011799308s
Jan 18 15:44:53.924: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Running", Reason="", readiness=true. Elapsed: 20.011907325s
Jan 18 15:44:55.924: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Running", Reason="", readiness=true. Elapsed: 22.011539644s
Jan 18 15:44:57.923: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Running", Reason="", readiness=false. Elapsed: 24.01068858s
Jan 18 15:44:59.923: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.011195749s
STEP: Saw pod success 01/18/23 15:44:59.923
Jan 18 15:44:59.924: INFO: Pod "pod-subpath-test-secret-mcj4" satisfied condition "Succeeded or Failed"
Jan 18 15:44:59.928: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-subpath-test-secret-mcj4 container test-container-subpath-secret-mcj4: <nil>
STEP: delete the pod 01/18/23 15:44:59.94
Jan 18 15:44:59.955: INFO: Waiting for pod pod-subpath-test-secret-mcj4 to disappear
Jan 18 15:44:59.960: INFO: Pod pod-subpath-test-secret-mcj4 no longer exists
STEP: Deleting pod pod-subpath-test-secret-mcj4 01/18/23 15:44:59.961
Jan 18 15:44:59.961: INFO: Deleting pod "pod-subpath-test-secret-mcj4" in namespace "subpath-2016"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 18 15:44:59.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2016" for this suite. 01/18/23 15:44:59.97
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":154,"skipped":2740,"failed":0}
------------------------------
• [SLOW TEST] [26.281 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:44:33.696
    Jan 18 15:44:33.697: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename subpath 01/18/23 15:44:33.698
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:44:33.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:44:33.885
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/18/23 15:44:33.89
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-mcj4 01/18/23 15:44:33.902
    STEP: Creating a pod to test atomic-volume-subpath 01/18/23 15:44:33.902
    Jan 18 15:44:33.912: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-mcj4" in namespace "subpath-2016" to be "Succeeded or Failed"
    Jan 18 15:44:33.918: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.665345ms
    Jan 18 15:44:35.924: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011535358s
    Jan 18 15:44:37.922: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Running", Reason="", readiness=true. Elapsed: 4.010396814s
    Jan 18 15:44:39.925: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Running", Reason="", readiness=true. Elapsed: 6.012511536s
    Jan 18 15:44:41.925: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Running", Reason="", readiness=true. Elapsed: 8.012760739s
    Jan 18 15:44:43.923: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Running", Reason="", readiness=true. Elapsed: 10.010870575s
    Jan 18 15:44:45.924: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Running", Reason="", readiness=true. Elapsed: 12.012283654s
    Jan 18 15:44:47.924: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Running", Reason="", readiness=true. Elapsed: 14.012310653s
    Jan 18 15:44:49.923: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Running", Reason="", readiness=true. Elapsed: 16.011221925s
    Jan 18 15:44:51.924: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Running", Reason="", readiness=true. Elapsed: 18.011799308s
    Jan 18 15:44:53.924: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Running", Reason="", readiness=true. Elapsed: 20.011907325s
    Jan 18 15:44:55.924: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Running", Reason="", readiness=true. Elapsed: 22.011539644s
    Jan 18 15:44:57.923: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Running", Reason="", readiness=false. Elapsed: 24.01068858s
    Jan 18 15:44:59.923: INFO: Pod "pod-subpath-test-secret-mcj4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.011195749s
    STEP: Saw pod success 01/18/23 15:44:59.923
    Jan 18 15:44:59.924: INFO: Pod "pod-subpath-test-secret-mcj4" satisfied condition "Succeeded or Failed"
    Jan 18 15:44:59.928: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-subpath-test-secret-mcj4 container test-container-subpath-secret-mcj4: <nil>
    STEP: delete the pod 01/18/23 15:44:59.94
    Jan 18 15:44:59.955: INFO: Waiting for pod pod-subpath-test-secret-mcj4 to disappear
    Jan 18 15:44:59.960: INFO: Pod pod-subpath-test-secret-mcj4 no longer exists
    STEP: Deleting pod pod-subpath-test-secret-mcj4 01/18/23 15:44:59.961
    Jan 18 15:44:59.961: INFO: Deleting pod "pod-subpath-test-secret-mcj4" in namespace "subpath-2016"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 18 15:44:59.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-2016" for this suite. 01/18/23 15:44:59.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:44:59.982
Jan 18 15:44:59.982: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename kubectl 01/18/23 15:44:59.985
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:45:00.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:45:00.012
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 01/18/23 15:45:00.018
Jan 18 15:45:00.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2090 api-versions'
Jan 18 15:45:00.127: INFO: stderr: ""
Jan 18 15:45:00.127: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nhelm.toolkit.fluxcd.io/v2beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnotification.toolkit.fluxcd.io/v1beta1\nnotification.toolkit.fluxcd.io/v1beta2\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsource.toolkit.fluxcd.io/v1beta1\nsource.toolkit.fluxcd.io/v1beta2\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 15:45:00.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2090" for this suite. 01/18/23 15:45:00.133
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":155,"skipped":2753,"failed":0}
------------------------------
• [0.157 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:44:59.982
    Jan 18 15:44:59.982: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename kubectl 01/18/23 15:44:59.985
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:45:00.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:45:00.012
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 01/18/23 15:45:00.018
    Jan 18 15:45:00.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2090 api-versions'
    Jan 18 15:45:00.127: INFO: stderr: ""
    Jan 18 15:45:00.127: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nhelm.toolkit.fluxcd.io/v2beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnotification.toolkit.fluxcd.io/v1beta1\nnotification.toolkit.fluxcd.io/v1beta2\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsource.toolkit.fluxcd.io/v1beta1\nsource.toolkit.fluxcd.io/v1beta2\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 15:45:00.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2090" for this suite. 01/18/23 15:45:00.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:45:00.14
Jan 18 15:45:00.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename dns 01/18/23 15:45:00.143
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:45:00.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:45:00.166
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 01/18/23 15:45:00.171
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-938.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-938.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 01/18/23 15:45:00.178
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-938.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-938.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 01/18/23 15:45:00.179
STEP: creating a pod to probe DNS 01/18/23 15:45:00.179
STEP: submitting the pod to kubernetes 01/18/23 15:45:00.181
Jan 18 15:45:00.193: INFO: Waiting up to 15m0s for pod "dns-test-64854e92-7c29-451e-ba9b-9fbc8e0c4e68" in namespace "dns-938" to be "running"
Jan 18 15:45:00.201: INFO: Pod "dns-test-64854e92-7c29-451e-ba9b-9fbc8e0c4e68": Phase="Pending", Reason="", readiness=false. Elapsed: 7.497333ms
Jan 18 15:45:02.209: INFO: Pod "dns-test-64854e92-7c29-451e-ba9b-9fbc8e0c4e68": Phase="Running", Reason="", readiness=true. Elapsed: 2.01527251s
Jan 18 15:45:02.209: INFO: Pod "dns-test-64854e92-7c29-451e-ba9b-9fbc8e0c4e68" satisfied condition "running"
STEP: retrieving the pod 01/18/23 15:45:02.209
STEP: looking for the results for each expected name from probers 01/18/23 15:45:02.215
Jan 18 15:45:02.236: INFO: DNS probes using dns-938/dns-test-64854e92-7c29-451e-ba9b-9fbc8e0c4e68 succeeded

STEP: deleting the pod 01/18/23 15:45:02.236
STEP: deleting the test headless service 01/18/23 15:45:02.262
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 18 15:45:02.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-938" for this suite. 01/18/23 15:45:02.291
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":156,"skipped":2759,"failed":0}
------------------------------
• [2.167 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:45:00.14
    Jan 18 15:45:00.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename dns 01/18/23 15:45:00.143
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:45:00.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:45:00.166
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 01/18/23 15:45:00.171
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-938.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-938.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     01/18/23 15:45:00.178
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-938.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-938.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     01/18/23 15:45:00.179
    STEP: creating a pod to probe DNS 01/18/23 15:45:00.179
    STEP: submitting the pod to kubernetes 01/18/23 15:45:00.181
    Jan 18 15:45:00.193: INFO: Waiting up to 15m0s for pod "dns-test-64854e92-7c29-451e-ba9b-9fbc8e0c4e68" in namespace "dns-938" to be "running"
    Jan 18 15:45:00.201: INFO: Pod "dns-test-64854e92-7c29-451e-ba9b-9fbc8e0c4e68": Phase="Pending", Reason="", readiness=false. Elapsed: 7.497333ms
    Jan 18 15:45:02.209: INFO: Pod "dns-test-64854e92-7c29-451e-ba9b-9fbc8e0c4e68": Phase="Running", Reason="", readiness=true. Elapsed: 2.01527251s
    Jan 18 15:45:02.209: INFO: Pod "dns-test-64854e92-7c29-451e-ba9b-9fbc8e0c4e68" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 15:45:02.209
    STEP: looking for the results for each expected name from probers 01/18/23 15:45:02.215
    Jan 18 15:45:02.236: INFO: DNS probes using dns-938/dns-test-64854e92-7c29-451e-ba9b-9fbc8e0c4e68 succeeded

    STEP: deleting the pod 01/18/23 15:45:02.236
    STEP: deleting the test headless service 01/18/23 15:45:02.262
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 18 15:45:02.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-938" for this suite. 01/18/23 15:45:02.291
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:45:02.309
Jan 18 15:45:02.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename replicaset 01/18/23 15:45:02.311
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:45:02.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:45:02.341
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 01/18/23 15:45:02.364
STEP: Verify that the required pods have come up. 01/18/23 15:45:02.37
Jan 18 15:45:02.376: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 18 15:45:07.386: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 15:45:07.387
STEP: Getting /status 01/18/23 15:45:07.387
Jan 18 15:45:07.416: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 01/18/23 15:45:07.416
Jan 18 15:45:07.429: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 01/18/23 15:45:07.429
Jan 18 15:45:07.433: INFO: Observed &ReplicaSet event: ADDED
Jan 18 15:45:07.434: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 15:45:07.434: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 15:45:07.435: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 15:45:07.435: INFO: Found replicaset test-rs in namespace replicaset-8880 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 18 15:45:07.435: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 01/18/23 15:45:07.436
Jan 18 15:45:07.436: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 18 15:45:07.447: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 01/18/23 15:45:07.447
Jan 18 15:45:07.453: INFO: Observed &ReplicaSet event: ADDED
Jan 18 15:45:07.453: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 15:45:07.453: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 15:45:07.453: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 15:45:07.453: INFO: Observed replicaset test-rs in namespace replicaset-8880 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 18 15:45:07.454: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 15:45:07.454: INFO: Found replicaset test-rs in namespace replicaset-8880 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan 18 15:45:07.454: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 18 15:45:07.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8880" for this suite. 01/18/23 15:45:07.461
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":157,"skipped":2782,"failed":0}
------------------------------
• [SLOW TEST] [5.160 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:45:02.309
    Jan 18 15:45:02.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename replicaset 01/18/23 15:45:02.311
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:45:02.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:45:02.341
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 01/18/23 15:45:02.364
    STEP: Verify that the required pods have come up. 01/18/23 15:45:02.37
    Jan 18 15:45:02.376: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 18 15:45:07.386: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 15:45:07.387
    STEP: Getting /status 01/18/23 15:45:07.387
    Jan 18 15:45:07.416: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 01/18/23 15:45:07.416
    Jan 18 15:45:07.429: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 01/18/23 15:45:07.429
    Jan 18 15:45:07.433: INFO: Observed &ReplicaSet event: ADDED
    Jan 18 15:45:07.434: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 15:45:07.434: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 15:45:07.435: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 15:45:07.435: INFO: Found replicaset test-rs in namespace replicaset-8880 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 18 15:45:07.435: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 01/18/23 15:45:07.436
    Jan 18 15:45:07.436: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 18 15:45:07.447: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 01/18/23 15:45:07.447
    Jan 18 15:45:07.453: INFO: Observed &ReplicaSet event: ADDED
    Jan 18 15:45:07.453: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 15:45:07.453: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 15:45:07.453: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 15:45:07.453: INFO: Observed replicaset test-rs in namespace replicaset-8880 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 18 15:45:07.454: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 15:45:07.454: INFO: Found replicaset test-rs in namespace replicaset-8880 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jan 18 15:45:07.454: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 18 15:45:07.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-8880" for this suite. 01/18/23 15:45:07.461
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:45:07.477
Jan 18 15:45:07.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename containers 01/18/23 15:45:07.479
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:45:07.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:45:07.508
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 01/18/23 15:45:07.514
Jan 18 15:45:07.525: INFO: Waiting up to 5m0s for pod "client-containers-3edbea75-518d-43a1-afbf-661892a77dfe" in namespace "containers-7743" to be "Succeeded or Failed"
Jan 18 15:45:07.530: INFO: Pod "client-containers-3edbea75-518d-43a1-afbf-661892a77dfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.742415ms
Jan 18 15:45:09.535: INFO: Pod "client-containers-3edbea75-518d-43a1-afbf-661892a77dfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010272262s
Jan 18 15:45:11.535: INFO: Pod "client-containers-3edbea75-518d-43a1-afbf-661892a77dfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009567373s
STEP: Saw pod success 01/18/23 15:45:11.535
Jan 18 15:45:11.536: INFO: Pod "client-containers-3edbea75-518d-43a1-afbf-661892a77dfe" satisfied condition "Succeeded or Failed"
Jan 18 15:45:11.539: INFO: Trying to get logs from node v1-25-1-18760-w2 pod client-containers-3edbea75-518d-43a1-afbf-661892a77dfe container agnhost-container: <nil>
STEP: delete the pod 01/18/23 15:45:11.545
Jan 18 15:45:11.565: INFO: Waiting for pod client-containers-3edbea75-518d-43a1-afbf-661892a77dfe to disappear
Jan 18 15:45:11.569: INFO: Pod client-containers-3edbea75-518d-43a1-afbf-661892a77dfe no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 18 15:45:11.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7743" for this suite. 01/18/23 15:45:11.576
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":158,"skipped":2789,"failed":0}
------------------------------
• [4.108 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:45:07.477
    Jan 18 15:45:07.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename containers 01/18/23 15:45:07.479
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:45:07.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:45:07.508
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 01/18/23 15:45:07.514
    Jan 18 15:45:07.525: INFO: Waiting up to 5m0s for pod "client-containers-3edbea75-518d-43a1-afbf-661892a77dfe" in namespace "containers-7743" to be "Succeeded or Failed"
    Jan 18 15:45:07.530: INFO: Pod "client-containers-3edbea75-518d-43a1-afbf-661892a77dfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.742415ms
    Jan 18 15:45:09.535: INFO: Pod "client-containers-3edbea75-518d-43a1-afbf-661892a77dfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010272262s
    Jan 18 15:45:11.535: INFO: Pod "client-containers-3edbea75-518d-43a1-afbf-661892a77dfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009567373s
    STEP: Saw pod success 01/18/23 15:45:11.535
    Jan 18 15:45:11.536: INFO: Pod "client-containers-3edbea75-518d-43a1-afbf-661892a77dfe" satisfied condition "Succeeded or Failed"
    Jan 18 15:45:11.539: INFO: Trying to get logs from node v1-25-1-18760-w2 pod client-containers-3edbea75-518d-43a1-afbf-661892a77dfe container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 15:45:11.545
    Jan 18 15:45:11.565: INFO: Waiting for pod client-containers-3edbea75-518d-43a1-afbf-661892a77dfe to disappear
    Jan 18 15:45:11.569: INFO: Pod client-containers-3edbea75-518d-43a1-afbf-661892a77dfe no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 18 15:45:11.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-7743" for this suite. 01/18/23 15:45:11.576
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:45:11.587
Jan 18 15:45:11.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 15:45:11.589
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:45:11.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:45:11.613
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-ab8a23ab-fce9-4d99-8361-f44474578fe8 01/18/23 15:45:11.625
STEP: Creating the pod 01/18/23 15:45:11.637
Jan 18 15:45:11.645: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c7b5dbf1-408d-4c29-b639-a3e32e1f23e8" in namespace "projected-1209" to be "running and ready"
Jan 18 15:45:11.651: INFO: Pod "pod-projected-configmaps-c7b5dbf1-408d-4c29-b639-a3e32e1f23e8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.713089ms
Jan 18 15:45:11.651: INFO: The phase of Pod pod-projected-configmaps-c7b5dbf1-408d-4c29-b639-a3e32e1f23e8 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:45:13.656: INFO: Pod "pod-projected-configmaps-c7b5dbf1-408d-4c29-b639-a3e32e1f23e8": Phase="Running", Reason="", readiness=true. Elapsed: 2.010884369s
Jan 18 15:45:13.656: INFO: The phase of Pod pod-projected-configmaps-c7b5dbf1-408d-4c29-b639-a3e32e1f23e8 is Running (Ready = true)
Jan 18 15:45:13.656: INFO: Pod "pod-projected-configmaps-c7b5dbf1-408d-4c29-b639-a3e32e1f23e8" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-ab8a23ab-fce9-4d99-8361-f44474578fe8 01/18/23 15:45:13.672
STEP: waiting to observe update in volume 01/18/23 15:45:13.68
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 18 15:45:15.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1209" for this suite. 01/18/23 15:45:15.706
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":159,"skipped":2802,"failed":0}
------------------------------
• [4.128 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:45:11.587
    Jan 18 15:45:11.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 15:45:11.589
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:45:11.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:45:11.613
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-ab8a23ab-fce9-4d99-8361-f44474578fe8 01/18/23 15:45:11.625
    STEP: Creating the pod 01/18/23 15:45:11.637
    Jan 18 15:45:11.645: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c7b5dbf1-408d-4c29-b639-a3e32e1f23e8" in namespace "projected-1209" to be "running and ready"
    Jan 18 15:45:11.651: INFO: Pod "pod-projected-configmaps-c7b5dbf1-408d-4c29-b639-a3e32e1f23e8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.713089ms
    Jan 18 15:45:11.651: INFO: The phase of Pod pod-projected-configmaps-c7b5dbf1-408d-4c29-b639-a3e32e1f23e8 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:45:13.656: INFO: Pod "pod-projected-configmaps-c7b5dbf1-408d-4c29-b639-a3e32e1f23e8": Phase="Running", Reason="", readiness=true. Elapsed: 2.010884369s
    Jan 18 15:45:13.656: INFO: The phase of Pod pod-projected-configmaps-c7b5dbf1-408d-4c29-b639-a3e32e1f23e8 is Running (Ready = true)
    Jan 18 15:45:13.656: INFO: Pod "pod-projected-configmaps-c7b5dbf1-408d-4c29-b639-a3e32e1f23e8" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-ab8a23ab-fce9-4d99-8361-f44474578fe8 01/18/23 15:45:13.672
    STEP: waiting to observe update in volume 01/18/23 15:45:13.68
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 18 15:45:15.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1209" for this suite. 01/18/23 15:45:15.706
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:45:15.719
Jan 18 15:45:15.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename emptydir 01/18/23 15:45:15.723
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:45:15.743
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:45:15.748
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/18/23 15:45:15.752
Jan 18 15:45:15.761: INFO: Waiting up to 5m0s for pod "pod-652eb86b-72ed-46fb-aea8-0103ceea186d" in namespace "emptydir-5739" to be "Succeeded or Failed"
Jan 18 15:45:15.773: INFO: Pod "pod-652eb86b-72ed-46fb-aea8-0103ceea186d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.969897ms
Jan 18 15:45:17.778: INFO: Pod "pod-652eb86b-72ed-46fb-aea8-0103ceea186d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016542509s
Jan 18 15:45:19.778: INFO: Pod "pod-652eb86b-72ed-46fb-aea8-0103ceea186d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016544991s
STEP: Saw pod success 01/18/23 15:45:19.779
Jan 18 15:45:19.779: INFO: Pod "pod-652eb86b-72ed-46fb-aea8-0103ceea186d" satisfied condition "Succeeded or Failed"
Jan 18 15:45:19.783: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-652eb86b-72ed-46fb-aea8-0103ceea186d container test-container: <nil>
STEP: delete the pod 01/18/23 15:45:19.791
Jan 18 15:45:19.810: INFO: Waiting for pod pod-652eb86b-72ed-46fb-aea8-0103ceea186d to disappear
Jan 18 15:45:19.822: INFO: Pod pod-652eb86b-72ed-46fb-aea8-0103ceea186d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 15:45:19.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5739" for this suite. 01/18/23 15:45:19.831
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":160,"skipped":2810,"failed":0}
------------------------------
• [4.120 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:45:15.719
    Jan 18 15:45:15.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename emptydir 01/18/23 15:45:15.723
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:45:15.743
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:45:15.748
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/18/23 15:45:15.752
    Jan 18 15:45:15.761: INFO: Waiting up to 5m0s for pod "pod-652eb86b-72ed-46fb-aea8-0103ceea186d" in namespace "emptydir-5739" to be "Succeeded or Failed"
    Jan 18 15:45:15.773: INFO: Pod "pod-652eb86b-72ed-46fb-aea8-0103ceea186d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.969897ms
    Jan 18 15:45:17.778: INFO: Pod "pod-652eb86b-72ed-46fb-aea8-0103ceea186d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016542509s
    Jan 18 15:45:19.778: INFO: Pod "pod-652eb86b-72ed-46fb-aea8-0103ceea186d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016544991s
    STEP: Saw pod success 01/18/23 15:45:19.779
    Jan 18 15:45:19.779: INFO: Pod "pod-652eb86b-72ed-46fb-aea8-0103ceea186d" satisfied condition "Succeeded or Failed"
    Jan 18 15:45:19.783: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-652eb86b-72ed-46fb-aea8-0103ceea186d container test-container: <nil>
    STEP: delete the pod 01/18/23 15:45:19.791
    Jan 18 15:45:19.810: INFO: Waiting for pod pod-652eb86b-72ed-46fb-aea8-0103ceea186d to disappear
    Jan 18 15:45:19.822: INFO: Pod pod-652eb86b-72ed-46fb-aea8-0103ceea186d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 15:45:19.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5739" for this suite. 01/18/23 15:45:19.831
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:45:19.847
Jan 18 15:45:19.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename configmap 01/18/23 15:45:19.848
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:45:19.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:45:19.876
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-1e0b9db0-c089-42f0-8243-ea7094e63899 01/18/23 15:45:19.882
STEP: Creating a pod to test consume configMaps 01/18/23 15:45:19.887
Jan 18 15:45:19.898: INFO: Waiting up to 5m0s for pod "pod-configmaps-6df0a052-6110-4ded-b532-bd85456e50a7" in namespace "configmap-1796" to be "Succeeded or Failed"
Jan 18 15:45:19.909: INFO: Pod "pod-configmaps-6df0a052-6110-4ded-b532-bd85456e50a7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.359759ms
Jan 18 15:45:21.915: INFO: Pod "pod-configmaps-6df0a052-6110-4ded-b532-bd85456e50a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016671102s
Jan 18 15:45:23.914: INFO: Pod "pod-configmaps-6df0a052-6110-4ded-b532-bd85456e50a7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015318206s
Jan 18 15:45:25.915: INFO: Pod "pod-configmaps-6df0a052-6110-4ded-b532-bd85456e50a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017058947s
STEP: Saw pod success 01/18/23 15:45:25.916
Jan 18 15:45:25.916: INFO: Pod "pod-configmaps-6df0a052-6110-4ded-b532-bd85456e50a7" satisfied condition "Succeeded or Failed"
Jan 18 15:45:25.919: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-6df0a052-6110-4ded-b532-bd85456e50a7 container configmap-volume-test: <nil>
STEP: delete the pod 01/18/23 15:45:25.928
Jan 18 15:45:25.956: INFO: Waiting for pod pod-configmaps-6df0a052-6110-4ded-b532-bd85456e50a7 to disappear
Jan 18 15:45:25.961: INFO: Pod pod-configmaps-6df0a052-6110-4ded-b532-bd85456e50a7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 15:45:25.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1796" for this suite. 01/18/23 15:45:25.968
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":161,"skipped":2855,"failed":0}
------------------------------
• [SLOW TEST] [6.127 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:45:19.847
    Jan 18 15:45:19.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename configmap 01/18/23 15:45:19.848
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:45:19.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:45:19.876
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-1e0b9db0-c089-42f0-8243-ea7094e63899 01/18/23 15:45:19.882
    STEP: Creating a pod to test consume configMaps 01/18/23 15:45:19.887
    Jan 18 15:45:19.898: INFO: Waiting up to 5m0s for pod "pod-configmaps-6df0a052-6110-4ded-b532-bd85456e50a7" in namespace "configmap-1796" to be "Succeeded or Failed"
    Jan 18 15:45:19.909: INFO: Pod "pod-configmaps-6df0a052-6110-4ded-b532-bd85456e50a7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.359759ms
    Jan 18 15:45:21.915: INFO: Pod "pod-configmaps-6df0a052-6110-4ded-b532-bd85456e50a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016671102s
    Jan 18 15:45:23.914: INFO: Pod "pod-configmaps-6df0a052-6110-4ded-b532-bd85456e50a7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015318206s
    Jan 18 15:45:25.915: INFO: Pod "pod-configmaps-6df0a052-6110-4ded-b532-bd85456e50a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017058947s
    STEP: Saw pod success 01/18/23 15:45:25.916
    Jan 18 15:45:25.916: INFO: Pod "pod-configmaps-6df0a052-6110-4ded-b532-bd85456e50a7" satisfied condition "Succeeded or Failed"
    Jan 18 15:45:25.919: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-6df0a052-6110-4ded-b532-bd85456e50a7 container configmap-volume-test: <nil>
    STEP: delete the pod 01/18/23 15:45:25.928
    Jan 18 15:45:25.956: INFO: Waiting for pod pod-configmaps-6df0a052-6110-4ded-b532-bd85456e50a7 to disappear
    Jan 18 15:45:25.961: INFO: Pod pod-configmaps-6df0a052-6110-4ded-b532-bd85456e50a7 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 15:45:25.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1796" for this suite. 01/18/23 15:45:25.968
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:45:25.975
Jan 18 15:45:25.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename deployment 01/18/23 15:45:25.977
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:45:25.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:45:25.997
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jan 18 15:45:26.003: INFO: Creating deployment "webserver-deployment"
Jan 18 15:45:26.011: INFO: Waiting for observed generation 1
Jan 18 15:45:28.146: INFO: Waiting for all required pods to come up
Jan 18 15:45:28.156: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 01/18/23 15:45:28.156
Jan 18 15:45:28.156: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-zf49v" in namespace "deployment-4134" to be "running"
Jan 18 15:45:28.157: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-44m42" in namespace "deployment-4134" to be "running"
Jan 18 15:45:28.157: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-5pvnc" in namespace "deployment-4134" to be "running"
Jan 18 15:45:28.157: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-8xcp6" in namespace "deployment-4134" to be "running"
Jan 18 15:45:28.157: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-b4f2v" in namespace "deployment-4134" to be "running"
Jan 18 15:45:28.157: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-mfkzq" in namespace "deployment-4134" to be "running"
Jan 18 15:45:28.157: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-n2dtc" in namespace "deployment-4134" to be "running"
Jan 18 15:45:28.158: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rlhld" in namespace "deployment-4134" to be "running"
Jan 18 15:45:28.158: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-vg92x" in namespace "deployment-4134" to be "running"
Jan 18 15:45:28.163: INFO: Pod "webserver-deployment-845c8977d9-zf49v": Phase="Pending", Reason="", readiness=false. Elapsed: 6.256471ms
Jan 18 15:45:28.164: INFO: Pod "webserver-deployment-845c8977d9-44m42": Phase="Pending", Reason="", readiness=false. Elapsed: 7.020567ms
Jan 18 15:45:28.170: INFO: Pod "webserver-deployment-845c8977d9-5pvnc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.615833ms
Jan 18 15:45:28.170: INFO: Pod "webserver-deployment-845c8977d9-b4f2v": Phase="Pending", Reason="", readiness=false. Elapsed: 12.509036ms
Jan 18 15:45:28.170: INFO: Pod "webserver-deployment-845c8977d9-8xcp6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.873192ms
Jan 18 15:45:28.179: INFO: Pod "webserver-deployment-845c8977d9-rlhld": Phase="Pending", Reason="", readiness=false. Elapsed: 21.644763ms
Jan 18 15:45:28.187: INFO: Pod "webserver-deployment-845c8977d9-vg92x": Phase="Pending", Reason="", readiness=false. Elapsed: 29.07978ms
Jan 18 15:45:28.187: INFO: Pod "webserver-deployment-845c8977d9-n2dtc": Phase="Pending", Reason="", readiness=false. Elapsed: 29.424298ms
Jan 18 15:45:28.201: INFO: Pod "webserver-deployment-845c8977d9-mfkzq": Phase="Pending", Reason="", readiness=false. Elapsed: 43.608484ms
Jan 18 15:45:30.168: INFO: Pod "webserver-deployment-845c8977d9-zf49v": Phase="Running", Reason="", readiness=true. Elapsed: 2.01117259s
Jan 18 15:45:30.168: INFO: Pod "webserver-deployment-845c8977d9-zf49v" satisfied condition "running"
Jan 18 15:45:30.168: INFO: Pod "webserver-deployment-845c8977d9-44m42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011695003s
Jan 18 15:45:30.177: INFO: Pod "webserver-deployment-845c8977d9-8xcp6": Phase="Running", Reason="", readiness=true. Elapsed: 2.019629206s
Jan 18 15:45:30.177: INFO: Pod "webserver-deployment-845c8977d9-8xcp6" satisfied condition "running"
Jan 18 15:45:30.177: INFO: Pod "webserver-deployment-845c8977d9-b4f2v": Phase="Running", Reason="", readiness=true. Elapsed: 2.01966447s
Jan 18 15:45:30.177: INFO: Pod "webserver-deployment-845c8977d9-b4f2v" satisfied condition "running"
Jan 18 15:45:30.182: INFO: Pod "webserver-deployment-845c8977d9-5pvnc": Phase="Running", Reason="", readiness=true. Elapsed: 2.025481359s
Jan 18 15:45:30.183: INFO: Pod "webserver-deployment-845c8977d9-5pvnc" satisfied condition "running"
Jan 18 15:45:30.192: INFO: Pod "webserver-deployment-845c8977d9-rlhld": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034682315s
Jan 18 15:45:30.200: INFO: Pod "webserver-deployment-845c8977d9-n2dtc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042872293s
Jan 18 15:45:30.201: INFO: Pod "webserver-deployment-845c8977d9-vg92x": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043524783s
Jan 18 15:45:30.208: INFO: Pod "webserver-deployment-845c8977d9-mfkzq": Phase="Running", Reason="", readiness=true. Elapsed: 2.050591538s
Jan 18 15:45:30.208: INFO: Pod "webserver-deployment-845c8977d9-mfkzq" satisfied condition "running"
Jan 18 15:45:32.169: INFO: Pod "webserver-deployment-845c8977d9-44m42": Phase="Running", Reason="", readiness=true. Elapsed: 4.012243959s
Jan 18 15:45:32.169: INFO: Pod "webserver-deployment-845c8977d9-44m42" satisfied condition "running"
Jan 18 15:45:32.185: INFO: Pod "webserver-deployment-845c8977d9-rlhld": Phase="Running", Reason="", readiness=true. Elapsed: 4.027635272s
Jan 18 15:45:32.185: INFO: Pod "webserver-deployment-845c8977d9-rlhld" satisfied condition "running"
Jan 18 15:45:32.192: INFO: Pod "webserver-deployment-845c8977d9-vg92x": Phase="Running", Reason="", readiness=true. Elapsed: 4.034016493s
Jan 18 15:45:32.192: INFO: Pod "webserver-deployment-845c8977d9-n2dtc": Phase="Running", Reason="", readiness=true. Elapsed: 4.034778154s
Jan 18 15:45:32.192: INFO: Pod "webserver-deployment-845c8977d9-n2dtc" satisfied condition "running"
Jan 18 15:45:32.192: INFO: Pod "webserver-deployment-845c8977d9-vg92x" satisfied condition "running"
Jan 18 15:45:32.192: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 18 15:45:32.201: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 18 15:45:32.215: INFO: Updating deployment webserver-deployment
Jan 18 15:45:32.215: INFO: Waiting for observed generation 2
Jan 18 15:45:34.234: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 18 15:45:34.244: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 18 15:45:34.254: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 18 15:45:34.273: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 18 15:45:34.273: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 18 15:45:34.279: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 18 15:45:34.289: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 18 15:45:34.289: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 18 15:45:34.328: INFO: Updating deployment webserver-deployment
Jan 18 15:45:34.328: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 18 15:45:34.362: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 18 15:45:34.438: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 15:45:34.542: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-4134  6354e399-b7a5-43b9-b99b-48f855cdf4ab 46729 3 2023-01-18 15:45:26 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a1cf58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-01-18 15:45:32 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-18 15:45:34 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 18 15:45:34.599: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-4134  b73800d6-33b5-4b7e-8fd4-f049d33372b1 46686 3 2023-01-18 15:45:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 6354e399-b7a5-43b9-b99b-48f855cdf4ab 0xc004434207 0xc004434208}] [] [{kube-controller-manager Update apps/v1 2023-01-18 15:45:32 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6354e399-b7a5-43b9-b99b-48f855cdf4ab\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044342a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 15:45:34.600: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 18 15:45:34.600: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-4134  b6815195-9a21-4097-ba18-044f54cfaf8d 46683 3 2023-01-18 15:45:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 6354e399-b7a5-43b9-b99b-48f855cdf4ab 0xc004434307 0xc004434308}] [] [{kube-controller-manager Update apps/v1 2023-01-18 15:45:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6354e399-b7a5-43b9-b99b-48f855cdf4ab\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004434398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 18 15:45:34.632: INFO: Pod "webserver-deployment-69b7448995-22kgf" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-22kgf webserver-deployment-69b7448995- deployment-4134  c49a3334-7f60-4841-b0b2-344d16c6316a 46746 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b73800d6-33b5-4b7e-8fd4-f049d33372b1 0xc003a1d377 0xc003a1d378}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b73800d6-33b5-4b7e-8fd4-f049d33372b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-58dm4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-58dm4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:,StartTime:2023-01-18 15:45:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.632: INFO: Pod "webserver-deployment-69b7448995-2gwc4" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-2gwc4 webserver-deployment-69b7448995- deployment-4134  eaa17d99-c636-4b0c-be48-44e848139820 46657 0 2023-01-18 15:45:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:3b0a19f03623576cee413da66b008dedbdeb5a399510f12a0b8f261d9abe0999 cni.projectcalico.org/podIP:10.233.78.114/32 cni.projectcalico.org/podIPs:10.233.78.114/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b73800d6-33b5-4b7e-8fd4-f049d33372b1 0xc003a1d587 0xc003a1d588}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b73800d6-33b5-4b7e-8fd4-f049d33372b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 15:45:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 15:45:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mbwwd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mbwwd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:,StartTime:2023-01-18 15:45:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.633: INFO: Pod "webserver-deployment-69b7448995-8l68n" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-8l68n webserver-deployment-69b7448995- deployment-4134  ac110a05-3465-48a0-9270-f0579daa1bb6 46735 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b73800d6-33b5-4b7e-8fd4-f049d33372b1 0xc003a1d7a7 0xc003a1d7a8}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b73800d6-33b5-4b7e-8fd4-f049d33372b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wgt9f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wgt9f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.634: INFO: Pod "webserver-deployment-69b7448995-h6rzj" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-h6rzj webserver-deployment-69b7448995- deployment-4134  f414f5e7-2050-4aaf-9bde-5f0172034b9a 46691 0 2023-01-18 15:45:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:7223b600ca186fd096a86155d5b7051a119a4b68a7560dd87d95218e85bf016c cni.projectcalico.org/podIP:10.233.68.41/32 cni.projectcalico.org/podIPs:10.233.68.41/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b73800d6-33b5-4b7e-8fd4-f049d33372b1 0xc003a1d927 0xc003a1d928}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b73800d6-33b5-4b7e-8fd4-f049d33372b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fhpd6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fhpd6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:,StartTime:2023-01-18 15:45:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.634: INFO: Pod "webserver-deployment-69b7448995-hwh6f" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-hwh6f webserver-deployment-69b7448995- deployment-4134  0d22c133-bc55-4243-b1cf-a0ba2f68312a 46741 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b73800d6-33b5-4b7e-8fd4-f049d33372b1 0xc003a1db37 0xc003a1db38}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b73800d6-33b5-4b7e-8fd4-f049d33372b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8fbf8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8fbf8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.635: INFO: Pod "webserver-deployment-69b7448995-hzf58" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-hzf58 webserver-deployment-69b7448995- deployment-4134  8f9955ed-a59d-4a5d-b864-969868aea0cd 46675 0 2023-01-18 15:45:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:53a2a4ea0cbd1ca1037c7367255f3ecede0cc6f23b8fb09bc0de0aa61154dff6 cni.projectcalico.org/podIP:10.233.68.27/32 cni.projectcalico.org/podIPs:10.233.68.27/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b73800d6-33b5-4b7e-8fd4-f049d33372b1 0xc003a1dcd0 0xc003a1dcd1}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b73800d6-33b5-4b7e-8fd4-f049d33372b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 15:45:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 15:45:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tpf4d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tpf4d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:,StartTime:2023-01-18 15:45:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.636: INFO: Pod "webserver-deployment-69b7448995-j4cct" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-j4cct webserver-deployment-69b7448995- deployment-4134  30a8c39b-a8db-42ae-87ca-315e9f3347f3 46709 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b73800d6-33b5-4b7e-8fd4-f049d33372b1 0xc003a1dee7 0xc003a1dee8}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b73800d6-33b5-4b7e-8fd4-f049d33372b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qhfmn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qhfmn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.638: INFO: Pod "webserver-deployment-69b7448995-mqhmc" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-mqhmc webserver-deployment-69b7448995- deployment-4134  ac15ae81-df11-44bd-8b67-6fd1ab659c59 46737 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b73800d6-33b5-4b7e-8fd4-f049d33372b1 0xc004642060 0xc004642061}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b73800d6-33b5-4b7e-8fd4-f049d33372b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tvvbj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tvvbj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.639: INFO: Pod "webserver-deployment-69b7448995-qs5fl" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-qs5fl webserver-deployment-69b7448995- deployment-4134  7bbb36f4-3491-4a54-80cd-1521b94c4432 46666 0 2023-01-18 15:45:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:57d6c36373df85a927d01f77dd9bbcf08768c18aba10a4a3cfe6b92173ed0f89 cni.projectcalico.org/podIP:10.233.78.115/32 cni.projectcalico.org/podIPs:10.233.78.115/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b73800d6-33b5-4b7e-8fd4-f049d33372b1 0xc0046425b0 0xc0046425b1}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b73800d6-33b5-4b7e-8fd4-f049d33372b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 15:45:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 15:45:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mj2w7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mj2w7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:,StartTime:2023-01-18 15:45:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.640: INFO: Pod "webserver-deployment-69b7448995-srdxb" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-srdxb webserver-deployment-69b7448995- deployment-4134  a1c5f06b-3aa2-4642-a2e5-6b6c4788ceb0 46721 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b73800d6-33b5-4b7e-8fd4-f049d33372b1 0xc004642817 0xc004642818}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b73800d6-33b5-4b7e-8fd4-f049d33372b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t8lcg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t8lcg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:,StartTime:2023-01-18 15:45:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.640: INFO: Pod "webserver-deployment-69b7448995-t9fgg" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-t9fgg webserver-deployment-69b7448995- deployment-4134  c0b0aecb-1d58-4c26-9e8e-7fd28e812aff 46736 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b73800d6-33b5-4b7e-8fd4-f049d33372b1 0xc004642a07 0xc004642a08}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b73800d6-33b5-4b7e-8fd4-f049d33372b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j22gf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j22gf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.641: INFO: Pod "webserver-deployment-69b7448995-tdx74" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-tdx74 webserver-deployment-69b7448995- deployment-4134  2a6f8e13-8f54-4622-82d4-dea1752237b3 46676 0 2023-01-18 15:45:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:8a8b732ada0c342a72bcc5436711b7bd3e438e29596ec066b39754de1bc08f63 cni.projectcalico.org/podIP:10.233.68.5/32 cni.projectcalico.org/podIPs:10.233.68.5/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b73800d6-33b5-4b7e-8fd4-f049d33372b1 0xc004642ba0 0xc004642ba1}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b73800d6-33b5-4b7e-8fd4-f049d33372b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 15:45:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 15:45:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mndzr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mndzr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:,StartTime:2023-01-18 15:45:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.642: INFO: Pod "webserver-deployment-69b7448995-tw9bg" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-tw9bg webserver-deployment-69b7448995- deployment-4134  feaea7b3-0eab-4bdc-8664-c3d78a41c2f0 46734 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b73800d6-33b5-4b7e-8fd4-f049d33372b1 0xc004642da7 0xc004642da8}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b73800d6-33b5-4b7e-8fd4-f049d33372b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m2pmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m2pmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.643: INFO: Pod "webserver-deployment-845c8977d9-46rz8" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-46rz8 webserver-deployment-845c8977d9- deployment-4134  b5ff1df5-a242-4139-93df-cf264ba131f2 46723 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc004642f20 0xc004642f21}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qbh26,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qbh26,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.643: INFO: Pod "webserver-deployment-845c8977d9-5pvnc" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-5pvnc webserver-deployment-845c8977d9- deployment-4134  e454d0c3-2a2e-4693-961d-92ad5e96631b 46559 0 2023-01-18 15:45:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:65609dc4a7f95b5a53abcbcbc2c78b7ef29dd47c904285e17b4170ecdcfc02cf cni.projectcalico.org/podIP:10.233.78.113/32 cni.projectcalico.org/podIPs:10.233.78.113/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc004643077 0xc004643078}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 15:45:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 15:45:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.113\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dtdm4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dtdm4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:10.233.78.113,StartTime:2023-01-18 15:45:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 15:45:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://17518ee6c3c31ef10e998412f0acf9ea13c11c96727eda348a4d5d59c0d1a036,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.113,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.644: INFO: Pod "webserver-deployment-845c8977d9-7gz9w" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-7gz9w webserver-deployment-845c8977d9- deployment-4134  624fceff-bd0a-45b8-a408-5d265f6f8630 46720 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc004643287 0xc004643288}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nrtsh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nrtsh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.644: INFO: Pod "webserver-deployment-845c8977d9-7l466" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-7l466 webserver-deployment-845c8977d9- deployment-4134  814a296a-07cf-4938-aff9-eb63dd3ec1fc 46726 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc0046433f0 0xc0046433f1}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mwgcf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mwgcf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.645: INFO: Pod "webserver-deployment-845c8977d9-8xcp6" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-8xcp6 webserver-deployment-845c8977d9- deployment-4134  ccea1629-dbe5-4364-a71d-40c06ee828ee 46550 0 2023-01-18 15:45:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:aa23d11d96da5152b08a9ad247bb026224fc6753188dd038c470b500a001fb80 cni.projectcalico.org/podIP:10.233.78.110/32 cni.projectcalico.org/podIPs:10.233.78.110/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc004643570 0xc004643571}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 15:45:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 15:45:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.110\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nffqt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nffqt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:10.233.78.110,StartTime:2023-01-18 15:45:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 15:45:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://20157011c81508a9481dca2f3fa95130b37ff437351d01476677b2c9b90fb10d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.645: INFO: Pod "webserver-deployment-845c8977d9-b4f2v" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-b4f2v webserver-deployment-845c8977d9- deployment-4134  2b6440fc-a32b-46eb-9dad-703ae91eaaad 46542 0 2023-01-18 15:45:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8bfc4724ebdf491a524fb807f1e5d45b71cf424525d74c4bfdcc2da7167e3649 cni.projectcalico.org/podIP:10.233.78.111/32 cni.projectcalico.org/podIPs:10.233.78.111/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc004643797 0xc004643798}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 15:45:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 15:45:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.111\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-57chj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-57chj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:10.233.78.111,StartTime:2023-01-18 15:45:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 15:45:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4fa5923c1b3b15b1a9567cae823a69f9938f6ae761d9ad98c93837db6f2175a9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.111,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.646: INFO: Pod "webserver-deployment-845c8977d9-b5k7r" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-b5k7r webserver-deployment-845c8977d9- deployment-4134  dc07eb94-bd49-43ab-af4b-0b434e090a7d 46702 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc004643a47 0xc004643a48}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fp98g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fp98g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.647: INFO: Pod "webserver-deployment-845c8977d9-b6sxf" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-b6sxf webserver-deployment-845c8977d9- deployment-4134  8ddb7daf-3ed4-42b9-9dfb-15c604dce5ed 46724 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc004643c20 0xc004643c21}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bf5rj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bf5rj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.648: INFO: Pod "webserver-deployment-845c8977d9-gmndx" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-gmndx webserver-deployment-845c8977d9- deployment-4134  e1d00999-76b7-42c0-bd50-ec5d0a7d56da 46727 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc004643d57 0xc004643d58}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ccbqr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ccbqr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.649: INFO: Pod "webserver-deployment-845c8977d9-lvtz8" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-lvtz8 webserver-deployment-845c8977d9- deployment-4134  40b60527-0671-46e2-a5aa-85aa4e3b858e 46696 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc004643ec0 0xc004643ec1}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h55tq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h55tq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.663: INFO: Pod "webserver-deployment-845c8977d9-mbjmr" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-mbjmr webserver-deployment-845c8977d9- deployment-4134  f1ca1b58-d8c2-457c-9de4-d1929c81a3ea 46728 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc0043c2020 0xc0043c2021}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bl5bh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bl5bh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.663: INFO: Pod "webserver-deployment-845c8977d9-mfkzq" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-mfkzq webserver-deployment-845c8977d9- deployment-4134  86c2ffaf-9bad-4982-8d53-439fba39aa71 46546 0 2023-01-18 15:45:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4250620e6a37cac005316fd699bd5b40103afd7a9f5f752446a7bd91a9367840 cni.projectcalico.org/podIP:10.233.78.112/32 cni.projectcalico.org/podIPs:10.233.78.112/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc0043c2177 0xc0043c2178}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 15:45:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 15:45:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6m89l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6m89l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:10.233.78.112,StartTime:2023-01-18 15:45:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 15:45:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2c7cccbf4ba062664b8f473ad011ad69463f8ae58c67fe6826ec1fff395d3cb4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.664: INFO: Pod "webserver-deployment-845c8977d9-n2dtc" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-n2dtc webserver-deployment-845c8977d9- deployment-4134  7958a8f7-808d-48ab-b5ac-eb017b83b4f4 46592 0 2023-01-18 15:45:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:5c6f22863115ecfd75565f6f54e6fc6d9c9f46322f4900720c2ec5f04d33f172 cni.projectcalico.org/podIP:10.233.68.4/32 cni.projectcalico.org/podIPs:10.233.68.4/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc0043c23b7 0xc0043c23b8}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 15:45:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 15:45:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fnhvb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fnhvb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.4,StartTime:2023-01-18 15:45:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 15:45:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5994c716e2f09d3f3d59730f4466256e93cc2c0a8414c935e4c384297c26d71e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.664: INFO: Pod "webserver-deployment-845c8977d9-vg92x" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-vg92x webserver-deployment-845c8977d9- deployment-4134  e47805f5-7ec2-4541-aa0e-914426f493d3 46575 0 2023-01-18 15:45:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:224f0db206a09789f854de94cd575cbf6700b9569e151b30ba46c292dc908f93 cni.projectcalico.org/podIP:10.233.68.255/32 cni.projectcalico.org/podIPs:10.233.68.255/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc0043c25f0 0xc0043c25f1}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 15:45:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 15:45:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.255\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jxxkf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jxxkf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.255,StartTime:2023-01-18 15:45:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 15:45:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b980f6d66dce4ec05e8ca8fc85f594c45297adc7ed4c8df79e023818c1a946e3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.255,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.665: INFO: Pod "webserver-deployment-845c8977d9-x8k67" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-x8k67 webserver-deployment-845c8977d9- deployment-4134  ff5a3308-3331-4a7e-92ec-17b226412ba5 46504 0 2023-01-18 15:45:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8a7c5dbff8d6f1a38460fca01bda78fe96b1b3ebced497530e3e756b2687fed9 cni.projectcalico.org/podIP:10.233.78.109/32 cni.projectcalico.org/podIPs:10.233.78.109/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc0043c2817 0xc0043c2818}] [] [{calico Update v1 2023-01-18 15:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-18 15:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 15:45:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.109\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qvtn9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qvtn9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:10.233.78.109,StartTime:2023-01-18 15:45:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 15:45:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://7178dc162d557a3c685d00907699475bb586ae269f4a29b66cec9158aa94552f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.665: INFO: Pod "webserver-deployment-845c8977d9-xkswd" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-xkswd webserver-deployment-845c8977d9- deployment-4134  eb6bfac5-2ad2-4197-91fb-a2538f69433a 46690 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc0043c2a17 0xc0043c2a18}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gj9c6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gj9c6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.665: INFO: Pod "webserver-deployment-845c8977d9-xwq2t" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-xwq2t webserver-deployment-845c8977d9- deployment-4134  5a3fbc3d-9986-4245-998d-a266fd81d1ad 46742 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc0043c2b90 0xc0043c2b91}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sdbl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sdbl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.666: INFO: Pod "webserver-deployment-845c8977d9-z2lxs" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-z2lxs webserver-deployment-845c8977d9- deployment-4134  6d0fe2f2-b4e2-4ee5-8e07-7b80ceb1dae8 46725 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc0043c2cf0 0xc0043c2cf1}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-db2b5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-db2b5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.666: INFO: Pod "webserver-deployment-845c8977d9-z2vbz" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-z2vbz webserver-deployment-845c8977d9- deployment-4134  6b7620c1-811f-45fc-8ea9-5087226b9f1a 46714 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc0043c2e27 0xc0043c2e28}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2696w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2696w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 15:45:34.667: INFO: Pod "webserver-deployment-845c8977d9-zf49v" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-zf49v webserver-deployment-845c8977d9- deployment-4134  92d64145-29f1-42a6-90bc-babef42b6f1a 46564 0 2023-01-18 15:45:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:60beedb6fddeed33db6b54e3463cd799a9b42b3a898deeac2bda5547af2fbb4d cni.projectcalico.org/podIP:10.233.68.254/32 cni.projectcalico.org/podIPs:10.233.68.254/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc0043c2fb0 0xc0043c2fb1}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 15:45:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 15:45:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.254\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ntvqt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ntvqt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.254,StartTime:2023-01-18 15:45:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 15:45:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://de1750ffcf805c20538ec60e43c77a98dd0b9e2cee831575e7c22a2160459e19,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.254,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 18 15:45:34.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4134" for this suite. 01/18/23 15:45:34.698
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":162,"skipped":2859,"failed":0}
------------------------------
• [SLOW TEST] [8.746 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:45:25.975
    Jan 18 15:45:25.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename deployment 01/18/23 15:45:25.977
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:45:25.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:45:25.997
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jan 18 15:45:26.003: INFO: Creating deployment "webserver-deployment"
    Jan 18 15:45:26.011: INFO: Waiting for observed generation 1
    Jan 18 15:45:28.146: INFO: Waiting for all required pods to come up
    Jan 18 15:45:28.156: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 01/18/23 15:45:28.156
    Jan 18 15:45:28.156: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-zf49v" in namespace "deployment-4134" to be "running"
    Jan 18 15:45:28.157: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-44m42" in namespace "deployment-4134" to be "running"
    Jan 18 15:45:28.157: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-5pvnc" in namespace "deployment-4134" to be "running"
    Jan 18 15:45:28.157: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-8xcp6" in namespace "deployment-4134" to be "running"
    Jan 18 15:45:28.157: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-b4f2v" in namespace "deployment-4134" to be "running"
    Jan 18 15:45:28.157: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-mfkzq" in namespace "deployment-4134" to be "running"
    Jan 18 15:45:28.157: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-n2dtc" in namespace "deployment-4134" to be "running"
    Jan 18 15:45:28.158: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rlhld" in namespace "deployment-4134" to be "running"
    Jan 18 15:45:28.158: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-vg92x" in namespace "deployment-4134" to be "running"
    Jan 18 15:45:28.163: INFO: Pod "webserver-deployment-845c8977d9-zf49v": Phase="Pending", Reason="", readiness=false. Elapsed: 6.256471ms
    Jan 18 15:45:28.164: INFO: Pod "webserver-deployment-845c8977d9-44m42": Phase="Pending", Reason="", readiness=false. Elapsed: 7.020567ms
    Jan 18 15:45:28.170: INFO: Pod "webserver-deployment-845c8977d9-5pvnc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.615833ms
    Jan 18 15:45:28.170: INFO: Pod "webserver-deployment-845c8977d9-b4f2v": Phase="Pending", Reason="", readiness=false. Elapsed: 12.509036ms
    Jan 18 15:45:28.170: INFO: Pod "webserver-deployment-845c8977d9-8xcp6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.873192ms
    Jan 18 15:45:28.179: INFO: Pod "webserver-deployment-845c8977d9-rlhld": Phase="Pending", Reason="", readiness=false. Elapsed: 21.644763ms
    Jan 18 15:45:28.187: INFO: Pod "webserver-deployment-845c8977d9-vg92x": Phase="Pending", Reason="", readiness=false. Elapsed: 29.07978ms
    Jan 18 15:45:28.187: INFO: Pod "webserver-deployment-845c8977d9-n2dtc": Phase="Pending", Reason="", readiness=false. Elapsed: 29.424298ms
    Jan 18 15:45:28.201: INFO: Pod "webserver-deployment-845c8977d9-mfkzq": Phase="Pending", Reason="", readiness=false. Elapsed: 43.608484ms
    Jan 18 15:45:30.168: INFO: Pod "webserver-deployment-845c8977d9-zf49v": Phase="Running", Reason="", readiness=true. Elapsed: 2.01117259s
    Jan 18 15:45:30.168: INFO: Pod "webserver-deployment-845c8977d9-zf49v" satisfied condition "running"
    Jan 18 15:45:30.168: INFO: Pod "webserver-deployment-845c8977d9-44m42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011695003s
    Jan 18 15:45:30.177: INFO: Pod "webserver-deployment-845c8977d9-8xcp6": Phase="Running", Reason="", readiness=true. Elapsed: 2.019629206s
    Jan 18 15:45:30.177: INFO: Pod "webserver-deployment-845c8977d9-8xcp6" satisfied condition "running"
    Jan 18 15:45:30.177: INFO: Pod "webserver-deployment-845c8977d9-b4f2v": Phase="Running", Reason="", readiness=true. Elapsed: 2.01966447s
    Jan 18 15:45:30.177: INFO: Pod "webserver-deployment-845c8977d9-b4f2v" satisfied condition "running"
    Jan 18 15:45:30.182: INFO: Pod "webserver-deployment-845c8977d9-5pvnc": Phase="Running", Reason="", readiness=true. Elapsed: 2.025481359s
    Jan 18 15:45:30.183: INFO: Pod "webserver-deployment-845c8977d9-5pvnc" satisfied condition "running"
    Jan 18 15:45:30.192: INFO: Pod "webserver-deployment-845c8977d9-rlhld": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034682315s
    Jan 18 15:45:30.200: INFO: Pod "webserver-deployment-845c8977d9-n2dtc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042872293s
    Jan 18 15:45:30.201: INFO: Pod "webserver-deployment-845c8977d9-vg92x": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043524783s
    Jan 18 15:45:30.208: INFO: Pod "webserver-deployment-845c8977d9-mfkzq": Phase="Running", Reason="", readiness=true. Elapsed: 2.050591538s
    Jan 18 15:45:30.208: INFO: Pod "webserver-deployment-845c8977d9-mfkzq" satisfied condition "running"
    Jan 18 15:45:32.169: INFO: Pod "webserver-deployment-845c8977d9-44m42": Phase="Running", Reason="", readiness=true. Elapsed: 4.012243959s
    Jan 18 15:45:32.169: INFO: Pod "webserver-deployment-845c8977d9-44m42" satisfied condition "running"
    Jan 18 15:45:32.185: INFO: Pod "webserver-deployment-845c8977d9-rlhld": Phase="Running", Reason="", readiness=true. Elapsed: 4.027635272s
    Jan 18 15:45:32.185: INFO: Pod "webserver-deployment-845c8977d9-rlhld" satisfied condition "running"
    Jan 18 15:45:32.192: INFO: Pod "webserver-deployment-845c8977d9-vg92x": Phase="Running", Reason="", readiness=true. Elapsed: 4.034016493s
    Jan 18 15:45:32.192: INFO: Pod "webserver-deployment-845c8977d9-n2dtc": Phase="Running", Reason="", readiness=true. Elapsed: 4.034778154s
    Jan 18 15:45:32.192: INFO: Pod "webserver-deployment-845c8977d9-n2dtc" satisfied condition "running"
    Jan 18 15:45:32.192: INFO: Pod "webserver-deployment-845c8977d9-vg92x" satisfied condition "running"
    Jan 18 15:45:32.192: INFO: Waiting for deployment "webserver-deployment" to complete
    Jan 18 15:45:32.201: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jan 18 15:45:32.215: INFO: Updating deployment webserver-deployment
    Jan 18 15:45:32.215: INFO: Waiting for observed generation 2
    Jan 18 15:45:34.234: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jan 18 15:45:34.244: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jan 18 15:45:34.254: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 18 15:45:34.273: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jan 18 15:45:34.273: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jan 18 15:45:34.279: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 18 15:45:34.289: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jan 18 15:45:34.289: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jan 18 15:45:34.328: INFO: Updating deployment webserver-deployment
    Jan 18 15:45:34.328: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jan 18 15:45:34.362: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jan 18 15:45:34.438: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 15:45:34.542: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-4134  6354e399-b7a5-43b9-b99b-48f855cdf4ab 46729 3 2023-01-18 15:45:26 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a1cf58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-01-18 15:45:32 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-18 15:45:34 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jan 18 15:45:34.599: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-4134  b73800d6-33b5-4b7e-8fd4-f049d33372b1 46686 3 2023-01-18 15:45:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 6354e399-b7a5-43b9-b99b-48f855cdf4ab 0xc004434207 0xc004434208}] [] [{kube-controller-manager Update apps/v1 2023-01-18 15:45:32 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6354e399-b7a5-43b9-b99b-48f855cdf4ab\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044342a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 15:45:34.600: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jan 18 15:45:34.600: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-4134  b6815195-9a21-4097-ba18-044f54cfaf8d 46683 3 2023-01-18 15:45:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 6354e399-b7a5-43b9-b99b-48f855cdf4ab 0xc004434307 0xc004434308}] [] [{kube-controller-manager Update apps/v1 2023-01-18 15:45:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6354e399-b7a5-43b9-b99b-48f855cdf4ab\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004434398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 15:45:34.632: INFO: Pod "webserver-deployment-69b7448995-22kgf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-22kgf webserver-deployment-69b7448995- deployment-4134  c49a3334-7f60-4841-b0b2-344d16c6316a 46746 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b73800d6-33b5-4b7e-8fd4-f049d33372b1 0xc003a1d377 0xc003a1d378}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b73800d6-33b5-4b7e-8fd4-f049d33372b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-58dm4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-58dm4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:,StartTime:2023-01-18 15:45:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.632: INFO: Pod "webserver-deployment-69b7448995-2gwc4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-2gwc4 webserver-deployment-69b7448995- deployment-4134  eaa17d99-c636-4b0c-be48-44e848139820 46657 0 2023-01-18 15:45:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:3b0a19f03623576cee413da66b008dedbdeb5a399510f12a0b8f261d9abe0999 cni.projectcalico.org/podIP:10.233.78.114/32 cni.projectcalico.org/podIPs:10.233.78.114/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b73800d6-33b5-4b7e-8fd4-f049d33372b1 0xc003a1d587 0xc003a1d588}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b73800d6-33b5-4b7e-8fd4-f049d33372b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 15:45:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 15:45:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mbwwd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mbwwd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:,StartTime:2023-01-18 15:45:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.633: INFO: Pod "webserver-deployment-69b7448995-8l68n" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-8l68n webserver-deployment-69b7448995- deployment-4134  ac110a05-3465-48a0-9270-f0579daa1bb6 46735 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b73800d6-33b5-4b7e-8fd4-f049d33372b1 0xc003a1d7a7 0xc003a1d7a8}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b73800d6-33b5-4b7e-8fd4-f049d33372b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wgt9f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wgt9f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.634: INFO: Pod "webserver-deployment-69b7448995-h6rzj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-h6rzj webserver-deployment-69b7448995- deployment-4134  f414f5e7-2050-4aaf-9bde-5f0172034b9a 46691 0 2023-01-18 15:45:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:7223b600ca186fd096a86155d5b7051a119a4b68a7560dd87d95218e85bf016c cni.projectcalico.org/podIP:10.233.68.41/32 cni.projectcalico.org/podIPs:10.233.68.41/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b73800d6-33b5-4b7e-8fd4-f049d33372b1 0xc003a1d927 0xc003a1d928}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b73800d6-33b5-4b7e-8fd4-f049d33372b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fhpd6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fhpd6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:,StartTime:2023-01-18 15:45:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.634: INFO: Pod "webserver-deployment-69b7448995-hwh6f" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-hwh6f webserver-deployment-69b7448995- deployment-4134  0d22c133-bc55-4243-b1cf-a0ba2f68312a 46741 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b73800d6-33b5-4b7e-8fd4-f049d33372b1 0xc003a1db37 0xc003a1db38}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b73800d6-33b5-4b7e-8fd4-f049d33372b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8fbf8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8fbf8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.635: INFO: Pod "webserver-deployment-69b7448995-hzf58" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-hzf58 webserver-deployment-69b7448995- deployment-4134  8f9955ed-a59d-4a5d-b864-969868aea0cd 46675 0 2023-01-18 15:45:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:53a2a4ea0cbd1ca1037c7367255f3ecede0cc6f23b8fb09bc0de0aa61154dff6 cni.projectcalico.org/podIP:10.233.68.27/32 cni.projectcalico.org/podIPs:10.233.68.27/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b73800d6-33b5-4b7e-8fd4-f049d33372b1 0xc003a1dcd0 0xc003a1dcd1}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b73800d6-33b5-4b7e-8fd4-f049d33372b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 15:45:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 15:45:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tpf4d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tpf4d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:,StartTime:2023-01-18 15:45:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.636: INFO: Pod "webserver-deployment-69b7448995-j4cct" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-j4cct webserver-deployment-69b7448995- deployment-4134  30a8c39b-a8db-42ae-87ca-315e9f3347f3 46709 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b73800d6-33b5-4b7e-8fd4-f049d33372b1 0xc003a1dee7 0xc003a1dee8}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b73800d6-33b5-4b7e-8fd4-f049d33372b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qhfmn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qhfmn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.638: INFO: Pod "webserver-deployment-69b7448995-mqhmc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-mqhmc webserver-deployment-69b7448995- deployment-4134  ac15ae81-df11-44bd-8b67-6fd1ab659c59 46737 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b73800d6-33b5-4b7e-8fd4-f049d33372b1 0xc004642060 0xc004642061}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b73800d6-33b5-4b7e-8fd4-f049d33372b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tvvbj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tvvbj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.639: INFO: Pod "webserver-deployment-69b7448995-qs5fl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-qs5fl webserver-deployment-69b7448995- deployment-4134  7bbb36f4-3491-4a54-80cd-1521b94c4432 46666 0 2023-01-18 15:45:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:57d6c36373df85a927d01f77dd9bbcf08768c18aba10a4a3cfe6b92173ed0f89 cni.projectcalico.org/podIP:10.233.78.115/32 cni.projectcalico.org/podIPs:10.233.78.115/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b73800d6-33b5-4b7e-8fd4-f049d33372b1 0xc0046425b0 0xc0046425b1}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b73800d6-33b5-4b7e-8fd4-f049d33372b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 15:45:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 15:45:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mj2w7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mj2w7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:,StartTime:2023-01-18 15:45:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.640: INFO: Pod "webserver-deployment-69b7448995-srdxb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-srdxb webserver-deployment-69b7448995- deployment-4134  a1c5f06b-3aa2-4642-a2e5-6b6c4788ceb0 46721 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b73800d6-33b5-4b7e-8fd4-f049d33372b1 0xc004642817 0xc004642818}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b73800d6-33b5-4b7e-8fd4-f049d33372b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t8lcg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t8lcg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:,StartTime:2023-01-18 15:45:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.640: INFO: Pod "webserver-deployment-69b7448995-t9fgg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-t9fgg webserver-deployment-69b7448995- deployment-4134  c0b0aecb-1d58-4c26-9e8e-7fd28e812aff 46736 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b73800d6-33b5-4b7e-8fd4-f049d33372b1 0xc004642a07 0xc004642a08}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b73800d6-33b5-4b7e-8fd4-f049d33372b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j22gf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j22gf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.641: INFO: Pod "webserver-deployment-69b7448995-tdx74" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-tdx74 webserver-deployment-69b7448995- deployment-4134  2a6f8e13-8f54-4622-82d4-dea1752237b3 46676 0 2023-01-18 15:45:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:8a8b732ada0c342a72bcc5436711b7bd3e438e29596ec066b39754de1bc08f63 cni.projectcalico.org/podIP:10.233.68.5/32 cni.projectcalico.org/podIPs:10.233.68.5/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b73800d6-33b5-4b7e-8fd4-f049d33372b1 0xc004642ba0 0xc004642ba1}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b73800d6-33b5-4b7e-8fd4-f049d33372b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 15:45:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 15:45:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mndzr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mndzr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:,StartTime:2023-01-18 15:45:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.642: INFO: Pod "webserver-deployment-69b7448995-tw9bg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-tw9bg webserver-deployment-69b7448995- deployment-4134  feaea7b3-0eab-4bdc-8664-c3d78a41c2f0 46734 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b73800d6-33b5-4b7e-8fd4-f049d33372b1 0xc004642da7 0xc004642da8}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b73800d6-33b5-4b7e-8fd4-f049d33372b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m2pmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m2pmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.643: INFO: Pod "webserver-deployment-845c8977d9-46rz8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-46rz8 webserver-deployment-845c8977d9- deployment-4134  b5ff1df5-a242-4139-93df-cf264ba131f2 46723 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc004642f20 0xc004642f21}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qbh26,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qbh26,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.643: INFO: Pod "webserver-deployment-845c8977d9-5pvnc" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-5pvnc webserver-deployment-845c8977d9- deployment-4134  e454d0c3-2a2e-4693-961d-92ad5e96631b 46559 0 2023-01-18 15:45:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:65609dc4a7f95b5a53abcbcbc2c78b7ef29dd47c904285e17b4170ecdcfc02cf cni.projectcalico.org/podIP:10.233.78.113/32 cni.projectcalico.org/podIPs:10.233.78.113/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc004643077 0xc004643078}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 15:45:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 15:45:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.113\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dtdm4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dtdm4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:10.233.78.113,StartTime:2023-01-18 15:45:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 15:45:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://17518ee6c3c31ef10e998412f0acf9ea13c11c96727eda348a4d5d59c0d1a036,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.113,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.644: INFO: Pod "webserver-deployment-845c8977d9-7gz9w" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-7gz9w webserver-deployment-845c8977d9- deployment-4134  624fceff-bd0a-45b8-a408-5d265f6f8630 46720 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc004643287 0xc004643288}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nrtsh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nrtsh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.644: INFO: Pod "webserver-deployment-845c8977d9-7l466" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-7l466 webserver-deployment-845c8977d9- deployment-4134  814a296a-07cf-4938-aff9-eb63dd3ec1fc 46726 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc0046433f0 0xc0046433f1}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mwgcf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mwgcf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.645: INFO: Pod "webserver-deployment-845c8977d9-8xcp6" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-8xcp6 webserver-deployment-845c8977d9- deployment-4134  ccea1629-dbe5-4364-a71d-40c06ee828ee 46550 0 2023-01-18 15:45:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:aa23d11d96da5152b08a9ad247bb026224fc6753188dd038c470b500a001fb80 cni.projectcalico.org/podIP:10.233.78.110/32 cni.projectcalico.org/podIPs:10.233.78.110/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc004643570 0xc004643571}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 15:45:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 15:45:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.110\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nffqt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nffqt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:10.233.78.110,StartTime:2023-01-18 15:45:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 15:45:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://20157011c81508a9481dca2f3fa95130b37ff437351d01476677b2c9b90fb10d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.645: INFO: Pod "webserver-deployment-845c8977d9-b4f2v" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-b4f2v webserver-deployment-845c8977d9- deployment-4134  2b6440fc-a32b-46eb-9dad-703ae91eaaad 46542 0 2023-01-18 15:45:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8bfc4724ebdf491a524fb807f1e5d45b71cf424525d74c4bfdcc2da7167e3649 cni.projectcalico.org/podIP:10.233.78.111/32 cni.projectcalico.org/podIPs:10.233.78.111/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc004643797 0xc004643798}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 15:45:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 15:45:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.111\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-57chj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-57chj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:10.233.78.111,StartTime:2023-01-18 15:45:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 15:45:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4fa5923c1b3b15b1a9567cae823a69f9938f6ae761d9ad98c93837db6f2175a9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.111,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.646: INFO: Pod "webserver-deployment-845c8977d9-b5k7r" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-b5k7r webserver-deployment-845c8977d9- deployment-4134  dc07eb94-bd49-43ab-af4b-0b434e090a7d 46702 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc004643a47 0xc004643a48}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fp98g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fp98g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.647: INFO: Pod "webserver-deployment-845c8977d9-b6sxf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-b6sxf webserver-deployment-845c8977d9- deployment-4134  8ddb7daf-3ed4-42b9-9dfb-15c604dce5ed 46724 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc004643c20 0xc004643c21}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bf5rj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bf5rj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.648: INFO: Pod "webserver-deployment-845c8977d9-gmndx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-gmndx webserver-deployment-845c8977d9- deployment-4134  e1d00999-76b7-42c0-bd50-ec5d0a7d56da 46727 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc004643d57 0xc004643d58}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ccbqr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ccbqr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.649: INFO: Pod "webserver-deployment-845c8977d9-lvtz8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-lvtz8 webserver-deployment-845c8977d9- deployment-4134  40b60527-0671-46e2-a5aa-85aa4e3b858e 46696 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc004643ec0 0xc004643ec1}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h55tq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h55tq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.663: INFO: Pod "webserver-deployment-845c8977d9-mbjmr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-mbjmr webserver-deployment-845c8977d9- deployment-4134  f1ca1b58-d8c2-457c-9de4-d1929c81a3ea 46728 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc0043c2020 0xc0043c2021}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bl5bh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bl5bh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.663: INFO: Pod "webserver-deployment-845c8977d9-mfkzq" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-mfkzq webserver-deployment-845c8977d9- deployment-4134  86c2ffaf-9bad-4982-8d53-439fba39aa71 46546 0 2023-01-18 15:45:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4250620e6a37cac005316fd699bd5b40103afd7a9f5f752446a7bd91a9367840 cni.projectcalico.org/podIP:10.233.78.112/32 cni.projectcalico.org/podIPs:10.233.78.112/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc0043c2177 0xc0043c2178}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 15:45:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 15:45:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6m89l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6m89l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:10.233.78.112,StartTime:2023-01-18 15:45:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 15:45:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2c7cccbf4ba062664b8f473ad011ad69463f8ae58c67fe6826ec1fff395d3cb4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.664: INFO: Pod "webserver-deployment-845c8977d9-n2dtc" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-n2dtc webserver-deployment-845c8977d9- deployment-4134  7958a8f7-808d-48ab-b5ac-eb017b83b4f4 46592 0 2023-01-18 15:45:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:5c6f22863115ecfd75565f6f54e6fc6d9c9f46322f4900720c2ec5f04d33f172 cni.projectcalico.org/podIP:10.233.68.4/32 cni.projectcalico.org/podIPs:10.233.68.4/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc0043c23b7 0xc0043c23b8}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 15:45:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 15:45:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fnhvb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fnhvb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.4,StartTime:2023-01-18 15:45:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 15:45:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5994c716e2f09d3f3d59730f4466256e93cc2c0a8414c935e4c384297c26d71e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.664: INFO: Pod "webserver-deployment-845c8977d9-vg92x" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-vg92x webserver-deployment-845c8977d9- deployment-4134  e47805f5-7ec2-4541-aa0e-914426f493d3 46575 0 2023-01-18 15:45:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:224f0db206a09789f854de94cd575cbf6700b9569e151b30ba46c292dc908f93 cni.projectcalico.org/podIP:10.233.68.255/32 cni.projectcalico.org/podIPs:10.233.68.255/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc0043c25f0 0xc0043c25f1}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 15:45:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 15:45:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.255\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jxxkf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jxxkf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.255,StartTime:2023-01-18 15:45:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 15:45:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b980f6d66dce4ec05e8ca8fc85f594c45297adc7ed4c8df79e023818c1a946e3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.255,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.665: INFO: Pod "webserver-deployment-845c8977d9-x8k67" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-x8k67 webserver-deployment-845c8977d9- deployment-4134  ff5a3308-3331-4a7e-92ec-17b226412ba5 46504 0 2023-01-18 15:45:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8a7c5dbff8d6f1a38460fca01bda78fe96b1b3ebced497530e3e756b2687fed9 cni.projectcalico.org/podIP:10.233.78.109/32 cni.projectcalico.org/podIPs:10.233.78.109/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc0043c2817 0xc0043c2818}] [] [{calico Update v1 2023-01-18 15:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-18 15:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 15:45:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.109\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qvtn9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qvtn9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:10.233.78.109,StartTime:2023-01-18 15:45:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 15:45:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://7178dc162d557a3c685d00907699475bb586ae269f4a29b66cec9158aa94552f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.665: INFO: Pod "webserver-deployment-845c8977d9-xkswd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-xkswd webserver-deployment-845c8977d9- deployment-4134  eb6bfac5-2ad2-4197-91fb-a2538f69433a 46690 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc0043c2a17 0xc0043c2a18}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gj9c6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gj9c6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.665: INFO: Pod "webserver-deployment-845c8977d9-xwq2t" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-xwq2t webserver-deployment-845c8977d9- deployment-4134  5a3fbc3d-9986-4245-998d-a266fd81d1ad 46742 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc0043c2b90 0xc0043c2b91}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sdbl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sdbl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.666: INFO: Pod "webserver-deployment-845c8977d9-z2lxs" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-z2lxs webserver-deployment-845c8977d9- deployment-4134  6d0fe2f2-b4e2-4ee5-8e07-7b80ceb1dae8 46725 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc0043c2cf0 0xc0043c2cf1}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-db2b5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-db2b5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.666: INFO: Pod "webserver-deployment-845c8977d9-z2vbz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-z2vbz webserver-deployment-845c8977d9- deployment-4134  6b7620c1-811f-45fc-8ea9-5087226b9f1a 46714 0 2023-01-18 15:45:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc0043c2e27 0xc0043c2e28}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2696w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2696w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 15:45:34.667: INFO: Pod "webserver-deployment-845c8977d9-zf49v" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-zf49v webserver-deployment-845c8977d9- deployment-4134  92d64145-29f1-42a6-90bc-babef42b6f1a 46564 0 2023-01-18 15:45:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:60beedb6fddeed33db6b54e3463cd799a9b42b3a898deeac2bda5547af2fbb4d cni.projectcalico.org/podIP:10.233.68.254/32 cni.projectcalico.org/podIPs:10.233.68.254/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b6815195-9a21-4097-ba18-044f54cfaf8d 0xc0043c2fb0 0xc0043c2fb1}] [] [{kube-controller-manager Update v1 2023-01-18 15:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6815195-9a21-4097-ba18-044f54cfaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 15:45:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 15:45:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.254\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ntvqt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ntvqt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:45:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.254,StartTime:2023-01-18 15:45:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 15:45:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://de1750ffcf805c20538ec60e43c77a98dd0b9e2cee831575e7c22a2160459e19,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.254,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 18 15:45:34.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4134" for this suite. 01/18/23 15:45:34.698
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:45:34.728
Jan 18 15:45:34.728: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename gc 01/18/23 15:45:34.735
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:45:34.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:45:34.794
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jan 18 15:45:34.877: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"f27398cb-80f4-4b27-9293-a99d42de6262", Controller:(*bool)(0xc00267002a), BlockOwnerDeletion:(*bool)(0xc00267002b)}}
Jan 18 15:45:34.894: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"aba57b0f-bdb0-4ee6-bb52-684e96a0ea84", Controller:(*bool)(0xc002670286), BlockOwnerDeletion:(*bool)(0xc002670287)}}
Jan 18 15:45:34.905: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"dccc5fbe-01b7-47cb-aa96-6c38d7fa7d72", Controller:(*bool)(0xc00267050e), BlockOwnerDeletion:(*bool)(0xc00267050f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 18 15:45:39.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7767" for this suite. 01/18/23 15:45:39.993
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":163,"skipped":2863,"failed":0}
------------------------------
• [SLOW TEST] [5.306 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:45:34.728
    Jan 18 15:45:34.728: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename gc 01/18/23 15:45:34.735
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:45:34.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:45:34.794
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jan 18 15:45:34.877: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"f27398cb-80f4-4b27-9293-a99d42de6262", Controller:(*bool)(0xc00267002a), BlockOwnerDeletion:(*bool)(0xc00267002b)}}
    Jan 18 15:45:34.894: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"aba57b0f-bdb0-4ee6-bb52-684e96a0ea84", Controller:(*bool)(0xc002670286), BlockOwnerDeletion:(*bool)(0xc002670287)}}
    Jan 18 15:45:34.905: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"dccc5fbe-01b7-47cb-aa96-6c38d7fa7d72", Controller:(*bool)(0xc00267050e), BlockOwnerDeletion:(*bool)(0xc00267050f)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 18 15:45:39.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7767" for this suite. 01/18/23 15:45:39.993
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:45:40.036
Jan 18 15:45:40.036: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename job 01/18/23 15:45:40.038
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:45:40.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:45:40.085
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 01/18/23 15:45:40.114
STEP: Patching the Job 01/18/23 15:45:40.143
STEP: Watching for Job to be patched 01/18/23 15:45:40.195
Jan 18 15:45:40.203: INFO: Event ADDED observed for Job e2e-q4gcl in namespace job-728 with labels: map[e2e-job-label:e2e-q4gcl] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 18 15:45:40.203: INFO: Event MODIFIED observed for Job e2e-q4gcl in namespace job-728 with labels: map[e2e-job-label:e2e-q4gcl] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 18 15:45:40.203: INFO: Event MODIFIED found for Job e2e-q4gcl in namespace job-728 with labels: map[e2e-job-label:e2e-q4gcl e2e-q4gcl:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 01/18/23 15:45:40.203
STEP: Watching for Job to be updated 01/18/23 15:45:40.221
Jan 18 15:45:40.224: INFO: Event MODIFIED found for Job e2e-q4gcl in namespace job-728 with labels: map[e2e-job-label:e2e-q4gcl e2e-q4gcl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 15:45:40.224: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 01/18/23 15:45:40.224
Jan 18 15:45:40.232: INFO: Job: e2e-q4gcl as labels: map[e2e-job-label:e2e-q4gcl e2e-q4gcl:patched]
STEP: Waiting for job to complete 01/18/23 15:45:40.232
STEP: Delete a job collection with a labelselector 01/18/23 15:45:56.242
STEP: Watching for Job to be deleted 01/18/23 15:45:56.25
Jan 18 15:45:56.257: INFO: Event MODIFIED observed for Job e2e-q4gcl in namespace job-728 with labels: map[e2e-job-label:e2e-q4gcl e2e-q4gcl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 15:45:56.257: INFO: Event MODIFIED observed for Job e2e-q4gcl in namespace job-728 with labels: map[e2e-job-label:e2e-q4gcl e2e-q4gcl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 15:45:56.257: INFO: Event MODIFIED observed for Job e2e-q4gcl in namespace job-728 with labels: map[e2e-job-label:e2e-q4gcl e2e-q4gcl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 15:45:56.257: INFO: Event MODIFIED observed for Job e2e-q4gcl in namespace job-728 with labels: map[e2e-job-label:e2e-q4gcl e2e-q4gcl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 15:45:56.257: INFO: Event MODIFIED observed for Job e2e-q4gcl in namespace job-728 with labels: map[e2e-job-label:e2e-q4gcl e2e-q4gcl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 15:45:56.257: INFO: Event DELETED found for Job e2e-q4gcl in namespace job-728 with labels: map[e2e-job-label:e2e-q4gcl e2e-q4gcl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 01/18/23 15:45:56.258
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 18 15:45:56.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-728" for this suite. 01/18/23 15:45:56.279
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":164,"skipped":2865,"failed":0}
------------------------------
• [SLOW TEST] [16.262 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:45:40.036
    Jan 18 15:45:40.036: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename job 01/18/23 15:45:40.038
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:45:40.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:45:40.085
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 01/18/23 15:45:40.114
    STEP: Patching the Job 01/18/23 15:45:40.143
    STEP: Watching for Job to be patched 01/18/23 15:45:40.195
    Jan 18 15:45:40.203: INFO: Event ADDED observed for Job e2e-q4gcl in namespace job-728 with labels: map[e2e-job-label:e2e-q4gcl] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 18 15:45:40.203: INFO: Event MODIFIED observed for Job e2e-q4gcl in namespace job-728 with labels: map[e2e-job-label:e2e-q4gcl] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 18 15:45:40.203: INFO: Event MODIFIED found for Job e2e-q4gcl in namespace job-728 with labels: map[e2e-job-label:e2e-q4gcl e2e-q4gcl:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 01/18/23 15:45:40.203
    STEP: Watching for Job to be updated 01/18/23 15:45:40.221
    Jan 18 15:45:40.224: INFO: Event MODIFIED found for Job e2e-q4gcl in namespace job-728 with labels: map[e2e-job-label:e2e-q4gcl e2e-q4gcl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 15:45:40.224: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 01/18/23 15:45:40.224
    Jan 18 15:45:40.232: INFO: Job: e2e-q4gcl as labels: map[e2e-job-label:e2e-q4gcl e2e-q4gcl:patched]
    STEP: Waiting for job to complete 01/18/23 15:45:40.232
    STEP: Delete a job collection with a labelselector 01/18/23 15:45:56.242
    STEP: Watching for Job to be deleted 01/18/23 15:45:56.25
    Jan 18 15:45:56.257: INFO: Event MODIFIED observed for Job e2e-q4gcl in namespace job-728 with labels: map[e2e-job-label:e2e-q4gcl e2e-q4gcl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 15:45:56.257: INFO: Event MODIFIED observed for Job e2e-q4gcl in namespace job-728 with labels: map[e2e-job-label:e2e-q4gcl e2e-q4gcl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 15:45:56.257: INFO: Event MODIFIED observed for Job e2e-q4gcl in namespace job-728 with labels: map[e2e-job-label:e2e-q4gcl e2e-q4gcl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 15:45:56.257: INFO: Event MODIFIED observed for Job e2e-q4gcl in namespace job-728 with labels: map[e2e-job-label:e2e-q4gcl e2e-q4gcl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 15:45:56.257: INFO: Event MODIFIED observed for Job e2e-q4gcl in namespace job-728 with labels: map[e2e-job-label:e2e-q4gcl e2e-q4gcl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 15:45:56.257: INFO: Event DELETED found for Job e2e-q4gcl in namespace job-728 with labels: map[e2e-job-label:e2e-q4gcl e2e-q4gcl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 01/18/23 15:45:56.258
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 18 15:45:56.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-728" for this suite. 01/18/23 15:45:56.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:45:56.301
Jan 18 15:45:56.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename runtimeclass 01/18/23 15:45:56.309
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:45:56.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:45:56.403
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-6295-delete-me 01/18/23 15:45:56.418
STEP: Waiting for the RuntimeClass to disappear 01/18/23 15:45:56.427
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 18 15:45:56.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-6295" for this suite. 01/18/23 15:45:56.453
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":165,"skipped":2872,"failed":0}
------------------------------
• [0.158 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:45:56.301
    Jan 18 15:45:56.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename runtimeclass 01/18/23 15:45:56.309
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:45:56.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:45:56.403
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-6295-delete-me 01/18/23 15:45:56.418
    STEP: Waiting for the RuntimeClass to disappear 01/18/23 15:45:56.427
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 18 15:45:56.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-6295" for this suite. 01/18/23 15:45:56.453
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:45:56.465
Jan 18 15:45:56.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename resourcequota 01/18/23 15:45:56.469
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:45:56.487
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:45:56.503
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 01/18/23 15:46:13.513
STEP: Creating a ResourceQuota 01/18/23 15:46:18.524
STEP: Ensuring resource quota status is calculated 01/18/23 15:46:18.533
STEP: Creating a ConfigMap 01/18/23 15:46:20.539
STEP: Ensuring resource quota status captures configMap creation 01/18/23 15:46:20.558
STEP: Deleting a ConfigMap 01/18/23 15:46:22.569
STEP: Ensuring resource quota status released usage 01/18/23 15:46:22.578
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 15:46:24.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2773" for this suite. 01/18/23 15:46:24.59
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":166,"skipped":2888,"failed":0}
------------------------------
• [SLOW TEST] [28.132 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:45:56.465
    Jan 18 15:45:56.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename resourcequota 01/18/23 15:45:56.469
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:45:56.487
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:45:56.503
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 01/18/23 15:46:13.513
    STEP: Creating a ResourceQuota 01/18/23 15:46:18.524
    STEP: Ensuring resource quota status is calculated 01/18/23 15:46:18.533
    STEP: Creating a ConfigMap 01/18/23 15:46:20.539
    STEP: Ensuring resource quota status captures configMap creation 01/18/23 15:46:20.558
    STEP: Deleting a ConfigMap 01/18/23 15:46:22.569
    STEP: Ensuring resource quota status released usage 01/18/23 15:46:22.578
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 15:46:24.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2773" for this suite. 01/18/23 15:46:24.59
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:46:24.603
Jan 18 15:46:24.603: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 15:46:24.605
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:46:24.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:46:24.637
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-8db8dfcc-edb1-4001-bb46-44626663bb2a 01/18/23 15:46:24.641
STEP: Creating a pod to test consume configMaps 01/18/23 15:46:24.648
Jan 18 15:46:24.661: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0a2adeea-b21a-45dc-9879-e9dd6d081976" in namespace "projected-7022" to be "Succeeded or Failed"
Jan 18 15:46:24.673: INFO: Pod "pod-projected-configmaps-0a2adeea-b21a-45dc-9879-e9dd6d081976": Phase="Pending", Reason="", readiness=false. Elapsed: 11.897839ms
Jan 18 15:46:26.678: INFO: Pod "pod-projected-configmaps-0a2adeea-b21a-45dc-9879-e9dd6d081976": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017833277s
Jan 18 15:46:28.679: INFO: Pod "pod-projected-configmaps-0a2adeea-b21a-45dc-9879-e9dd6d081976": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018095638s
STEP: Saw pod success 01/18/23 15:46:28.679
Jan 18 15:46:28.680: INFO: Pod "pod-projected-configmaps-0a2adeea-b21a-45dc-9879-e9dd6d081976" satisfied condition "Succeeded or Failed"
Jan 18 15:46:28.684: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-0a2adeea-b21a-45dc-9879-e9dd6d081976 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 15:46:28.692
Jan 18 15:46:28.708: INFO: Waiting for pod pod-projected-configmaps-0a2adeea-b21a-45dc-9879-e9dd6d081976 to disappear
Jan 18 15:46:28.712: INFO: Pod pod-projected-configmaps-0a2adeea-b21a-45dc-9879-e9dd6d081976 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 18 15:46:28.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7022" for this suite. 01/18/23 15:46:28.717
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":167,"skipped":2911,"failed":0}
------------------------------
• [4.124 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:46:24.603
    Jan 18 15:46:24.603: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 15:46:24.605
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:46:24.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:46:24.637
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-8db8dfcc-edb1-4001-bb46-44626663bb2a 01/18/23 15:46:24.641
    STEP: Creating a pod to test consume configMaps 01/18/23 15:46:24.648
    Jan 18 15:46:24.661: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0a2adeea-b21a-45dc-9879-e9dd6d081976" in namespace "projected-7022" to be "Succeeded or Failed"
    Jan 18 15:46:24.673: INFO: Pod "pod-projected-configmaps-0a2adeea-b21a-45dc-9879-e9dd6d081976": Phase="Pending", Reason="", readiness=false. Elapsed: 11.897839ms
    Jan 18 15:46:26.678: INFO: Pod "pod-projected-configmaps-0a2adeea-b21a-45dc-9879-e9dd6d081976": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017833277s
    Jan 18 15:46:28.679: INFO: Pod "pod-projected-configmaps-0a2adeea-b21a-45dc-9879-e9dd6d081976": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018095638s
    STEP: Saw pod success 01/18/23 15:46:28.679
    Jan 18 15:46:28.680: INFO: Pod "pod-projected-configmaps-0a2adeea-b21a-45dc-9879-e9dd6d081976" satisfied condition "Succeeded or Failed"
    Jan 18 15:46:28.684: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-0a2adeea-b21a-45dc-9879-e9dd6d081976 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 15:46:28.692
    Jan 18 15:46:28.708: INFO: Waiting for pod pod-projected-configmaps-0a2adeea-b21a-45dc-9879-e9dd6d081976 to disappear
    Jan 18 15:46:28.712: INFO: Pod pod-projected-configmaps-0a2adeea-b21a-45dc-9879-e9dd6d081976 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 18 15:46:28.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7022" for this suite. 01/18/23 15:46:28.717
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:46:28.735
Jan 18 15:46:28.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 15:46:28.736
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:46:28.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:46:28.771
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/18/23 15:46:28.778
Jan 18 15:46:28.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 15:46:33.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 15:46:51.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1685" for this suite. 01/18/23 15:46:51.481
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":168,"skipped":2927,"failed":0}
------------------------------
• [SLOW TEST] [22.753 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:46:28.735
    Jan 18 15:46:28.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 15:46:28.736
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:46:28.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:46:28.771
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/18/23 15:46:28.778
    Jan 18 15:46:28.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 15:46:33.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 15:46:51.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1685" for this suite. 01/18/23 15:46:51.481
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:46:51.495
Jan 18 15:46:51.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename runtimeclass 01/18/23 15:46:51.497
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:46:51.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:46:51.523
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 18 15:46:51.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2809" for this suite. 01/18/23 15:46:51.557
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":169,"skipped":2948,"failed":0}
------------------------------
• [0.069 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:46:51.495
    Jan 18 15:46:51.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename runtimeclass 01/18/23 15:46:51.497
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:46:51.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:46:51.523
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 18 15:46:51.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-2809" for this suite. 01/18/23 15:46:51.557
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:46:51.565
Jan 18 15:46:51.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename resourcequota 01/18/23 15:46:51.567
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:46:51.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:46:51.59
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 01/18/23 15:46:51.594
STEP: Getting a ResourceQuota 01/18/23 15:46:51.599
STEP: Updating a ResourceQuota 01/18/23 15:46:51.608
STEP: Verifying a ResourceQuota was modified 01/18/23 15:46:51.619
STEP: Deleting a ResourceQuota 01/18/23 15:46:51.624
STEP: Verifying the deleted ResourceQuota 01/18/23 15:46:51.629
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 15:46:51.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1924" for this suite. 01/18/23 15:46:51.639
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":170,"skipped":2955,"failed":0}
------------------------------
• [0.081 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:46:51.565
    Jan 18 15:46:51.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename resourcequota 01/18/23 15:46:51.567
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:46:51.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:46:51.59
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 01/18/23 15:46:51.594
    STEP: Getting a ResourceQuota 01/18/23 15:46:51.599
    STEP: Updating a ResourceQuota 01/18/23 15:46:51.608
    STEP: Verifying a ResourceQuota was modified 01/18/23 15:46:51.619
    STEP: Deleting a ResourceQuota 01/18/23 15:46:51.624
    STEP: Verifying the deleted ResourceQuota 01/18/23 15:46:51.629
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 15:46:51.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1924" for this suite. 01/18/23 15:46:51.639
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:46:51.647
Jan 18 15:46:51.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename kubectl 01/18/23 15:46:51.65
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:46:51.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:46:51.672
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Jan 18 15:46:51.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3206 version'
Jan 18 15:46:51.788: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan 18 15:46:51.788: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4\", GitCommit:\"872a965c6c6526caa949f0c6ac028ef7aff3fb78\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:36:36Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4\", GitCommit:\"872a965c6c6526caa949f0c6ac028ef7aff3fb78\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:29:58Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 15:46:51.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3206" for this suite. 01/18/23 15:46:51.823
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":171,"skipped":2963,"failed":0}
------------------------------
• [0.183 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:46:51.647
    Jan 18 15:46:51.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename kubectl 01/18/23 15:46:51.65
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:46:51.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:46:51.672
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Jan 18 15:46:51.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3206 version'
    Jan 18 15:46:51.788: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jan 18 15:46:51.788: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4\", GitCommit:\"872a965c6c6526caa949f0c6ac028ef7aff3fb78\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:36:36Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4\", GitCommit:\"872a965c6c6526caa949f0c6ac028ef7aff3fb78\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:29:58Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 15:46:51.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3206" for this suite. 01/18/23 15:46:51.823
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:46:51.834
Jan 18 15:46:51.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename daemonsets 01/18/23 15:46:51.836
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:46:51.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:46:51.861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 01/18/23 15:46:51.885
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 15:46:51.896
Jan 18 15:46:51.905: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:46:51.910: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 15:46:51.910: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 15:46:52.917: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:46:52.924: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 15:46:52.924: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 15:46:53.917: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 15:46:53.923: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 15:46:53.923: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 01/18/23 15:46:53.928
STEP: DeleteCollection of the DaemonSets 01/18/23 15:46:53.937
STEP: Verify that ReplicaSets have been deleted 01/18/23 15:46:53.947
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Jan 18 15:46:53.981: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"47569"},"items":null}

Jan 18 15:46:53.990: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"47569"},"items":[{"metadata":{"name":"daemon-set-2flms","generateName":"daemon-set-","namespace":"daemonsets-6","uid":"d9edb3c3-1747-4347-88c7-2e824eb869f1","resourceVersion":"47567","creationTimestamp":"2023-01-18T15:46:51Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"aa8263cd220d757da484d3c3afd93dd0cc85190142d01a34bd64e73434e777bb","cni.projectcalico.org/podIP":"10.233.68.52/32","cni.projectcalico.org/podIPs":"10.233.68.52/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"13063719-899a-4828-909b-422f15b89d80","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-18T15:46:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13063719-899a-4828-909b-422f15b89d80\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-18T15:46:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-18T15:46:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-hmrs2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-hmrs2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"v1-25-1-18760-w2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["v1-25-1-18760-w2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T15:46:51Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T15:46:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T15:46:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T15:46:51Z"}],"hostIP":"192.168.101.216","podIP":"10.233.68.52","podIPs":[{"ip":"10.233.68.52"}],"startTime":"2023-01-18T15:46:51Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-18T15:46:52Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://3701aaa7f7006dd298a3085c91aacb2c3baa8331e0026c1e19d9dac605718272","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-8bwph","generateName":"daemon-set-","namespace":"daemonsets-6","uid":"b09e831f-fe20-4c72-a435-b799425ba409","resourceVersion":"47565","creationTimestamp":"2023-01-18T15:46:51Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"cb34b778c5b29797cd7ef9d08464982d6cdd848aac6d2eff81ca386700e264c1","cni.projectcalico.org/podIP":"10.233.78.117/32","cni.projectcalico.org/podIPs":"10.233.78.117/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"13063719-899a-4828-909b-422f15b89d80","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-18T15:46:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13063719-899a-4828-909b-422f15b89d80\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-18T15:46:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-18T15:46:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-dm64h","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-dm64h","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"v1-25-1-18760-w","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["v1-25-1-18760-w"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T15:46:51Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T15:46:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T15:46:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T15:46:51Z"}],"hostIP":"192.168.101.168","podIP":"10.233.78.117","podIPs":[{"ip":"10.233.78.117"}],"startTime":"2023-01-18T15:46:51Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-18T15:46:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://7533f30f1018bb5a5c2c61118e3de2b13b4ecf175c5cbda2d20a51a50b33af79","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 18 15:46:54.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6" for this suite. 01/18/23 15:46:54.046
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":172,"skipped":3005,"failed":0}
------------------------------
• [2.226 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:46:51.834
    Jan 18 15:46:51.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename daemonsets 01/18/23 15:46:51.836
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:46:51.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:46:51.861
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 01/18/23 15:46:51.885
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 15:46:51.896
    Jan 18 15:46:51.905: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:46:51.910: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 15:46:51.910: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 15:46:52.917: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:46:52.924: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 15:46:52.924: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 15:46:53.917: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 15:46:53.923: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 15:46:53.923: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 01/18/23 15:46:53.928
    STEP: DeleteCollection of the DaemonSets 01/18/23 15:46:53.937
    STEP: Verify that ReplicaSets have been deleted 01/18/23 15:46:53.947
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Jan 18 15:46:53.981: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"47569"},"items":null}

    Jan 18 15:46:53.990: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"47569"},"items":[{"metadata":{"name":"daemon-set-2flms","generateName":"daemon-set-","namespace":"daemonsets-6","uid":"d9edb3c3-1747-4347-88c7-2e824eb869f1","resourceVersion":"47567","creationTimestamp":"2023-01-18T15:46:51Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"aa8263cd220d757da484d3c3afd93dd0cc85190142d01a34bd64e73434e777bb","cni.projectcalico.org/podIP":"10.233.68.52/32","cni.projectcalico.org/podIPs":"10.233.68.52/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"13063719-899a-4828-909b-422f15b89d80","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-18T15:46:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13063719-899a-4828-909b-422f15b89d80\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-18T15:46:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-18T15:46:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-hmrs2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-hmrs2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"v1-25-1-18760-w2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["v1-25-1-18760-w2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T15:46:51Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T15:46:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T15:46:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T15:46:51Z"}],"hostIP":"192.168.101.216","podIP":"10.233.68.52","podIPs":[{"ip":"10.233.68.52"}],"startTime":"2023-01-18T15:46:51Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-18T15:46:52Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://3701aaa7f7006dd298a3085c91aacb2c3baa8331e0026c1e19d9dac605718272","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-8bwph","generateName":"daemon-set-","namespace":"daemonsets-6","uid":"b09e831f-fe20-4c72-a435-b799425ba409","resourceVersion":"47565","creationTimestamp":"2023-01-18T15:46:51Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"cb34b778c5b29797cd7ef9d08464982d6cdd848aac6d2eff81ca386700e264c1","cni.projectcalico.org/podIP":"10.233.78.117/32","cni.projectcalico.org/podIPs":"10.233.78.117/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"13063719-899a-4828-909b-422f15b89d80","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-18T15:46:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13063719-899a-4828-909b-422f15b89d80\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-18T15:46:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-18T15:46:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-dm64h","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-dm64h","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"v1-25-1-18760-w","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["v1-25-1-18760-w"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T15:46:51Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T15:46:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T15:46:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T15:46:51Z"}],"hostIP":"192.168.101.168","podIP":"10.233.78.117","podIPs":[{"ip":"10.233.78.117"}],"startTime":"2023-01-18T15:46:51Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-18T15:46:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://7533f30f1018bb5a5c2c61118e3de2b13b4ecf175c5cbda2d20a51a50b33af79","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 15:46:54.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-6" for this suite. 01/18/23 15:46:54.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:46:54.069
Jan 18 15:46:54.069: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename runtimeclass 01/18/23 15:46:54.071
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:46:54.098
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:46:54.105
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jan 18 15:46:54.130: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9359 to be scheduled
Jan 18 15:46:54.135: INFO: 1 pods are not scheduled: [runtimeclass-9359/test-runtimeclass-runtimeclass-9359-preconfigured-handler-hk45n(5bc5fa54-ca90-4af4-b330-eb1ae6f59b73)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 18 15:46:56.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9359" for this suite. 01/18/23 15:46:56.159
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":173,"skipped":3018,"failed":0}
------------------------------
• [2.104 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:46:54.069
    Jan 18 15:46:54.069: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename runtimeclass 01/18/23 15:46:54.071
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:46:54.098
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:46:54.105
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jan 18 15:46:54.130: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9359 to be scheduled
    Jan 18 15:46:54.135: INFO: 1 pods are not scheduled: [runtimeclass-9359/test-runtimeclass-runtimeclass-9359-preconfigured-handler-hk45n(5bc5fa54-ca90-4af4-b330-eb1ae6f59b73)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 18 15:46:56.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-9359" for this suite. 01/18/23 15:46:56.159
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:46:56.177
Jan 18 15:46:56.178: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename emptydir 01/18/23 15:46:56.179
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:46:56.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:46:56.21
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/18/23 15:46:56.217
Jan 18 15:46:56.226: INFO: Waiting up to 5m0s for pod "pod-c44508e0-3442-4a99-a517-d87d56725ca9" in namespace "emptydir-9956" to be "Succeeded or Failed"
Jan 18 15:46:56.239: INFO: Pod "pod-c44508e0-3442-4a99-a517-d87d56725ca9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.890593ms
Jan 18 15:46:58.245: INFO: Pod "pod-c44508e0-3442-4a99-a517-d87d56725ca9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019306333s
Jan 18 15:47:00.445: INFO: Pod "pod-c44508e0-3442-4a99-a517-d87d56725ca9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.219257262s
STEP: Saw pod success 01/18/23 15:47:00.446
Jan 18 15:47:00.446: INFO: Pod "pod-c44508e0-3442-4a99-a517-d87d56725ca9" satisfied condition "Succeeded or Failed"
Jan 18 15:47:00.456: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-c44508e0-3442-4a99-a517-d87d56725ca9 container test-container: <nil>
STEP: delete the pod 01/18/23 15:47:00.464
Jan 18 15:47:00.482: INFO: Waiting for pod pod-c44508e0-3442-4a99-a517-d87d56725ca9 to disappear
Jan 18 15:47:00.487: INFO: Pod pod-c44508e0-3442-4a99-a517-d87d56725ca9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 15:47:00.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9956" for this suite. 01/18/23 15:47:00.496
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":174,"skipped":3041,"failed":0}
------------------------------
• [4.324 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:46:56.177
    Jan 18 15:46:56.178: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename emptydir 01/18/23 15:46:56.179
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:46:56.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:46:56.21
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/18/23 15:46:56.217
    Jan 18 15:46:56.226: INFO: Waiting up to 5m0s for pod "pod-c44508e0-3442-4a99-a517-d87d56725ca9" in namespace "emptydir-9956" to be "Succeeded or Failed"
    Jan 18 15:46:56.239: INFO: Pod "pod-c44508e0-3442-4a99-a517-d87d56725ca9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.890593ms
    Jan 18 15:46:58.245: INFO: Pod "pod-c44508e0-3442-4a99-a517-d87d56725ca9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019306333s
    Jan 18 15:47:00.445: INFO: Pod "pod-c44508e0-3442-4a99-a517-d87d56725ca9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.219257262s
    STEP: Saw pod success 01/18/23 15:47:00.446
    Jan 18 15:47:00.446: INFO: Pod "pod-c44508e0-3442-4a99-a517-d87d56725ca9" satisfied condition "Succeeded or Failed"
    Jan 18 15:47:00.456: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-c44508e0-3442-4a99-a517-d87d56725ca9 container test-container: <nil>
    STEP: delete the pod 01/18/23 15:47:00.464
    Jan 18 15:47:00.482: INFO: Waiting for pod pod-c44508e0-3442-4a99-a517-d87d56725ca9 to disappear
    Jan 18 15:47:00.487: INFO: Pod pod-c44508e0-3442-4a99-a517-d87d56725ca9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 15:47:00.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9956" for this suite. 01/18/23 15:47:00.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:47:00.517
Jan 18 15:47:00.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename cronjob 01/18/23 15:47:00.52
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:47:00.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:47:00.543
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 01/18/23 15:47:00.549
STEP: Ensuring a job is scheduled 01/18/23 15:47:00.558
STEP: Ensuring exactly one is scheduled 01/18/23 15:48:00.564
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/18/23 15:48:00.57
STEP: Ensuring the job is replaced with a new one 01/18/23 15:48:00.579
STEP: Removing cronjob 01/18/23 15:49:00.585
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 18 15:49:00.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5890" for this suite. 01/18/23 15:49:00.606
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":175,"skipped":3049,"failed":0}
------------------------------
• [SLOW TEST] [120.113 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:47:00.517
    Jan 18 15:47:00.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename cronjob 01/18/23 15:47:00.52
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:47:00.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:47:00.543
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 01/18/23 15:47:00.549
    STEP: Ensuring a job is scheduled 01/18/23 15:47:00.558
    STEP: Ensuring exactly one is scheduled 01/18/23 15:48:00.564
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/18/23 15:48:00.57
    STEP: Ensuring the job is replaced with a new one 01/18/23 15:48:00.579
    STEP: Removing cronjob 01/18/23 15:49:00.585
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 18 15:49:00.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-5890" for this suite. 01/18/23 15:49:00.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:49:00.643
Jan 18 15:49:00.644: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename emptydir 01/18/23 15:49:00.646
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:49:00.674
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:49:00.678
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 01/18/23 15:49:00.683
Jan 18 15:49:00.692: INFO: Waiting up to 5m0s for pod "pod-3f26dc99-3f72-4e3c-9800-840c4054d518" in namespace "emptydir-9370" to be "Succeeded or Failed"
Jan 18 15:49:00.706: INFO: Pod "pod-3f26dc99-3f72-4e3c-9800-840c4054d518": Phase="Pending", Reason="", readiness=false. Elapsed: 14.452273ms
Jan 18 15:49:02.715: INFO: Pod "pod-3f26dc99-3f72-4e3c-9800-840c4054d518": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022889485s
Jan 18 15:49:04.715: INFO: Pod "pod-3f26dc99-3f72-4e3c-9800-840c4054d518": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02337273s
STEP: Saw pod success 01/18/23 15:49:04.715
Jan 18 15:49:04.716: INFO: Pod "pod-3f26dc99-3f72-4e3c-9800-840c4054d518" satisfied condition "Succeeded or Failed"
Jan 18 15:49:04.721: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-3f26dc99-3f72-4e3c-9800-840c4054d518 container test-container: <nil>
STEP: delete the pod 01/18/23 15:49:04.742
Jan 18 15:49:04.759: INFO: Waiting for pod pod-3f26dc99-3f72-4e3c-9800-840c4054d518 to disappear
Jan 18 15:49:04.764: INFO: Pod pod-3f26dc99-3f72-4e3c-9800-840c4054d518 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 15:49:04.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9370" for this suite. 01/18/23 15:49:04.769
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":176,"skipped":3072,"failed":0}
------------------------------
• [4.134 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:49:00.643
    Jan 18 15:49:00.644: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename emptydir 01/18/23 15:49:00.646
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:49:00.674
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:49:00.678
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/18/23 15:49:00.683
    Jan 18 15:49:00.692: INFO: Waiting up to 5m0s for pod "pod-3f26dc99-3f72-4e3c-9800-840c4054d518" in namespace "emptydir-9370" to be "Succeeded or Failed"
    Jan 18 15:49:00.706: INFO: Pod "pod-3f26dc99-3f72-4e3c-9800-840c4054d518": Phase="Pending", Reason="", readiness=false. Elapsed: 14.452273ms
    Jan 18 15:49:02.715: INFO: Pod "pod-3f26dc99-3f72-4e3c-9800-840c4054d518": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022889485s
    Jan 18 15:49:04.715: INFO: Pod "pod-3f26dc99-3f72-4e3c-9800-840c4054d518": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02337273s
    STEP: Saw pod success 01/18/23 15:49:04.715
    Jan 18 15:49:04.716: INFO: Pod "pod-3f26dc99-3f72-4e3c-9800-840c4054d518" satisfied condition "Succeeded or Failed"
    Jan 18 15:49:04.721: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-3f26dc99-3f72-4e3c-9800-840c4054d518 container test-container: <nil>
    STEP: delete the pod 01/18/23 15:49:04.742
    Jan 18 15:49:04.759: INFO: Waiting for pod pod-3f26dc99-3f72-4e3c-9800-840c4054d518 to disappear
    Jan 18 15:49:04.764: INFO: Pod pod-3f26dc99-3f72-4e3c-9800-840c4054d518 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 15:49:04.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9370" for this suite. 01/18/23 15:49:04.769
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:49:04.779
Jan 18 15:49:04.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename resourcequota 01/18/23 15:49:04.782
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:49:04.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:49:04.823
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 01/18/23 15:49:04.828
STEP: Ensuring ResourceQuota status is calculated 01/18/23 15:49:04.839
STEP: Creating a ResourceQuota with not terminating scope 01/18/23 15:49:06.845
STEP: Ensuring ResourceQuota status is calculated 01/18/23 15:49:06.85
STEP: Creating a long running pod 01/18/23 15:49:08.859
STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/18/23 15:49:08.892
STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/18/23 15:49:10.899
STEP: Deleting the pod 01/18/23 15:49:12.905
STEP: Ensuring resource quota status released the pod usage 01/18/23 15:49:12.922
STEP: Creating a terminating pod 01/18/23 15:49:14.93
STEP: Ensuring resource quota with terminating scope captures the pod usage 01/18/23 15:49:14.951
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/18/23 15:49:16.957
STEP: Deleting the pod 01/18/23 15:49:18.964
STEP: Ensuring resource quota status released the pod usage 01/18/23 15:49:18.981
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 15:49:20.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6078" for this suite. 01/18/23 15:49:20.993
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":177,"skipped":3076,"failed":0}
------------------------------
• [SLOW TEST] [16.231 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:49:04.779
    Jan 18 15:49:04.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename resourcequota 01/18/23 15:49:04.782
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:49:04.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:49:04.823
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 01/18/23 15:49:04.828
    STEP: Ensuring ResourceQuota status is calculated 01/18/23 15:49:04.839
    STEP: Creating a ResourceQuota with not terminating scope 01/18/23 15:49:06.845
    STEP: Ensuring ResourceQuota status is calculated 01/18/23 15:49:06.85
    STEP: Creating a long running pod 01/18/23 15:49:08.859
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/18/23 15:49:08.892
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/18/23 15:49:10.899
    STEP: Deleting the pod 01/18/23 15:49:12.905
    STEP: Ensuring resource quota status released the pod usage 01/18/23 15:49:12.922
    STEP: Creating a terminating pod 01/18/23 15:49:14.93
    STEP: Ensuring resource quota with terminating scope captures the pod usage 01/18/23 15:49:14.951
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/18/23 15:49:16.957
    STEP: Deleting the pod 01/18/23 15:49:18.964
    STEP: Ensuring resource quota status released the pod usage 01/18/23 15:49:18.981
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 15:49:20.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6078" for this suite. 01/18/23 15:49:20.993
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:49:21.014
Jan 18 15:49:21.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename services 01/18/23 15:49:21.017
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:49:21.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:49:21.051
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 01/18/23 15:49:21.055
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 15:49:21.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4865" for this suite. 01/18/23 15:49:21.067
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":178,"skipped":3088,"failed":0}
------------------------------
• [0.070 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:49:21.014
    Jan 18 15:49:21.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename services 01/18/23 15:49:21.017
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:49:21.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:49:21.051
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 01/18/23 15:49:21.055
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 15:49:21.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4865" for this suite. 01/18/23 15:49:21.067
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:49:21.093
Jan 18 15:49:21.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename var-expansion 01/18/23 15:49:21.094
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:49:21.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:49:21.125
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 01/18/23 15:49:21.131
STEP: waiting for pod running 01/18/23 15:49:21.146
Jan 18 15:49:21.147: INFO: Waiting up to 2m0s for pod "var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63" in namespace "var-expansion-9743" to be "running"
Jan 18 15:49:21.152: INFO: Pod "var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63": Phase="Pending", Reason="", readiness=false. Elapsed: 5.274846ms
Jan 18 15:49:23.158: INFO: Pod "var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63": Phase="Running", Reason="", readiness=true. Elapsed: 2.011261961s
Jan 18 15:49:23.158: INFO: Pod "var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63" satisfied condition "running"
STEP: creating a file in subpath 01/18/23 15:49:23.158
Jan 18 15:49:23.163: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9743 PodName:var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 15:49:23.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 15:49:23.164: INFO: ExecWithOptions: Clientset creation
Jan 18 15:49:23.164: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-9743/pods/var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 01/18/23 15:49:23.269
Jan 18 15:49:23.274: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9743 PodName:var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 15:49:23.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 15:49:23.275: INFO: ExecWithOptions: Clientset creation
Jan 18 15:49:23.275: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-9743/pods/var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 01/18/23 15:49:23.373
Jan 18 15:49:23.892: INFO: Successfully updated pod "var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63"
STEP: waiting for annotated pod running 01/18/23 15:49:23.892
Jan 18 15:49:23.893: INFO: Waiting up to 2m0s for pod "var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63" in namespace "var-expansion-9743" to be "running"
Jan 18 15:49:23.897: INFO: Pod "var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63": Phase="Running", Reason="", readiness=true. Elapsed: 4.210593ms
Jan 18 15:49:23.897: INFO: Pod "var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63" satisfied condition "running"
STEP: deleting the pod gracefully 01/18/23 15:49:23.898
Jan 18 15:49:23.898: INFO: Deleting pod "var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63" in namespace "var-expansion-9743"
Jan 18 15:49:23.908: INFO: Wait up to 5m0s for pod "var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 18 15:49:57.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9743" for this suite. 01/18/23 15:49:57.924
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":179,"skipped":3112,"failed":0}
------------------------------
• [SLOW TEST] [36.839 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:49:21.093
    Jan 18 15:49:21.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename var-expansion 01/18/23 15:49:21.094
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:49:21.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:49:21.125
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 01/18/23 15:49:21.131
    STEP: waiting for pod running 01/18/23 15:49:21.146
    Jan 18 15:49:21.147: INFO: Waiting up to 2m0s for pod "var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63" in namespace "var-expansion-9743" to be "running"
    Jan 18 15:49:21.152: INFO: Pod "var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63": Phase="Pending", Reason="", readiness=false. Elapsed: 5.274846ms
    Jan 18 15:49:23.158: INFO: Pod "var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63": Phase="Running", Reason="", readiness=true. Elapsed: 2.011261961s
    Jan 18 15:49:23.158: INFO: Pod "var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63" satisfied condition "running"
    STEP: creating a file in subpath 01/18/23 15:49:23.158
    Jan 18 15:49:23.163: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9743 PodName:var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 15:49:23.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 15:49:23.164: INFO: ExecWithOptions: Clientset creation
    Jan 18 15:49:23.164: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-9743/pods/var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 01/18/23 15:49:23.269
    Jan 18 15:49:23.274: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9743 PodName:var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 15:49:23.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 15:49:23.275: INFO: ExecWithOptions: Clientset creation
    Jan 18 15:49:23.275: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-9743/pods/var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 01/18/23 15:49:23.373
    Jan 18 15:49:23.892: INFO: Successfully updated pod "var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63"
    STEP: waiting for annotated pod running 01/18/23 15:49:23.892
    Jan 18 15:49:23.893: INFO: Waiting up to 2m0s for pod "var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63" in namespace "var-expansion-9743" to be "running"
    Jan 18 15:49:23.897: INFO: Pod "var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63": Phase="Running", Reason="", readiness=true. Elapsed: 4.210593ms
    Jan 18 15:49:23.897: INFO: Pod "var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63" satisfied condition "running"
    STEP: deleting the pod gracefully 01/18/23 15:49:23.898
    Jan 18 15:49:23.898: INFO: Deleting pod "var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63" in namespace "var-expansion-9743"
    Jan 18 15:49:23.908: INFO: Wait up to 5m0s for pod "var-expansion-bac8f7ec-577d-4872-937b-7e86d8686e63" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 18 15:49:57.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-9743" for this suite. 01/18/23 15:49:57.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:49:57.935
Jan 18 15:49:57.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename container-probe 01/18/23 15:49:57.937
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:49:57.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:49:57.961
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-9e5654b6-ff38-4de8-971e-9529466c1c84 in namespace container-probe-1668 01/18/23 15:49:57.965
Jan 18 15:49:57.979: INFO: Waiting up to 5m0s for pod "test-webserver-9e5654b6-ff38-4de8-971e-9529466c1c84" in namespace "container-probe-1668" to be "not pending"
Jan 18 15:49:57.986: INFO: Pod "test-webserver-9e5654b6-ff38-4de8-971e-9529466c1c84": Phase="Pending", Reason="", readiness=false. Elapsed: 6.733303ms
Jan 18 15:49:59.991: INFO: Pod "test-webserver-9e5654b6-ff38-4de8-971e-9529466c1c84": Phase="Running", Reason="", readiness=true. Elapsed: 2.011924615s
Jan 18 15:49:59.992: INFO: Pod "test-webserver-9e5654b6-ff38-4de8-971e-9529466c1c84" satisfied condition "not pending"
Jan 18 15:49:59.992: INFO: Started pod test-webserver-9e5654b6-ff38-4de8-971e-9529466c1c84 in namespace container-probe-1668
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 15:49:59.992
Jan 18 15:49:59.996: INFO: Initial restart count of pod test-webserver-9e5654b6-ff38-4de8-971e-9529466c1c84 is 0
STEP: deleting the pod 01/18/23 15:54:01.101
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 18 15:54:01.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1668" for this suite. 01/18/23 15:54:01.141
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":180,"skipped":3155,"failed":0}
------------------------------
• [SLOW TEST] [243.226 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:49:57.935
    Jan 18 15:49:57.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename container-probe 01/18/23 15:49:57.937
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:49:57.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:49:57.961
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-9e5654b6-ff38-4de8-971e-9529466c1c84 in namespace container-probe-1668 01/18/23 15:49:57.965
    Jan 18 15:49:57.979: INFO: Waiting up to 5m0s for pod "test-webserver-9e5654b6-ff38-4de8-971e-9529466c1c84" in namespace "container-probe-1668" to be "not pending"
    Jan 18 15:49:57.986: INFO: Pod "test-webserver-9e5654b6-ff38-4de8-971e-9529466c1c84": Phase="Pending", Reason="", readiness=false. Elapsed: 6.733303ms
    Jan 18 15:49:59.991: INFO: Pod "test-webserver-9e5654b6-ff38-4de8-971e-9529466c1c84": Phase="Running", Reason="", readiness=true. Elapsed: 2.011924615s
    Jan 18 15:49:59.992: INFO: Pod "test-webserver-9e5654b6-ff38-4de8-971e-9529466c1c84" satisfied condition "not pending"
    Jan 18 15:49:59.992: INFO: Started pod test-webserver-9e5654b6-ff38-4de8-971e-9529466c1c84 in namespace container-probe-1668
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 15:49:59.992
    Jan 18 15:49:59.996: INFO: Initial restart count of pod test-webserver-9e5654b6-ff38-4de8-971e-9529466c1c84 is 0
    STEP: deleting the pod 01/18/23 15:54:01.101
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 18 15:54:01.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-1668" for this suite. 01/18/23 15:54:01.141
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:54:01.161
Jan 18 15:54:01.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename deployment 01/18/23 15:54:01.166
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:54:01.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:54:01.224
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jan 18 15:54:01.228: INFO: Creating deployment "test-recreate-deployment"
Jan 18 15:54:01.237: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 18 15:54:01.250: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan 18 15:54:03.269: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 18 15:54:03.274: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 18 15:54:03.286: INFO: Updating deployment test-recreate-deployment
Jan 18 15:54:03.286: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 15:54:03.463: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2442  9e446486-df90-40ab-8869-1d13050c5482 49460 2 2023-01-18 15:54:01 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 15:54:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 15:54:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cb0f88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-18 15:54:03 +0000 UTC,LastTransitionTime:2023-01-18 15:54:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-01-18 15:54:03 +0000 UTC,LastTransitionTime:2023-01-18 15:54:01 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 18 15:54:03.469: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-2442  5d765982-66e4-4ae7-8747-004372091807 49459 1 2023-01-18 15:54:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 9e446486-df90-40ab-8869-1d13050c5482 0xc003cb1470 0xc003cb1471}] [] [{kube-controller-manager Update apps/v1 2023-01-18 15:54:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9e446486-df90-40ab-8869-1d13050c5482\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 15:54:03 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cb1508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 15:54:03.469: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 18 15:54:03.469: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-2442  bc6799a6-0f18-4a34-b6b0-a80967f6ad6f 49448 2 2023-01-18 15:54:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 9e446486-df90-40ab-8869-1d13050c5482 0xc003cb1357 0xc003cb1358}] [] [{kube-controller-manager Update apps/v1 2023-01-18 15:54:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9e446486-df90-40ab-8869-1d13050c5482\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 15:54:03 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cb1408 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 15:54:03.475: INFO: Pod "test-recreate-deployment-9d58999df-6qzcx" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-6qzcx test-recreate-deployment-9d58999df- deployment-2442  097ee539-1d7c-42d5-9d79-e76d6d97a1fd 49458 0 2023-01-18 15:54:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 5d765982-66e4-4ae7-8747-004372091807 0xc003cb1990 0xc003cb1991}] [] [{kube-controller-manager Update v1 2023-01-18 15:54:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d765982-66e4-4ae7-8747-004372091807\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 15:54:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qsdwb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qsdwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:54:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:54:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:54:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:54:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:,StartTime:2023-01-18 15:54:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 18 15:54:03.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2442" for this suite. 01/18/23 15:54:03.483
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":181,"skipped":3155,"failed":0}
------------------------------
• [2.329 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:54:01.161
    Jan 18 15:54:01.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename deployment 01/18/23 15:54:01.166
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:54:01.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:54:01.224
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jan 18 15:54:01.228: INFO: Creating deployment "test-recreate-deployment"
    Jan 18 15:54:01.237: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jan 18 15:54:01.250: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Jan 18 15:54:03.269: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jan 18 15:54:03.274: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jan 18 15:54:03.286: INFO: Updating deployment test-recreate-deployment
    Jan 18 15:54:03.286: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 15:54:03.463: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-2442  9e446486-df90-40ab-8869-1d13050c5482 49460 2 2023-01-18 15:54:01 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 15:54:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 15:54:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cb0f88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-18 15:54:03 +0000 UTC,LastTransitionTime:2023-01-18 15:54:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-01-18 15:54:03 +0000 UTC,LastTransitionTime:2023-01-18 15:54:01 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 18 15:54:03.469: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-2442  5d765982-66e4-4ae7-8747-004372091807 49459 1 2023-01-18 15:54:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 9e446486-df90-40ab-8869-1d13050c5482 0xc003cb1470 0xc003cb1471}] [] [{kube-controller-manager Update apps/v1 2023-01-18 15:54:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9e446486-df90-40ab-8869-1d13050c5482\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 15:54:03 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cb1508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 15:54:03.469: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jan 18 15:54:03.469: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-2442  bc6799a6-0f18-4a34-b6b0-a80967f6ad6f 49448 2 2023-01-18 15:54:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 9e446486-df90-40ab-8869-1d13050c5482 0xc003cb1357 0xc003cb1358}] [] [{kube-controller-manager Update apps/v1 2023-01-18 15:54:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9e446486-df90-40ab-8869-1d13050c5482\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 15:54:03 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cb1408 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 15:54:03.475: INFO: Pod "test-recreate-deployment-9d58999df-6qzcx" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-6qzcx test-recreate-deployment-9d58999df- deployment-2442  097ee539-1d7c-42d5-9d79-e76d6d97a1fd 49458 0 2023-01-18 15:54:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 5d765982-66e4-4ae7-8747-004372091807 0xc003cb1990 0xc003cb1991}] [] [{kube-controller-manager Update v1 2023-01-18 15:54:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d765982-66e4-4ae7-8747-004372091807\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 15:54:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qsdwb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qsdwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:54:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:54:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:54:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:54:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:,StartTime:2023-01-18 15:54:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 18 15:54:03.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2442" for this suite. 01/18/23 15:54:03.483
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:54:03.496
Jan 18 15:54:03.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename services 01/18/23 15:54:03.505
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:54:03.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:54:03.546
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 01/18/23 15:54:03.554
STEP: watching for the Service to be added 01/18/23 15:54:03.605
Jan 18 15:54:03.607: INFO: Found Service test-service-db7f8 in namespace services-5981 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan 18 15:54:03.608: INFO: Service test-service-db7f8 created
STEP: Getting /status 01/18/23 15:54:03.608
Jan 18 15:54:03.629: INFO: Service test-service-db7f8 has LoadBalancer: {[]}
STEP: patching the ServiceStatus 01/18/23 15:54:03.629
STEP: watching for the Service to be patched 01/18/23 15:54:03.638
Jan 18 15:54:03.644: INFO: observed Service test-service-db7f8 in namespace services-5981 with annotations: map[] & LoadBalancer: {[]}
Jan 18 15:54:03.645: INFO: Found Service test-service-db7f8 in namespace services-5981 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan 18 15:54:03.645: INFO: Service test-service-db7f8 has service status patched
STEP: updating the ServiceStatus 01/18/23 15:54:03.645
Jan 18 15:54:03.665: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 01/18/23 15:54:03.665
Jan 18 15:54:03.668: INFO: Observed Service test-service-db7f8 in namespace services-5981 with annotations: map[] & Conditions: {[]}
Jan 18 15:54:03.668: INFO: Observed event: &Service{ObjectMeta:{test-service-db7f8  services-5981  4094dc9b-287f-4415-b949-3bc73c1f983a 49469 0 2023-01-18 15:54:03 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-18 15:54:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-18 15:54:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.1.195,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.1.195],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan 18 15:54:03.669: INFO: Found Service test-service-db7f8 in namespace services-5981 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 18 15:54:03.669: INFO: Service test-service-db7f8 has service status updated
STEP: patching the service 01/18/23 15:54:03.669
STEP: watching for the Service to be patched 01/18/23 15:54:03.691
Jan 18 15:54:03.698: INFO: observed Service test-service-db7f8 in namespace services-5981 with labels: map[test-service-static:true]
Jan 18 15:54:03.698: INFO: observed Service test-service-db7f8 in namespace services-5981 with labels: map[test-service-static:true]
Jan 18 15:54:03.698: INFO: observed Service test-service-db7f8 in namespace services-5981 with labels: map[test-service-static:true]
Jan 18 15:54:03.699: INFO: Found Service test-service-db7f8 in namespace services-5981 with labels: map[test-service:patched test-service-static:true]
Jan 18 15:54:03.699: INFO: Service test-service-db7f8 patched
STEP: deleting the service 01/18/23 15:54:03.699
STEP: watching for the Service to be deleted 01/18/23 15:54:03.718
Jan 18 15:54:03.722: INFO: Observed event: ADDED
Jan 18 15:54:03.722: INFO: Observed event: MODIFIED
Jan 18 15:54:03.722: INFO: Observed event: MODIFIED
Jan 18 15:54:03.722: INFO: Observed event: MODIFIED
Jan 18 15:54:03.722: INFO: Found Service test-service-db7f8 in namespace services-5981 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan 18 15:54:03.722: INFO: Service test-service-db7f8 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 15:54:03.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5981" for this suite. 01/18/23 15:54:03.727
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":182,"skipped":3161,"failed":0}
------------------------------
• [0.243 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:54:03.496
    Jan 18 15:54:03.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename services 01/18/23 15:54:03.505
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:54:03.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:54:03.546
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 01/18/23 15:54:03.554
    STEP: watching for the Service to be added 01/18/23 15:54:03.605
    Jan 18 15:54:03.607: INFO: Found Service test-service-db7f8 in namespace services-5981 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jan 18 15:54:03.608: INFO: Service test-service-db7f8 created
    STEP: Getting /status 01/18/23 15:54:03.608
    Jan 18 15:54:03.629: INFO: Service test-service-db7f8 has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 01/18/23 15:54:03.629
    STEP: watching for the Service to be patched 01/18/23 15:54:03.638
    Jan 18 15:54:03.644: INFO: observed Service test-service-db7f8 in namespace services-5981 with annotations: map[] & LoadBalancer: {[]}
    Jan 18 15:54:03.645: INFO: Found Service test-service-db7f8 in namespace services-5981 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jan 18 15:54:03.645: INFO: Service test-service-db7f8 has service status patched
    STEP: updating the ServiceStatus 01/18/23 15:54:03.645
    Jan 18 15:54:03.665: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 01/18/23 15:54:03.665
    Jan 18 15:54:03.668: INFO: Observed Service test-service-db7f8 in namespace services-5981 with annotations: map[] & Conditions: {[]}
    Jan 18 15:54:03.668: INFO: Observed event: &Service{ObjectMeta:{test-service-db7f8  services-5981  4094dc9b-287f-4415-b949-3bc73c1f983a 49469 0 2023-01-18 15:54:03 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-18 15:54:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-18 15:54:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.1.195,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.1.195],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jan 18 15:54:03.669: INFO: Found Service test-service-db7f8 in namespace services-5981 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 18 15:54:03.669: INFO: Service test-service-db7f8 has service status updated
    STEP: patching the service 01/18/23 15:54:03.669
    STEP: watching for the Service to be patched 01/18/23 15:54:03.691
    Jan 18 15:54:03.698: INFO: observed Service test-service-db7f8 in namespace services-5981 with labels: map[test-service-static:true]
    Jan 18 15:54:03.698: INFO: observed Service test-service-db7f8 in namespace services-5981 with labels: map[test-service-static:true]
    Jan 18 15:54:03.698: INFO: observed Service test-service-db7f8 in namespace services-5981 with labels: map[test-service-static:true]
    Jan 18 15:54:03.699: INFO: Found Service test-service-db7f8 in namespace services-5981 with labels: map[test-service:patched test-service-static:true]
    Jan 18 15:54:03.699: INFO: Service test-service-db7f8 patched
    STEP: deleting the service 01/18/23 15:54:03.699
    STEP: watching for the Service to be deleted 01/18/23 15:54:03.718
    Jan 18 15:54:03.722: INFO: Observed event: ADDED
    Jan 18 15:54:03.722: INFO: Observed event: MODIFIED
    Jan 18 15:54:03.722: INFO: Observed event: MODIFIED
    Jan 18 15:54:03.722: INFO: Observed event: MODIFIED
    Jan 18 15:54:03.722: INFO: Found Service test-service-db7f8 in namespace services-5981 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jan 18 15:54:03.722: INFO: Service test-service-db7f8 deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 15:54:03.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5981" for this suite. 01/18/23 15:54:03.727
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:54:03.747
Jan 18 15:54:03.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename kubectl 01/18/23 15:54:03.749
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:54:03.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:54:03.782
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/18/23 15:54:03.79
Jan 18 15:54:03.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-7051 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Jan 18 15:54:03.934: INFO: stderr: ""
Jan 18 15:54:03.934: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 01/18/23 15:54:03.934
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Jan 18 15:54:03.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-7051 delete pods e2e-test-httpd-pod'
Jan 18 15:54:06.318: INFO: stderr: ""
Jan 18 15:54:06.318: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 15:54:06.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7051" for this suite. 01/18/23 15:54:06.341
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":183,"skipped":3203,"failed":0}
------------------------------
• [2.603 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:54:03.747
    Jan 18 15:54:03.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename kubectl 01/18/23 15:54:03.749
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:54:03.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:54:03.782
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/18/23 15:54:03.79
    Jan 18 15:54:03.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-7051 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Jan 18 15:54:03.934: INFO: stderr: ""
    Jan 18 15:54:03.934: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 01/18/23 15:54:03.934
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Jan 18 15:54:03.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-7051 delete pods e2e-test-httpd-pod'
    Jan 18 15:54:06.318: INFO: stderr: ""
    Jan 18 15:54:06.318: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 15:54:06.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7051" for this suite. 01/18/23 15:54:06.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:54:06.351
Jan 18 15:54:06.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 15:54:06.353
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:54:06.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:54:06.399
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jan 18 15:54:06.405: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 15:54:12.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7215" for this suite. 01/18/23 15:54:12.962
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":184,"skipped":3209,"failed":0}
------------------------------
• [SLOW TEST] [6.616 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:54:06.351
    Jan 18 15:54:06.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 15:54:06.353
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:54:06.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:54:06.399
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jan 18 15:54:06.405: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 15:54:12.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-7215" for this suite. 01/18/23 15:54:12.962
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:54:12.969
Jan 18 15:54:12.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 15:54:12.972
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:54:12.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:54:13.004
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-be0c61d9-31fd-4cf7-b752-d16c6fbf8802 01/18/23 15:54:13.008
STEP: Creating a pod to test consume configMaps 01/18/23 15:54:13.017
Jan 18 15:54:13.036: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-be3ede51-fc8b-47a2-b6ca-629ae5a6899c" in namespace "projected-7034" to be "Succeeded or Failed"
Jan 18 15:54:13.047: INFO: Pod "pod-projected-configmaps-be3ede51-fc8b-47a2-b6ca-629ae5a6899c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.507412ms
Jan 18 15:54:15.053: INFO: Pod "pod-projected-configmaps-be3ede51-fc8b-47a2-b6ca-629ae5a6899c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017691995s
Jan 18 15:54:17.052: INFO: Pod "pod-projected-configmaps-be3ede51-fc8b-47a2-b6ca-629ae5a6899c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0167268s
STEP: Saw pod success 01/18/23 15:54:17.053
Jan 18 15:54:17.053: INFO: Pod "pod-projected-configmaps-be3ede51-fc8b-47a2-b6ca-629ae5a6899c" satisfied condition "Succeeded or Failed"
Jan 18 15:54:17.056: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-be3ede51-fc8b-47a2-b6ca-629ae5a6899c container agnhost-container: <nil>
STEP: delete the pod 01/18/23 15:54:17.08
Jan 18 15:54:17.093: INFO: Waiting for pod pod-projected-configmaps-be3ede51-fc8b-47a2-b6ca-629ae5a6899c to disappear
Jan 18 15:54:17.100: INFO: Pod pod-projected-configmaps-be3ede51-fc8b-47a2-b6ca-629ae5a6899c no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 18 15:54:17.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7034" for this suite. 01/18/23 15:54:17.106
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":185,"skipped":3215,"failed":0}
------------------------------
• [4.143 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:54:12.969
    Jan 18 15:54:12.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 15:54:12.972
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:54:12.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:54:13.004
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-be0c61d9-31fd-4cf7-b752-d16c6fbf8802 01/18/23 15:54:13.008
    STEP: Creating a pod to test consume configMaps 01/18/23 15:54:13.017
    Jan 18 15:54:13.036: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-be3ede51-fc8b-47a2-b6ca-629ae5a6899c" in namespace "projected-7034" to be "Succeeded or Failed"
    Jan 18 15:54:13.047: INFO: Pod "pod-projected-configmaps-be3ede51-fc8b-47a2-b6ca-629ae5a6899c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.507412ms
    Jan 18 15:54:15.053: INFO: Pod "pod-projected-configmaps-be3ede51-fc8b-47a2-b6ca-629ae5a6899c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017691995s
    Jan 18 15:54:17.052: INFO: Pod "pod-projected-configmaps-be3ede51-fc8b-47a2-b6ca-629ae5a6899c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0167268s
    STEP: Saw pod success 01/18/23 15:54:17.053
    Jan 18 15:54:17.053: INFO: Pod "pod-projected-configmaps-be3ede51-fc8b-47a2-b6ca-629ae5a6899c" satisfied condition "Succeeded or Failed"
    Jan 18 15:54:17.056: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-be3ede51-fc8b-47a2-b6ca-629ae5a6899c container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 15:54:17.08
    Jan 18 15:54:17.093: INFO: Waiting for pod pod-projected-configmaps-be3ede51-fc8b-47a2-b6ca-629ae5a6899c to disappear
    Jan 18 15:54:17.100: INFO: Pod pod-projected-configmaps-be3ede51-fc8b-47a2-b6ca-629ae5a6899c no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 18 15:54:17.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7034" for this suite. 01/18/23 15:54:17.106
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:54:17.112
Jan 18 15:54:17.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename sysctl 01/18/23 15:54:17.116
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:54:17.135
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:54:17.141
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 01/18/23 15:54:17.147
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 18 15:54:17.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-5744" for this suite. 01/18/23 15:54:17.158
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":186,"skipped":3216,"failed":0}
------------------------------
• [0.060 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:54:17.112
    Jan 18 15:54:17.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename sysctl 01/18/23 15:54:17.116
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:54:17.135
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:54:17.141
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 01/18/23 15:54:17.147
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 18 15:54:17.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-5744" for this suite. 01/18/23 15:54:17.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:54:17.178
Jan 18 15:54:17.178: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename secrets 01/18/23 15:54:17.18
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:54:17.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:54:17.202
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-eeb229fe-d92e-4611-b5dc-af0f5f39d105 01/18/23 15:54:17.207
STEP: Creating a pod to test consume secrets 01/18/23 15:54:17.213
Jan 18 15:54:17.222: INFO: Waiting up to 5m0s for pod "pod-secrets-de9f08fb-908c-40c1-a481-8fd71c4249d8" in namespace "secrets-5036" to be "Succeeded or Failed"
Jan 18 15:54:17.229: INFO: Pod "pod-secrets-de9f08fb-908c-40c1-a481-8fd71c4249d8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.299429ms
Jan 18 15:54:19.234: INFO: Pod "pod-secrets-de9f08fb-908c-40c1-a481-8fd71c4249d8": Phase="Running", Reason="", readiness=true. Elapsed: 2.011646997s
Jan 18 15:54:21.235: INFO: Pod "pod-secrets-de9f08fb-908c-40c1-a481-8fd71c4249d8": Phase="Running", Reason="", readiness=false. Elapsed: 4.012610157s
Jan 18 15:54:23.235: INFO: Pod "pod-secrets-de9f08fb-908c-40c1-a481-8fd71c4249d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012578424s
STEP: Saw pod success 01/18/23 15:54:23.236
Jan 18 15:54:23.236: INFO: Pod "pod-secrets-de9f08fb-908c-40c1-a481-8fd71c4249d8" satisfied condition "Succeeded or Failed"
Jan 18 15:54:23.243: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-de9f08fb-908c-40c1-a481-8fd71c4249d8 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 15:54:23.252
Jan 18 15:54:23.264: INFO: Waiting for pod pod-secrets-de9f08fb-908c-40c1-a481-8fd71c4249d8 to disappear
Jan 18 15:54:23.267: INFO: Pod pod-secrets-de9f08fb-908c-40c1-a481-8fd71c4249d8 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 18 15:54:23.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5036" for this suite. 01/18/23 15:54:23.272
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":187,"skipped":3231,"failed":0}
------------------------------
• [SLOW TEST] [6.100 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:54:17.178
    Jan 18 15:54:17.178: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename secrets 01/18/23 15:54:17.18
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:54:17.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:54:17.202
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-eeb229fe-d92e-4611-b5dc-af0f5f39d105 01/18/23 15:54:17.207
    STEP: Creating a pod to test consume secrets 01/18/23 15:54:17.213
    Jan 18 15:54:17.222: INFO: Waiting up to 5m0s for pod "pod-secrets-de9f08fb-908c-40c1-a481-8fd71c4249d8" in namespace "secrets-5036" to be "Succeeded or Failed"
    Jan 18 15:54:17.229: INFO: Pod "pod-secrets-de9f08fb-908c-40c1-a481-8fd71c4249d8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.299429ms
    Jan 18 15:54:19.234: INFO: Pod "pod-secrets-de9f08fb-908c-40c1-a481-8fd71c4249d8": Phase="Running", Reason="", readiness=true. Elapsed: 2.011646997s
    Jan 18 15:54:21.235: INFO: Pod "pod-secrets-de9f08fb-908c-40c1-a481-8fd71c4249d8": Phase="Running", Reason="", readiness=false. Elapsed: 4.012610157s
    Jan 18 15:54:23.235: INFO: Pod "pod-secrets-de9f08fb-908c-40c1-a481-8fd71c4249d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012578424s
    STEP: Saw pod success 01/18/23 15:54:23.236
    Jan 18 15:54:23.236: INFO: Pod "pod-secrets-de9f08fb-908c-40c1-a481-8fd71c4249d8" satisfied condition "Succeeded or Failed"
    Jan 18 15:54:23.243: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-de9f08fb-908c-40c1-a481-8fd71c4249d8 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 15:54:23.252
    Jan 18 15:54:23.264: INFO: Waiting for pod pod-secrets-de9f08fb-908c-40c1-a481-8fd71c4249d8 to disappear
    Jan 18 15:54:23.267: INFO: Pod pod-secrets-de9f08fb-908c-40c1-a481-8fd71c4249d8 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 15:54:23.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5036" for this suite. 01/18/23 15:54:23.272
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:54:23.281
Jan 18 15:54:23.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 15:54:23.283
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:54:23.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:54:23.306
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/18/23 15:54:23.322
Jan 18 15:54:23.332: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4221" to be "running and ready"
Jan 18 15:54:23.343: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 10.965297ms
Jan 18 15:54:23.343: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:54:25.349: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016957718s
Jan 18 15:54:25.350: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:54:27.348: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.015928908s
Jan 18 15:54:27.348: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 18 15:54:27.348: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 01/18/23 15:54:27.354
Jan 18 15:54:27.361: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4221" to be "running and ready"
Jan 18 15:54:27.366: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.90278ms
Jan 18 15:54:27.366: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:54:29.372: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010700737s
Jan 18 15:54:29.372: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:54:31.371: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.010393914s
Jan 18 15:54:31.371: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jan 18 15:54:31.371: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/18/23 15:54:31.376
STEP: delete the pod with lifecycle hook 01/18/23 15:54:31.384
Jan 18 15:54:31.395: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 18 15:54:31.405: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 18 15:54:33.407: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 18 15:54:33.414: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 18 15:54:33.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4221" for this suite. 01/18/23 15:54:33.42
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":188,"skipped":3260,"failed":0}
------------------------------
• [SLOW TEST] [10.149 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:54:23.281
    Jan 18 15:54:23.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 15:54:23.283
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:54:23.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:54:23.306
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/18/23 15:54:23.322
    Jan 18 15:54:23.332: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4221" to be "running and ready"
    Jan 18 15:54:23.343: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 10.965297ms
    Jan 18 15:54:23.343: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:54:25.349: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016957718s
    Jan 18 15:54:25.350: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:54:27.348: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.015928908s
    Jan 18 15:54:27.348: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 18 15:54:27.348: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 01/18/23 15:54:27.354
    Jan 18 15:54:27.361: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4221" to be "running and ready"
    Jan 18 15:54:27.366: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.90278ms
    Jan 18 15:54:27.366: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:54:29.372: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010700737s
    Jan 18 15:54:29.372: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:54:31.371: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.010393914s
    Jan 18 15:54:31.371: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jan 18 15:54:31.371: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/18/23 15:54:31.376
    STEP: delete the pod with lifecycle hook 01/18/23 15:54:31.384
    Jan 18 15:54:31.395: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 18 15:54:31.405: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 18 15:54:33.407: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 18 15:54:33.414: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 18 15:54:33.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-4221" for this suite. 01/18/23 15:54:33.42
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:54:33.432
Jan 18 15:54:33.432: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename sched-preemption 01/18/23 15:54:33.434
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:54:33.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:54:33.462
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 18 15:54:33.481: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 15:55:33.532: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:55:33.537
Jan 18 15:55:33.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename sched-preemption-path 01/18/23 15:55:33.54
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:55:33.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:55:33.571
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 01/18/23 15:55:33.576
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 15:55:33.577
Jan 18 15:55:33.589: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-9221" to be "running"
Jan 18 15:55:33.593: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.717309ms
Jan 18 15:55:35.602: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012536702s
Jan 18 15:55:37.599: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.009594457s
Jan 18 15:55:37.599: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 15:55:37.602
Jan 18 15:55:37.623: INFO: found a healthy node: v1-25-1-18760-w2
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Jan 18 15:55:49.876: INFO: pods created so far: [1 1 1]
Jan 18 15:55:49.876: INFO: length of pods created so far: 3
Jan 18 15:55:51.898: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Jan 18 15:55:58.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9221" for this suite. 01/18/23 15:55:58.91
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 18 15:55:58.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-619" for this suite. 01/18/23 15:55:58.961
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":189,"skipped":3264,"failed":0}
------------------------------
• [SLOW TEST] [85.589 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:54:33.432
    Jan 18 15:54:33.432: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename sched-preemption 01/18/23 15:54:33.434
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:54:33.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:54:33.462
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 18 15:54:33.481: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 15:55:33.532: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:55:33.537
    Jan 18 15:55:33.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename sched-preemption-path 01/18/23 15:55:33.54
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:55:33.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:55:33.571
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 01/18/23 15:55:33.576
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 15:55:33.577
    Jan 18 15:55:33.589: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-9221" to be "running"
    Jan 18 15:55:33.593: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.717309ms
    Jan 18 15:55:35.602: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012536702s
    Jan 18 15:55:37.599: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.009594457s
    Jan 18 15:55:37.599: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 15:55:37.602
    Jan 18 15:55:37.623: INFO: found a healthy node: v1-25-1-18760-w2
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Jan 18 15:55:49.876: INFO: pods created so far: [1 1 1]
    Jan 18 15:55:49.876: INFO: length of pods created so far: 3
    Jan 18 15:55:51.898: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Jan 18 15:55:58.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-9221" for this suite. 01/18/23 15:55:58.91
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 15:55:58.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-619" for this suite. 01/18/23 15:55:58.961
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:55:59.027
Jan 18 15:55:59.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename ingressclass 01/18/23 15:55:59.03
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:55:59.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:55:59.065
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 01/18/23 15:55:59.07
STEP: getting /apis/networking.k8s.io 01/18/23 15:55:59.075
STEP: getting /apis/networking.k8s.iov1 01/18/23 15:55:59.077
STEP: creating 01/18/23 15:55:59.079
STEP: getting 01/18/23 15:55:59.097
STEP: listing 01/18/23 15:55:59.105
STEP: watching 01/18/23 15:55:59.111
Jan 18 15:55:59.111: INFO: starting watch
STEP: patching 01/18/23 15:55:59.114
STEP: updating 01/18/23 15:55:59.122
Jan 18 15:55:59.131: INFO: waiting for watch events with expected annotations
Jan 18 15:55:59.131: INFO: saw patched and updated annotations
STEP: deleting 01/18/23 15:55:59.131
STEP: deleting a collection 01/18/23 15:55:59.145
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Jan 18 15:55:59.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-9043" for this suite. 01/18/23 15:55:59.184
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":190,"skipped":3278,"failed":0}
------------------------------
• [0.164 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:55:59.027
    Jan 18 15:55:59.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename ingressclass 01/18/23 15:55:59.03
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:55:59.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:55:59.065
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 01/18/23 15:55:59.07
    STEP: getting /apis/networking.k8s.io 01/18/23 15:55:59.075
    STEP: getting /apis/networking.k8s.iov1 01/18/23 15:55:59.077
    STEP: creating 01/18/23 15:55:59.079
    STEP: getting 01/18/23 15:55:59.097
    STEP: listing 01/18/23 15:55:59.105
    STEP: watching 01/18/23 15:55:59.111
    Jan 18 15:55:59.111: INFO: starting watch
    STEP: patching 01/18/23 15:55:59.114
    STEP: updating 01/18/23 15:55:59.122
    Jan 18 15:55:59.131: INFO: waiting for watch events with expected annotations
    Jan 18 15:55:59.131: INFO: saw patched and updated annotations
    STEP: deleting 01/18/23 15:55:59.131
    STEP: deleting a collection 01/18/23 15:55:59.145
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Jan 18 15:55:59.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-9043" for this suite. 01/18/23 15:55:59.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:55:59.194
Jan 18 15:55:59.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename security-context-test 01/18/23 15:55:59.198
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:55:59.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:55:59.232
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Jan 18 15:55:59.248: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-344a3ec6-b5a6-45b9-97ff-8e8ccdadccab" in namespace "security-context-test-761" to be "Succeeded or Failed"
Jan 18 15:55:59.256: INFO: Pod "alpine-nnp-false-344a3ec6-b5a6-45b9-97ff-8e8ccdadccab": Phase="Pending", Reason="", readiness=false. Elapsed: 7.444814ms
Jan 18 15:56:01.262: INFO: Pod "alpine-nnp-false-344a3ec6-b5a6-45b9-97ff-8e8ccdadccab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013670948s
Jan 18 15:56:03.263: INFO: Pod "alpine-nnp-false-344a3ec6-b5a6-45b9-97ff-8e8ccdadccab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01425097s
Jan 18 15:56:05.269: INFO: Pod "alpine-nnp-false-344a3ec6-b5a6-45b9-97ff-8e8ccdadccab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020721401s
Jan 18 15:56:05.269: INFO: Pod "alpine-nnp-false-344a3ec6-b5a6-45b9-97ff-8e8ccdadccab" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 18 15:56:05.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-761" for this suite. 01/18/23 15:56:05.326
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":191,"skipped":3287,"failed":0}
------------------------------
• [SLOW TEST] [6.160 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:55:59.194
    Jan 18 15:55:59.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename security-context-test 01/18/23 15:55:59.198
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:55:59.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:55:59.232
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Jan 18 15:55:59.248: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-344a3ec6-b5a6-45b9-97ff-8e8ccdadccab" in namespace "security-context-test-761" to be "Succeeded or Failed"
    Jan 18 15:55:59.256: INFO: Pod "alpine-nnp-false-344a3ec6-b5a6-45b9-97ff-8e8ccdadccab": Phase="Pending", Reason="", readiness=false. Elapsed: 7.444814ms
    Jan 18 15:56:01.262: INFO: Pod "alpine-nnp-false-344a3ec6-b5a6-45b9-97ff-8e8ccdadccab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013670948s
    Jan 18 15:56:03.263: INFO: Pod "alpine-nnp-false-344a3ec6-b5a6-45b9-97ff-8e8ccdadccab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01425097s
    Jan 18 15:56:05.269: INFO: Pod "alpine-nnp-false-344a3ec6-b5a6-45b9-97ff-8e8ccdadccab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020721401s
    Jan 18 15:56:05.269: INFO: Pod "alpine-nnp-false-344a3ec6-b5a6-45b9-97ff-8e8ccdadccab" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 18 15:56:05.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-761" for this suite. 01/18/23 15:56:05.326
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:56:05.357
Jan 18 15:56:05.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename containers 01/18/23 15:56:05.361
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:56:05.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:56:05.4
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 01/18/23 15:56:05.408
Jan 18 15:56:05.426: INFO: Waiting up to 5m0s for pod "client-containers-53c737a6-ad17-445a-aae9-0570b7c5f1e6" in namespace "containers-4843" to be "Succeeded or Failed"
Jan 18 15:56:05.429: INFO: Pod "client-containers-53c737a6-ad17-445a-aae9-0570b7c5f1e6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.255459ms
Jan 18 15:56:07.436: INFO: Pod "client-containers-53c737a6-ad17-445a-aae9-0570b7c5f1e6": Phase="Running", Reason="", readiness=true. Elapsed: 2.009931519s
Jan 18 15:56:09.435: INFO: Pod "client-containers-53c737a6-ad17-445a-aae9-0570b7c5f1e6": Phase="Running", Reason="", readiness=false. Elapsed: 4.008494402s
Jan 18 15:56:11.435: INFO: Pod "client-containers-53c737a6-ad17-445a-aae9-0570b7c5f1e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008612611s
STEP: Saw pod success 01/18/23 15:56:11.435
Jan 18 15:56:11.435: INFO: Pod "client-containers-53c737a6-ad17-445a-aae9-0570b7c5f1e6" satisfied condition "Succeeded or Failed"
Jan 18 15:56:11.440: INFO: Trying to get logs from node v1-25-1-18760-w2 pod client-containers-53c737a6-ad17-445a-aae9-0570b7c5f1e6 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 15:56:11.449
Jan 18 15:56:11.460: INFO: Waiting for pod client-containers-53c737a6-ad17-445a-aae9-0570b7c5f1e6 to disappear
Jan 18 15:56:11.465: INFO: Pod client-containers-53c737a6-ad17-445a-aae9-0570b7c5f1e6 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 18 15:56:11.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4843" for this suite. 01/18/23 15:56:11.472
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":192,"skipped":3287,"failed":0}
------------------------------
• [SLOW TEST] [6.125 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:56:05.357
    Jan 18 15:56:05.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename containers 01/18/23 15:56:05.361
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:56:05.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:56:05.4
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 01/18/23 15:56:05.408
    Jan 18 15:56:05.426: INFO: Waiting up to 5m0s for pod "client-containers-53c737a6-ad17-445a-aae9-0570b7c5f1e6" in namespace "containers-4843" to be "Succeeded or Failed"
    Jan 18 15:56:05.429: INFO: Pod "client-containers-53c737a6-ad17-445a-aae9-0570b7c5f1e6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.255459ms
    Jan 18 15:56:07.436: INFO: Pod "client-containers-53c737a6-ad17-445a-aae9-0570b7c5f1e6": Phase="Running", Reason="", readiness=true. Elapsed: 2.009931519s
    Jan 18 15:56:09.435: INFO: Pod "client-containers-53c737a6-ad17-445a-aae9-0570b7c5f1e6": Phase="Running", Reason="", readiness=false. Elapsed: 4.008494402s
    Jan 18 15:56:11.435: INFO: Pod "client-containers-53c737a6-ad17-445a-aae9-0570b7c5f1e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008612611s
    STEP: Saw pod success 01/18/23 15:56:11.435
    Jan 18 15:56:11.435: INFO: Pod "client-containers-53c737a6-ad17-445a-aae9-0570b7c5f1e6" satisfied condition "Succeeded or Failed"
    Jan 18 15:56:11.440: INFO: Trying to get logs from node v1-25-1-18760-w2 pod client-containers-53c737a6-ad17-445a-aae9-0570b7c5f1e6 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 15:56:11.449
    Jan 18 15:56:11.460: INFO: Waiting for pod client-containers-53c737a6-ad17-445a-aae9-0570b7c5f1e6 to disappear
    Jan 18 15:56:11.465: INFO: Pod client-containers-53c737a6-ad17-445a-aae9-0570b7c5f1e6 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 18 15:56:11.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-4843" for this suite. 01/18/23 15:56:11.472
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:56:11.484
Jan 18 15:56:11.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename sched-pred 01/18/23 15:56:11.485
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:56:11.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:56:11.521
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 18 15:56:11.526: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 18 15:56:11.536: INFO: Waiting for terminating namespaces to be deleted...
Jan 18 15:56:11.541: INFO: 
Logging pods the apiserver thinks is on node v1-25-1-18760-w before test
Jan 18 15:56:11.558: INFO: helm-controller-578bd944df-dzr2c from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 15:56:11.558: INFO: 	Container manager ready: true, restart count 0
Jan 18 15:56:11.558: INFO: notification-controller-6459696d4f-gdmj8 from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 15:56:11.559: INFO: 	Container manager ready: true, restart count 0
Jan 18 15:56:11.559: INFO: source-controller-84f6bcbfb8-ljjjm from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 15:56:11.559: INFO: 	Container manager ready: true, restart count 0
Jan 18 15:56:11.559: INFO: calico-node-59tts from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
Jan 18 15:56:11.559: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 15:56:11.559: INFO: coredns-coredns-5f9b955d9-pjqxb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 15:56:11.559: INFO: 	Container coredns ready: true, restart count 0
Jan 18 15:56:11.560: INFO: coredns-coredns-autoscaler-7d98f7496d-7zbwb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 15:56:11.560: INFO: 	Container autoscaler ready: true, restart count 0
Jan 18 15:56:11.560: INFO: kube-proxy-rzflq from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
Jan 18 15:56:11.560: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 15:56:11.560: INFO: metrics-server-58498b9c56-q88cp from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 15:56:11.560: INFO: 	Container metrics-server ready: true, restart count 0
Jan 18 15:56:11.560: INFO: nginx-proxy-v1-25-1-18760-w from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
Jan 18 15:56:11.560: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 18 15:56:11.560: INFO: nodelocaldns-skdxm from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
Jan 18 15:56:11.560: INFO: 	Container node-cache ready: true, restart count 0
Jan 18 15:56:11.560: INFO: openstack-cinder-csi-controllerplugin-55d57dd799-6496v from kube-system started at 2023-01-18 14:39:48 +0000 UTC (6 container statuses recorded)
Jan 18 15:56:11.560: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 18 15:56:11.560: INFO: 	Container csi-attacher ready: true, restart count 0
Jan 18 15:56:11.561: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan 18 15:56:11.561: INFO: 	Container csi-resizer ready: true, restart count 0
Jan 18 15:56:11.561: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan 18 15:56:11.561: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 18 15:56:11.561: INFO: openstack-cinder-csi-nodeplugin-w7sxz from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
Jan 18 15:56:11.561: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 18 15:56:11.561: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 18 15:56:11.561: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 18 15:56:11.561: INFO: sonobuoy-systemd-logs-daemon-set-e45d8d00b64540c2-j5v5z from sonobuoy started at 2023-01-18 15:05:41 +0000 UTC (2 container statuses recorded)
Jan 18 15:56:11.561: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 15:56:11.561: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 15:56:11.561: INFO: 
Logging pods the apiserver thinks is on node v1-25-1-18760-w2 before test
Jan 18 15:56:11.573: INFO: calico-node-qx4f9 from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
Jan 18 15:56:11.573: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 15:56:11.573: INFO: coredns-coredns-5f9b955d9-qggng from kube-system started at 2023-01-18 14:40:34 +0000 UTC (1 container statuses recorded)
Jan 18 15:56:11.574: INFO: 	Container coredns ready: true, restart count 0
Jan 18 15:56:11.574: INFO: kube-proxy-8r2pt from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
Jan 18 15:56:11.574: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 15:56:11.574: INFO: nginx-proxy-v1-25-1-18760-w2 from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
Jan 18 15:56:11.574: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 18 15:56:11.574: INFO: nodelocaldns-dzwbj from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
Jan 18 15:56:11.575: INFO: 	Container node-cache ready: true, restart count 0
Jan 18 15:56:11.575: INFO: openstack-cinder-csi-nodeplugin-qm575 from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
Jan 18 15:56:11.575: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 18 15:56:11.575: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 18 15:56:11.575: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 18 15:56:11.575: INFO: sonobuoy from sonobuoy started at 2023-01-18 15:05:32 +0000 UTC (1 container statuses recorded)
Jan 18 15:56:11.576: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 18 15:56:11.576: INFO: sonobuoy-e2e-job-97bb2aa868f3486c from sonobuoy started at 2023-01-18 15:05:41 +0000 UTC (2 container statuses recorded)
Jan 18 15:56:11.576: INFO: 	Container e2e ready: true, restart count 0
Jan 18 15:56:11.576: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 15:56:11.576: INFO: sonobuoy-systemd-logs-daemon-set-e45d8d00b64540c2-488l9 from sonobuoy started at 2023-01-18 15:05:41 +0000 UTC (2 container statuses recorded)
Jan 18 15:56:11.576: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 15:56:11.576: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 01/18/23 15:56:11.576
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.173b724f58e860f9], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 01/18/23 15:56:11.656
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 18 15:56:12.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2151" for this suite. 01/18/23 15:56:12.625
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":193,"skipped":3307,"failed":0}
------------------------------
• [1.147 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:56:11.484
    Jan 18 15:56:11.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename sched-pred 01/18/23 15:56:11.485
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:56:11.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:56:11.521
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 18 15:56:11.526: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 18 15:56:11.536: INFO: Waiting for terminating namespaces to be deleted...
    Jan 18 15:56:11.541: INFO: 
    Logging pods the apiserver thinks is on node v1-25-1-18760-w before test
    Jan 18 15:56:11.558: INFO: helm-controller-578bd944df-dzr2c from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 15:56:11.558: INFO: 	Container manager ready: true, restart count 0
    Jan 18 15:56:11.558: INFO: notification-controller-6459696d4f-gdmj8 from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 15:56:11.559: INFO: 	Container manager ready: true, restart count 0
    Jan 18 15:56:11.559: INFO: source-controller-84f6bcbfb8-ljjjm from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 15:56:11.559: INFO: 	Container manager ready: true, restart count 0
    Jan 18 15:56:11.559: INFO: calico-node-59tts from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
    Jan 18 15:56:11.559: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 15:56:11.559: INFO: coredns-coredns-5f9b955d9-pjqxb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 15:56:11.559: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 15:56:11.560: INFO: coredns-coredns-autoscaler-7d98f7496d-7zbwb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 15:56:11.560: INFO: 	Container autoscaler ready: true, restart count 0
    Jan 18 15:56:11.560: INFO: kube-proxy-rzflq from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
    Jan 18 15:56:11.560: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 15:56:11.560: INFO: metrics-server-58498b9c56-q88cp from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 15:56:11.560: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 18 15:56:11.560: INFO: nginx-proxy-v1-25-1-18760-w from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
    Jan 18 15:56:11.560: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 18 15:56:11.560: INFO: nodelocaldns-skdxm from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
    Jan 18 15:56:11.560: INFO: 	Container node-cache ready: true, restart count 0
    Jan 18 15:56:11.560: INFO: openstack-cinder-csi-controllerplugin-55d57dd799-6496v from kube-system started at 2023-01-18 14:39:48 +0000 UTC (6 container statuses recorded)
    Jan 18 15:56:11.560: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 18 15:56:11.560: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan 18 15:56:11.561: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan 18 15:56:11.561: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan 18 15:56:11.561: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan 18 15:56:11.561: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 18 15:56:11.561: INFO: openstack-cinder-csi-nodeplugin-w7sxz from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
    Jan 18 15:56:11.561: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 18 15:56:11.561: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 18 15:56:11.561: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 18 15:56:11.561: INFO: sonobuoy-systemd-logs-daemon-set-e45d8d00b64540c2-j5v5z from sonobuoy started at 2023-01-18 15:05:41 +0000 UTC (2 container statuses recorded)
    Jan 18 15:56:11.561: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 15:56:11.561: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 18 15:56:11.561: INFO: 
    Logging pods the apiserver thinks is on node v1-25-1-18760-w2 before test
    Jan 18 15:56:11.573: INFO: calico-node-qx4f9 from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
    Jan 18 15:56:11.573: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 15:56:11.573: INFO: coredns-coredns-5f9b955d9-qggng from kube-system started at 2023-01-18 14:40:34 +0000 UTC (1 container statuses recorded)
    Jan 18 15:56:11.574: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 15:56:11.574: INFO: kube-proxy-8r2pt from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
    Jan 18 15:56:11.574: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 15:56:11.574: INFO: nginx-proxy-v1-25-1-18760-w2 from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
    Jan 18 15:56:11.574: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 18 15:56:11.574: INFO: nodelocaldns-dzwbj from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
    Jan 18 15:56:11.575: INFO: 	Container node-cache ready: true, restart count 0
    Jan 18 15:56:11.575: INFO: openstack-cinder-csi-nodeplugin-qm575 from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
    Jan 18 15:56:11.575: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 18 15:56:11.575: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 18 15:56:11.575: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 18 15:56:11.575: INFO: sonobuoy from sonobuoy started at 2023-01-18 15:05:32 +0000 UTC (1 container statuses recorded)
    Jan 18 15:56:11.576: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 18 15:56:11.576: INFO: sonobuoy-e2e-job-97bb2aa868f3486c from sonobuoy started at 2023-01-18 15:05:41 +0000 UTC (2 container statuses recorded)
    Jan 18 15:56:11.576: INFO: 	Container e2e ready: true, restart count 0
    Jan 18 15:56:11.576: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 15:56:11.576: INFO: sonobuoy-systemd-logs-daemon-set-e45d8d00b64540c2-488l9 from sonobuoy started at 2023-01-18 15:05:41 +0000 UTC (2 container statuses recorded)
    Jan 18 15:56:11.576: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 15:56:11.576: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 01/18/23 15:56:11.576
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.173b724f58e860f9], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 01/18/23 15:56:11.656
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 15:56:12.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-2151" for this suite. 01/18/23 15:56:12.625
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:56:12.635
Jan 18 15:56:12.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename kubelet-test 01/18/23 15:56:12.638
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:56:12.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:56:12.674
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jan 18 15:56:12.693: INFO: Waiting up to 5m0s for pod "busybox-scheduling-4d198a91-3648-44db-ae03-9af7ab2d552f" in namespace "kubelet-test-4366" to be "running and ready"
Jan 18 15:56:12.701: INFO: Pod "busybox-scheduling-4d198a91-3648-44db-ae03-9af7ab2d552f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.446781ms
Jan 18 15:56:12.701: INFO: The phase of Pod busybox-scheduling-4d198a91-3648-44db-ae03-9af7ab2d552f is Pending, waiting for it to be Running (with Ready = true)
Jan 18 15:56:14.713: INFO: Pod "busybox-scheduling-4d198a91-3648-44db-ae03-9af7ab2d552f": Phase="Running", Reason="", readiness=true. Elapsed: 2.019030601s
Jan 18 15:56:14.713: INFO: The phase of Pod busybox-scheduling-4d198a91-3648-44db-ae03-9af7ab2d552f is Running (Ready = true)
Jan 18 15:56:14.714: INFO: Pod "busybox-scheduling-4d198a91-3648-44db-ae03-9af7ab2d552f" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 18 15:56:14.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4366" for this suite. 01/18/23 15:56:14.732
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":194,"skipped":3311,"failed":0}
------------------------------
• [2.104 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:56:12.635
    Jan 18 15:56:12.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename kubelet-test 01/18/23 15:56:12.638
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:56:12.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:56:12.674
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jan 18 15:56:12.693: INFO: Waiting up to 5m0s for pod "busybox-scheduling-4d198a91-3648-44db-ae03-9af7ab2d552f" in namespace "kubelet-test-4366" to be "running and ready"
    Jan 18 15:56:12.701: INFO: Pod "busybox-scheduling-4d198a91-3648-44db-ae03-9af7ab2d552f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.446781ms
    Jan 18 15:56:12.701: INFO: The phase of Pod busybox-scheduling-4d198a91-3648-44db-ae03-9af7ab2d552f is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 15:56:14.713: INFO: Pod "busybox-scheduling-4d198a91-3648-44db-ae03-9af7ab2d552f": Phase="Running", Reason="", readiness=true. Elapsed: 2.019030601s
    Jan 18 15:56:14.713: INFO: The phase of Pod busybox-scheduling-4d198a91-3648-44db-ae03-9af7ab2d552f is Running (Ready = true)
    Jan 18 15:56:14.714: INFO: Pod "busybox-scheduling-4d198a91-3648-44db-ae03-9af7ab2d552f" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 18 15:56:14.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-4366" for this suite. 01/18/23 15:56:14.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:56:14.753
Jan 18 15:56:14.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename security-context-test 01/18/23 15:56:14.754
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:56:14.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:56:14.782
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Jan 18 15:56:14.796: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-e0c6776b-b66d-4923-ae94-3415d2bb68bf" in namespace "security-context-test-1674" to be "Succeeded or Failed"
Jan 18 15:56:15.016: INFO: Pod "busybox-readonly-false-e0c6776b-b66d-4923-ae94-3415d2bb68bf": Phase="Pending", Reason="", readiness=false. Elapsed: 219.394356ms
Jan 18 15:56:17.023: INFO: Pod "busybox-readonly-false-e0c6776b-b66d-4923-ae94-3415d2bb68bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.226846351s
Jan 18 15:56:19.023: INFO: Pod "busybox-readonly-false-e0c6776b-b66d-4923-ae94-3415d2bb68bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.226470985s
Jan 18 15:56:19.023: INFO: Pod "busybox-readonly-false-e0c6776b-b66d-4923-ae94-3415d2bb68bf" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 18 15:56:19.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1674" for this suite. 01/18/23 15:56:19.031
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":195,"skipped":3378,"failed":0}
------------------------------
• [4.286 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:56:14.753
    Jan 18 15:56:14.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename security-context-test 01/18/23 15:56:14.754
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:56:14.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:56:14.782
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Jan 18 15:56:14.796: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-e0c6776b-b66d-4923-ae94-3415d2bb68bf" in namespace "security-context-test-1674" to be "Succeeded or Failed"
    Jan 18 15:56:15.016: INFO: Pod "busybox-readonly-false-e0c6776b-b66d-4923-ae94-3415d2bb68bf": Phase="Pending", Reason="", readiness=false. Elapsed: 219.394356ms
    Jan 18 15:56:17.023: INFO: Pod "busybox-readonly-false-e0c6776b-b66d-4923-ae94-3415d2bb68bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.226846351s
    Jan 18 15:56:19.023: INFO: Pod "busybox-readonly-false-e0c6776b-b66d-4923-ae94-3415d2bb68bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.226470985s
    Jan 18 15:56:19.023: INFO: Pod "busybox-readonly-false-e0c6776b-b66d-4923-ae94-3415d2bb68bf" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 18 15:56:19.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-1674" for this suite. 01/18/23 15:56:19.031
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:56:19.059
Jan 18 15:56:19.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename deployment 01/18/23 15:56:19.06
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:56:19.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:56:19.084
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jan 18 15:56:19.088: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 18 15:56:19.100: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 18 15:56:24.113: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 15:56:24.113
Jan 18 15:56:24.114: INFO: Creating deployment "test-rolling-update-deployment"
Jan 18 15:56:24.121: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 18 15:56:24.150: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 18 15:56:26.165: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 18 15:56:26.170: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 15:56:26.190: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2464  87302a3f-d415-4eee-b929-5891b9a645eb 50638 1 2023-01-18 15:56:24 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-18 15:56:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 15:56:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e05ba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 15:56:24 +0000 UTC,LastTransitionTime:2023-01-18 15:56:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-01-18 15:56:26 +0000 UTC,LastTransitionTime:2023-01-18 15:56:24 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 18 15:56:26.195: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-2464  c7dc1d14-9d8e-4e31-a758-e6882530c514 50628 1 2023-01-18 15:56:24 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 87302a3f-d415-4eee-b929-5891b9a645eb 0xc005496367 0xc005496368}] [] [{kube-controller-manager Update apps/v1 2023-01-18 15:56:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"87302a3f-d415-4eee-b929-5891b9a645eb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 15:56:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005496418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 15:56:26.195: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 18 15:56:26.196: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2464  65b59210-98d8-470d-9eef-64081c94a4c9 50637 2 2023-01-18 15:56:19 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 87302a3f-d415-4eee-b929-5891b9a645eb 0xc005496237 0xc005496238}] [] [{e2e.test Update apps/v1 2023-01-18 15:56:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 15:56:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"87302a3f-d415-4eee-b929-5891b9a645eb\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-18 15:56:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0054962f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 15:56:26.201: INFO: Pod "test-rolling-update-deployment-78f575d8ff-78dth" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-78dth test-rolling-update-deployment-78f575d8ff- deployment-2464  040baad7-0328-4d80-b0fd-9ac2fefbbb9c 50627 0 2023-01-18 15:56:24 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:91ac506f72a4b29e60125d7ef32a3c64f19fc0b2f2ec154462054fd552149870 cni.projectcalico.org/podIP:10.233.68.43/32 cni.projectcalico.org/podIPs:10.233.68.43/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff c7dc1d14-9d8e-4e31-a758-e6882530c514 0xc0054968b7 0xc0054968b8}] [] [{calico Update v1 2023-01-18 15:56:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-18 15:56:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c7dc1d14-9d8e-4e31-a758-e6882530c514\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 15:56:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zfgsv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zfgsv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:56:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:56:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:56:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:56:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.43,StartTime:2023-01-18 15:56:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 15:56:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://9748f94ee1cf08707bd10da116d3f2fb83c0a5e3dd976d854c19424ca7ffeb2d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.43,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 18 15:56:26.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2464" for this suite. 01/18/23 15:56:26.21
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":196,"skipped":3469,"failed":0}
------------------------------
• [SLOW TEST] [7.163 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:56:19.059
    Jan 18 15:56:19.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename deployment 01/18/23 15:56:19.06
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:56:19.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:56:19.084
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jan 18 15:56:19.088: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jan 18 15:56:19.100: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 18 15:56:24.113: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 15:56:24.113
    Jan 18 15:56:24.114: INFO: Creating deployment "test-rolling-update-deployment"
    Jan 18 15:56:24.121: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jan 18 15:56:24.150: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jan 18 15:56:26.165: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jan 18 15:56:26.170: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 15:56:26.190: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2464  87302a3f-d415-4eee-b929-5891b9a645eb 50638 1 2023-01-18 15:56:24 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-18 15:56:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 15:56:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e05ba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 15:56:24 +0000 UTC,LastTransitionTime:2023-01-18 15:56:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-01-18 15:56:26 +0000 UTC,LastTransitionTime:2023-01-18 15:56:24 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 18 15:56:26.195: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-2464  c7dc1d14-9d8e-4e31-a758-e6882530c514 50628 1 2023-01-18 15:56:24 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 87302a3f-d415-4eee-b929-5891b9a645eb 0xc005496367 0xc005496368}] [] [{kube-controller-manager Update apps/v1 2023-01-18 15:56:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"87302a3f-d415-4eee-b929-5891b9a645eb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 15:56:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005496418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 15:56:26.195: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jan 18 15:56:26.196: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2464  65b59210-98d8-470d-9eef-64081c94a4c9 50637 2 2023-01-18 15:56:19 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 87302a3f-d415-4eee-b929-5891b9a645eb 0xc005496237 0xc005496238}] [] [{e2e.test Update apps/v1 2023-01-18 15:56:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 15:56:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"87302a3f-d415-4eee-b929-5891b9a645eb\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-18 15:56:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0054962f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 15:56:26.201: INFO: Pod "test-rolling-update-deployment-78f575d8ff-78dth" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-78dth test-rolling-update-deployment-78f575d8ff- deployment-2464  040baad7-0328-4d80-b0fd-9ac2fefbbb9c 50627 0 2023-01-18 15:56:24 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:91ac506f72a4b29e60125d7ef32a3c64f19fc0b2f2ec154462054fd552149870 cni.projectcalico.org/podIP:10.233.68.43/32 cni.projectcalico.org/podIPs:10.233.68.43/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff c7dc1d14-9d8e-4e31-a758-e6882530c514 0xc0054968b7 0xc0054968b8}] [] [{calico Update v1 2023-01-18 15:56:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-18 15:56:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c7dc1d14-9d8e-4e31-a758-e6882530c514\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 15:56:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zfgsv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zfgsv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:56:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:56:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:56:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 15:56:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.43,StartTime:2023-01-18 15:56:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 15:56:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://9748f94ee1cf08707bd10da116d3f2fb83c0a5e3dd976d854c19424ca7ffeb2d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.43,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 18 15:56:26.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2464" for this suite. 01/18/23 15:56:26.21
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 15:56:26.23
Jan 18 15:56:26.231: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename cronjob 01/18/23 15:56:26.232
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:56:26.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:56:26.261
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 01/18/23 15:56:26.267
STEP: Ensuring no jobs are scheduled 01/18/23 15:56:26.273
STEP: Ensuring no job exists by listing jobs explicitly 01/18/23 16:01:26.284
STEP: Removing cronjob 01/18/23 16:01:26.29
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 18 16:01:26.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2550" for this suite. 01/18/23 16:01:26.311
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":197,"skipped":3532,"failed":0}
------------------------------
• [SLOW TEST] [300.092 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 15:56:26.23
    Jan 18 15:56:26.231: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename cronjob 01/18/23 15:56:26.232
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 15:56:26.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 15:56:26.261
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 01/18/23 15:56:26.267
    STEP: Ensuring no jobs are scheduled 01/18/23 15:56:26.273
    STEP: Ensuring no job exists by listing jobs explicitly 01/18/23 16:01:26.284
    STEP: Removing cronjob 01/18/23 16:01:26.29
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 18 16:01:26.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-2550" for this suite. 01/18/23 16:01:26.311
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:01:26.325
Jan 18 16:01:26.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename secrets 01/18/23 16:01:26.329
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:01:26.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:01:26.358
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-f219b81d-e8e8-49ab-a63c-31123977bcc4 01/18/23 16:01:26.367
STEP: Creating a pod to test consume secrets 01/18/23 16:01:26.391
Jan 18 16:01:26.413: INFO: Waiting up to 5m0s for pod "pod-secrets-e8153ac8-babc-47a1-9ad6-30d38fda7510" in namespace "secrets-9798" to be "Succeeded or Failed"
Jan 18 16:01:26.419: INFO: Pod "pod-secrets-e8153ac8-babc-47a1-9ad6-30d38fda7510": Phase="Pending", Reason="", readiness=false. Elapsed: 5.896016ms
Jan 18 16:01:28.424: INFO: Pod "pod-secrets-e8153ac8-babc-47a1-9ad6-30d38fda7510": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011758682s
Jan 18 16:01:30.424: INFO: Pod "pod-secrets-e8153ac8-babc-47a1-9ad6-30d38fda7510": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011265587s
STEP: Saw pod success 01/18/23 16:01:30.424
Jan 18 16:01:30.425: INFO: Pod "pod-secrets-e8153ac8-babc-47a1-9ad6-30d38fda7510" satisfied condition "Succeeded or Failed"
Jan 18 16:01:30.429: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-e8153ac8-babc-47a1-9ad6-30d38fda7510 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 16:01:30.455
Jan 18 16:01:30.472: INFO: Waiting for pod pod-secrets-e8153ac8-babc-47a1-9ad6-30d38fda7510 to disappear
Jan 18 16:01:30.488: INFO: Pod pod-secrets-e8153ac8-babc-47a1-9ad6-30d38fda7510 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 18 16:01:30.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9798" for this suite. 01/18/23 16:01:30.494
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":198,"skipped":3545,"failed":0}
------------------------------
• [4.179 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:01:26.325
    Jan 18 16:01:26.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename secrets 01/18/23 16:01:26.329
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:01:26.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:01:26.358
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-f219b81d-e8e8-49ab-a63c-31123977bcc4 01/18/23 16:01:26.367
    STEP: Creating a pod to test consume secrets 01/18/23 16:01:26.391
    Jan 18 16:01:26.413: INFO: Waiting up to 5m0s for pod "pod-secrets-e8153ac8-babc-47a1-9ad6-30d38fda7510" in namespace "secrets-9798" to be "Succeeded or Failed"
    Jan 18 16:01:26.419: INFO: Pod "pod-secrets-e8153ac8-babc-47a1-9ad6-30d38fda7510": Phase="Pending", Reason="", readiness=false. Elapsed: 5.896016ms
    Jan 18 16:01:28.424: INFO: Pod "pod-secrets-e8153ac8-babc-47a1-9ad6-30d38fda7510": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011758682s
    Jan 18 16:01:30.424: INFO: Pod "pod-secrets-e8153ac8-babc-47a1-9ad6-30d38fda7510": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011265587s
    STEP: Saw pod success 01/18/23 16:01:30.424
    Jan 18 16:01:30.425: INFO: Pod "pod-secrets-e8153ac8-babc-47a1-9ad6-30d38fda7510" satisfied condition "Succeeded or Failed"
    Jan 18 16:01:30.429: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-e8153ac8-babc-47a1-9ad6-30d38fda7510 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 16:01:30.455
    Jan 18 16:01:30.472: INFO: Waiting for pod pod-secrets-e8153ac8-babc-47a1-9ad6-30d38fda7510 to disappear
    Jan 18 16:01:30.488: INFO: Pod pod-secrets-e8153ac8-babc-47a1-9ad6-30d38fda7510 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 16:01:30.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9798" for this suite. 01/18/23 16:01:30.494
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:01:30.505
Jan 18 16:01:30.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename svcaccounts 01/18/23 16:01:30.508
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:01:30.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:01:30.532
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  01/18/23 16:01:30.536
Jan 18 16:01:30.546: INFO: Waiting up to 5m0s for pod "test-pod-3d969ebc-0ac1-49f7-bc09-0cb2826a6a56" in namespace "svcaccounts-5520" to be "Succeeded or Failed"
Jan 18 16:01:30.552: INFO: Pod "test-pod-3d969ebc-0ac1-49f7-bc09-0cb2826a6a56": Phase="Pending", Reason="", readiness=false. Elapsed: 5.747061ms
Jan 18 16:01:32.559: INFO: Pod "test-pod-3d969ebc-0ac1-49f7-bc09-0cb2826a6a56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012474244s
Jan 18 16:01:34.557: INFO: Pod "test-pod-3d969ebc-0ac1-49f7-bc09-0cb2826a6a56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010568812s
STEP: Saw pod success 01/18/23 16:01:34.557
Jan 18 16:01:34.557: INFO: Pod "test-pod-3d969ebc-0ac1-49f7-bc09-0cb2826a6a56" satisfied condition "Succeeded or Failed"
Jan 18 16:01:34.561: INFO: Trying to get logs from node v1-25-1-18760-w2 pod test-pod-3d969ebc-0ac1-49f7-bc09-0cb2826a6a56 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 16:01:34.572
Jan 18 16:01:34.593: INFO: Waiting for pod test-pod-3d969ebc-0ac1-49f7-bc09-0cb2826a6a56 to disappear
Jan 18 16:01:34.598: INFO: Pod test-pod-3d969ebc-0ac1-49f7-bc09-0cb2826a6a56 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 18 16:01:34.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5520" for this suite. 01/18/23 16:01:34.605
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":199,"skipped":3549,"failed":0}
------------------------------
• [4.108 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:01:30.505
    Jan 18 16:01:30.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 16:01:30.508
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:01:30.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:01:30.532
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  01/18/23 16:01:30.536
    Jan 18 16:01:30.546: INFO: Waiting up to 5m0s for pod "test-pod-3d969ebc-0ac1-49f7-bc09-0cb2826a6a56" in namespace "svcaccounts-5520" to be "Succeeded or Failed"
    Jan 18 16:01:30.552: INFO: Pod "test-pod-3d969ebc-0ac1-49f7-bc09-0cb2826a6a56": Phase="Pending", Reason="", readiness=false. Elapsed: 5.747061ms
    Jan 18 16:01:32.559: INFO: Pod "test-pod-3d969ebc-0ac1-49f7-bc09-0cb2826a6a56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012474244s
    Jan 18 16:01:34.557: INFO: Pod "test-pod-3d969ebc-0ac1-49f7-bc09-0cb2826a6a56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010568812s
    STEP: Saw pod success 01/18/23 16:01:34.557
    Jan 18 16:01:34.557: INFO: Pod "test-pod-3d969ebc-0ac1-49f7-bc09-0cb2826a6a56" satisfied condition "Succeeded or Failed"
    Jan 18 16:01:34.561: INFO: Trying to get logs from node v1-25-1-18760-w2 pod test-pod-3d969ebc-0ac1-49f7-bc09-0cb2826a6a56 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 16:01:34.572
    Jan 18 16:01:34.593: INFO: Waiting for pod test-pod-3d969ebc-0ac1-49f7-bc09-0cb2826a6a56 to disappear
    Jan 18 16:01:34.598: INFO: Pod test-pod-3d969ebc-0ac1-49f7-bc09-0cb2826a6a56 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 18 16:01:34.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5520" for this suite. 01/18/23 16:01:34.605
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:01:34.617
Jan 18 16:01:34.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename watch 01/18/23 16:01:34.619
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:01:34.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:01:34.683
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 01/18/23 16:01:34.689
STEP: modifying the configmap once 01/18/23 16:01:34.694
STEP: modifying the configmap a second time 01/18/23 16:01:34.704
STEP: deleting the configmap 01/18/23 16:01:34.714
STEP: creating a watch on configmaps from the resource version returned by the first update 01/18/23 16:01:34.719
STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/18/23 16:01:34.722
Jan 18 16:01:34.723: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3272  e4e282a6-caba-48c7-90f5-dc5db76ffa67 51846 0 2023-01-18 16:01:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-18 16:01:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 16:01:34.724: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3272  e4e282a6-caba-48c7-90f5-dc5db76ffa67 51847 0 2023-01-18 16:01:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-18 16:01:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 18 16:01:34.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3272" for this suite. 01/18/23 16:01:34.731
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":200,"skipped":3577,"failed":0}
------------------------------
• [0.122 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:01:34.617
    Jan 18 16:01:34.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename watch 01/18/23 16:01:34.619
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:01:34.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:01:34.683
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 01/18/23 16:01:34.689
    STEP: modifying the configmap once 01/18/23 16:01:34.694
    STEP: modifying the configmap a second time 01/18/23 16:01:34.704
    STEP: deleting the configmap 01/18/23 16:01:34.714
    STEP: creating a watch on configmaps from the resource version returned by the first update 01/18/23 16:01:34.719
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/18/23 16:01:34.722
    Jan 18 16:01:34.723: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3272  e4e282a6-caba-48c7-90f5-dc5db76ffa67 51846 0 2023-01-18 16:01:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-18 16:01:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 16:01:34.724: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3272  e4e282a6-caba-48c7-90f5-dc5db76ffa67 51847 0 2023-01-18 16:01:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-18 16:01:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 18 16:01:34.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-3272" for this suite. 01/18/23 16:01:34.731
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:01:34.742
Jan 18 16:01:34.742: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename secrets 01/18/23 16:01:34.744
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:01:34.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:01:34.767
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-2282970c-511c-4abf-9a77-76b6ce2b4d2e 01/18/23 16:01:34.777
STEP: Creating a pod to test consume secrets 01/18/23 16:01:34.784
Jan 18 16:01:34.799: INFO: Waiting up to 5m0s for pod "pod-secrets-0949b7bb-eddc-4b37-8c36-cfc04f7c5ed9" in namespace "secrets-6307" to be "Succeeded or Failed"
Jan 18 16:01:34.811: INFO: Pod "pod-secrets-0949b7bb-eddc-4b37-8c36-cfc04f7c5ed9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.312523ms
Jan 18 16:01:36.819: INFO: Pod "pod-secrets-0949b7bb-eddc-4b37-8c36-cfc04f7c5ed9": Phase="Running", Reason="", readiness=true. Elapsed: 2.020185414s
Jan 18 16:01:38.817: INFO: Pod "pod-secrets-0949b7bb-eddc-4b37-8c36-cfc04f7c5ed9": Phase="Running", Reason="", readiness=false. Elapsed: 4.017796909s
Jan 18 16:01:40.816: INFO: Pod "pod-secrets-0949b7bb-eddc-4b37-8c36-cfc04f7c5ed9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017451089s
STEP: Saw pod success 01/18/23 16:01:40.817
Jan 18 16:01:40.817: INFO: Pod "pod-secrets-0949b7bb-eddc-4b37-8c36-cfc04f7c5ed9" satisfied condition "Succeeded or Failed"
Jan 18 16:01:40.821: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-0949b7bb-eddc-4b37-8c36-cfc04f7c5ed9 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 16:01:40.829
Jan 18 16:01:40.869: INFO: Waiting for pod pod-secrets-0949b7bb-eddc-4b37-8c36-cfc04f7c5ed9 to disappear
Jan 18 16:01:40.872: INFO: Pod pod-secrets-0949b7bb-eddc-4b37-8c36-cfc04f7c5ed9 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 18 16:01:40.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6307" for this suite. 01/18/23 16:01:40.878
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":201,"skipped":3579,"failed":0}
------------------------------
• [SLOW TEST] [6.145 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:01:34.742
    Jan 18 16:01:34.742: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename secrets 01/18/23 16:01:34.744
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:01:34.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:01:34.767
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-2282970c-511c-4abf-9a77-76b6ce2b4d2e 01/18/23 16:01:34.777
    STEP: Creating a pod to test consume secrets 01/18/23 16:01:34.784
    Jan 18 16:01:34.799: INFO: Waiting up to 5m0s for pod "pod-secrets-0949b7bb-eddc-4b37-8c36-cfc04f7c5ed9" in namespace "secrets-6307" to be "Succeeded or Failed"
    Jan 18 16:01:34.811: INFO: Pod "pod-secrets-0949b7bb-eddc-4b37-8c36-cfc04f7c5ed9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.312523ms
    Jan 18 16:01:36.819: INFO: Pod "pod-secrets-0949b7bb-eddc-4b37-8c36-cfc04f7c5ed9": Phase="Running", Reason="", readiness=true. Elapsed: 2.020185414s
    Jan 18 16:01:38.817: INFO: Pod "pod-secrets-0949b7bb-eddc-4b37-8c36-cfc04f7c5ed9": Phase="Running", Reason="", readiness=false. Elapsed: 4.017796909s
    Jan 18 16:01:40.816: INFO: Pod "pod-secrets-0949b7bb-eddc-4b37-8c36-cfc04f7c5ed9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017451089s
    STEP: Saw pod success 01/18/23 16:01:40.817
    Jan 18 16:01:40.817: INFO: Pod "pod-secrets-0949b7bb-eddc-4b37-8c36-cfc04f7c5ed9" satisfied condition "Succeeded or Failed"
    Jan 18 16:01:40.821: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-0949b7bb-eddc-4b37-8c36-cfc04f7c5ed9 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 16:01:40.829
    Jan 18 16:01:40.869: INFO: Waiting for pod pod-secrets-0949b7bb-eddc-4b37-8c36-cfc04f7c5ed9 to disappear
    Jan 18 16:01:40.872: INFO: Pod pod-secrets-0949b7bb-eddc-4b37-8c36-cfc04f7c5ed9 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 16:01:40.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6307" for this suite. 01/18/23 16:01:40.878
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:01:40.892
Jan 18 16:01:40.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename var-expansion 01/18/23 16:01:40.894
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:01:40.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:01:40.931
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Jan 18 16:01:40.942: INFO: Waiting up to 2m0s for pod "var-expansion-c7d40d53-3f6c-4221-915d-3fd6e54654ce" in namespace "var-expansion-7965" to be "container 0 failed with reason CreateContainerConfigError"
Jan 18 16:01:40.960: INFO: Pod "var-expansion-c7d40d53-3f6c-4221-915d-3fd6e54654ce": Phase="Pending", Reason="", readiness=false. Elapsed: 17.194226ms
Jan 18 16:01:42.967: INFO: Pod "var-expansion-c7d40d53-3f6c-4221-915d-3fd6e54654ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024450087s
Jan 18 16:01:42.968: INFO: Pod "var-expansion-c7d40d53-3f6c-4221-915d-3fd6e54654ce" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 18 16:01:42.968: INFO: Deleting pod "var-expansion-c7d40d53-3f6c-4221-915d-3fd6e54654ce" in namespace "var-expansion-7965"
Jan 18 16:01:42.977: INFO: Wait up to 5m0s for pod "var-expansion-c7d40d53-3f6c-4221-915d-3fd6e54654ce" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 18 16:01:46.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7965" for this suite. 01/18/23 16:01:46.998
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":202,"skipped":3588,"failed":0}
------------------------------
• [SLOW TEST] [6.113 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:01:40.892
    Jan 18 16:01:40.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename var-expansion 01/18/23 16:01:40.894
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:01:40.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:01:40.931
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Jan 18 16:01:40.942: INFO: Waiting up to 2m0s for pod "var-expansion-c7d40d53-3f6c-4221-915d-3fd6e54654ce" in namespace "var-expansion-7965" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 18 16:01:40.960: INFO: Pod "var-expansion-c7d40d53-3f6c-4221-915d-3fd6e54654ce": Phase="Pending", Reason="", readiness=false. Elapsed: 17.194226ms
    Jan 18 16:01:42.967: INFO: Pod "var-expansion-c7d40d53-3f6c-4221-915d-3fd6e54654ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024450087s
    Jan 18 16:01:42.968: INFO: Pod "var-expansion-c7d40d53-3f6c-4221-915d-3fd6e54654ce" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 18 16:01:42.968: INFO: Deleting pod "var-expansion-c7d40d53-3f6c-4221-915d-3fd6e54654ce" in namespace "var-expansion-7965"
    Jan 18 16:01:42.977: INFO: Wait up to 5m0s for pod "var-expansion-c7d40d53-3f6c-4221-915d-3fd6e54654ce" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 18 16:01:46.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7965" for this suite. 01/18/23 16:01:46.998
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:01:47.012
Jan 18 16:01:47.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename dns 01/18/23 16:01:47.014
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:01:47.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:01:47.038
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 01/18/23 16:01:47.042
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4330.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4330.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local;sleep 1; done
 01/18/23 16:01:47.048
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4330.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4330.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local;sleep 1; done
 01/18/23 16:01:47.048
STEP: creating a pod to probe DNS 01/18/23 16:01:47.048
STEP: submitting the pod to kubernetes 01/18/23 16:01:47.048
Jan 18 16:01:47.066: INFO: Waiting up to 15m0s for pod "dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b" in namespace "dns-4330" to be "running"
Jan 18 16:01:47.070: INFO: Pod "dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.112418ms
Jan 18 16:01:49.082: INFO: Pod "dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b": Phase="Running", Reason="", readiness=true. Elapsed: 2.015577039s
Jan 18 16:01:49.082: INFO: Pod "dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b" satisfied condition "running"
STEP: retrieving the pod 01/18/23 16:01:49.082
STEP: looking for the results for each expected name from probers 01/18/23 16:01:49.096
Jan 18 16:01:49.105: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:01:49.112: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:01:49.125: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:01:49.131: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:01:49.135: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:01:49.140: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:01:49.147: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:01:49.152: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:01:49.152: INFO: Lookups using dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local]

Jan 18 16:01:54.160: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:01:54.164: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:01:54.167: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:01:54.172: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:01:54.177: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:01:54.181: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:01:54.189: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:01:54.196: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:01:54.196: INFO: Lookups using dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local]

Jan 18 16:01:59.168: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:01:59.174: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:01:59.179: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:01:59.184: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:01:59.192: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:01:59.198: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:01:59.205: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:01:59.214: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:01:59.214: INFO: Lookups using dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local]

Jan 18 16:02:04.161: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:04.167: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:04.172: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:04.178: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:04.188: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:04.199: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:04.205: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:04.209: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:04.209: INFO: Lookups using dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local]

Jan 18 16:02:09.165: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:09.170: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:09.174: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:09.178: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:09.183: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:09.187: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:09.197: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:09.204: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:09.204: INFO: Lookups using dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local]

Jan 18 16:02:14.158: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:14.166: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:14.172: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:14.176: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:14.181: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:14.187: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:14.199: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:14.204: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:14.205: INFO: Lookups using dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local]

Jan 18 16:02:19.174: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:19.180: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:19.186: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:19.191: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:19.208: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:19.219: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:19.226: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:19.231: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
Jan 18 16:02:19.231: INFO: Lookups using dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local]

Jan 18 16:02:24.202: INFO: DNS probes using dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b succeeded

STEP: deleting the pod 01/18/23 16:02:24.202
STEP: deleting the test headless service 01/18/23 16:02:24.244
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 18 16:02:24.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4330" for this suite. 01/18/23 16:02:24.279
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":203,"skipped":3624,"failed":0}
------------------------------
• [SLOW TEST] [37.289 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:01:47.012
    Jan 18 16:01:47.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename dns 01/18/23 16:01:47.014
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:01:47.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:01:47.038
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 01/18/23 16:01:47.042
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4330.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4330.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local;sleep 1; done
     01/18/23 16:01:47.048
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4330.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4330.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local;sleep 1; done
     01/18/23 16:01:47.048
    STEP: creating a pod to probe DNS 01/18/23 16:01:47.048
    STEP: submitting the pod to kubernetes 01/18/23 16:01:47.048
    Jan 18 16:01:47.066: INFO: Waiting up to 15m0s for pod "dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b" in namespace "dns-4330" to be "running"
    Jan 18 16:01:47.070: INFO: Pod "dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.112418ms
    Jan 18 16:01:49.082: INFO: Pod "dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b": Phase="Running", Reason="", readiness=true. Elapsed: 2.015577039s
    Jan 18 16:01:49.082: INFO: Pod "dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 16:01:49.082
    STEP: looking for the results for each expected name from probers 01/18/23 16:01:49.096
    Jan 18 16:01:49.105: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:01:49.112: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:01:49.125: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:01:49.131: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:01:49.135: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:01:49.140: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:01:49.147: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:01:49.152: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:01:49.152: INFO: Lookups using dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local]

    Jan 18 16:01:54.160: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:01:54.164: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:01:54.167: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:01:54.172: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:01:54.177: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:01:54.181: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:01:54.189: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:01:54.196: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:01:54.196: INFO: Lookups using dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local]

    Jan 18 16:01:59.168: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:01:59.174: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:01:59.179: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:01:59.184: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:01:59.192: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:01:59.198: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:01:59.205: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:01:59.214: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:01:59.214: INFO: Lookups using dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local]

    Jan 18 16:02:04.161: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:04.167: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:04.172: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:04.178: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:04.188: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:04.199: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:04.205: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:04.209: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:04.209: INFO: Lookups using dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local]

    Jan 18 16:02:09.165: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:09.170: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:09.174: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:09.178: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:09.183: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:09.187: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:09.197: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:09.204: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:09.204: INFO: Lookups using dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local]

    Jan 18 16:02:14.158: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:14.166: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:14.172: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:14.176: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:14.181: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:14.187: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:14.199: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:14.204: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:14.205: INFO: Lookups using dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local]

    Jan 18 16:02:19.174: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:19.180: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:19.186: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:19.191: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:19.208: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:19.219: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:19.226: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:19.231: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local from pod dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b: the server could not find the requested resource (get pods dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b)
    Jan 18 16:02:19.231: INFO: Lookups using dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4330.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4330.svc.cluster.local jessie_udp@dns-test-service-2.dns-4330.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4330.svc.cluster.local]

    Jan 18 16:02:24.202: INFO: DNS probes using dns-4330/dns-test-bdafa77f-15b4-45d2-8ed5-fea73ee6d36b succeeded

    STEP: deleting the pod 01/18/23 16:02:24.202
    STEP: deleting the test headless service 01/18/23 16:02:24.244
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 18 16:02:24.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4330" for this suite. 01/18/23 16:02:24.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:02:24.316
Jan 18 16:02:24.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename statefulset 01/18/23 16:02:24.325
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:02:24.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:02:24.353
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9283 01/18/23 16:02:24.358
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-9283 01/18/23 16:02:24.362
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9283 01/18/23 16:02:24.371
Jan 18 16:02:24.381: INFO: Found 0 stateful pods, waiting for 1
Jan 18 16:02:34.389: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/18/23 16:02:34.389
Jan 18 16:02:34.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-9283 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 16:02:34.605: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 16:02:34.605: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 16:02:34.605: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 16:02:34.610: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 18 16:02:44.617: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 16:02:44.617: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 16:02:44.651: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Jan 18 16:02:44.652: INFO: ss-0  v1-25-1-18760-w2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:24 +0000 UTC  }]
Jan 18 16:02:44.653: INFO: ss-1                    Pending         []
Jan 18 16:02:44.653: INFO: 
Jan 18 16:02:44.653: INFO: StatefulSet ss has not reached scale 3, at 2
Jan 18 16:02:45.662: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.979209361s
Jan 18 16:02:46.669: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.971236525s
Jan 18 16:02:47.677: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.963481896s
Jan 18 16:02:48.683: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.955660373s
Jan 18 16:02:49.691: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.950398214s
Jan 18 16:02:50.697: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.942113104s
Jan 18 16:02:51.705: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.936410602s
Jan 18 16:02:52.713: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.927895085s
Jan 18 16:02:53.719: INFO: Verifying statefulset ss doesn't scale past 3 for another 919.43709ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9283 01/18/23 16:02:54.719
Jan 18 16:02:54.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-9283 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 16:02:54.990: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 16:02:54.990: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 16:02:54.990: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 16:02:54.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-9283 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 16:02:55.184: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 18 16:02:55.184: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 16:02:55.184: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 16:02:55.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-9283 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 16:02:55.412: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 18 16:02:55.412: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 16:02:55.412: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 16:02:55.423: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jan 18 16:03:05.430: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 16:03:05.430: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 16:03:05.430: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 01/18/23 16:03:05.431
Jan 18 16:03:05.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-9283 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 16:03:05.625: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 16:03:05.625: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 16:03:05.625: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 16:03:05.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-9283 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 16:03:05.866: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 16:03:05.866: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 16:03:05.866: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 16:03:05.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-9283 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 16:03:06.112: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 16:03:06.112: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 16:03:06.112: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 16:03:06.112: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 16:03:06.117: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jan 18 16:03:16.223: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 16:03:16.223: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 16:03:16.223: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 16:03:16.242: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Jan 18 16:03:16.242: INFO: ss-0  v1-25-1-18760-w2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:03:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:03:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:24 +0000 UTC  }]
Jan 18 16:03:16.243: INFO: ss-1  v1-25-1-18760-w2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:03:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:03:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:44 +0000 UTC  }]
Jan 18 16:03:16.243: INFO: ss-2  v1-25-1-18760-w   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:03:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:03:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:44 +0000 UTC  }]
Jan 18 16:03:16.243: INFO: 
Jan 18 16:03:16.243: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 18 16:03:17.248: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Jan 18 16:03:17.248: INFO: ss-0  v1-25-1-18760-w2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:03:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:03:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:24 +0000 UTC  }]
Jan 18 16:03:17.248: INFO: ss-1  v1-25-1-18760-w2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:03:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:03:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:44 +0000 UTC  }]
Jan 18 16:03:17.248: INFO: 
Jan 18 16:03:17.248: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 18 16:03:18.253: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.987630752s
Jan 18 16:03:19.258: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.982916714s
Jan 18 16:03:20.263: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.977767795s
Jan 18 16:03:21.270: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.972658063s
Jan 18 16:03:22.275: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.965495047s
Jan 18 16:03:23.280: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.960005546s
Jan 18 16:03:24.286: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.955213059s
Jan 18 16:03:25.291: INFO: Verifying statefulset ss doesn't scale past 0 for another 949.99354ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9283 01/18/23 16:03:26.291
Jan 18 16:03:26.319: INFO: Scaling statefulset ss to 0
Jan 18 16:03:26.334: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 16:03:26.338: INFO: Deleting all statefulset in ns statefulset-9283
Jan 18 16:03:26.360: INFO: Scaling statefulset ss to 0
Jan 18 16:03:26.383: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 16:03:26.386: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 18 16:03:26.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9283" for this suite. 01/18/23 16:03:26.428
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":204,"skipped":3660,"failed":0}
------------------------------
• [SLOW TEST] [62.119 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:02:24.316
    Jan 18 16:02:24.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename statefulset 01/18/23 16:02:24.325
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:02:24.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:02:24.353
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9283 01/18/23 16:02:24.358
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-9283 01/18/23 16:02:24.362
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9283 01/18/23 16:02:24.371
    Jan 18 16:02:24.381: INFO: Found 0 stateful pods, waiting for 1
    Jan 18 16:02:34.389: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/18/23 16:02:34.389
    Jan 18 16:02:34.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-9283 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 16:02:34.605: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 16:02:34.605: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 16:02:34.605: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 16:02:34.610: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 18 16:02:44.617: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 16:02:44.617: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 16:02:44.651: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
    Jan 18 16:02:44.652: INFO: ss-0  v1-25-1-18760-w2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:24 +0000 UTC  }]
    Jan 18 16:02:44.653: INFO: ss-1                    Pending         []
    Jan 18 16:02:44.653: INFO: 
    Jan 18 16:02:44.653: INFO: StatefulSet ss has not reached scale 3, at 2
    Jan 18 16:02:45.662: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.979209361s
    Jan 18 16:02:46.669: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.971236525s
    Jan 18 16:02:47.677: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.963481896s
    Jan 18 16:02:48.683: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.955660373s
    Jan 18 16:02:49.691: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.950398214s
    Jan 18 16:02:50.697: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.942113104s
    Jan 18 16:02:51.705: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.936410602s
    Jan 18 16:02:52.713: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.927895085s
    Jan 18 16:02:53.719: INFO: Verifying statefulset ss doesn't scale past 3 for another 919.43709ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9283 01/18/23 16:02:54.719
    Jan 18 16:02:54.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-9283 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 16:02:54.990: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 16:02:54.990: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 16:02:54.990: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 16:02:54.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-9283 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 16:02:55.184: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 18 16:02:55.184: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 16:02:55.184: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 16:02:55.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-9283 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 16:02:55.412: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 18 16:02:55.412: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 16:02:55.412: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 16:02:55.423: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Jan 18 16:03:05.430: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 16:03:05.430: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 16:03:05.430: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 01/18/23 16:03:05.431
    Jan 18 16:03:05.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-9283 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 16:03:05.625: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 16:03:05.625: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 16:03:05.625: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 16:03:05.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-9283 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 16:03:05.866: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 16:03:05.866: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 16:03:05.866: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 16:03:05.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=statefulset-9283 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 16:03:06.112: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 16:03:06.112: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 16:03:06.112: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 16:03:06.112: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 16:03:06.117: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
    Jan 18 16:03:16.223: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 16:03:16.223: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 16:03:16.223: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 16:03:16.242: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
    Jan 18 16:03:16.242: INFO: ss-0  v1-25-1-18760-w2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:03:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:03:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:24 +0000 UTC  }]
    Jan 18 16:03:16.243: INFO: ss-1  v1-25-1-18760-w2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:03:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:03:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:44 +0000 UTC  }]
    Jan 18 16:03:16.243: INFO: ss-2  v1-25-1-18760-w   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:03:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:03:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:44 +0000 UTC  }]
    Jan 18 16:03:16.243: INFO: 
    Jan 18 16:03:16.243: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan 18 16:03:17.248: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
    Jan 18 16:03:17.248: INFO: ss-0  v1-25-1-18760-w2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:03:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:03:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:24 +0000 UTC  }]
    Jan 18 16:03:17.248: INFO: ss-1  v1-25-1-18760-w2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:03:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:03:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:02:44 +0000 UTC  }]
    Jan 18 16:03:17.248: INFO: 
    Jan 18 16:03:17.248: INFO: StatefulSet ss has not reached scale 0, at 2
    Jan 18 16:03:18.253: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.987630752s
    Jan 18 16:03:19.258: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.982916714s
    Jan 18 16:03:20.263: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.977767795s
    Jan 18 16:03:21.270: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.972658063s
    Jan 18 16:03:22.275: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.965495047s
    Jan 18 16:03:23.280: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.960005546s
    Jan 18 16:03:24.286: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.955213059s
    Jan 18 16:03:25.291: INFO: Verifying statefulset ss doesn't scale past 0 for another 949.99354ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9283 01/18/23 16:03:26.291
    Jan 18 16:03:26.319: INFO: Scaling statefulset ss to 0
    Jan 18 16:03:26.334: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 18 16:03:26.338: INFO: Deleting all statefulset in ns statefulset-9283
    Jan 18 16:03:26.360: INFO: Scaling statefulset ss to 0
    Jan 18 16:03:26.383: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 16:03:26.386: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 18 16:03:26.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9283" for this suite. 01/18/23 16:03:26.428
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:03:26.439
Jan 18 16:03:26.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename sched-preemption 01/18/23 16:03:26.443
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:03:26.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:03:26.48
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 18 16:03:26.508: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 16:04:26.561: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:04:26.565
Jan 18 16:04:26.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename sched-preemption-path 01/18/23 16:04:26.568
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:04:26.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:04:26.603
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Jan 18 16:04:26.629: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jan 18 16:04:26.634: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Jan 18 16:04:26.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-418" for this suite. 01/18/23 16:04:26.671
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 18 16:04:26.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5058" for this suite. 01/18/23 16:04:26.696
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":205,"skipped":3664,"failed":0}
------------------------------
• [SLOW TEST] [60.314 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:03:26.439
    Jan 18 16:03:26.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename sched-preemption 01/18/23 16:03:26.443
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:03:26.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:03:26.48
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 18 16:03:26.508: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 16:04:26.561: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:04:26.565
    Jan 18 16:04:26.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename sched-preemption-path 01/18/23 16:04:26.568
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:04:26.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:04:26.603
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Jan 18 16:04:26.629: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jan 18 16:04:26.634: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Jan 18 16:04:26.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-418" for this suite. 01/18/23 16:04:26.671
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 16:04:26.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-5058" for this suite. 01/18/23 16:04:26.696
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:04:26.756
Jan 18 16:04:26.756: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename services 01/18/23 16:04:26.757
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:04:26.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:04:26.78
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 01/18/23 16:04:26.784
Jan 18 16:04:26.785: INFO: Creating e2e-svc-a-l8qgs
Jan 18 16:04:26.795: INFO: Creating e2e-svc-b-wrscb
Jan 18 16:04:26.814: INFO: Creating e2e-svc-c-fdcqs
STEP: deleting service collection 01/18/23 16:04:26.838
Jan 18 16:04:26.886: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 16:04:26.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1536" for this suite. 01/18/23 16:04:26.892
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":206,"skipped":3672,"failed":0}
------------------------------
• [0.145 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:04:26.756
    Jan 18 16:04:26.756: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename services 01/18/23 16:04:26.757
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:04:26.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:04:26.78
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 01/18/23 16:04:26.784
    Jan 18 16:04:26.785: INFO: Creating e2e-svc-a-l8qgs
    Jan 18 16:04:26.795: INFO: Creating e2e-svc-b-wrscb
    Jan 18 16:04:26.814: INFO: Creating e2e-svc-c-fdcqs
    STEP: deleting service collection 01/18/23 16:04:26.838
    Jan 18 16:04:26.886: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 16:04:26.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1536" for this suite. 01/18/23 16:04:26.892
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:04:26.901
Jan 18 16:04:26.902: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename watch 01/18/23 16:04:26.906
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:04:26.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:04:26.93
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 01/18/23 16:04:26.936
STEP: starting a background goroutine to produce watch events 01/18/23 16:04:26.941
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/18/23 16:04:26.941
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 18 16:04:29.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8070" for this suite. 01/18/23 16:04:29.763
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":207,"skipped":3677,"failed":0}
------------------------------
• [2.913 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:04:26.901
    Jan 18 16:04:26.902: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename watch 01/18/23 16:04:26.906
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:04:26.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:04:26.93
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 01/18/23 16:04:26.936
    STEP: starting a background goroutine to produce watch events 01/18/23 16:04:26.941
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/18/23 16:04:26.941
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 18 16:04:29.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-8070" for this suite. 01/18/23 16:04:29.763
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:04:29.817
Jan 18 16:04:29.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename emptydir 01/18/23 16:04:29.819
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:04:29.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:04:29.863
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 01/18/23 16:04:29.87
Jan 18 16:04:29.883: INFO: Waiting up to 5m0s for pod "pod-4987cee2-9ee4-4b41-b5e4-cb57a3ce1f33" in namespace "emptydir-3136" to be "Succeeded or Failed"
Jan 18 16:04:29.897: INFO: Pod "pod-4987cee2-9ee4-4b41-b5e4-cb57a3ce1f33": Phase="Pending", Reason="", readiness=false. Elapsed: 13.354075ms
Jan 18 16:04:31.908: INFO: Pod "pod-4987cee2-9ee4-4b41-b5e4-cb57a3ce1f33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024215226s
Jan 18 16:04:33.903: INFO: Pod "pod-4987cee2-9ee4-4b41-b5e4-cb57a3ce1f33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019256781s
STEP: Saw pod success 01/18/23 16:04:33.903
Jan 18 16:04:33.903: INFO: Pod "pod-4987cee2-9ee4-4b41-b5e4-cb57a3ce1f33" satisfied condition "Succeeded or Failed"
Jan 18 16:04:33.908: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-4987cee2-9ee4-4b41-b5e4-cb57a3ce1f33 container test-container: <nil>
STEP: delete the pod 01/18/23 16:04:33.927
Jan 18 16:04:33.948: INFO: Waiting for pod pod-4987cee2-9ee4-4b41-b5e4-cb57a3ce1f33 to disappear
Jan 18 16:04:33.952: INFO: Pod pod-4987cee2-9ee4-4b41-b5e4-cb57a3ce1f33 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 16:04:33.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3136" for this suite. 01/18/23 16:04:33.962
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":208,"skipped":3711,"failed":0}
------------------------------
• [4.153 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:04:29.817
    Jan 18 16:04:29.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename emptydir 01/18/23 16:04:29.819
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:04:29.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:04:29.863
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/18/23 16:04:29.87
    Jan 18 16:04:29.883: INFO: Waiting up to 5m0s for pod "pod-4987cee2-9ee4-4b41-b5e4-cb57a3ce1f33" in namespace "emptydir-3136" to be "Succeeded or Failed"
    Jan 18 16:04:29.897: INFO: Pod "pod-4987cee2-9ee4-4b41-b5e4-cb57a3ce1f33": Phase="Pending", Reason="", readiness=false. Elapsed: 13.354075ms
    Jan 18 16:04:31.908: INFO: Pod "pod-4987cee2-9ee4-4b41-b5e4-cb57a3ce1f33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024215226s
    Jan 18 16:04:33.903: INFO: Pod "pod-4987cee2-9ee4-4b41-b5e4-cb57a3ce1f33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019256781s
    STEP: Saw pod success 01/18/23 16:04:33.903
    Jan 18 16:04:33.903: INFO: Pod "pod-4987cee2-9ee4-4b41-b5e4-cb57a3ce1f33" satisfied condition "Succeeded or Failed"
    Jan 18 16:04:33.908: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-4987cee2-9ee4-4b41-b5e4-cb57a3ce1f33 container test-container: <nil>
    STEP: delete the pod 01/18/23 16:04:33.927
    Jan 18 16:04:33.948: INFO: Waiting for pod pod-4987cee2-9ee4-4b41-b5e4-cb57a3ce1f33 to disappear
    Jan 18 16:04:33.952: INFO: Pod pod-4987cee2-9ee4-4b41-b5e4-cb57a3ce1f33 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 16:04:33.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3136" for this suite. 01/18/23 16:04:33.962
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:04:33.979
Jan 18 16:04:33.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename daemonsets 01/18/23 16:04:33.98
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:04:34.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:04:34.007
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Jan 18 16:04:34.053: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 01/18/23 16:04:34.061
Jan 18 16:04:34.065: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 16:04:34.066: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 01/18/23 16:04:34.066
Jan 18 16:04:34.109: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 16:04:34.109: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
Jan 18 16:04:35.278: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 16:04:35.278: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
Jan 18 16:04:36.115: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 16:04:36.115: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 01/18/23 16:04:36.119
Jan 18 16:04:36.159: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 16:04:36.159: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/18/23 16:04:36.16
Jan 18 16:04:36.180: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 16:04:36.180: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
Jan 18 16:04:37.184: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 16:04:37.185: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
Jan 18 16:04:38.186: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 16:04:38.186: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
Jan 18 16:04:39.184: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 16:04:39.184: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
Jan 18 16:04:40.186: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 16:04:40.186: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
Jan 18 16:04:41.187: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 16:04:41.187: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/18/23 16:04:41.207
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2999, will wait for the garbage collector to delete the pods 01/18/23 16:04:41.207
Jan 18 16:04:41.274: INFO: Deleting DaemonSet.extensions daemon-set took: 7.159796ms
Jan 18 16:04:41.375: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.801128ms
Jan 18 16:04:43.780: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 16:04:43.780: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 16:04:43.783: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"53141"},"items":null}

Jan 18 16:04:43.787: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"53141"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 18 16:04:43.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2999" for this suite. 01/18/23 16:04:43.849
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":209,"skipped":3727,"failed":0}
------------------------------
• [SLOW TEST] [9.881 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:04:33.979
    Jan 18 16:04:33.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename daemonsets 01/18/23 16:04:33.98
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:04:34.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:04:34.007
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Jan 18 16:04:34.053: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 01/18/23 16:04:34.061
    Jan 18 16:04:34.065: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 16:04:34.066: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 01/18/23 16:04:34.066
    Jan 18 16:04:34.109: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 16:04:34.109: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
    Jan 18 16:04:35.278: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 16:04:35.278: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
    Jan 18 16:04:36.115: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 16:04:36.115: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 01/18/23 16:04:36.119
    Jan 18 16:04:36.159: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 16:04:36.159: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/18/23 16:04:36.16
    Jan 18 16:04:36.180: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 16:04:36.180: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
    Jan 18 16:04:37.184: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 16:04:37.185: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
    Jan 18 16:04:38.186: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 16:04:38.186: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
    Jan 18 16:04:39.184: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 16:04:39.184: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
    Jan 18 16:04:40.186: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 16:04:40.186: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
    Jan 18 16:04:41.187: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 16:04:41.187: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 16:04:41.207
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2999, will wait for the garbage collector to delete the pods 01/18/23 16:04:41.207
    Jan 18 16:04:41.274: INFO: Deleting DaemonSet.extensions daemon-set took: 7.159796ms
    Jan 18 16:04:41.375: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.801128ms
    Jan 18 16:04:43.780: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 16:04:43.780: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 16:04:43.783: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"53141"},"items":null}

    Jan 18 16:04:43.787: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"53141"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 16:04:43.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-2999" for this suite. 01/18/23 16:04:43.849
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:04:43.868
Jan 18 16:04:43.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename downward-api 01/18/23 16:04:43.871
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:04:43.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:04:43.911
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 01/18/23 16:04:43.929
Jan 18 16:04:43.938: INFO: Waiting up to 5m0s for pod "downward-api-39612de7-f5c6-46be-805d-def6364c6c95" in namespace "downward-api-3472" to be "Succeeded or Failed"
Jan 18 16:04:43.962: INFO: Pod "downward-api-39612de7-f5c6-46be-805d-def6364c6c95": Phase="Pending", Reason="", readiness=false. Elapsed: 24.283972ms
Jan 18 16:04:45.975: INFO: Pod "downward-api-39612de7-f5c6-46be-805d-def6364c6c95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036565669s
Jan 18 16:04:47.969: INFO: Pod "downward-api-39612de7-f5c6-46be-805d-def6364c6c95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030648259s
STEP: Saw pod success 01/18/23 16:04:47.969
Jan 18 16:04:47.970: INFO: Pod "downward-api-39612de7-f5c6-46be-805d-def6364c6c95" satisfied condition "Succeeded or Failed"
Jan 18 16:04:47.975: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downward-api-39612de7-f5c6-46be-805d-def6364c6c95 container dapi-container: <nil>
STEP: delete the pod 01/18/23 16:04:47.986
Jan 18 16:04:48.001: INFO: Waiting for pod downward-api-39612de7-f5c6-46be-805d-def6364c6c95 to disappear
Jan 18 16:04:48.004: INFO: Pod downward-api-39612de7-f5c6-46be-805d-def6364c6c95 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 18 16:04:48.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3472" for this suite. 01/18/23 16:04:48.009
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":210,"skipped":3763,"failed":0}
------------------------------
• [4.151 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:04:43.868
    Jan 18 16:04:43.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename downward-api 01/18/23 16:04:43.871
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:04:43.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:04:43.911
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 01/18/23 16:04:43.929
    Jan 18 16:04:43.938: INFO: Waiting up to 5m0s for pod "downward-api-39612de7-f5c6-46be-805d-def6364c6c95" in namespace "downward-api-3472" to be "Succeeded or Failed"
    Jan 18 16:04:43.962: INFO: Pod "downward-api-39612de7-f5c6-46be-805d-def6364c6c95": Phase="Pending", Reason="", readiness=false. Elapsed: 24.283972ms
    Jan 18 16:04:45.975: INFO: Pod "downward-api-39612de7-f5c6-46be-805d-def6364c6c95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036565669s
    Jan 18 16:04:47.969: INFO: Pod "downward-api-39612de7-f5c6-46be-805d-def6364c6c95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030648259s
    STEP: Saw pod success 01/18/23 16:04:47.969
    Jan 18 16:04:47.970: INFO: Pod "downward-api-39612de7-f5c6-46be-805d-def6364c6c95" satisfied condition "Succeeded or Failed"
    Jan 18 16:04:47.975: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downward-api-39612de7-f5c6-46be-805d-def6364c6c95 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 16:04:47.986
    Jan 18 16:04:48.001: INFO: Waiting for pod downward-api-39612de7-f5c6-46be-805d-def6364c6c95 to disappear
    Jan 18 16:04:48.004: INFO: Pod downward-api-39612de7-f5c6-46be-805d-def6364c6c95 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 18 16:04:48.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3472" for this suite. 01/18/23 16:04:48.009
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:04:48.027
Jan 18 16:04:48.027: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename services 01/18/23 16:04:48.028
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:04:48.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:04:48.075
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-6645 01/18/23 16:04:48.082
Jan 18 16:04:48.092: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-6645" to be "running and ready"
Jan 18 16:04:48.163: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 71.0375ms
Jan 18 16:04:48.164: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 18 16:04:50.170: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.077686598s
Jan 18 16:04:50.170: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 18 16:04:50.170: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Jan 18 16:04:50.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6645 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 18 16:04:50.407: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 18 16:04:50.407: INFO: stdout: "ipvs"
Jan 18 16:04:50.407: INFO: proxyMode: ipvs
Jan 18 16:04:50.429: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 18 16:04:50.432: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-6645 01/18/23 16:04:50.432
STEP: creating replication controller affinity-nodeport-timeout in namespace services-6645 01/18/23 16:04:50.47
I0118 16:04:50.479041      19 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-6645, replica count: 3
I0118 16:04:53.531452      19 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 16:04:53.546: INFO: Creating new exec pod
Jan 18 16:04:53.551: INFO: Waiting up to 5m0s for pod "execpod-affinityxtvg4" in namespace "services-6645" to be "running"
Jan 18 16:04:53.559: INFO: Pod "execpod-affinityxtvg4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.656695ms
Jan 18 16:04:55.566: INFO: Pod "execpod-affinityxtvg4": Phase="Running", Reason="", readiness=true. Elapsed: 2.014291312s
Jan 18 16:04:55.567: INFO: Pod "execpod-affinityxtvg4" satisfied condition "running"
Jan 18 16:04:56.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6645 exec execpod-affinityxtvg4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jan 18 16:04:56.789: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jan 18 16:04:56.789: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 16:04:56.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6645 exec execpod-affinityxtvg4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.48.224 80'
Jan 18 16:04:56.994: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.48.224 80\nConnection to 10.233.48.224 80 port [tcp/http] succeeded!\n"
Jan 18 16:04:56.994: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 16:04:56.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6645 exec execpod-affinityxtvg4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.168 32149'
Jan 18 16:04:57.180: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.168 32149\nConnection to 192.168.101.168 32149 port [tcp/*] succeeded!\n"
Jan 18 16:04:57.180: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 16:04:57.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6645 exec execpod-affinityxtvg4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 32149'
Jan 18 16:04:57.357: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.216 32149\nConnection to 192.168.101.216 32149 port [tcp/*] succeeded!\n"
Jan 18 16:04:57.357: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 16:04:57.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6645 exec execpod-affinityxtvg4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.101.168:32149/ ; done'
Jan 18 16:04:57.698: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n"
Jan 18 16:04:57.698: INFO: stdout: "\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4"
Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
Jan 18 16:04:57.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6645 exec execpod-affinityxtvg4 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.101.168:32149/'
Jan 18 16:04:57.919: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n"
Jan 18 16:04:57.919: INFO: stdout: "affinity-nodeport-timeout-dg6f4"
Jan 18 16:07:07.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6645 exec execpod-affinityxtvg4 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.101.168:32149/'
Jan 18 16:07:08.134: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n"
Jan 18 16:07:08.134: INFO: stdout: "affinity-nodeport-timeout-rrphh"
Jan 18 16:07:08.134: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-6645, will wait for the garbage collector to delete the pods 01/18/23 16:07:08.15
Jan 18 16:07:08.239: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 23.95319ms
Jan 18 16:07:08.342: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 103.084883ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 16:07:10.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6645" for this suite. 01/18/23 16:07:10.977
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":211,"skipped":3824,"failed":0}
------------------------------
• [SLOW TEST] [142.960 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:04:48.027
    Jan 18 16:04:48.027: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename services 01/18/23 16:04:48.028
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:04:48.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:04:48.075
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-6645 01/18/23 16:04:48.082
    Jan 18 16:04:48.092: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-6645" to be "running and ready"
    Jan 18 16:04:48.163: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 71.0375ms
    Jan 18 16:04:48.164: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 16:04:50.170: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.077686598s
    Jan 18 16:04:50.170: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Jan 18 16:04:50.170: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Jan 18 16:04:50.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6645 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Jan 18 16:04:50.407: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Jan 18 16:04:50.407: INFO: stdout: "ipvs"
    Jan 18 16:04:50.407: INFO: proxyMode: ipvs
    Jan 18 16:04:50.429: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Jan 18 16:04:50.432: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-6645 01/18/23 16:04:50.432
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-6645 01/18/23 16:04:50.47
    I0118 16:04:50.479041      19 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-6645, replica count: 3
    I0118 16:04:53.531452      19 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 16:04:53.546: INFO: Creating new exec pod
    Jan 18 16:04:53.551: INFO: Waiting up to 5m0s for pod "execpod-affinityxtvg4" in namespace "services-6645" to be "running"
    Jan 18 16:04:53.559: INFO: Pod "execpod-affinityxtvg4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.656695ms
    Jan 18 16:04:55.566: INFO: Pod "execpod-affinityxtvg4": Phase="Running", Reason="", readiness=true. Elapsed: 2.014291312s
    Jan 18 16:04:55.567: INFO: Pod "execpod-affinityxtvg4" satisfied condition "running"
    Jan 18 16:04:56.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6645 exec execpod-affinityxtvg4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Jan 18 16:04:56.789: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Jan 18 16:04:56.789: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 16:04:56.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6645 exec execpod-affinityxtvg4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.48.224 80'
    Jan 18 16:04:56.994: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.48.224 80\nConnection to 10.233.48.224 80 port [tcp/http] succeeded!\n"
    Jan 18 16:04:56.994: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 16:04:56.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6645 exec execpod-affinityxtvg4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.168 32149'
    Jan 18 16:04:57.180: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.168 32149\nConnection to 192.168.101.168 32149 port [tcp/*] succeeded!\n"
    Jan 18 16:04:57.180: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 16:04:57.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6645 exec execpod-affinityxtvg4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 32149'
    Jan 18 16:04:57.357: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.216 32149\nConnection to 192.168.101.216 32149 port [tcp/*] succeeded!\n"
    Jan 18 16:04:57.357: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 16:04:57.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6645 exec execpod-affinityxtvg4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.101.168:32149/ ; done'
    Jan 18 16:04:57.698: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n"
    Jan 18 16:04:57.698: INFO: stdout: "\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4\naffinity-nodeport-timeout-dg6f4"
    Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
    Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
    Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
    Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
    Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
    Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
    Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
    Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
    Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
    Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
    Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
    Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
    Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
    Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
    Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
    Jan 18 16:04:57.698: INFO: Received response from host: affinity-nodeport-timeout-dg6f4
    Jan 18 16:04:57.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6645 exec execpod-affinityxtvg4 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.101.168:32149/'
    Jan 18 16:04:57.919: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n"
    Jan 18 16:04:57.919: INFO: stdout: "affinity-nodeport-timeout-dg6f4"
    Jan 18 16:07:07.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6645 exec execpod-affinityxtvg4 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.101.168:32149/'
    Jan 18 16:07:08.134: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.101.168:32149/\n"
    Jan 18 16:07:08.134: INFO: stdout: "affinity-nodeport-timeout-rrphh"
    Jan 18 16:07:08.134: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-6645, will wait for the garbage collector to delete the pods 01/18/23 16:07:08.15
    Jan 18 16:07:08.239: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 23.95319ms
    Jan 18 16:07:08.342: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 103.084883ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 16:07:10.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6645" for this suite. 01/18/23 16:07:10.977
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:07:10.99
Jan 18 16:07:10.992: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename events 01/18/23 16:07:10.994
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:07:11.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:07:11.038
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 01/18/23 16:07:11.049
Jan 18 16:07:11.054: INFO: created test-event-1
Jan 18 16:07:11.064: INFO: created test-event-2
Jan 18 16:07:11.070: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 01/18/23 16:07:11.07
STEP: delete collection of events 01/18/23 16:07:11.075
Jan 18 16:07:11.075: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/18/23 16:07:11.099
Jan 18 16:07:11.099: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jan 18 16:07:11.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3421" for this suite. 01/18/23 16:07:11.113
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":212,"skipped":3838,"failed":0}
------------------------------
• [0.133 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:07:10.99
    Jan 18 16:07:10.992: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename events 01/18/23 16:07:10.994
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:07:11.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:07:11.038
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 01/18/23 16:07:11.049
    Jan 18 16:07:11.054: INFO: created test-event-1
    Jan 18 16:07:11.064: INFO: created test-event-2
    Jan 18 16:07:11.070: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 01/18/23 16:07:11.07
    STEP: delete collection of events 01/18/23 16:07:11.075
    Jan 18 16:07:11.075: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/18/23 16:07:11.099
    Jan 18 16:07:11.099: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jan 18 16:07:11.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-3421" for this suite. 01/18/23 16:07:11.113
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:07:11.126
Jan 18 16:07:11.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 16:07:11.128
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:07:11.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:07:11.157
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 01/18/23 16:07:11.161
Jan 18 16:07:11.169: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99d6ec6c-2e0b-49fe-b8c0-bf3f852d0d23" in namespace "projected-5289" to be "Succeeded or Failed"
Jan 18 16:07:11.177: INFO: Pod "downwardapi-volume-99d6ec6c-2e0b-49fe-b8c0-bf3f852d0d23": Phase="Pending", Reason="", readiness=false. Elapsed: 7.874875ms
Jan 18 16:07:13.183: INFO: Pod "downwardapi-volume-99d6ec6c-2e0b-49fe-b8c0-bf3f852d0d23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014325514s
Jan 18 16:07:15.183: INFO: Pod "downwardapi-volume-99d6ec6c-2e0b-49fe-b8c0-bf3f852d0d23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013833691s
STEP: Saw pod success 01/18/23 16:07:15.183
Jan 18 16:07:15.183: INFO: Pod "downwardapi-volume-99d6ec6c-2e0b-49fe-b8c0-bf3f852d0d23" satisfied condition "Succeeded or Failed"
Jan 18 16:07:15.189: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-99d6ec6c-2e0b-49fe-b8c0-bf3f852d0d23 container client-container: <nil>
STEP: delete the pod 01/18/23 16:07:15.21
Jan 18 16:07:15.234: INFO: Waiting for pod downwardapi-volume-99d6ec6c-2e0b-49fe-b8c0-bf3f852d0d23 to disappear
Jan 18 16:07:15.238: INFO: Pod downwardapi-volume-99d6ec6c-2e0b-49fe-b8c0-bf3f852d0d23 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 16:07:15.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5289" for this suite. 01/18/23 16:07:15.246
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":213,"skipped":3853,"failed":0}
------------------------------
• [4.126 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:07:11.126
    Jan 18 16:07:11.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 16:07:11.128
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:07:11.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:07:11.157
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 01/18/23 16:07:11.161
    Jan 18 16:07:11.169: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99d6ec6c-2e0b-49fe-b8c0-bf3f852d0d23" in namespace "projected-5289" to be "Succeeded or Failed"
    Jan 18 16:07:11.177: INFO: Pod "downwardapi-volume-99d6ec6c-2e0b-49fe-b8c0-bf3f852d0d23": Phase="Pending", Reason="", readiness=false. Elapsed: 7.874875ms
    Jan 18 16:07:13.183: INFO: Pod "downwardapi-volume-99d6ec6c-2e0b-49fe-b8c0-bf3f852d0d23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014325514s
    Jan 18 16:07:15.183: INFO: Pod "downwardapi-volume-99d6ec6c-2e0b-49fe-b8c0-bf3f852d0d23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013833691s
    STEP: Saw pod success 01/18/23 16:07:15.183
    Jan 18 16:07:15.183: INFO: Pod "downwardapi-volume-99d6ec6c-2e0b-49fe-b8c0-bf3f852d0d23" satisfied condition "Succeeded or Failed"
    Jan 18 16:07:15.189: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-99d6ec6c-2e0b-49fe-b8c0-bf3f852d0d23 container client-container: <nil>
    STEP: delete the pod 01/18/23 16:07:15.21
    Jan 18 16:07:15.234: INFO: Waiting for pod downwardapi-volume-99d6ec6c-2e0b-49fe-b8c0-bf3f852d0d23 to disappear
    Jan 18 16:07:15.238: INFO: Pod downwardapi-volume-99d6ec6c-2e0b-49fe-b8c0-bf3f852d0d23 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 16:07:15.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5289" for this suite. 01/18/23 16:07:15.246
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:07:15.258
Jan 18 16:07:15.258: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename services 01/18/23 16:07:15.26
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:07:15.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:07:15.286
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4750 01/18/23 16:07:15.29
STEP: changing the ExternalName service to type=NodePort 01/18/23 16:07:15.296
STEP: creating replication controller externalname-service in namespace services-4750 01/18/23 16:07:15.324
I0118 16:07:15.338805      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4750, replica count: 2
I0118 16:07:18.389843      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 16:07:18.389: INFO: Creating new exec pod
Jan 18 16:07:18.399: INFO: Waiting up to 5m0s for pod "execpodjscm9" in namespace "services-4750" to be "running"
Jan 18 16:07:18.404: INFO: Pod "execpodjscm9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.679174ms
Jan 18 16:07:20.423: INFO: Pod "execpodjscm9": Phase="Running", Reason="", readiness=true. Elapsed: 2.02381411s
Jan 18 16:07:20.423: INFO: Pod "execpodjscm9" satisfied condition "running"
Jan 18 16:07:21.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4750 exec execpodjscm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 18 16:07:21.660: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 18 16:07:21.660: INFO: stdout: ""
Jan 18 16:07:22.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4750 exec execpodjscm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 18 16:07:22.904: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 18 16:07:22.904: INFO: stdout: "externalname-service-vpppk"
Jan 18 16:07:22.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4750 exec execpodjscm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.4.112 80'
Jan 18 16:07:23.126: INFO: stderr: "+ nc -v -t -w 2 10.233.4.112 80\nConnection to 10.233.4.112 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Jan 18 16:07:23.126: INFO: stdout: ""
Jan 18 16:07:24.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4750 exec execpodjscm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.4.112 80'
Jan 18 16:07:24.323: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.4.112 80\nConnection to 10.233.4.112 80 port [tcp/http] succeeded!\n"
Jan 18 16:07:24.323: INFO: stdout: ""
Jan 18 16:07:25.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4750 exec execpodjscm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.4.112 80'
Jan 18 16:07:25.324: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.4.112 80\nConnection to 10.233.4.112 80 port [tcp/http] succeeded!\n"
Jan 18 16:07:25.324: INFO: stdout: ""
Jan 18 16:07:26.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4750 exec execpodjscm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.4.112 80'
Jan 18 16:07:26.339: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.4.112 80\nConnection to 10.233.4.112 80 port [tcp/http] succeeded!\n"
Jan 18 16:07:26.339: INFO: stdout: ""
Jan 18 16:07:27.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4750 exec execpodjscm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.4.112 80'
Jan 18 16:07:27.317: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.4.112 80\nConnection to 10.233.4.112 80 port [tcp/http] succeeded!\n"
Jan 18 16:07:27.317: INFO: stdout: "externalname-service-fgvm7"
Jan 18 16:07:27.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4750 exec execpodjscm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.168 31388'
Jan 18 16:07:27.545: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.168 31388\nConnection to 192.168.101.168 31388 port [tcp/*] succeeded!\n"
Jan 18 16:07:27.545: INFO: stdout: ""
Jan 18 16:07:28.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4750 exec execpodjscm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.168 31388'
Jan 18 16:07:28.753: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.168 31388\nConnection to 192.168.101.168 31388 port [tcp/*] succeeded!\n"
Jan 18 16:07:28.754: INFO: stdout: "externalname-service-vpppk"
Jan 18 16:07:28.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4750 exec execpodjscm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 31388'
Jan 18 16:07:28.961: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.216 31388\nConnection to 192.168.101.216 31388 port [tcp/*] succeeded!\n"
Jan 18 16:07:28.961: INFO: stdout: ""
Jan 18 16:07:29.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4750 exec execpodjscm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 31388'
Jan 18 16:07:30.156: INFO: stderr: "+ + nc -v -t -w 2 192.168.101.216 31388\necho hostName\nConnection to 192.168.101.216 31388 port [tcp/*] succeeded!\n"
Jan 18 16:07:30.156: INFO: stdout: ""
Jan 18 16:07:30.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4750 exec execpodjscm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 31388'
Jan 18 16:07:31.145: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.216 31388\nConnection to 192.168.101.216 31388 port [tcp/*] succeeded!\n"
Jan 18 16:07:31.145: INFO: stdout: "externalname-service-fgvm7"
Jan 18 16:07:31.145: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 16:07:31.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4750" for this suite. 01/18/23 16:07:31.192
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":214,"skipped":3916,"failed":0}
------------------------------
• [SLOW TEST] [15.941 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:07:15.258
    Jan 18 16:07:15.258: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename services 01/18/23 16:07:15.26
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:07:15.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:07:15.286
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-4750 01/18/23 16:07:15.29
    STEP: changing the ExternalName service to type=NodePort 01/18/23 16:07:15.296
    STEP: creating replication controller externalname-service in namespace services-4750 01/18/23 16:07:15.324
    I0118 16:07:15.338805      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4750, replica count: 2
    I0118 16:07:18.389843      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 16:07:18.389: INFO: Creating new exec pod
    Jan 18 16:07:18.399: INFO: Waiting up to 5m0s for pod "execpodjscm9" in namespace "services-4750" to be "running"
    Jan 18 16:07:18.404: INFO: Pod "execpodjscm9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.679174ms
    Jan 18 16:07:20.423: INFO: Pod "execpodjscm9": Phase="Running", Reason="", readiness=true. Elapsed: 2.02381411s
    Jan 18 16:07:20.423: INFO: Pod "execpodjscm9" satisfied condition "running"
    Jan 18 16:07:21.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4750 exec execpodjscm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 18 16:07:21.660: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 18 16:07:21.660: INFO: stdout: ""
    Jan 18 16:07:22.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4750 exec execpodjscm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 18 16:07:22.904: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 18 16:07:22.904: INFO: stdout: "externalname-service-vpppk"
    Jan 18 16:07:22.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4750 exec execpodjscm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.4.112 80'
    Jan 18 16:07:23.126: INFO: stderr: "+ nc -v -t -w 2 10.233.4.112 80\nConnection to 10.233.4.112 80 port [tcp/http] succeeded!\n+ echo hostName\n"
    Jan 18 16:07:23.126: INFO: stdout: ""
    Jan 18 16:07:24.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4750 exec execpodjscm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.4.112 80'
    Jan 18 16:07:24.323: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.4.112 80\nConnection to 10.233.4.112 80 port [tcp/http] succeeded!\n"
    Jan 18 16:07:24.323: INFO: stdout: ""
    Jan 18 16:07:25.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4750 exec execpodjscm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.4.112 80'
    Jan 18 16:07:25.324: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.4.112 80\nConnection to 10.233.4.112 80 port [tcp/http] succeeded!\n"
    Jan 18 16:07:25.324: INFO: stdout: ""
    Jan 18 16:07:26.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4750 exec execpodjscm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.4.112 80'
    Jan 18 16:07:26.339: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.4.112 80\nConnection to 10.233.4.112 80 port [tcp/http] succeeded!\n"
    Jan 18 16:07:26.339: INFO: stdout: ""
    Jan 18 16:07:27.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4750 exec execpodjscm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.4.112 80'
    Jan 18 16:07:27.317: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.4.112 80\nConnection to 10.233.4.112 80 port [tcp/http] succeeded!\n"
    Jan 18 16:07:27.317: INFO: stdout: "externalname-service-fgvm7"
    Jan 18 16:07:27.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4750 exec execpodjscm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.168 31388'
    Jan 18 16:07:27.545: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.168 31388\nConnection to 192.168.101.168 31388 port [tcp/*] succeeded!\n"
    Jan 18 16:07:27.545: INFO: stdout: ""
    Jan 18 16:07:28.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4750 exec execpodjscm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.168 31388'
    Jan 18 16:07:28.753: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.168 31388\nConnection to 192.168.101.168 31388 port [tcp/*] succeeded!\n"
    Jan 18 16:07:28.754: INFO: stdout: "externalname-service-vpppk"
    Jan 18 16:07:28.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4750 exec execpodjscm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 31388'
    Jan 18 16:07:28.961: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.216 31388\nConnection to 192.168.101.216 31388 port [tcp/*] succeeded!\n"
    Jan 18 16:07:28.961: INFO: stdout: ""
    Jan 18 16:07:29.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4750 exec execpodjscm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 31388'
    Jan 18 16:07:30.156: INFO: stderr: "+ + nc -v -t -w 2 192.168.101.216 31388\necho hostName\nConnection to 192.168.101.216 31388 port [tcp/*] succeeded!\n"
    Jan 18 16:07:30.156: INFO: stdout: ""
    Jan 18 16:07:30.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4750 exec execpodjscm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 31388'
    Jan 18 16:07:31.145: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.216 31388\nConnection to 192.168.101.216 31388 port [tcp/*] succeeded!\n"
    Jan 18 16:07:31.145: INFO: stdout: "externalname-service-fgvm7"
    Jan 18 16:07:31.145: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 16:07:31.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4750" for this suite. 01/18/23 16:07:31.192
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:07:31.2
Jan 18 16:07:31.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 16:07:31.207
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:07:31.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:07:31.236
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/18/23 16:07:31.249
Jan 18 16:07:31.263: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5336" to be "running and ready"
Jan 18 16:07:31.277: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 13.502122ms
Jan 18 16:07:31.277: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 16:07:33.285: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.021764887s
Jan 18 16:07:33.285: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 18 16:07:33.285: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 01/18/23 16:07:33.291
Jan 18 16:07:33.299: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-5336" to be "running and ready"
Jan 18 16:07:33.304: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.310362ms
Jan 18 16:07:33.304: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 18 16:07:35.311: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011764036s
Jan 18 16:07:35.311: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jan 18 16:07:35.311: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/18/23 16:07:35.316
Jan 18 16:07:35.327: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 18 16:07:35.331: INFO: Pod pod-with-prestop-http-hook still exists
Jan 18 16:07:37.333: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 18 16:07:37.348: INFO: Pod pod-with-prestop-http-hook still exists
Jan 18 16:07:39.332: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 18 16:07:39.339: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 01/18/23 16:07:39.339
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 18 16:07:39.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5336" for this suite. 01/18/23 16:07:39.37
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":215,"skipped":3918,"failed":0}
------------------------------
• [SLOW TEST] [8.176 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:07:31.2
    Jan 18 16:07:31.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 16:07:31.207
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:07:31.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:07:31.236
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/18/23 16:07:31.249
    Jan 18 16:07:31.263: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5336" to be "running and ready"
    Jan 18 16:07:31.277: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 13.502122ms
    Jan 18 16:07:31.277: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 16:07:33.285: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.021764887s
    Jan 18 16:07:33.285: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 18 16:07:33.285: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 01/18/23 16:07:33.291
    Jan 18 16:07:33.299: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-5336" to be "running and ready"
    Jan 18 16:07:33.304: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.310362ms
    Jan 18 16:07:33.304: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 16:07:35.311: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011764036s
    Jan 18 16:07:35.311: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jan 18 16:07:35.311: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/18/23 16:07:35.316
    Jan 18 16:07:35.327: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 18 16:07:35.331: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 18 16:07:37.333: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 18 16:07:37.348: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 18 16:07:39.332: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 18 16:07:39.339: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 01/18/23 16:07:39.339
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 18 16:07:39.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-5336" for this suite. 01/18/23 16:07:39.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:07:39.385
Jan 18 16:07:39.386: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename lease-test 01/18/23 16:07:39.387
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:07:39.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:07:39.416
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Jan 18 16:07:39.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-9539" for this suite. 01/18/23 16:07:39.544
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":216,"skipped":3946,"failed":0}
------------------------------
• [0.172 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:07:39.385
    Jan 18 16:07:39.386: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename lease-test 01/18/23 16:07:39.387
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:07:39.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:07:39.416
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Jan 18 16:07:39.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-9539" for this suite. 01/18/23 16:07:39.544
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:07:39.559
Jan 18 16:07:39.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename sched-preemption 01/18/23 16:07:39.561
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:07:39.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:07:39.585
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 18 16:07:39.606: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 16:08:39.653: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 01/18/23 16:08:39.657
Jan 18 16:08:39.703: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 18 16:08:39.720: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 18 16:08:39.764: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 18 16:08:39.777: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/18/23 16:08:39.777
Jan 18 16:08:39.777: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5087" to be "running"
Jan 18 16:08:39.789: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 11.442685ms
Jan 18 16:08:41.806: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029199278s
Jan 18 16:08:43.797: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01940823s
Jan 18 16:08:45.795: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018138204s
Jan 18 16:08:47.796: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018388224s
Jan 18 16:08:49.795: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018216471s
Jan 18 16:08:51.794: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.017102769s
Jan 18 16:08:51.794: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 18 16:08:51.794: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5087" to be "running"
Jan 18 16:08:51.799: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.681058ms
Jan 18 16:08:51.800: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 18 16:08:51.800: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5087" to be "running"
Jan 18 16:08:51.806: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.774066ms
Jan 18 16:08:51.806: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 18 16:08:51.807: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5087" to be "running"
Jan 18 16:08:51.811: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.163332ms
Jan 18 16:08:51.811: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/18/23 16:08:51.811
Jan 18 16:08:51.819: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-5087" to be "running"
Jan 18 16:08:51.825: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.253026ms
Jan 18 16:08:53.837: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016702241s
Jan 18 16:08:55.829: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009276969s
Jan 18 16:08:55.829: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 18 16:08:55.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5087" for this suite. 01/18/23 16:08:55.86
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":217,"skipped":3947,"failed":0}
------------------------------
• [SLOW TEST] [76.359 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:07:39.559
    Jan 18 16:07:39.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename sched-preemption 01/18/23 16:07:39.561
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:07:39.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:07:39.585
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 18 16:07:39.606: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 16:08:39.653: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 01/18/23 16:08:39.657
    Jan 18 16:08:39.703: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 18 16:08:39.720: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 18 16:08:39.764: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 18 16:08:39.777: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/18/23 16:08:39.777
    Jan 18 16:08:39.777: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5087" to be "running"
    Jan 18 16:08:39.789: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 11.442685ms
    Jan 18 16:08:41.806: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029199278s
    Jan 18 16:08:43.797: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01940823s
    Jan 18 16:08:45.795: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018138204s
    Jan 18 16:08:47.796: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018388224s
    Jan 18 16:08:49.795: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018216471s
    Jan 18 16:08:51.794: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.017102769s
    Jan 18 16:08:51.794: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 18 16:08:51.794: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5087" to be "running"
    Jan 18 16:08:51.799: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.681058ms
    Jan 18 16:08:51.800: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 18 16:08:51.800: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5087" to be "running"
    Jan 18 16:08:51.806: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.774066ms
    Jan 18 16:08:51.806: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 18 16:08:51.807: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5087" to be "running"
    Jan 18 16:08:51.811: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.163332ms
    Jan 18 16:08:51.811: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/18/23 16:08:51.811
    Jan 18 16:08:51.819: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-5087" to be "running"
    Jan 18 16:08:51.825: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.253026ms
    Jan 18 16:08:53.837: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016702241s
    Jan 18 16:08:55.829: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009276969s
    Jan 18 16:08:55.829: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 16:08:55.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-5087" for this suite. 01/18/23 16:08:55.86
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:08:55.928
Jan 18 16:08:55.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename dns 01/18/23 16:08:55.93
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:08:55.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:08:55.953
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/18/23 16:08:55.956
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/18/23 16:08:55.956
STEP: creating a pod to probe DNS 01/18/23 16:08:55.956
STEP: submitting the pod to kubernetes 01/18/23 16:08:55.957
Jan 18 16:08:55.964: INFO: Waiting up to 15m0s for pod "dns-test-c0134df6-d75c-4361-b111-8f874a70cc54" in namespace "dns-6488" to be "running"
Jan 18 16:08:55.991: INFO: Pod "dns-test-c0134df6-d75c-4361-b111-8f874a70cc54": Phase="Pending", Reason="", readiness=false. Elapsed: 26.432391ms
Jan 18 16:08:58.003: INFO: Pod "dns-test-c0134df6-d75c-4361-b111-8f874a70cc54": Phase="Running", Reason="", readiness=true. Elapsed: 2.038915866s
Jan 18 16:08:58.003: INFO: Pod "dns-test-c0134df6-d75c-4361-b111-8f874a70cc54" satisfied condition "running"
STEP: retrieving the pod 01/18/23 16:08:58.003
STEP: looking for the results for each expected name from probers 01/18/23 16:08:58.009
Jan 18 16:08:58.044: INFO: DNS probes using dns-6488/dns-test-c0134df6-d75c-4361-b111-8f874a70cc54 succeeded

STEP: deleting the pod 01/18/23 16:08:58.044
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 18 16:08:58.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6488" for this suite. 01/18/23 16:08:58.086
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":218,"skipped":3962,"failed":0}
------------------------------
• [2.186 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:08:55.928
    Jan 18 16:08:55.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename dns 01/18/23 16:08:55.93
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:08:55.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:08:55.953
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/18/23 16:08:55.956
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/18/23 16:08:55.956
    STEP: creating a pod to probe DNS 01/18/23 16:08:55.956
    STEP: submitting the pod to kubernetes 01/18/23 16:08:55.957
    Jan 18 16:08:55.964: INFO: Waiting up to 15m0s for pod "dns-test-c0134df6-d75c-4361-b111-8f874a70cc54" in namespace "dns-6488" to be "running"
    Jan 18 16:08:55.991: INFO: Pod "dns-test-c0134df6-d75c-4361-b111-8f874a70cc54": Phase="Pending", Reason="", readiness=false. Elapsed: 26.432391ms
    Jan 18 16:08:58.003: INFO: Pod "dns-test-c0134df6-d75c-4361-b111-8f874a70cc54": Phase="Running", Reason="", readiness=true. Elapsed: 2.038915866s
    Jan 18 16:08:58.003: INFO: Pod "dns-test-c0134df6-d75c-4361-b111-8f874a70cc54" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 16:08:58.003
    STEP: looking for the results for each expected name from probers 01/18/23 16:08:58.009
    Jan 18 16:08:58.044: INFO: DNS probes using dns-6488/dns-test-c0134df6-d75c-4361-b111-8f874a70cc54 succeeded

    STEP: deleting the pod 01/18/23 16:08:58.044
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 18 16:08:58.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6488" for this suite. 01/18/23 16:08:58.086
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:08:58.122
Jan 18 16:08:58.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename security-context 01/18/23 16:08:58.124
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:08:58.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:08:58.159
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/18/23 16:08:58.164
Jan 18 16:08:58.174: INFO: Waiting up to 5m0s for pod "security-context-d1448cd4-bc78-40a8-8b05-95d9fa5753ea" in namespace "security-context-3534" to be "Succeeded or Failed"
Jan 18 16:08:58.185: INFO: Pod "security-context-d1448cd4-bc78-40a8-8b05-95d9fa5753ea": Phase="Pending", Reason="", readiness=false. Elapsed: 10.987215ms
Jan 18 16:09:00.192: INFO: Pod "security-context-d1448cd4-bc78-40a8-8b05-95d9fa5753ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017794195s
Jan 18 16:09:02.192: INFO: Pod "security-context-d1448cd4-bc78-40a8-8b05-95d9fa5753ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018374867s
STEP: Saw pod success 01/18/23 16:09:02.192
Jan 18 16:09:02.192: INFO: Pod "security-context-d1448cd4-bc78-40a8-8b05-95d9fa5753ea" satisfied condition "Succeeded or Failed"
Jan 18 16:09:02.208: INFO: Trying to get logs from node v1-25-1-18760-w2 pod security-context-d1448cd4-bc78-40a8-8b05-95d9fa5753ea container test-container: <nil>
STEP: delete the pod 01/18/23 16:09:02.236
Jan 18 16:09:02.250: INFO: Waiting for pod security-context-d1448cd4-bc78-40a8-8b05-95d9fa5753ea to disappear
Jan 18 16:09:02.258: INFO: Pod security-context-d1448cd4-bc78-40a8-8b05-95d9fa5753ea no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 18 16:09:02.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-3534" for this suite. 01/18/23 16:09:02.266
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":219,"skipped":4001,"failed":0}
------------------------------
• [4.156 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:08:58.122
    Jan 18 16:08:58.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename security-context 01/18/23 16:08:58.124
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:08:58.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:08:58.159
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/18/23 16:08:58.164
    Jan 18 16:08:58.174: INFO: Waiting up to 5m0s for pod "security-context-d1448cd4-bc78-40a8-8b05-95d9fa5753ea" in namespace "security-context-3534" to be "Succeeded or Failed"
    Jan 18 16:08:58.185: INFO: Pod "security-context-d1448cd4-bc78-40a8-8b05-95d9fa5753ea": Phase="Pending", Reason="", readiness=false. Elapsed: 10.987215ms
    Jan 18 16:09:00.192: INFO: Pod "security-context-d1448cd4-bc78-40a8-8b05-95d9fa5753ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017794195s
    Jan 18 16:09:02.192: INFO: Pod "security-context-d1448cd4-bc78-40a8-8b05-95d9fa5753ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018374867s
    STEP: Saw pod success 01/18/23 16:09:02.192
    Jan 18 16:09:02.192: INFO: Pod "security-context-d1448cd4-bc78-40a8-8b05-95d9fa5753ea" satisfied condition "Succeeded or Failed"
    Jan 18 16:09:02.208: INFO: Trying to get logs from node v1-25-1-18760-w2 pod security-context-d1448cd4-bc78-40a8-8b05-95d9fa5753ea container test-container: <nil>
    STEP: delete the pod 01/18/23 16:09:02.236
    Jan 18 16:09:02.250: INFO: Waiting for pod security-context-d1448cd4-bc78-40a8-8b05-95d9fa5753ea to disappear
    Jan 18 16:09:02.258: INFO: Pod security-context-d1448cd4-bc78-40a8-8b05-95d9fa5753ea no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 18 16:09:02.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-3534" for this suite. 01/18/23 16:09:02.266
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:09:02.283
Jan 18 16:09:02.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename downward-api 01/18/23 16:09:02.285
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:09:02.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:09:02.332
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 01/18/23 16:09:02.341
Jan 18 16:09:02.353: INFO: Waiting up to 5m0s for pod "downward-api-8ffa55cf-a123-415f-94e8-f6b152bf40a6" in namespace "downward-api-4530" to be "Succeeded or Failed"
Jan 18 16:09:02.367: INFO: Pod "downward-api-8ffa55cf-a123-415f-94e8-f6b152bf40a6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.79879ms
Jan 18 16:09:04.372: INFO: Pod "downward-api-8ffa55cf-a123-415f-94e8-f6b152bf40a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018002699s
Jan 18 16:09:06.374: INFO: Pod "downward-api-8ffa55cf-a123-415f-94e8-f6b152bf40a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019775546s
STEP: Saw pod success 01/18/23 16:09:06.374
Jan 18 16:09:06.374: INFO: Pod "downward-api-8ffa55cf-a123-415f-94e8-f6b152bf40a6" satisfied condition "Succeeded or Failed"
Jan 18 16:09:06.383: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downward-api-8ffa55cf-a123-415f-94e8-f6b152bf40a6 container dapi-container: <nil>
STEP: delete the pod 01/18/23 16:09:06.396
Jan 18 16:09:06.412: INFO: Waiting for pod downward-api-8ffa55cf-a123-415f-94e8-f6b152bf40a6 to disappear
Jan 18 16:09:06.420: INFO: Pod downward-api-8ffa55cf-a123-415f-94e8-f6b152bf40a6 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 18 16:09:06.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4530" for this suite. 01/18/23 16:09:06.425
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":220,"skipped":4011,"failed":0}
------------------------------
• [4.151 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:09:02.283
    Jan 18 16:09:02.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename downward-api 01/18/23 16:09:02.285
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:09:02.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:09:02.332
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 01/18/23 16:09:02.341
    Jan 18 16:09:02.353: INFO: Waiting up to 5m0s for pod "downward-api-8ffa55cf-a123-415f-94e8-f6b152bf40a6" in namespace "downward-api-4530" to be "Succeeded or Failed"
    Jan 18 16:09:02.367: INFO: Pod "downward-api-8ffa55cf-a123-415f-94e8-f6b152bf40a6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.79879ms
    Jan 18 16:09:04.372: INFO: Pod "downward-api-8ffa55cf-a123-415f-94e8-f6b152bf40a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018002699s
    Jan 18 16:09:06.374: INFO: Pod "downward-api-8ffa55cf-a123-415f-94e8-f6b152bf40a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019775546s
    STEP: Saw pod success 01/18/23 16:09:06.374
    Jan 18 16:09:06.374: INFO: Pod "downward-api-8ffa55cf-a123-415f-94e8-f6b152bf40a6" satisfied condition "Succeeded or Failed"
    Jan 18 16:09:06.383: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downward-api-8ffa55cf-a123-415f-94e8-f6b152bf40a6 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 16:09:06.396
    Jan 18 16:09:06.412: INFO: Waiting for pod downward-api-8ffa55cf-a123-415f-94e8-f6b152bf40a6 to disappear
    Jan 18 16:09:06.420: INFO: Pod downward-api-8ffa55cf-a123-415f-94e8-f6b152bf40a6 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 18 16:09:06.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4530" for this suite. 01/18/23 16:09:06.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:09:06.436
Jan 18 16:09:06.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename resourcequota 01/18/23 16:09:06.438
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:09:06.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:09:06.46
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 01/18/23 16:09:06.464
STEP: Creating a ResourceQuota 01/18/23 16:09:11.471
STEP: Ensuring resource quota status is calculated 01/18/23 16:09:11.477
STEP: Creating a ReplicaSet 01/18/23 16:09:13.482
STEP: Ensuring resource quota status captures replicaset creation 01/18/23 16:09:13.502
STEP: Deleting a ReplicaSet 01/18/23 16:09:15.509
STEP: Ensuring resource quota status released usage 01/18/23 16:09:15.516
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 16:09:17.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6323" for this suite. 01/18/23 16:09:17.527
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":221,"skipped":4016,"failed":0}
------------------------------
• [SLOW TEST] [11.099 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:09:06.436
    Jan 18 16:09:06.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename resourcequota 01/18/23 16:09:06.438
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:09:06.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:09:06.46
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 01/18/23 16:09:06.464
    STEP: Creating a ResourceQuota 01/18/23 16:09:11.471
    STEP: Ensuring resource quota status is calculated 01/18/23 16:09:11.477
    STEP: Creating a ReplicaSet 01/18/23 16:09:13.482
    STEP: Ensuring resource quota status captures replicaset creation 01/18/23 16:09:13.502
    STEP: Deleting a ReplicaSet 01/18/23 16:09:15.509
    STEP: Ensuring resource quota status released usage 01/18/23 16:09:15.516
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 16:09:17.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6323" for this suite. 01/18/23 16:09:17.527
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:09:17.542
Jan 18 16:09:17.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename emptydir-wrapper 01/18/23 16:09:17.543
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:09:17.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:09:17.573
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jan 18 16:09:17.613: INFO: Waiting up to 5m0s for pod "pod-secrets-eb156058-4a5b-4454-b168-3abb357efe23" in namespace "emptydir-wrapper-2920" to be "running and ready"
Jan 18 16:09:17.624: INFO: Pod "pod-secrets-eb156058-4a5b-4454-b168-3abb357efe23": Phase="Pending", Reason="", readiness=false. Elapsed: 10.992615ms
Jan 18 16:09:17.625: INFO: The phase of Pod pod-secrets-eb156058-4a5b-4454-b168-3abb357efe23 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 16:09:19.631: INFO: Pod "pod-secrets-eb156058-4a5b-4454-b168-3abb357efe23": Phase="Running", Reason="", readiness=true. Elapsed: 2.017340454s
Jan 18 16:09:19.631: INFO: The phase of Pod pod-secrets-eb156058-4a5b-4454-b168-3abb357efe23 is Running (Ready = true)
Jan 18 16:09:19.631: INFO: Pod "pod-secrets-eb156058-4a5b-4454-b168-3abb357efe23" satisfied condition "running and ready"
STEP: Cleaning up the secret 01/18/23 16:09:19.636
STEP: Cleaning up the configmap 01/18/23 16:09:19.642
STEP: Cleaning up the pod 01/18/23 16:09:19.658
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jan 18 16:09:19.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2920" for this suite. 01/18/23 16:09:19.695
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":222,"skipped":4043,"failed":0}
------------------------------
• [2.164 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:09:17.542
    Jan 18 16:09:17.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename emptydir-wrapper 01/18/23 16:09:17.543
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:09:17.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:09:17.573
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jan 18 16:09:17.613: INFO: Waiting up to 5m0s for pod "pod-secrets-eb156058-4a5b-4454-b168-3abb357efe23" in namespace "emptydir-wrapper-2920" to be "running and ready"
    Jan 18 16:09:17.624: INFO: Pod "pod-secrets-eb156058-4a5b-4454-b168-3abb357efe23": Phase="Pending", Reason="", readiness=false. Elapsed: 10.992615ms
    Jan 18 16:09:17.625: INFO: The phase of Pod pod-secrets-eb156058-4a5b-4454-b168-3abb357efe23 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 16:09:19.631: INFO: Pod "pod-secrets-eb156058-4a5b-4454-b168-3abb357efe23": Phase="Running", Reason="", readiness=true. Elapsed: 2.017340454s
    Jan 18 16:09:19.631: INFO: The phase of Pod pod-secrets-eb156058-4a5b-4454-b168-3abb357efe23 is Running (Ready = true)
    Jan 18 16:09:19.631: INFO: Pod "pod-secrets-eb156058-4a5b-4454-b168-3abb357efe23" satisfied condition "running and ready"
    STEP: Cleaning up the secret 01/18/23 16:09:19.636
    STEP: Cleaning up the configmap 01/18/23 16:09:19.642
    STEP: Cleaning up the pod 01/18/23 16:09:19.658
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jan 18 16:09:19.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-2920" for this suite. 01/18/23 16:09:19.695
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:09:19.709
Jan 18 16:09:19.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename secrets 01/18/23 16:09:19.711
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:09:19.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:09:19.738
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 18 16:09:19.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2593" for this suite. 01/18/23 16:09:19.812
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":223,"skipped":4064,"failed":0}
------------------------------
• [0.112 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:09:19.709
    Jan 18 16:09:19.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename secrets 01/18/23 16:09:19.711
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:09:19.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:09:19.738
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 16:09:19.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2593" for this suite. 01/18/23 16:09:19.812
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:09:19.841
Jan 18 16:09:19.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename downward-api 01/18/23 16:09:19.843
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:09:19.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:09:19.882
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 01/18/23 16:09:19.891
Jan 18 16:09:19.905: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e630039-3340-4257-91fe-8e444f5d0449" in namespace "downward-api-772" to be "Succeeded or Failed"
Jan 18 16:09:19.913: INFO: Pod "downwardapi-volume-4e630039-3340-4257-91fe-8e444f5d0449": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023892ms
Jan 18 16:09:21.921: INFO: Pod "downwardapi-volume-4e630039-3340-4257-91fe-8e444f5d0449": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015706071s
Jan 18 16:09:23.919: INFO: Pod "downwardapi-volume-4e630039-3340-4257-91fe-8e444f5d0449": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013568293s
Jan 18 16:09:25.920: INFO: Pod "downwardapi-volume-4e630039-3340-4257-91fe-8e444f5d0449": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014823544s
STEP: Saw pod success 01/18/23 16:09:25.92
Jan 18 16:09:25.920: INFO: Pod "downwardapi-volume-4e630039-3340-4257-91fe-8e444f5d0449" satisfied condition "Succeeded or Failed"
Jan 18 16:09:25.924: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-4e630039-3340-4257-91fe-8e444f5d0449 container client-container: <nil>
STEP: delete the pod 01/18/23 16:09:25.938
Jan 18 16:09:25.951: INFO: Waiting for pod downwardapi-volume-4e630039-3340-4257-91fe-8e444f5d0449 to disappear
Jan 18 16:09:25.958: INFO: Pod downwardapi-volume-4e630039-3340-4257-91fe-8e444f5d0449 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 16:09:25.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-772" for this suite. 01/18/23 16:09:25.964
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":224,"skipped":4096,"failed":0}
------------------------------
• [SLOW TEST] [6.130 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:09:19.841
    Jan 18 16:09:19.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename downward-api 01/18/23 16:09:19.843
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:09:19.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:09:19.882
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 01/18/23 16:09:19.891
    Jan 18 16:09:19.905: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e630039-3340-4257-91fe-8e444f5d0449" in namespace "downward-api-772" to be "Succeeded or Failed"
    Jan 18 16:09:19.913: INFO: Pod "downwardapi-volume-4e630039-3340-4257-91fe-8e444f5d0449": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023892ms
    Jan 18 16:09:21.921: INFO: Pod "downwardapi-volume-4e630039-3340-4257-91fe-8e444f5d0449": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015706071s
    Jan 18 16:09:23.919: INFO: Pod "downwardapi-volume-4e630039-3340-4257-91fe-8e444f5d0449": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013568293s
    Jan 18 16:09:25.920: INFO: Pod "downwardapi-volume-4e630039-3340-4257-91fe-8e444f5d0449": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014823544s
    STEP: Saw pod success 01/18/23 16:09:25.92
    Jan 18 16:09:25.920: INFO: Pod "downwardapi-volume-4e630039-3340-4257-91fe-8e444f5d0449" satisfied condition "Succeeded or Failed"
    Jan 18 16:09:25.924: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-4e630039-3340-4257-91fe-8e444f5d0449 container client-container: <nil>
    STEP: delete the pod 01/18/23 16:09:25.938
    Jan 18 16:09:25.951: INFO: Waiting for pod downwardapi-volume-4e630039-3340-4257-91fe-8e444f5d0449 to disappear
    Jan 18 16:09:25.958: INFO: Pod downwardapi-volume-4e630039-3340-4257-91fe-8e444f5d0449 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 16:09:25.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-772" for this suite. 01/18/23 16:09:25.964
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:09:25.977
Jan 18 16:09:25.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename init-container 01/18/23 16:09:25.979
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:09:26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:09:26.005
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 01/18/23 16:09:26.012
Jan 18 16:09:26.013: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 18 16:09:31.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5287" for this suite. 01/18/23 16:09:31.026
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":225,"skipped":4096,"failed":0}
------------------------------
• [SLOW TEST] [5.057 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:09:25.977
    Jan 18 16:09:25.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename init-container 01/18/23 16:09:25.979
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:09:26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:09:26.005
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 01/18/23 16:09:26.012
    Jan 18 16:09:26.013: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 18 16:09:31.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-5287" for this suite. 01/18/23 16:09:31.026
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:09:31.065
Jan 18 16:09:31.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename kubectl 01/18/23 16:09:31.067
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:09:31.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:09:31.092
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 01/18/23 16:09:31.104
Jan 18 16:09:31.104: INFO: namespace kubectl-3487
Jan 18 16:09:31.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3487 create -f -'
Jan 18 16:09:32.814: INFO: stderr: ""
Jan 18 16:09:32.814: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/18/23 16:09:32.814
Jan 18 16:09:33.835: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 16:09:33.835: INFO: Found 0 / 1
Jan 18 16:09:34.820: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 16:09:34.820: INFO: Found 1 / 1
Jan 18 16:09:34.820: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 18 16:09:34.825: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 16:09:34.825: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 18 16:09:34.825: INFO: wait on agnhost-primary startup in kubectl-3487 
Jan 18 16:09:34.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3487 logs agnhost-primary-5b6v2 agnhost-primary'
Jan 18 16:09:34.970: INFO: stderr: ""
Jan 18 16:09:34.970: INFO: stdout: "Paused\n"
STEP: exposing RC 01/18/23 16:09:34.97
Jan 18 16:09:34.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3487 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan 18 16:09:35.109: INFO: stderr: ""
Jan 18 16:09:35.109: INFO: stdout: "service/rm2 exposed\n"
Jan 18 16:09:35.126: INFO: Service rm2 in namespace kubectl-3487 found.
STEP: exposing service 01/18/23 16:09:37.135
Jan 18 16:09:37.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3487 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan 18 16:09:37.271: INFO: stderr: ""
Jan 18 16:09:37.271: INFO: stdout: "service/rm3 exposed\n"
Jan 18 16:09:37.276: INFO: Service rm3 in namespace kubectl-3487 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 16:09:39.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3487" for this suite. 01/18/23 16:09:39.293
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":226,"skipped":4157,"failed":0}
------------------------------
• [SLOW TEST] [8.241 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:09:31.065
    Jan 18 16:09:31.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename kubectl 01/18/23 16:09:31.067
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:09:31.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:09:31.092
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 01/18/23 16:09:31.104
    Jan 18 16:09:31.104: INFO: namespace kubectl-3487
    Jan 18 16:09:31.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3487 create -f -'
    Jan 18 16:09:32.814: INFO: stderr: ""
    Jan 18 16:09:32.814: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/18/23 16:09:32.814
    Jan 18 16:09:33.835: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 16:09:33.835: INFO: Found 0 / 1
    Jan 18 16:09:34.820: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 16:09:34.820: INFO: Found 1 / 1
    Jan 18 16:09:34.820: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 18 16:09:34.825: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 16:09:34.825: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 18 16:09:34.825: INFO: wait on agnhost-primary startup in kubectl-3487 
    Jan 18 16:09:34.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3487 logs agnhost-primary-5b6v2 agnhost-primary'
    Jan 18 16:09:34.970: INFO: stderr: ""
    Jan 18 16:09:34.970: INFO: stdout: "Paused\n"
    STEP: exposing RC 01/18/23 16:09:34.97
    Jan 18 16:09:34.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3487 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jan 18 16:09:35.109: INFO: stderr: ""
    Jan 18 16:09:35.109: INFO: stdout: "service/rm2 exposed\n"
    Jan 18 16:09:35.126: INFO: Service rm2 in namespace kubectl-3487 found.
    STEP: exposing service 01/18/23 16:09:37.135
    Jan 18 16:09:37.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3487 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jan 18 16:09:37.271: INFO: stderr: ""
    Jan 18 16:09:37.271: INFO: stdout: "service/rm3 exposed\n"
    Jan 18 16:09:37.276: INFO: Service rm3 in namespace kubectl-3487 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 16:09:39.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3487" for this suite. 01/18/23 16:09:39.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:09:39.312
Jan 18 16:09:39.313: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename webhook 01/18/23 16:09:39.314
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:09:39.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:09:39.347
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 16:09:39.371
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 16:09:39.838
STEP: Deploying the webhook pod 01/18/23 16:09:39.858
STEP: Wait for the deployment to be ready 01/18/23 16:09:39.872
Jan 18 16:09:39.879: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/18/23 16:09:41.911
STEP: Verifying the service has paired with the endpoint 01/18/23 16:09:41.935
Jan 18 16:09:42.936: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 01/18/23 16:09:42.94
STEP: create a pod that should be denied by the webhook 01/18/23 16:09:42.965
STEP: create a pod that causes the webhook to hang 01/18/23 16:09:42.986
STEP: create a configmap that should be denied by the webhook 01/18/23 16:09:52.998
STEP: create a configmap that should be admitted by the webhook 01/18/23 16:09:53.042
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/18/23 16:09:53.058
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/18/23 16:09:53.07
STEP: create a namespace that bypass the webhook 01/18/23 16:09:53.081
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/18/23 16:09:53.092
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 16:09:53.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7834" for this suite. 01/18/23 16:09:53.133
STEP: Destroying namespace "webhook-7834-markers" for this suite. 01/18/23 16:09:53.141
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":227,"skipped":4211,"failed":0}
------------------------------
• [SLOW TEST] [13.896 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:09:39.312
    Jan 18 16:09:39.313: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename webhook 01/18/23 16:09:39.314
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:09:39.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:09:39.347
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 16:09:39.371
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 16:09:39.838
    STEP: Deploying the webhook pod 01/18/23 16:09:39.858
    STEP: Wait for the deployment to be ready 01/18/23 16:09:39.872
    Jan 18 16:09:39.879: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/18/23 16:09:41.911
    STEP: Verifying the service has paired with the endpoint 01/18/23 16:09:41.935
    Jan 18 16:09:42.936: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 01/18/23 16:09:42.94
    STEP: create a pod that should be denied by the webhook 01/18/23 16:09:42.965
    STEP: create a pod that causes the webhook to hang 01/18/23 16:09:42.986
    STEP: create a configmap that should be denied by the webhook 01/18/23 16:09:52.998
    STEP: create a configmap that should be admitted by the webhook 01/18/23 16:09:53.042
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/18/23 16:09:53.058
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/18/23 16:09:53.07
    STEP: create a namespace that bypass the webhook 01/18/23 16:09:53.081
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/18/23 16:09:53.092
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 16:09:53.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7834" for this suite. 01/18/23 16:09:53.133
    STEP: Destroying namespace "webhook-7834-markers" for this suite. 01/18/23 16:09:53.141
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:09:53.212
Jan 18 16:09:53.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename kubectl 01/18/23 16:09:53.213
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:09:53.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:09:53.277
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 01/18/23 16:09:53.284
Jan 18 16:09:53.284: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-7015 proxy --unix-socket=/tmp/kubectl-proxy-unix4013026179/test'
STEP: retrieving proxy /api/ output 01/18/23 16:09:53.487
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 16:09:53.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7015" for this suite. 01/18/23 16:09:53.496
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":228,"skipped":4227,"failed":0}
------------------------------
• [0.293 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:09:53.212
    Jan 18 16:09:53.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename kubectl 01/18/23 16:09:53.213
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:09:53.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:09:53.277
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 01/18/23 16:09:53.284
    Jan 18 16:09:53.284: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-7015 proxy --unix-socket=/tmp/kubectl-proxy-unix4013026179/test'
    STEP: retrieving proxy /api/ output 01/18/23 16:09:53.487
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 16:09:53.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7015" for this suite. 01/18/23 16:09:53.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:09:53.516
Jan 18 16:09:53.516: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 16:09:53.518
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:09:53.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:09:53.549
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 01/18/23 16:09:53.561
Jan 18 16:09:53.583: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d83e532a-b2b7-455b-9702-368762e5a050" in namespace "projected-6446" to be "Succeeded or Failed"
Jan 18 16:09:53.592: INFO: Pod "downwardapi-volume-d83e532a-b2b7-455b-9702-368762e5a050": Phase="Pending", Reason="", readiness=false. Elapsed: 8.831329ms
Jan 18 16:09:55.599: INFO: Pod "downwardapi-volume-d83e532a-b2b7-455b-9702-368762e5a050": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015808324s
Jan 18 16:09:57.600: INFO: Pod "downwardapi-volume-d83e532a-b2b7-455b-9702-368762e5a050": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017157565s
STEP: Saw pod success 01/18/23 16:09:57.601
Jan 18 16:09:57.602: INFO: Pod "downwardapi-volume-d83e532a-b2b7-455b-9702-368762e5a050" satisfied condition "Succeeded or Failed"
Jan 18 16:09:57.607: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-d83e532a-b2b7-455b-9702-368762e5a050 container client-container: <nil>
STEP: delete the pod 01/18/23 16:09:57.619
Jan 18 16:09:57.645: INFO: Waiting for pod downwardapi-volume-d83e532a-b2b7-455b-9702-368762e5a050 to disappear
Jan 18 16:09:57.651: INFO: Pod downwardapi-volume-d83e532a-b2b7-455b-9702-368762e5a050 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 16:09:57.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6446" for this suite. 01/18/23 16:09:57.665
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":229,"skipped":4253,"failed":0}
------------------------------
• [4.159 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:09:53.516
    Jan 18 16:09:53.516: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 16:09:53.518
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:09:53.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:09:53.549
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 01/18/23 16:09:53.561
    Jan 18 16:09:53.583: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d83e532a-b2b7-455b-9702-368762e5a050" in namespace "projected-6446" to be "Succeeded or Failed"
    Jan 18 16:09:53.592: INFO: Pod "downwardapi-volume-d83e532a-b2b7-455b-9702-368762e5a050": Phase="Pending", Reason="", readiness=false. Elapsed: 8.831329ms
    Jan 18 16:09:55.599: INFO: Pod "downwardapi-volume-d83e532a-b2b7-455b-9702-368762e5a050": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015808324s
    Jan 18 16:09:57.600: INFO: Pod "downwardapi-volume-d83e532a-b2b7-455b-9702-368762e5a050": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017157565s
    STEP: Saw pod success 01/18/23 16:09:57.601
    Jan 18 16:09:57.602: INFO: Pod "downwardapi-volume-d83e532a-b2b7-455b-9702-368762e5a050" satisfied condition "Succeeded or Failed"
    Jan 18 16:09:57.607: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-d83e532a-b2b7-455b-9702-368762e5a050 container client-container: <nil>
    STEP: delete the pod 01/18/23 16:09:57.619
    Jan 18 16:09:57.645: INFO: Waiting for pod downwardapi-volume-d83e532a-b2b7-455b-9702-368762e5a050 to disappear
    Jan 18 16:09:57.651: INFO: Pod downwardapi-volume-d83e532a-b2b7-455b-9702-368762e5a050 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 16:09:57.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6446" for this suite. 01/18/23 16:09:57.665
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:09:57.684
Jan 18 16:09:57.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename conformance-tests 01/18/23 16:09:57.686
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:09:57.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:09:57.714
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 01/18/23 16:09:57.719
Jan 18 16:09:57.720: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Jan 18 16:09:57.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-3901" for this suite. 01/18/23 16:09:57.733
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":230,"skipped":4266,"failed":0}
------------------------------
• [0.061 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:09:57.684
    Jan 18 16:09:57.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename conformance-tests 01/18/23 16:09:57.686
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:09:57.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:09:57.714
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 01/18/23 16:09:57.719
    Jan 18 16:09:57.720: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Jan 18 16:09:57.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-3901" for this suite. 01/18/23 16:09:57.733
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:09:57.746
Jan 18 16:09:57.746: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename sysctl 01/18/23 16:09:57.747
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:09:57.785
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:09:57.791
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/18/23 16:09:57.797
STEP: Watching for error events or started pod 01/18/23 16:09:57.805
STEP: Waiting for pod completion 01/18/23 16:09:59.813
Jan 18 16:09:59.813: INFO: Waiting up to 3m0s for pod "sysctl-c32a14f7-c8d5-4a66-805b-f81df73eefad" in namespace "sysctl-7439" to be "completed"
Jan 18 16:09:59.818: INFO: Pod "sysctl-c32a14f7-c8d5-4a66-805b-f81df73eefad": Phase="Running", Reason="", readiness=true. Elapsed: 4.790916ms
Jan 18 16:10:01.824: INFO: Pod "sysctl-c32a14f7-c8d5-4a66-805b-f81df73eefad": Phase="Running", Reason="", readiness=false. Elapsed: 2.010115665s
Jan 18 16:10:03.824: INFO: Pod "sysctl-c32a14f7-c8d5-4a66-805b-f81df73eefad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011025711s
Jan 18 16:10:03.825: INFO: Pod "sysctl-c32a14f7-c8d5-4a66-805b-f81df73eefad" satisfied condition "completed"
STEP: Checking that the pod succeeded 01/18/23 16:10:03.84
STEP: Getting logs from the pod 01/18/23 16:10:03.841
STEP: Checking that the sysctl is actually updated 01/18/23 16:10:03.85
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 18 16:10:03.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-7439" for this suite. 01/18/23 16:10:03.857
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":231,"skipped":4270,"failed":0}
------------------------------
• [SLOW TEST] [6.132 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:09:57.746
    Jan 18 16:09:57.746: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename sysctl 01/18/23 16:09:57.747
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:09:57.785
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:09:57.791
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/18/23 16:09:57.797
    STEP: Watching for error events or started pod 01/18/23 16:09:57.805
    STEP: Waiting for pod completion 01/18/23 16:09:59.813
    Jan 18 16:09:59.813: INFO: Waiting up to 3m0s for pod "sysctl-c32a14f7-c8d5-4a66-805b-f81df73eefad" in namespace "sysctl-7439" to be "completed"
    Jan 18 16:09:59.818: INFO: Pod "sysctl-c32a14f7-c8d5-4a66-805b-f81df73eefad": Phase="Running", Reason="", readiness=true. Elapsed: 4.790916ms
    Jan 18 16:10:01.824: INFO: Pod "sysctl-c32a14f7-c8d5-4a66-805b-f81df73eefad": Phase="Running", Reason="", readiness=false. Elapsed: 2.010115665s
    Jan 18 16:10:03.824: INFO: Pod "sysctl-c32a14f7-c8d5-4a66-805b-f81df73eefad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011025711s
    Jan 18 16:10:03.825: INFO: Pod "sysctl-c32a14f7-c8d5-4a66-805b-f81df73eefad" satisfied condition "completed"
    STEP: Checking that the pod succeeded 01/18/23 16:10:03.84
    STEP: Getting logs from the pod 01/18/23 16:10:03.841
    STEP: Checking that the sysctl is actually updated 01/18/23 16:10:03.85
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 18 16:10:03.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-7439" for this suite. 01/18/23 16:10:03.857
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:10:03.883
Jan 18 16:10:03.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename endpointslice 01/18/23 16:10:03.884
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:10:03.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:10:03.915
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 01/18/23 16:10:09.066
STEP: referencing matching pods with named port 01/18/23 16:10:14.08
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/18/23 16:10:19.091
STEP: recreating EndpointSlices after they've been deleted 01/18/23 16:10:24.105
Jan 18 16:10:24.134: INFO: EndpointSlice for Service endpointslice-5896/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 18 16:10:34.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-5896" for this suite. 01/18/23 16:10:34.153
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":232,"skipped":4307,"failed":0}
------------------------------
• [SLOW TEST] [30.290 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:10:03.883
    Jan 18 16:10:03.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename endpointslice 01/18/23 16:10:03.884
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:10:03.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:10:03.915
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 01/18/23 16:10:09.066
    STEP: referencing matching pods with named port 01/18/23 16:10:14.08
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/18/23 16:10:19.091
    STEP: recreating EndpointSlices after they've been deleted 01/18/23 16:10:24.105
    Jan 18 16:10:24.134: INFO: EndpointSlice for Service endpointslice-5896/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 18 16:10:34.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-5896" for this suite. 01/18/23 16:10:34.153
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:10:34.18
Jan 18 16:10:34.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename secrets 01/18/23 16:10:34.182
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:10:34.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:10:34.211
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 01/18/23 16:10:34.216
STEP: listing secrets in all namespaces to ensure that there are more than zero 01/18/23 16:10:34.222
STEP: patching the secret 01/18/23 16:10:34.232
STEP: deleting the secret using a LabelSelector 01/18/23 16:10:34.248
STEP: listing secrets in all namespaces, searching for label name and value in patch 01/18/23 16:10:34.258
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 18 16:10:34.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7937" for this suite. 01/18/23 16:10:34.273
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":233,"skipped":4355,"failed":0}
------------------------------
• [0.103 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:10:34.18
    Jan 18 16:10:34.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename secrets 01/18/23 16:10:34.182
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:10:34.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:10:34.211
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 01/18/23 16:10:34.216
    STEP: listing secrets in all namespaces to ensure that there are more than zero 01/18/23 16:10:34.222
    STEP: patching the secret 01/18/23 16:10:34.232
    STEP: deleting the secret using a LabelSelector 01/18/23 16:10:34.248
    STEP: listing secrets in all namespaces, searching for label name and value in patch 01/18/23 16:10:34.258
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 16:10:34.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7937" for this suite. 01/18/23 16:10:34.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:10:34.291
Jan 18 16:10:34.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename svcaccounts 01/18/23 16:10:34.293
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:10:34.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:10:34.329
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Jan 18 16:10:34.354: INFO: Waiting up to 5m0s for pod "pod-service-account-b1140f91-158b-4e1a-ab09-496db4c80b1f" in namespace "svcaccounts-9224" to be "running"
Jan 18 16:10:34.359: INFO: Pod "pod-service-account-b1140f91-158b-4e1a-ab09-496db4c80b1f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.042594ms
Jan 18 16:10:36.364: INFO: Pod "pod-service-account-b1140f91-158b-4e1a-ab09-496db4c80b1f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009904227s
Jan 18 16:10:36.364: INFO: Pod "pod-service-account-b1140f91-158b-4e1a-ab09-496db4c80b1f" satisfied condition "running"
STEP: reading a file in the container 01/18/23 16:10:36.364
Jan 18 16:10:36.364: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9224 pod-service-account-b1140f91-158b-4e1a-ab09-496db4c80b1f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 01/18/23 16:10:36.577
Jan 18 16:10:36.577: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9224 pod-service-account-b1140f91-158b-4e1a-ab09-496db4c80b1f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 01/18/23 16:10:36.77
Jan 18 16:10:36.770: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9224 pod-service-account-b1140f91-158b-4e1a-ab09-496db4c80b1f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan 18 16:10:36.977: INFO: Got root ca configmap in namespace "svcaccounts-9224"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 18 16:10:36.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9224" for this suite. 01/18/23 16:10:36.986
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":234,"skipped":4380,"failed":0}
------------------------------
• [2.703 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:10:34.291
    Jan 18 16:10:34.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 16:10:34.293
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:10:34.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:10:34.329
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Jan 18 16:10:34.354: INFO: Waiting up to 5m0s for pod "pod-service-account-b1140f91-158b-4e1a-ab09-496db4c80b1f" in namespace "svcaccounts-9224" to be "running"
    Jan 18 16:10:34.359: INFO: Pod "pod-service-account-b1140f91-158b-4e1a-ab09-496db4c80b1f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.042594ms
    Jan 18 16:10:36.364: INFO: Pod "pod-service-account-b1140f91-158b-4e1a-ab09-496db4c80b1f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009904227s
    Jan 18 16:10:36.364: INFO: Pod "pod-service-account-b1140f91-158b-4e1a-ab09-496db4c80b1f" satisfied condition "running"
    STEP: reading a file in the container 01/18/23 16:10:36.364
    Jan 18 16:10:36.364: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9224 pod-service-account-b1140f91-158b-4e1a-ab09-496db4c80b1f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 01/18/23 16:10:36.577
    Jan 18 16:10:36.577: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9224 pod-service-account-b1140f91-158b-4e1a-ab09-496db4c80b1f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 01/18/23 16:10:36.77
    Jan 18 16:10:36.770: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9224 pod-service-account-b1140f91-158b-4e1a-ab09-496db4c80b1f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jan 18 16:10:36.977: INFO: Got root ca configmap in namespace "svcaccounts-9224"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 18 16:10:36.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-9224" for this suite. 01/18/23 16:10:36.986
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:10:36.998
Jan 18 16:10:36.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename subpath 01/18/23 16:10:36.999
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:10:37.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:10:37.047
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/18/23 16:10:37.052
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-q4vw 01/18/23 16:10:37.064
STEP: Creating a pod to test atomic-volume-subpath 01/18/23 16:10:37.064
Jan 18 16:10:37.090: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-q4vw" in namespace "subpath-4137" to be "Succeeded or Failed"
Jan 18 16:10:37.101: INFO: Pod "pod-subpath-test-projected-q4vw": Phase="Pending", Reason="", readiness=false. Elapsed: 10.66385ms
Jan 18 16:10:39.109: INFO: Pod "pod-subpath-test-projected-q4vw": Phase="Running", Reason="", readiness=true. Elapsed: 2.018979239s
Jan 18 16:10:41.111: INFO: Pod "pod-subpath-test-projected-q4vw": Phase="Running", Reason="", readiness=true. Elapsed: 4.02080622s
Jan 18 16:10:43.110: INFO: Pod "pod-subpath-test-projected-q4vw": Phase="Running", Reason="", readiness=true. Elapsed: 6.020010156s
Jan 18 16:10:45.117: INFO: Pod "pod-subpath-test-projected-q4vw": Phase="Running", Reason="", readiness=true. Elapsed: 8.026369939s
Jan 18 16:10:47.109: INFO: Pod "pod-subpath-test-projected-q4vw": Phase="Running", Reason="", readiness=true. Elapsed: 10.019199933s
Jan 18 16:10:49.110: INFO: Pod "pod-subpath-test-projected-q4vw": Phase="Running", Reason="", readiness=true. Elapsed: 12.019562783s
Jan 18 16:10:51.113: INFO: Pod "pod-subpath-test-projected-q4vw": Phase="Running", Reason="", readiness=true. Elapsed: 14.022793686s
Jan 18 16:10:53.110: INFO: Pod "pod-subpath-test-projected-q4vw": Phase="Running", Reason="", readiness=true. Elapsed: 16.01941316s
Jan 18 16:10:55.110: INFO: Pod "pod-subpath-test-projected-q4vw": Phase="Running", Reason="", readiness=true. Elapsed: 18.020159468s
Jan 18 16:10:57.110: INFO: Pod "pod-subpath-test-projected-q4vw": Phase="Running", Reason="", readiness=true. Elapsed: 20.019954748s
Jan 18 16:10:59.112: INFO: Pod "pod-subpath-test-projected-q4vw": Phase="Running", Reason="", readiness=false. Elapsed: 22.021490296s
Jan 18 16:11:01.111: INFO: Pod "pod-subpath-test-projected-q4vw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.020438403s
STEP: Saw pod success 01/18/23 16:11:01.111
Jan 18 16:11:01.111: INFO: Pod "pod-subpath-test-projected-q4vw" satisfied condition "Succeeded or Failed"
Jan 18 16:11:01.117: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-subpath-test-projected-q4vw container test-container-subpath-projected-q4vw: <nil>
STEP: delete the pod 01/18/23 16:11:01.126
Jan 18 16:11:01.149: INFO: Waiting for pod pod-subpath-test-projected-q4vw to disappear
Jan 18 16:11:01.154: INFO: Pod pod-subpath-test-projected-q4vw no longer exists
STEP: Deleting pod pod-subpath-test-projected-q4vw 01/18/23 16:11:01.155
Jan 18 16:11:01.155: INFO: Deleting pod "pod-subpath-test-projected-q4vw" in namespace "subpath-4137"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 18 16:11:01.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4137" for this suite. 01/18/23 16:11:01.164
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":235,"skipped":4409,"failed":0}
------------------------------
• [SLOW TEST] [24.180 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:10:36.998
    Jan 18 16:10:36.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename subpath 01/18/23 16:10:36.999
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:10:37.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:10:37.047
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/18/23 16:10:37.052
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-q4vw 01/18/23 16:10:37.064
    STEP: Creating a pod to test atomic-volume-subpath 01/18/23 16:10:37.064
    Jan 18 16:10:37.090: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-q4vw" in namespace "subpath-4137" to be "Succeeded or Failed"
    Jan 18 16:10:37.101: INFO: Pod "pod-subpath-test-projected-q4vw": Phase="Pending", Reason="", readiness=false. Elapsed: 10.66385ms
    Jan 18 16:10:39.109: INFO: Pod "pod-subpath-test-projected-q4vw": Phase="Running", Reason="", readiness=true. Elapsed: 2.018979239s
    Jan 18 16:10:41.111: INFO: Pod "pod-subpath-test-projected-q4vw": Phase="Running", Reason="", readiness=true. Elapsed: 4.02080622s
    Jan 18 16:10:43.110: INFO: Pod "pod-subpath-test-projected-q4vw": Phase="Running", Reason="", readiness=true. Elapsed: 6.020010156s
    Jan 18 16:10:45.117: INFO: Pod "pod-subpath-test-projected-q4vw": Phase="Running", Reason="", readiness=true. Elapsed: 8.026369939s
    Jan 18 16:10:47.109: INFO: Pod "pod-subpath-test-projected-q4vw": Phase="Running", Reason="", readiness=true. Elapsed: 10.019199933s
    Jan 18 16:10:49.110: INFO: Pod "pod-subpath-test-projected-q4vw": Phase="Running", Reason="", readiness=true. Elapsed: 12.019562783s
    Jan 18 16:10:51.113: INFO: Pod "pod-subpath-test-projected-q4vw": Phase="Running", Reason="", readiness=true. Elapsed: 14.022793686s
    Jan 18 16:10:53.110: INFO: Pod "pod-subpath-test-projected-q4vw": Phase="Running", Reason="", readiness=true. Elapsed: 16.01941316s
    Jan 18 16:10:55.110: INFO: Pod "pod-subpath-test-projected-q4vw": Phase="Running", Reason="", readiness=true. Elapsed: 18.020159468s
    Jan 18 16:10:57.110: INFO: Pod "pod-subpath-test-projected-q4vw": Phase="Running", Reason="", readiness=true. Elapsed: 20.019954748s
    Jan 18 16:10:59.112: INFO: Pod "pod-subpath-test-projected-q4vw": Phase="Running", Reason="", readiness=false. Elapsed: 22.021490296s
    Jan 18 16:11:01.111: INFO: Pod "pod-subpath-test-projected-q4vw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.020438403s
    STEP: Saw pod success 01/18/23 16:11:01.111
    Jan 18 16:11:01.111: INFO: Pod "pod-subpath-test-projected-q4vw" satisfied condition "Succeeded or Failed"
    Jan 18 16:11:01.117: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-subpath-test-projected-q4vw container test-container-subpath-projected-q4vw: <nil>
    STEP: delete the pod 01/18/23 16:11:01.126
    Jan 18 16:11:01.149: INFO: Waiting for pod pod-subpath-test-projected-q4vw to disappear
    Jan 18 16:11:01.154: INFO: Pod pod-subpath-test-projected-q4vw no longer exists
    STEP: Deleting pod pod-subpath-test-projected-q4vw 01/18/23 16:11:01.155
    Jan 18 16:11:01.155: INFO: Deleting pod "pod-subpath-test-projected-q4vw" in namespace "subpath-4137"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 18 16:11:01.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-4137" for this suite. 01/18/23 16:11:01.164
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:11:01.182
Jan 18 16:11:01.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename watch 01/18/23 16:11:01.185
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:11:01.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:11:01.219
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 01/18/23 16:11:01.224
STEP: creating a watch on configmaps with label B 01/18/23 16:11:01.227
STEP: creating a watch on configmaps with label A or B 01/18/23 16:11:01.229
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/18/23 16:11:01.235
Jan 18 16:11:01.242: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1273  49564751-8e6e-4b51-af8c-eb65448f5a8e 55687 0 2023-01-18 16:11:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 16:11:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 16:11:01.243: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1273  49564751-8e6e-4b51-af8c-eb65448f5a8e 55687 0 2023-01-18 16:11:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 16:11:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/18/23 16:11:01.243
Jan 18 16:11:01.255: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1273  49564751-8e6e-4b51-af8c-eb65448f5a8e 55688 0 2023-01-18 16:11:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 16:11:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 16:11:01.256: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1273  49564751-8e6e-4b51-af8c-eb65448f5a8e 55688 0 2023-01-18 16:11:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 16:11:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/18/23 16:11:01.256
Jan 18 16:11:01.268: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1273  49564751-8e6e-4b51-af8c-eb65448f5a8e 55689 0 2023-01-18 16:11:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 16:11:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 16:11:01.269: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1273  49564751-8e6e-4b51-af8c-eb65448f5a8e 55689 0 2023-01-18 16:11:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 16:11:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/18/23 16:11:01.269
Jan 18 16:11:01.276: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1273  49564751-8e6e-4b51-af8c-eb65448f5a8e 55690 0 2023-01-18 16:11:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 16:11:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 16:11:01.277: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1273  49564751-8e6e-4b51-af8c-eb65448f5a8e 55690 0 2023-01-18 16:11:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 16:11:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/18/23 16:11:01.277
Jan 18 16:11:01.285: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1273  15ea6edc-c412-48ef-b52a-d034882dbeeb 55691 0 2023-01-18 16:11:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 16:11:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 16:11:01.285: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1273  15ea6edc-c412-48ef-b52a-d034882dbeeb 55691 0 2023-01-18 16:11:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 16:11:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/18/23 16:11:11.287
Jan 18 16:11:11.297: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1273  15ea6edc-c412-48ef-b52a-d034882dbeeb 55739 0 2023-01-18 16:11:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 16:11:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 16:11:11.297: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1273  15ea6edc-c412-48ef-b52a-d034882dbeeb 55739 0 2023-01-18 16:11:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 16:11:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 18 16:11:21.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1273" for this suite. 01/18/23 16:11:21.308
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":236,"skipped":4422,"failed":0}
------------------------------
• [SLOW TEST] [20.137 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:11:01.182
    Jan 18 16:11:01.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename watch 01/18/23 16:11:01.185
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:11:01.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:11:01.219
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 01/18/23 16:11:01.224
    STEP: creating a watch on configmaps with label B 01/18/23 16:11:01.227
    STEP: creating a watch on configmaps with label A or B 01/18/23 16:11:01.229
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/18/23 16:11:01.235
    Jan 18 16:11:01.242: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1273  49564751-8e6e-4b51-af8c-eb65448f5a8e 55687 0 2023-01-18 16:11:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 16:11:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 16:11:01.243: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1273  49564751-8e6e-4b51-af8c-eb65448f5a8e 55687 0 2023-01-18 16:11:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 16:11:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/18/23 16:11:01.243
    Jan 18 16:11:01.255: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1273  49564751-8e6e-4b51-af8c-eb65448f5a8e 55688 0 2023-01-18 16:11:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 16:11:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 16:11:01.256: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1273  49564751-8e6e-4b51-af8c-eb65448f5a8e 55688 0 2023-01-18 16:11:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 16:11:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/18/23 16:11:01.256
    Jan 18 16:11:01.268: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1273  49564751-8e6e-4b51-af8c-eb65448f5a8e 55689 0 2023-01-18 16:11:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 16:11:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 16:11:01.269: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1273  49564751-8e6e-4b51-af8c-eb65448f5a8e 55689 0 2023-01-18 16:11:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 16:11:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/18/23 16:11:01.269
    Jan 18 16:11:01.276: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1273  49564751-8e6e-4b51-af8c-eb65448f5a8e 55690 0 2023-01-18 16:11:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 16:11:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 16:11:01.277: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1273  49564751-8e6e-4b51-af8c-eb65448f5a8e 55690 0 2023-01-18 16:11:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 16:11:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/18/23 16:11:01.277
    Jan 18 16:11:01.285: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1273  15ea6edc-c412-48ef-b52a-d034882dbeeb 55691 0 2023-01-18 16:11:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 16:11:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 16:11:01.285: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1273  15ea6edc-c412-48ef-b52a-d034882dbeeb 55691 0 2023-01-18 16:11:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 16:11:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/18/23 16:11:11.287
    Jan 18 16:11:11.297: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1273  15ea6edc-c412-48ef-b52a-d034882dbeeb 55739 0 2023-01-18 16:11:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 16:11:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 16:11:11.297: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1273  15ea6edc-c412-48ef-b52a-d034882dbeeb 55739 0 2023-01-18 16:11:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 16:11:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 18 16:11:21.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-1273" for this suite. 01/18/23 16:11:21.308
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:11:21.328
Jan 18 16:11:21.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 16:11:21.33
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:11:21.356
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:11:21.361
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-d704eb13-f22d-44ed-bf15-e5e77f19a4ed 01/18/23 16:11:21.372
STEP: Creating a pod to test consume configMaps 01/18/23 16:11:21.378
Jan 18 16:11:21.387: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fe93e4cd-4776-4347-9619-f9020245cb37" in namespace "projected-1534" to be "Succeeded or Failed"
Jan 18 16:11:21.399: INFO: Pod "pod-projected-configmaps-fe93e4cd-4776-4347-9619-f9020245cb37": Phase="Pending", Reason="", readiness=false. Elapsed: 11.537069ms
Jan 18 16:11:23.405: INFO: Pod "pod-projected-configmaps-fe93e4cd-4776-4347-9619-f9020245cb37": Phase="Running", Reason="", readiness=true. Elapsed: 2.017791462s
Jan 18 16:11:25.405: INFO: Pod "pod-projected-configmaps-fe93e4cd-4776-4347-9619-f9020245cb37": Phase="Running", Reason="", readiness=false. Elapsed: 4.01729375s
Jan 18 16:11:27.411: INFO: Pod "pod-projected-configmaps-fe93e4cd-4776-4347-9619-f9020245cb37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023267682s
STEP: Saw pod success 01/18/23 16:11:27.411
Jan 18 16:11:27.411: INFO: Pod "pod-projected-configmaps-fe93e4cd-4776-4347-9619-f9020245cb37" satisfied condition "Succeeded or Failed"
Jan 18 16:11:27.423: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-fe93e4cd-4776-4347-9619-f9020245cb37 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 16:11:27.438
Jan 18 16:11:27.471: INFO: Waiting for pod pod-projected-configmaps-fe93e4cd-4776-4347-9619-f9020245cb37 to disappear
Jan 18 16:11:27.477: INFO: Pod pod-projected-configmaps-fe93e4cd-4776-4347-9619-f9020245cb37 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 18 16:11:27.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1534" for this suite. 01/18/23 16:11:27.491
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":237,"skipped":4458,"failed":0}
------------------------------
• [SLOW TEST] [6.181 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:11:21.328
    Jan 18 16:11:21.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 16:11:21.33
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:11:21.356
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:11:21.361
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-d704eb13-f22d-44ed-bf15-e5e77f19a4ed 01/18/23 16:11:21.372
    STEP: Creating a pod to test consume configMaps 01/18/23 16:11:21.378
    Jan 18 16:11:21.387: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fe93e4cd-4776-4347-9619-f9020245cb37" in namespace "projected-1534" to be "Succeeded or Failed"
    Jan 18 16:11:21.399: INFO: Pod "pod-projected-configmaps-fe93e4cd-4776-4347-9619-f9020245cb37": Phase="Pending", Reason="", readiness=false. Elapsed: 11.537069ms
    Jan 18 16:11:23.405: INFO: Pod "pod-projected-configmaps-fe93e4cd-4776-4347-9619-f9020245cb37": Phase="Running", Reason="", readiness=true. Elapsed: 2.017791462s
    Jan 18 16:11:25.405: INFO: Pod "pod-projected-configmaps-fe93e4cd-4776-4347-9619-f9020245cb37": Phase="Running", Reason="", readiness=false. Elapsed: 4.01729375s
    Jan 18 16:11:27.411: INFO: Pod "pod-projected-configmaps-fe93e4cd-4776-4347-9619-f9020245cb37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023267682s
    STEP: Saw pod success 01/18/23 16:11:27.411
    Jan 18 16:11:27.411: INFO: Pod "pod-projected-configmaps-fe93e4cd-4776-4347-9619-f9020245cb37" satisfied condition "Succeeded or Failed"
    Jan 18 16:11:27.423: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-fe93e4cd-4776-4347-9619-f9020245cb37 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 16:11:27.438
    Jan 18 16:11:27.471: INFO: Waiting for pod pod-projected-configmaps-fe93e4cd-4776-4347-9619-f9020245cb37 to disappear
    Jan 18 16:11:27.477: INFO: Pod pod-projected-configmaps-fe93e4cd-4776-4347-9619-f9020245cb37 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 18 16:11:27.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1534" for this suite. 01/18/23 16:11:27.491
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:11:27.512
Jan 18 16:11:27.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename kubectl 01/18/23 16:11:27.513
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:11:27.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:11:27.551
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 01/18/23 16:11:27.557
Jan 18 16:11:27.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 create -f -'
Jan 18 16:11:27.976: INFO: stderr: ""
Jan 18 16:11:27.976: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 16:11:27.976
Jan 18 16:11:27.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 16:11:28.147: INFO: stderr: ""
Jan 18 16:11:28.147: INFO: stdout: "update-demo-nautilus-jww5d update-demo-nautilus-zvwf9 "
Jan 18 16:11:28.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods update-demo-nautilus-jww5d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 16:11:28.305: INFO: stderr: ""
Jan 18 16:11:28.305: INFO: stdout: ""
Jan 18 16:11:28.305: INFO: update-demo-nautilus-jww5d is created but not running
Jan 18 16:11:33.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 16:11:33.434: INFO: stderr: ""
Jan 18 16:11:33.434: INFO: stdout: "update-demo-nautilus-jww5d update-demo-nautilus-zvwf9 "
Jan 18 16:11:33.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods update-demo-nautilus-jww5d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 16:11:33.544: INFO: stderr: ""
Jan 18 16:11:33.544: INFO: stdout: "true"
Jan 18 16:11:33.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods update-demo-nautilus-jww5d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 16:11:33.661: INFO: stderr: ""
Jan 18 16:11:33.661: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 18 16:11:33.661: INFO: validating pod update-demo-nautilus-jww5d
Jan 18 16:11:33.673: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 16:11:33.673: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 16:11:33.673: INFO: update-demo-nautilus-jww5d is verified up and running
Jan 18 16:11:33.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods update-demo-nautilus-zvwf9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 16:11:33.817: INFO: stderr: ""
Jan 18 16:11:33.817: INFO: stdout: "true"
Jan 18 16:11:33.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods update-demo-nautilus-zvwf9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 16:11:33.955: INFO: stderr: ""
Jan 18 16:11:33.955: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 18 16:11:33.955: INFO: validating pod update-demo-nautilus-zvwf9
Jan 18 16:11:33.970: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 16:11:33.970: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 16:11:33.970: INFO: update-demo-nautilus-zvwf9 is verified up and running
STEP: scaling down the replication controller 01/18/23 16:11:33.971
Jan 18 16:11:33.973: INFO: scanned /root for discovery docs: <nil>
Jan 18 16:11:33.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan 18 16:11:35.129: INFO: stderr: ""
Jan 18 16:11:35.129: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 16:11:35.13
Jan 18 16:11:35.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 16:11:35.244: INFO: stderr: ""
Jan 18 16:11:35.244: INFO: stdout: "update-demo-nautilus-jww5d update-demo-nautilus-zvwf9 "
STEP: Replicas for name=update-demo: expected=1 actual=2 01/18/23 16:11:35.244
Jan 18 16:11:40.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 16:11:40.349: INFO: stderr: ""
Jan 18 16:11:40.349: INFO: stdout: "update-demo-nautilus-zvwf9 "
Jan 18 16:11:40.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods update-demo-nautilus-zvwf9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 16:11:40.454: INFO: stderr: ""
Jan 18 16:11:40.454: INFO: stdout: "true"
Jan 18 16:11:40.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods update-demo-nautilus-zvwf9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 16:11:40.571: INFO: stderr: ""
Jan 18 16:11:40.571: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 18 16:11:40.571: INFO: validating pod update-demo-nautilus-zvwf9
Jan 18 16:11:40.581: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 16:11:40.581: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 16:11:40.581: INFO: update-demo-nautilus-zvwf9 is verified up and running
STEP: scaling up the replication controller 01/18/23 16:11:40.581
Jan 18 16:11:40.584: INFO: scanned /root for discovery docs: <nil>
Jan 18 16:11:40.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan 18 16:11:41.746: INFO: stderr: ""
Jan 18 16:11:41.746: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 16:11:41.746
Jan 18 16:11:41.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 16:11:41.918: INFO: stderr: ""
Jan 18 16:11:41.918: INFO: stdout: "update-demo-nautilus-8bdtd update-demo-nautilus-zvwf9 "
Jan 18 16:11:41.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods update-demo-nautilus-8bdtd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 16:11:42.031: INFO: stderr: ""
Jan 18 16:11:42.031: INFO: stdout: ""
Jan 18 16:11:42.031: INFO: update-demo-nautilus-8bdtd is created but not running
Jan 18 16:11:47.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 16:11:47.148: INFO: stderr: ""
Jan 18 16:11:47.148: INFO: stdout: "update-demo-nautilus-8bdtd update-demo-nautilus-zvwf9 "
Jan 18 16:11:47.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods update-demo-nautilus-8bdtd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 16:11:47.259: INFO: stderr: ""
Jan 18 16:11:47.259: INFO: stdout: "true"
Jan 18 16:11:47.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods update-demo-nautilus-8bdtd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 16:11:47.374: INFO: stderr: ""
Jan 18 16:11:47.374: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 18 16:11:47.374: INFO: validating pod update-demo-nautilus-8bdtd
Jan 18 16:11:47.382: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 16:11:47.382: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 16:11:47.382: INFO: update-demo-nautilus-8bdtd is verified up and running
Jan 18 16:11:47.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods update-demo-nautilus-zvwf9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 16:11:47.494: INFO: stderr: ""
Jan 18 16:11:47.494: INFO: stdout: "true"
Jan 18 16:11:47.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods update-demo-nautilus-zvwf9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 16:11:47.603: INFO: stderr: ""
Jan 18 16:11:47.604: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 18 16:11:47.604: INFO: validating pod update-demo-nautilus-zvwf9
Jan 18 16:11:47.610: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 16:11:47.610: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 16:11:47.610: INFO: update-demo-nautilus-zvwf9 is verified up and running
STEP: using delete to clean up resources 01/18/23 16:11:47.61
Jan 18 16:11:47.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 delete --grace-period=0 --force -f -'
Jan 18 16:11:47.752: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 16:11:47.752: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 18 16:11:47.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get rc,svc -l name=update-demo --no-headers'
Jan 18 16:11:48.050: INFO: stderr: "No resources found in kubectl-2556 namespace.\n"
Jan 18 16:11:48.050: INFO: stdout: ""
Jan 18 16:11:48.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 18 16:11:48.227: INFO: stderr: ""
Jan 18 16:11:48.227: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 16:11:48.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2556" for this suite. 01/18/23 16:11:48.234
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":238,"skipped":4493,"failed":0}
------------------------------
• [SLOW TEST] [20.730 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:11:27.512
    Jan 18 16:11:27.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename kubectl 01/18/23 16:11:27.513
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:11:27.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:11:27.551
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 01/18/23 16:11:27.557
    Jan 18 16:11:27.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 create -f -'
    Jan 18 16:11:27.976: INFO: stderr: ""
    Jan 18 16:11:27.976: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 16:11:27.976
    Jan 18 16:11:27.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 16:11:28.147: INFO: stderr: ""
    Jan 18 16:11:28.147: INFO: stdout: "update-demo-nautilus-jww5d update-demo-nautilus-zvwf9 "
    Jan 18 16:11:28.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods update-demo-nautilus-jww5d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 16:11:28.305: INFO: stderr: ""
    Jan 18 16:11:28.305: INFO: stdout: ""
    Jan 18 16:11:28.305: INFO: update-demo-nautilus-jww5d is created but not running
    Jan 18 16:11:33.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 16:11:33.434: INFO: stderr: ""
    Jan 18 16:11:33.434: INFO: stdout: "update-demo-nautilus-jww5d update-demo-nautilus-zvwf9 "
    Jan 18 16:11:33.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods update-demo-nautilus-jww5d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 16:11:33.544: INFO: stderr: ""
    Jan 18 16:11:33.544: INFO: stdout: "true"
    Jan 18 16:11:33.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods update-demo-nautilus-jww5d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 16:11:33.661: INFO: stderr: ""
    Jan 18 16:11:33.661: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 18 16:11:33.661: INFO: validating pod update-demo-nautilus-jww5d
    Jan 18 16:11:33.673: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 16:11:33.673: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 16:11:33.673: INFO: update-demo-nautilus-jww5d is verified up and running
    Jan 18 16:11:33.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods update-demo-nautilus-zvwf9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 16:11:33.817: INFO: stderr: ""
    Jan 18 16:11:33.817: INFO: stdout: "true"
    Jan 18 16:11:33.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods update-demo-nautilus-zvwf9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 16:11:33.955: INFO: stderr: ""
    Jan 18 16:11:33.955: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 18 16:11:33.955: INFO: validating pod update-demo-nautilus-zvwf9
    Jan 18 16:11:33.970: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 16:11:33.970: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 16:11:33.970: INFO: update-demo-nautilus-zvwf9 is verified up and running
    STEP: scaling down the replication controller 01/18/23 16:11:33.971
    Jan 18 16:11:33.973: INFO: scanned /root for discovery docs: <nil>
    Jan 18 16:11:33.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jan 18 16:11:35.129: INFO: stderr: ""
    Jan 18 16:11:35.129: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 16:11:35.13
    Jan 18 16:11:35.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 16:11:35.244: INFO: stderr: ""
    Jan 18 16:11:35.244: INFO: stdout: "update-demo-nautilus-jww5d update-demo-nautilus-zvwf9 "
    STEP: Replicas for name=update-demo: expected=1 actual=2 01/18/23 16:11:35.244
    Jan 18 16:11:40.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 16:11:40.349: INFO: stderr: ""
    Jan 18 16:11:40.349: INFO: stdout: "update-demo-nautilus-zvwf9 "
    Jan 18 16:11:40.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods update-demo-nautilus-zvwf9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 16:11:40.454: INFO: stderr: ""
    Jan 18 16:11:40.454: INFO: stdout: "true"
    Jan 18 16:11:40.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods update-demo-nautilus-zvwf9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 16:11:40.571: INFO: stderr: ""
    Jan 18 16:11:40.571: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 18 16:11:40.571: INFO: validating pod update-demo-nautilus-zvwf9
    Jan 18 16:11:40.581: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 16:11:40.581: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 16:11:40.581: INFO: update-demo-nautilus-zvwf9 is verified up and running
    STEP: scaling up the replication controller 01/18/23 16:11:40.581
    Jan 18 16:11:40.584: INFO: scanned /root for discovery docs: <nil>
    Jan 18 16:11:40.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jan 18 16:11:41.746: INFO: stderr: ""
    Jan 18 16:11:41.746: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 16:11:41.746
    Jan 18 16:11:41.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 16:11:41.918: INFO: stderr: ""
    Jan 18 16:11:41.918: INFO: stdout: "update-demo-nautilus-8bdtd update-demo-nautilus-zvwf9 "
    Jan 18 16:11:41.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods update-demo-nautilus-8bdtd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 16:11:42.031: INFO: stderr: ""
    Jan 18 16:11:42.031: INFO: stdout: ""
    Jan 18 16:11:42.031: INFO: update-demo-nautilus-8bdtd is created but not running
    Jan 18 16:11:47.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 16:11:47.148: INFO: stderr: ""
    Jan 18 16:11:47.148: INFO: stdout: "update-demo-nautilus-8bdtd update-demo-nautilus-zvwf9 "
    Jan 18 16:11:47.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods update-demo-nautilus-8bdtd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 16:11:47.259: INFO: stderr: ""
    Jan 18 16:11:47.259: INFO: stdout: "true"
    Jan 18 16:11:47.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods update-demo-nautilus-8bdtd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 16:11:47.374: INFO: stderr: ""
    Jan 18 16:11:47.374: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 18 16:11:47.374: INFO: validating pod update-demo-nautilus-8bdtd
    Jan 18 16:11:47.382: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 16:11:47.382: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 16:11:47.382: INFO: update-demo-nautilus-8bdtd is verified up and running
    Jan 18 16:11:47.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods update-demo-nautilus-zvwf9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 16:11:47.494: INFO: stderr: ""
    Jan 18 16:11:47.494: INFO: stdout: "true"
    Jan 18 16:11:47.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods update-demo-nautilus-zvwf9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 16:11:47.603: INFO: stderr: ""
    Jan 18 16:11:47.604: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 18 16:11:47.604: INFO: validating pod update-demo-nautilus-zvwf9
    Jan 18 16:11:47.610: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 16:11:47.610: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 16:11:47.610: INFO: update-demo-nautilus-zvwf9 is verified up and running
    STEP: using delete to clean up resources 01/18/23 16:11:47.61
    Jan 18 16:11:47.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 delete --grace-period=0 --force -f -'
    Jan 18 16:11:47.752: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 16:11:47.752: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 18 16:11:47.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get rc,svc -l name=update-demo --no-headers'
    Jan 18 16:11:48.050: INFO: stderr: "No resources found in kubectl-2556 namespace.\n"
    Jan 18 16:11:48.050: INFO: stdout: ""
    Jan 18 16:11:48.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-2556 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 18 16:11:48.227: INFO: stderr: ""
    Jan 18 16:11:48.227: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 16:11:48.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2556" for this suite. 01/18/23 16:11:48.234
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:11:48.244
Jan 18 16:11:48.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename proxy 01/18/23 16:11:48.251
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:11:48.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:11:48.33
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jan 18 16:11:48.370: INFO: Creating pod...
Jan 18 16:11:48.389: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-1091" to be "running"
Jan 18 16:11:48.392: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.257741ms
Jan 18 16:11:50.399: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.010076995s
Jan 18 16:11:50.399: INFO: Pod "agnhost" satisfied condition "running"
Jan 18 16:11:50.399: INFO: Creating service...
Jan 18 16:11:50.427: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/pods/agnhost/proxy?method=DELETE
Jan 18 16:11:50.447: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 18 16:11:50.447: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/pods/agnhost/proxy?method=OPTIONS
Jan 18 16:11:50.454: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 18 16:11:50.454: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/pods/agnhost/proxy?method=PATCH
Jan 18 16:11:50.462: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 18 16:11:50.462: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/pods/agnhost/proxy?method=POST
Jan 18 16:11:50.467: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 18 16:11:50.467: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/pods/agnhost/proxy?method=PUT
Jan 18 16:11:50.472: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 18 16:11:50.472: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/services/e2e-proxy-test-service/proxy?method=DELETE
Jan 18 16:11:50.482: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 18 16:11:50.483: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan 18 16:11:50.492: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 18 16:11:50.492: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/services/e2e-proxy-test-service/proxy?method=PATCH
Jan 18 16:11:50.503: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 18 16:11:50.503: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/services/e2e-proxy-test-service/proxy?method=POST
Jan 18 16:11:50.526: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 18 16:11:50.526: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/services/e2e-proxy-test-service/proxy?method=PUT
Jan 18 16:11:50.535: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 18 16:11:50.536: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/pods/agnhost/proxy?method=GET
Jan 18 16:11:50.543: INFO: http.Client request:GET StatusCode:301
Jan 18 16:11:50.543: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/services/e2e-proxy-test-service/proxy?method=GET
Jan 18 16:11:50.554: INFO: http.Client request:GET StatusCode:301
Jan 18 16:11:50.555: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/pods/agnhost/proxy?method=HEAD
Jan 18 16:11:50.560: INFO: http.Client request:HEAD StatusCode:301
Jan 18 16:11:50.561: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/services/e2e-proxy-test-service/proxy?method=HEAD
Jan 18 16:11:50.572: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan 18 16:11:50.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1091" for this suite. 01/18/23 16:11:50.584
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":239,"skipped":4537,"failed":0}
------------------------------
• [2.348 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:11:48.244
    Jan 18 16:11:48.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename proxy 01/18/23 16:11:48.251
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:11:48.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:11:48.33
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jan 18 16:11:48.370: INFO: Creating pod...
    Jan 18 16:11:48.389: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-1091" to be "running"
    Jan 18 16:11:48.392: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.257741ms
    Jan 18 16:11:50.399: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.010076995s
    Jan 18 16:11:50.399: INFO: Pod "agnhost" satisfied condition "running"
    Jan 18 16:11:50.399: INFO: Creating service...
    Jan 18 16:11:50.427: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/pods/agnhost/proxy?method=DELETE
    Jan 18 16:11:50.447: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 18 16:11:50.447: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/pods/agnhost/proxy?method=OPTIONS
    Jan 18 16:11:50.454: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 18 16:11:50.454: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/pods/agnhost/proxy?method=PATCH
    Jan 18 16:11:50.462: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 18 16:11:50.462: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/pods/agnhost/proxy?method=POST
    Jan 18 16:11:50.467: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 18 16:11:50.467: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/pods/agnhost/proxy?method=PUT
    Jan 18 16:11:50.472: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 18 16:11:50.472: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/services/e2e-proxy-test-service/proxy?method=DELETE
    Jan 18 16:11:50.482: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 18 16:11:50.483: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jan 18 16:11:50.492: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 18 16:11:50.492: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/services/e2e-proxy-test-service/proxy?method=PATCH
    Jan 18 16:11:50.503: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 18 16:11:50.503: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/services/e2e-proxy-test-service/proxy?method=POST
    Jan 18 16:11:50.526: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 18 16:11:50.526: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/services/e2e-proxy-test-service/proxy?method=PUT
    Jan 18 16:11:50.535: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 18 16:11:50.536: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/pods/agnhost/proxy?method=GET
    Jan 18 16:11:50.543: INFO: http.Client request:GET StatusCode:301
    Jan 18 16:11:50.543: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/services/e2e-proxy-test-service/proxy?method=GET
    Jan 18 16:11:50.554: INFO: http.Client request:GET StatusCode:301
    Jan 18 16:11:50.555: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/pods/agnhost/proxy?method=HEAD
    Jan 18 16:11:50.560: INFO: http.Client request:HEAD StatusCode:301
    Jan 18 16:11:50.561: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1091/services/e2e-proxy-test-service/proxy?method=HEAD
    Jan 18 16:11:50.572: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan 18 16:11:50.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-1091" for this suite. 01/18/23 16:11:50.584
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:11:50.593
Jan 18 16:11:50.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename webhook 01/18/23 16:11:50.596
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:11:50.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:11:50.633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 16:11:50.661
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 16:11:51.258
STEP: Deploying the webhook pod 01/18/23 16:11:51.267
STEP: Wait for the deployment to be ready 01/18/23 16:11:51.281
Jan 18 16:11:51.301: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 18 16:11:53.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 16, 11, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 11, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 11, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 11, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/18/23 16:11:55.337
STEP: Verifying the service has paired with the endpoint 01/18/23 16:11:55.353
Jan 18 16:11:56.353: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/18/23 16:11:56.364
STEP: create a configmap that should be updated by the webhook 01/18/23 16:11:56.397
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 16:11:56.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7496" for this suite. 01/18/23 16:11:56.442
STEP: Destroying namespace "webhook-7496-markers" for this suite. 01/18/23 16:11:56.448
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":240,"skipped":4561,"failed":0}
------------------------------
• [SLOW TEST] [5.971 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:11:50.593
    Jan 18 16:11:50.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename webhook 01/18/23 16:11:50.596
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:11:50.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:11:50.633
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 16:11:50.661
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 16:11:51.258
    STEP: Deploying the webhook pod 01/18/23 16:11:51.267
    STEP: Wait for the deployment to be ready 01/18/23 16:11:51.281
    Jan 18 16:11:51.301: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 18 16:11:53.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 16, 11, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 11, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 11, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 11, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/18/23 16:11:55.337
    STEP: Verifying the service has paired with the endpoint 01/18/23 16:11:55.353
    Jan 18 16:11:56.353: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/18/23 16:11:56.364
    STEP: create a configmap that should be updated by the webhook 01/18/23 16:11:56.397
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 16:11:56.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7496" for this suite. 01/18/23 16:11:56.442
    STEP: Destroying namespace "webhook-7496-markers" for this suite. 01/18/23 16:11:56.448
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:11:56.574
Jan 18 16:11:56.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename kubectl 01/18/23 16:11:56.581
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:11:56.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:11:56.645
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 01/18/23 16:11:56.651
Jan 18 16:11:56.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-805 create -f -'
Jan 18 16:11:57.149: INFO: stderr: ""
Jan 18 16:11:57.149: INFO: stdout: "pod/pause created\n"
Jan 18 16:11:57.149: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 18 16:11:57.149: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-805" to be "running and ready"
Jan 18 16:11:57.167: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 18.142472ms
Jan 18 16:11:57.167: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'v1-25-1-18760-w2' to be 'Running' but was 'Pending'
Jan 18 16:11:59.172: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.022604136s
Jan 18 16:11:59.172: INFO: Pod "pause" satisfied condition "running and ready"
Jan 18 16:11:59.172: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 01/18/23 16:11:59.172
Jan 18 16:11:59.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-805 label pods pause testing-label=testing-label-value'
Jan 18 16:11:59.292: INFO: stderr: ""
Jan 18 16:11:59.292: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 01/18/23 16:11:59.292
Jan 18 16:11:59.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-805 get pod pause -L testing-label'
Jan 18 16:11:59.403: INFO: stderr: ""
Jan 18 16:11:59.403: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 01/18/23 16:11:59.403
Jan 18 16:11:59.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-805 label pods pause testing-label-'
Jan 18 16:11:59.524: INFO: stderr: ""
Jan 18 16:11:59.524: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 01/18/23 16:11:59.524
Jan 18 16:11:59.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-805 get pod pause -L testing-label'
Jan 18 16:11:59.629: INFO: stderr: ""
Jan 18 16:11:59.629: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 01/18/23 16:11:59.629
Jan 18 16:11:59.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-805 delete --grace-period=0 --force -f -'
Jan 18 16:11:59.766: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 16:11:59.766: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 18 16:11:59.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-805 get rc,svc -l name=pause --no-headers'
Jan 18 16:11:59.907: INFO: stderr: "No resources found in kubectl-805 namespace.\n"
Jan 18 16:11:59.907: INFO: stdout: ""
Jan 18 16:11:59.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-805 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 18 16:12:00.055: INFO: stderr: ""
Jan 18 16:12:00.055: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 16:12:00.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-805" for this suite. 01/18/23 16:12:00.064
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":241,"skipped":4578,"failed":0}
------------------------------
• [3.500 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:11:56.574
    Jan 18 16:11:56.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename kubectl 01/18/23 16:11:56.581
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:11:56.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:11:56.645
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 01/18/23 16:11:56.651
    Jan 18 16:11:56.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-805 create -f -'
    Jan 18 16:11:57.149: INFO: stderr: ""
    Jan 18 16:11:57.149: INFO: stdout: "pod/pause created\n"
    Jan 18 16:11:57.149: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jan 18 16:11:57.149: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-805" to be "running and ready"
    Jan 18 16:11:57.167: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 18.142472ms
    Jan 18 16:11:57.167: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'v1-25-1-18760-w2' to be 'Running' but was 'Pending'
    Jan 18 16:11:59.172: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.022604136s
    Jan 18 16:11:59.172: INFO: Pod "pause" satisfied condition "running and ready"
    Jan 18 16:11:59.172: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 01/18/23 16:11:59.172
    Jan 18 16:11:59.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-805 label pods pause testing-label=testing-label-value'
    Jan 18 16:11:59.292: INFO: stderr: ""
    Jan 18 16:11:59.292: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 01/18/23 16:11:59.292
    Jan 18 16:11:59.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-805 get pod pause -L testing-label'
    Jan 18 16:11:59.403: INFO: stderr: ""
    Jan 18 16:11:59.403: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 01/18/23 16:11:59.403
    Jan 18 16:11:59.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-805 label pods pause testing-label-'
    Jan 18 16:11:59.524: INFO: stderr: ""
    Jan 18 16:11:59.524: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 01/18/23 16:11:59.524
    Jan 18 16:11:59.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-805 get pod pause -L testing-label'
    Jan 18 16:11:59.629: INFO: stderr: ""
    Jan 18 16:11:59.629: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 01/18/23 16:11:59.629
    Jan 18 16:11:59.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-805 delete --grace-period=0 --force -f -'
    Jan 18 16:11:59.766: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 16:11:59.766: INFO: stdout: "pod \"pause\" force deleted\n"
    Jan 18 16:11:59.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-805 get rc,svc -l name=pause --no-headers'
    Jan 18 16:11:59.907: INFO: stderr: "No resources found in kubectl-805 namespace.\n"
    Jan 18 16:11:59.907: INFO: stdout: ""
    Jan 18 16:11:59.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-805 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 18 16:12:00.055: INFO: stderr: ""
    Jan 18 16:12:00.055: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 16:12:00.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-805" for this suite. 01/18/23 16:12:00.064
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:12:00.075
Jan 18 16:12:00.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename prestop 01/18/23 16:12:00.078
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:12:00.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:12:00.117
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-5377 01/18/23 16:12:00.122
STEP: Waiting for pods to come up. 01/18/23 16:12:00.134
Jan 18 16:12:00.135: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-5377" to be "running"
Jan 18 16:12:00.149: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 14.128535ms
Jan 18 16:12:02.154: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.019594745s
Jan 18 16:12:02.154: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-5377 01/18/23 16:12:02.157
Jan 18 16:12:02.171: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-5377" to be "running"
Jan 18 16:12:02.175: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 4.389204ms
Jan 18 16:12:04.181: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.010661314s
Jan 18 16:12:04.182: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 01/18/23 16:12:04.182
Jan 18 16:12:09.207: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 01/18/23 16:12:09.207
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Jan 18 16:12:09.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-5377" for this suite. 01/18/23 16:12:09.239
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":242,"skipped":4590,"failed":0}
------------------------------
• [SLOW TEST] [9.172 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:12:00.075
    Jan 18 16:12:00.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename prestop 01/18/23 16:12:00.078
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:12:00.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:12:00.117
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-5377 01/18/23 16:12:00.122
    STEP: Waiting for pods to come up. 01/18/23 16:12:00.134
    Jan 18 16:12:00.135: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-5377" to be "running"
    Jan 18 16:12:00.149: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 14.128535ms
    Jan 18 16:12:02.154: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.019594745s
    Jan 18 16:12:02.154: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-5377 01/18/23 16:12:02.157
    Jan 18 16:12:02.171: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-5377" to be "running"
    Jan 18 16:12:02.175: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 4.389204ms
    Jan 18 16:12:04.181: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.010661314s
    Jan 18 16:12:04.182: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 01/18/23 16:12:04.182
    Jan 18 16:12:09.207: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 01/18/23 16:12:09.207
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Jan 18 16:12:09.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-5377" for this suite. 01/18/23 16:12:09.239
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:12:09.25
Jan 18 16:12:09.252: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename job 01/18/23 16:12:09.253
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:12:09.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:12:09.282
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 01/18/23 16:12:09.291
STEP: Ensuring job reaches completions 01/18/23 16:12:09.299
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 18 16:12:23.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3311" for this suite. 01/18/23 16:12:23.312
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":243,"skipped":4593,"failed":0}
------------------------------
• [SLOW TEST] [14.071 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:12:09.25
    Jan 18 16:12:09.252: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename job 01/18/23 16:12:09.253
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:12:09.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:12:09.282
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 01/18/23 16:12:09.291
    STEP: Ensuring job reaches completions 01/18/23 16:12:09.299
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 18 16:12:23.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-3311" for this suite. 01/18/23 16:12:23.312
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:12:23.324
Jan 18 16:12:23.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename pods 01/18/23 16:12:23.327
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:12:23.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:12:23.354
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 01/18/23 16:12:23.363
Jan 18 16:12:23.373: INFO: created test-pod-1
Jan 18 16:12:23.609: INFO: created test-pod-2
Jan 18 16:12:23.630: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 01/18/23 16:12:23.63
Jan 18 16:12:23.631: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-7795' to be running and ready
Jan 18 16:12:23.651: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 16:12:23.651: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 16:12:23.651: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 16:12:23.651: INFO: 0 / 3 pods in namespace 'pods-7795' are running and ready (0 seconds elapsed)
Jan 18 16:12:23.651: INFO: expected 0 pod replicas in namespace 'pods-7795', 0 are Running and Ready.
Jan 18 16:12:23.651: INFO: POD         NODE              PHASE    GRACE  CONDITIONS
Jan 18 16:12:23.651: INFO: test-pod-1  v1-25-1-18760-w2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC  }]
Jan 18 16:12:23.651: INFO: test-pod-2  v1-25-1-18760-w2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC  }]
Jan 18 16:12:23.651: INFO: test-pod-3  v1-25-1-18760-w2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC  }]
Jan 18 16:12:23.651: INFO: 
Jan 18 16:12:25.665: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 16:12:25.677: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 16:12:25.678: INFO: 1 / 3 pods in namespace 'pods-7795' are running and ready (2 seconds elapsed)
Jan 18 16:12:25.678: INFO: expected 0 pod replicas in namespace 'pods-7795', 0 are Running and Ready.
Jan 18 16:12:25.678: INFO: POD         NODE              PHASE    GRACE  CONDITIONS
Jan 18 16:12:25.678: INFO: test-pod-2  v1-25-1-18760-w2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC  }]
Jan 18 16:12:25.678: INFO: test-pod-3  v1-25-1-18760-w2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC  }]
Jan 18 16:12:25.678: INFO: 
Jan 18 16:12:27.668: INFO: 3 / 3 pods in namespace 'pods-7795' are running and ready (4 seconds elapsed)
Jan 18 16:12:27.672: INFO: expected 0 pod replicas in namespace 'pods-7795', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 01/18/23 16:12:27.745
Jan 18 16:12:27.760: INFO: Pod quantity 3 is different from expected quantity 0
Jan 18 16:12:28.768: INFO: Pod quantity 3 is different from expected quantity 0
Jan 18 16:12:29.771: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 16:12:30.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7795" for this suite. 01/18/23 16:12:30.783
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":244,"skipped":4594,"failed":0}
------------------------------
• [SLOW TEST] [7.473 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:12:23.324
    Jan 18 16:12:23.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename pods 01/18/23 16:12:23.327
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:12:23.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:12:23.354
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 01/18/23 16:12:23.363
    Jan 18 16:12:23.373: INFO: created test-pod-1
    Jan 18 16:12:23.609: INFO: created test-pod-2
    Jan 18 16:12:23.630: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 01/18/23 16:12:23.63
    Jan 18 16:12:23.631: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-7795' to be running and ready
    Jan 18 16:12:23.651: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 18 16:12:23.651: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 18 16:12:23.651: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 18 16:12:23.651: INFO: 0 / 3 pods in namespace 'pods-7795' are running and ready (0 seconds elapsed)
    Jan 18 16:12:23.651: INFO: expected 0 pod replicas in namespace 'pods-7795', 0 are Running and Ready.
    Jan 18 16:12:23.651: INFO: POD         NODE              PHASE    GRACE  CONDITIONS
    Jan 18 16:12:23.651: INFO: test-pod-1  v1-25-1-18760-w2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC  }]
    Jan 18 16:12:23.651: INFO: test-pod-2  v1-25-1-18760-w2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC  }]
    Jan 18 16:12:23.651: INFO: test-pod-3  v1-25-1-18760-w2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC  }]
    Jan 18 16:12:23.651: INFO: 
    Jan 18 16:12:25.665: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 18 16:12:25.677: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 18 16:12:25.678: INFO: 1 / 3 pods in namespace 'pods-7795' are running and ready (2 seconds elapsed)
    Jan 18 16:12:25.678: INFO: expected 0 pod replicas in namespace 'pods-7795', 0 are Running and Ready.
    Jan 18 16:12:25.678: INFO: POD         NODE              PHASE    GRACE  CONDITIONS
    Jan 18 16:12:25.678: INFO: test-pod-2  v1-25-1-18760-w2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC  }]
    Jan 18 16:12:25.678: INFO: test-pod-3  v1-25-1-18760-w2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 16:12:23 +0000 UTC  }]
    Jan 18 16:12:25.678: INFO: 
    Jan 18 16:12:27.668: INFO: 3 / 3 pods in namespace 'pods-7795' are running and ready (4 seconds elapsed)
    Jan 18 16:12:27.672: INFO: expected 0 pod replicas in namespace 'pods-7795', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 01/18/23 16:12:27.745
    Jan 18 16:12:27.760: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 18 16:12:28.768: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 18 16:12:29.771: INFO: Pod quantity 1 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 16:12:30.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7795" for this suite. 01/18/23 16:12:30.783
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:12:30.807
Jan 18 16:12:30.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename emptydir 01/18/23 16:12:30.809
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:12:30.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:12:30.866
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 01/18/23 16:12:30.875
Jan 18 16:12:30.889: INFO: Waiting up to 5m0s for pod "pod-89f432e8-791d-4900-80c1-ddd277a3f9d8" in namespace "emptydir-6999" to be "Succeeded or Failed"
Jan 18 16:12:30.903: INFO: Pod "pod-89f432e8-791d-4900-80c1-ddd277a3f9d8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.430509ms
Jan 18 16:12:32.921: INFO: Pod "pod-89f432e8-791d-4900-80c1-ddd277a3f9d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031086249s
Jan 18 16:12:34.912: INFO: Pod "pod-89f432e8-791d-4900-80c1-ddd277a3f9d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022195632s
STEP: Saw pod success 01/18/23 16:12:34.912
Jan 18 16:12:34.912: INFO: Pod "pod-89f432e8-791d-4900-80c1-ddd277a3f9d8" satisfied condition "Succeeded or Failed"
Jan 18 16:12:34.922: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-89f432e8-791d-4900-80c1-ddd277a3f9d8 container test-container: <nil>
STEP: delete the pod 01/18/23 16:12:34.943
Jan 18 16:12:34.973: INFO: Waiting for pod pod-89f432e8-791d-4900-80c1-ddd277a3f9d8 to disappear
Jan 18 16:12:34.981: INFO: Pod pod-89f432e8-791d-4900-80c1-ddd277a3f9d8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 16:12:34.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6999" for this suite. 01/18/23 16:12:34.991
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":245,"skipped":4599,"failed":0}
------------------------------
• [4.202 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:12:30.807
    Jan 18 16:12:30.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename emptydir 01/18/23 16:12:30.809
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:12:30.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:12:30.866
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/18/23 16:12:30.875
    Jan 18 16:12:30.889: INFO: Waiting up to 5m0s for pod "pod-89f432e8-791d-4900-80c1-ddd277a3f9d8" in namespace "emptydir-6999" to be "Succeeded or Failed"
    Jan 18 16:12:30.903: INFO: Pod "pod-89f432e8-791d-4900-80c1-ddd277a3f9d8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.430509ms
    Jan 18 16:12:32.921: INFO: Pod "pod-89f432e8-791d-4900-80c1-ddd277a3f9d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031086249s
    Jan 18 16:12:34.912: INFO: Pod "pod-89f432e8-791d-4900-80c1-ddd277a3f9d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022195632s
    STEP: Saw pod success 01/18/23 16:12:34.912
    Jan 18 16:12:34.912: INFO: Pod "pod-89f432e8-791d-4900-80c1-ddd277a3f9d8" satisfied condition "Succeeded or Failed"
    Jan 18 16:12:34.922: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-89f432e8-791d-4900-80c1-ddd277a3f9d8 container test-container: <nil>
    STEP: delete the pod 01/18/23 16:12:34.943
    Jan 18 16:12:34.973: INFO: Waiting for pod pod-89f432e8-791d-4900-80c1-ddd277a3f9d8 to disappear
    Jan 18 16:12:34.981: INFO: Pod pod-89f432e8-791d-4900-80c1-ddd277a3f9d8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 16:12:34.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6999" for this suite. 01/18/23 16:12:34.991
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:12:35.017
Jan 18 16:12:35.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename configmap 01/18/23 16:12:35.019
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:12:35.044
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:12:35.049
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 01/18/23 16:12:35.062
STEP: fetching the ConfigMap 01/18/23 16:12:35.078
STEP: patching the ConfigMap 01/18/23 16:12:35.086
STEP: listing all ConfigMaps in all namespaces with a label selector 01/18/23 16:12:35.093
STEP: deleting the ConfigMap by collection with a label selector 01/18/23 16:12:35.108
STEP: listing all ConfigMaps in test namespace 01/18/23 16:12:35.126
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 16:12:35.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8570" for this suite. 01/18/23 16:12:35.145
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":246,"skipped":4605,"failed":0}
------------------------------
• [0.219 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:12:35.017
    Jan 18 16:12:35.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename configmap 01/18/23 16:12:35.019
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:12:35.044
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:12:35.049
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 01/18/23 16:12:35.062
    STEP: fetching the ConfigMap 01/18/23 16:12:35.078
    STEP: patching the ConfigMap 01/18/23 16:12:35.086
    STEP: listing all ConfigMaps in all namespaces with a label selector 01/18/23 16:12:35.093
    STEP: deleting the ConfigMap by collection with a label selector 01/18/23 16:12:35.108
    STEP: listing all ConfigMaps in test namespace 01/18/23 16:12:35.126
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 16:12:35.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8570" for this suite. 01/18/23 16:12:35.145
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:12:35.237
Jan 18 16:12:35.237: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename daemonsets 01/18/23 16:12:35.239
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:12:35.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:12:35.315
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Jan 18 16:12:35.350: INFO: Create a RollingUpdate DaemonSet
Jan 18 16:12:35.363: INFO: Check that daemon pods launch on every node of the cluster
Jan 18 16:12:35.376: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 16:12:35.390: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 16:12:35.390: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 16:12:36.425: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 16:12:36.437: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 16:12:36.437: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 16:12:37.397: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 16:12:37.403: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 16:12:37.403: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Jan 18 16:12:37.403: INFO: Update the DaemonSet to trigger a rollout
Jan 18 16:12:37.416: INFO: Updating DaemonSet daemon-set
Jan 18 16:12:40.460: INFO: Roll back the DaemonSet before rollout is complete
Jan 18 16:12:40.475: INFO: Updating DaemonSet daemon-set
Jan 18 16:12:40.475: INFO: Make sure DaemonSet rollback is complete
Jan 18 16:12:40.480: INFO: Wrong image for pod: daemon-set-mcvcq. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Jan 18 16:12:40.480: INFO: Pod daemon-set-mcvcq is not available
Jan 18 16:12:40.491: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 16:12:41.504: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 16:12:42.508: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 16:12:43.505: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 16:12:44.496: INFO: Pod daemon-set-h869n is not available
Jan 18 16:12:44.501: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/18/23 16:12:44.511
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5196, will wait for the garbage collector to delete the pods 01/18/23 16:12:44.512
Jan 18 16:12:44.579: INFO: Deleting DaemonSet.extensions daemon-set took: 11.330127ms
Jan 18 16:12:44.682: INFO: Terminating DaemonSet.extensions daemon-set pods took: 102.891251ms
Jan 18 16:12:47.287: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 16:12:47.287: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 16:12:47.291: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"56797"},"items":null}

Jan 18 16:12:47.296: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"56797"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 18 16:12:47.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5196" for this suite. 01/18/23 16:12:47.318
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":247,"skipped":4610,"failed":0}
------------------------------
• [SLOW TEST] [12.088 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:12:35.237
    Jan 18 16:12:35.237: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename daemonsets 01/18/23 16:12:35.239
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:12:35.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:12:35.315
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Jan 18 16:12:35.350: INFO: Create a RollingUpdate DaemonSet
    Jan 18 16:12:35.363: INFO: Check that daemon pods launch on every node of the cluster
    Jan 18 16:12:35.376: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 16:12:35.390: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 16:12:35.390: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 16:12:36.425: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 16:12:36.437: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 16:12:36.437: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 16:12:37.397: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 16:12:37.403: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 16:12:37.403: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Jan 18 16:12:37.403: INFO: Update the DaemonSet to trigger a rollout
    Jan 18 16:12:37.416: INFO: Updating DaemonSet daemon-set
    Jan 18 16:12:40.460: INFO: Roll back the DaemonSet before rollout is complete
    Jan 18 16:12:40.475: INFO: Updating DaemonSet daemon-set
    Jan 18 16:12:40.475: INFO: Make sure DaemonSet rollback is complete
    Jan 18 16:12:40.480: INFO: Wrong image for pod: daemon-set-mcvcq. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Jan 18 16:12:40.480: INFO: Pod daemon-set-mcvcq is not available
    Jan 18 16:12:40.491: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 16:12:41.504: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 16:12:42.508: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 16:12:43.505: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 16:12:44.496: INFO: Pod daemon-set-h869n is not available
    Jan 18 16:12:44.501: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 16:12:44.511
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5196, will wait for the garbage collector to delete the pods 01/18/23 16:12:44.512
    Jan 18 16:12:44.579: INFO: Deleting DaemonSet.extensions daemon-set took: 11.330127ms
    Jan 18 16:12:44.682: INFO: Terminating DaemonSet.extensions daemon-set pods took: 102.891251ms
    Jan 18 16:12:47.287: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 16:12:47.287: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 16:12:47.291: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"56797"},"items":null}

    Jan 18 16:12:47.296: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"56797"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 16:12:47.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-5196" for this suite. 01/18/23 16:12:47.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:12:47.329
Jan 18 16:12:47.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename gc 01/18/23 16:12:47.33
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:12:47.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:12:47.357
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 01/18/23 16:12:47.369
STEP: create the rc2 01/18/23 16:12:47.375
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/18/23 16:12:52.404
STEP: delete the rc simpletest-rc-to-be-deleted 01/18/23 16:12:54.143
STEP: wait for the rc to be deleted 01/18/23 16:12:54.197
Jan 18 16:12:59.305: INFO: 67 pods remaining
Jan 18 16:12:59.309: INFO: 67 pods has nil DeletionTimestamp
Jan 18 16:12:59.309: INFO: 
STEP: Gathering metrics 01/18/23 16:13:04.271
Jan 18 16:13:04.320: INFO: Waiting up to 5m0s for pod "kube-controller-manager-v1-25-1-18760-m" in namespace "kube-system" to be "running and ready"
Jan 18 16:13:04.332: INFO: Pod "kube-controller-manager-v1-25-1-18760-m": Phase="Running", Reason="", readiness=true. Elapsed: 11.921188ms
Jan 18 16:13:04.332: INFO: The phase of Pod kube-controller-manager-v1-25-1-18760-m is Running (Ready = true)
Jan 18 16:13:04.332: INFO: Pod "kube-controller-manager-v1-25-1-18760-m" satisfied condition "running and ready"
Jan 18 16:13:04.480: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 18 16:13:04.480: INFO: Deleting pod "simpletest-rc-to-be-deleted-2d8bs" in namespace "gc-9626"
Jan 18 16:13:04.519: INFO: Deleting pod "simpletest-rc-to-be-deleted-2g824" in namespace "gc-9626"
Jan 18 16:13:04.542: INFO: Deleting pod "simpletest-rc-to-be-deleted-2p5x9" in namespace "gc-9626"
Jan 18 16:13:04.558: INFO: Deleting pod "simpletest-rc-to-be-deleted-2wvqt" in namespace "gc-9626"
Jan 18 16:13:04.575: INFO: Deleting pod "simpletest-rc-to-be-deleted-45w99" in namespace "gc-9626"
Jan 18 16:13:04.592: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hvq6" in namespace "gc-9626"
Jan 18 16:13:04.635: INFO: Deleting pod "simpletest-rc-to-be-deleted-5m7rb" in namespace "gc-9626"
Jan 18 16:13:04.654: INFO: Deleting pod "simpletest-rc-to-be-deleted-698p2" in namespace "gc-9626"
Jan 18 16:13:04.672: INFO: Deleting pod "simpletest-rc-to-be-deleted-6h66d" in namespace "gc-9626"
Jan 18 16:13:04.684: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pzm9" in namespace "gc-9626"
Jan 18 16:13:04.706: INFO: Deleting pod "simpletest-rc-to-be-deleted-6rrlf" in namespace "gc-9626"
Jan 18 16:13:04.738: INFO: Deleting pod "simpletest-rc-to-be-deleted-747p7" in namespace "gc-9626"
Jan 18 16:13:04.778: INFO: Deleting pod "simpletest-rc-to-be-deleted-7gdpq" in namespace "gc-9626"
Jan 18 16:13:04.797: INFO: Deleting pod "simpletest-rc-to-be-deleted-7qj6z" in namespace "gc-9626"
Jan 18 16:13:04.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-85d26" in namespace "gc-9626"
Jan 18 16:13:04.857: INFO: Deleting pod "simpletest-rc-to-be-deleted-88nb2" in namespace "gc-9626"
Jan 18 16:13:04.882: INFO: Deleting pod "simpletest-rc-to-be-deleted-8d57p" in namespace "gc-9626"
Jan 18 16:13:04.914: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jtzc" in namespace "gc-9626"
Jan 18 16:13:04.926: INFO: Deleting pod "simpletest-rc-to-be-deleted-8vj9p" in namespace "gc-9626"
Jan 18 16:13:04.940: INFO: Deleting pod "simpletest-rc-to-be-deleted-8wwpk" in namespace "gc-9626"
Jan 18 16:13:04.958: INFO: Deleting pod "simpletest-rc-to-be-deleted-9gbwv" in namespace "gc-9626"
Jan 18 16:13:04.971: INFO: Deleting pod "simpletest-rc-to-be-deleted-9nlzg" in namespace "gc-9626"
Jan 18 16:13:04.988: INFO: Deleting pod "simpletest-rc-to-be-deleted-9p6dx" in namespace "gc-9626"
Jan 18 16:13:05.012: INFO: Deleting pod "simpletest-rc-to-be-deleted-b8f24" in namespace "gc-9626"
Jan 18 16:13:05.037: INFO: Deleting pod "simpletest-rc-to-be-deleted-bn9lz" in namespace "gc-9626"
Jan 18 16:13:05.063: INFO: Deleting pod "simpletest-rc-to-be-deleted-btnp6" in namespace "gc-9626"
Jan 18 16:13:05.092: INFO: Deleting pod "simpletest-rc-to-be-deleted-bzkk6" in namespace "gc-9626"
Jan 18 16:13:05.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7bv9" in namespace "gc-9626"
Jan 18 16:13:05.137: INFO: Deleting pod "simpletest-rc-to-be-deleted-cckwz" in namespace "gc-9626"
Jan 18 16:13:05.158: INFO: Deleting pod "simpletest-rc-to-be-deleted-cdg5w" in namespace "gc-9626"
Jan 18 16:13:05.170: INFO: Deleting pod "simpletest-rc-to-be-deleted-crcfd" in namespace "gc-9626"
Jan 18 16:13:05.182: INFO: Deleting pod "simpletest-rc-to-be-deleted-dclcj" in namespace "gc-9626"
Jan 18 16:13:05.225: INFO: Deleting pod "simpletest-rc-to-be-deleted-dgmn5" in namespace "gc-9626"
Jan 18 16:13:05.241: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8qbp" in namespace "gc-9626"
Jan 18 16:13:05.258: INFO: Deleting pod "simpletest-rc-to-be-deleted-fh8px" in namespace "gc-9626"
Jan 18 16:13:05.282: INFO: Deleting pod "simpletest-rc-to-be-deleted-fhfj4" in namespace "gc-9626"
Jan 18 16:13:05.299: INFO: Deleting pod "simpletest-rc-to-be-deleted-fhxq4" in namespace "gc-9626"
Jan 18 16:13:05.335: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvz9z" in namespace "gc-9626"
Jan 18 16:13:05.358: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwqbt" in namespace "gc-9626"
Jan 18 16:13:05.372: INFO: Deleting pod "simpletest-rc-to-be-deleted-fz4ld" in namespace "gc-9626"
Jan 18 16:13:05.402: INFO: Deleting pod "simpletest-rc-to-be-deleted-g49ln" in namespace "gc-9626"
Jan 18 16:13:05.427: INFO: Deleting pod "simpletest-rc-to-be-deleted-g4wzg" in namespace "gc-9626"
Jan 18 16:13:05.446: INFO: Deleting pod "simpletest-rc-to-be-deleted-gf5hg" in namespace "gc-9626"
Jan 18 16:13:05.504: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjkcn" in namespace "gc-9626"
Jan 18 16:13:05.536: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvvws" in namespace "gc-9626"
Jan 18 16:13:05.569: INFO: Deleting pod "simpletest-rc-to-be-deleted-hdt5f" in namespace "gc-9626"
Jan 18 16:13:05.583: INFO: Deleting pod "simpletest-rc-to-be-deleted-hmbmp" in namespace "gc-9626"
Jan 18 16:13:05.602: INFO: Deleting pod "simpletest-rc-to-be-deleted-jj5bj" in namespace "gc-9626"
Jan 18 16:13:05.622: INFO: Deleting pod "simpletest-rc-to-be-deleted-jmgpd" in namespace "gc-9626"
Jan 18 16:13:05.640: INFO: Deleting pod "simpletest-rc-to-be-deleted-jt8l8" in namespace "gc-9626"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 18 16:13:05.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9626" for this suite. 01/18/23 16:13:05.811
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":248,"skipped":4618,"failed":0}
------------------------------
• [SLOW TEST] [18.637 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:12:47.329
    Jan 18 16:12:47.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename gc 01/18/23 16:12:47.33
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:12:47.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:12:47.357
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 01/18/23 16:12:47.369
    STEP: create the rc2 01/18/23 16:12:47.375
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/18/23 16:12:52.404
    STEP: delete the rc simpletest-rc-to-be-deleted 01/18/23 16:12:54.143
    STEP: wait for the rc to be deleted 01/18/23 16:12:54.197
    Jan 18 16:12:59.305: INFO: 67 pods remaining
    Jan 18 16:12:59.309: INFO: 67 pods has nil DeletionTimestamp
    Jan 18 16:12:59.309: INFO: 
    STEP: Gathering metrics 01/18/23 16:13:04.271
    Jan 18 16:13:04.320: INFO: Waiting up to 5m0s for pod "kube-controller-manager-v1-25-1-18760-m" in namespace "kube-system" to be "running and ready"
    Jan 18 16:13:04.332: INFO: Pod "kube-controller-manager-v1-25-1-18760-m": Phase="Running", Reason="", readiness=true. Elapsed: 11.921188ms
    Jan 18 16:13:04.332: INFO: The phase of Pod kube-controller-manager-v1-25-1-18760-m is Running (Ready = true)
    Jan 18 16:13:04.332: INFO: Pod "kube-controller-manager-v1-25-1-18760-m" satisfied condition "running and ready"
    Jan 18 16:13:04.480: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 18 16:13:04.480: INFO: Deleting pod "simpletest-rc-to-be-deleted-2d8bs" in namespace "gc-9626"
    Jan 18 16:13:04.519: INFO: Deleting pod "simpletest-rc-to-be-deleted-2g824" in namespace "gc-9626"
    Jan 18 16:13:04.542: INFO: Deleting pod "simpletest-rc-to-be-deleted-2p5x9" in namespace "gc-9626"
    Jan 18 16:13:04.558: INFO: Deleting pod "simpletest-rc-to-be-deleted-2wvqt" in namespace "gc-9626"
    Jan 18 16:13:04.575: INFO: Deleting pod "simpletest-rc-to-be-deleted-45w99" in namespace "gc-9626"
    Jan 18 16:13:04.592: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hvq6" in namespace "gc-9626"
    Jan 18 16:13:04.635: INFO: Deleting pod "simpletest-rc-to-be-deleted-5m7rb" in namespace "gc-9626"
    Jan 18 16:13:04.654: INFO: Deleting pod "simpletest-rc-to-be-deleted-698p2" in namespace "gc-9626"
    Jan 18 16:13:04.672: INFO: Deleting pod "simpletest-rc-to-be-deleted-6h66d" in namespace "gc-9626"
    Jan 18 16:13:04.684: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pzm9" in namespace "gc-9626"
    Jan 18 16:13:04.706: INFO: Deleting pod "simpletest-rc-to-be-deleted-6rrlf" in namespace "gc-9626"
    Jan 18 16:13:04.738: INFO: Deleting pod "simpletest-rc-to-be-deleted-747p7" in namespace "gc-9626"
    Jan 18 16:13:04.778: INFO: Deleting pod "simpletest-rc-to-be-deleted-7gdpq" in namespace "gc-9626"
    Jan 18 16:13:04.797: INFO: Deleting pod "simpletest-rc-to-be-deleted-7qj6z" in namespace "gc-9626"
    Jan 18 16:13:04.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-85d26" in namespace "gc-9626"
    Jan 18 16:13:04.857: INFO: Deleting pod "simpletest-rc-to-be-deleted-88nb2" in namespace "gc-9626"
    Jan 18 16:13:04.882: INFO: Deleting pod "simpletest-rc-to-be-deleted-8d57p" in namespace "gc-9626"
    Jan 18 16:13:04.914: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jtzc" in namespace "gc-9626"
    Jan 18 16:13:04.926: INFO: Deleting pod "simpletest-rc-to-be-deleted-8vj9p" in namespace "gc-9626"
    Jan 18 16:13:04.940: INFO: Deleting pod "simpletest-rc-to-be-deleted-8wwpk" in namespace "gc-9626"
    Jan 18 16:13:04.958: INFO: Deleting pod "simpletest-rc-to-be-deleted-9gbwv" in namespace "gc-9626"
    Jan 18 16:13:04.971: INFO: Deleting pod "simpletest-rc-to-be-deleted-9nlzg" in namespace "gc-9626"
    Jan 18 16:13:04.988: INFO: Deleting pod "simpletest-rc-to-be-deleted-9p6dx" in namespace "gc-9626"
    Jan 18 16:13:05.012: INFO: Deleting pod "simpletest-rc-to-be-deleted-b8f24" in namespace "gc-9626"
    Jan 18 16:13:05.037: INFO: Deleting pod "simpletest-rc-to-be-deleted-bn9lz" in namespace "gc-9626"
    Jan 18 16:13:05.063: INFO: Deleting pod "simpletest-rc-to-be-deleted-btnp6" in namespace "gc-9626"
    Jan 18 16:13:05.092: INFO: Deleting pod "simpletest-rc-to-be-deleted-bzkk6" in namespace "gc-9626"
    Jan 18 16:13:05.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7bv9" in namespace "gc-9626"
    Jan 18 16:13:05.137: INFO: Deleting pod "simpletest-rc-to-be-deleted-cckwz" in namespace "gc-9626"
    Jan 18 16:13:05.158: INFO: Deleting pod "simpletest-rc-to-be-deleted-cdg5w" in namespace "gc-9626"
    Jan 18 16:13:05.170: INFO: Deleting pod "simpletest-rc-to-be-deleted-crcfd" in namespace "gc-9626"
    Jan 18 16:13:05.182: INFO: Deleting pod "simpletest-rc-to-be-deleted-dclcj" in namespace "gc-9626"
    Jan 18 16:13:05.225: INFO: Deleting pod "simpletest-rc-to-be-deleted-dgmn5" in namespace "gc-9626"
    Jan 18 16:13:05.241: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8qbp" in namespace "gc-9626"
    Jan 18 16:13:05.258: INFO: Deleting pod "simpletest-rc-to-be-deleted-fh8px" in namespace "gc-9626"
    Jan 18 16:13:05.282: INFO: Deleting pod "simpletest-rc-to-be-deleted-fhfj4" in namespace "gc-9626"
    Jan 18 16:13:05.299: INFO: Deleting pod "simpletest-rc-to-be-deleted-fhxq4" in namespace "gc-9626"
    Jan 18 16:13:05.335: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvz9z" in namespace "gc-9626"
    Jan 18 16:13:05.358: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwqbt" in namespace "gc-9626"
    Jan 18 16:13:05.372: INFO: Deleting pod "simpletest-rc-to-be-deleted-fz4ld" in namespace "gc-9626"
    Jan 18 16:13:05.402: INFO: Deleting pod "simpletest-rc-to-be-deleted-g49ln" in namespace "gc-9626"
    Jan 18 16:13:05.427: INFO: Deleting pod "simpletest-rc-to-be-deleted-g4wzg" in namespace "gc-9626"
    Jan 18 16:13:05.446: INFO: Deleting pod "simpletest-rc-to-be-deleted-gf5hg" in namespace "gc-9626"
    Jan 18 16:13:05.504: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjkcn" in namespace "gc-9626"
    Jan 18 16:13:05.536: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvvws" in namespace "gc-9626"
    Jan 18 16:13:05.569: INFO: Deleting pod "simpletest-rc-to-be-deleted-hdt5f" in namespace "gc-9626"
    Jan 18 16:13:05.583: INFO: Deleting pod "simpletest-rc-to-be-deleted-hmbmp" in namespace "gc-9626"
    Jan 18 16:13:05.602: INFO: Deleting pod "simpletest-rc-to-be-deleted-jj5bj" in namespace "gc-9626"
    Jan 18 16:13:05.622: INFO: Deleting pod "simpletest-rc-to-be-deleted-jmgpd" in namespace "gc-9626"
    Jan 18 16:13:05.640: INFO: Deleting pod "simpletest-rc-to-be-deleted-jt8l8" in namespace "gc-9626"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 18 16:13:05.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-9626" for this suite. 01/18/23 16:13:05.811
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:13:05.973
Jan 18 16:13:05.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename replicaset 01/18/23 16:13:05.975
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:13:06.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:13:06.085
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/18/23 16:13:06.092
Jan 18 16:13:06.108: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9108" to be "running and ready"
Jan 18 16:13:06.135: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 27.226977ms
Jan 18 16:13:06.135: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 18 16:13:08.150: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041914782s
Jan 18 16:13:08.150: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 18 16:13:10.141: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032676209s
Jan 18 16:13:10.141: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 18 16:13:12.166: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 6.058377179s
Jan 18 16:13:12.182: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 18 16:13:14.171: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 8.062563436s
Jan 18 16:13:14.171: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 18 16:13:16.154: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 10.045955161s
Jan 18 16:13:16.154: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 18 16:13:18.141: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 12.033353547s
Jan 18 16:13:18.141: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jan 18 16:13:18.142: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 01/18/23 16:13:18.145
STEP: Then the orphan pod is adopted 01/18/23 16:13:18.159
STEP: When the matched label of one of its pods change 01/18/23 16:13:19.168
Jan 18 16:13:19.174: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 01/18/23 16:13:19.195
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 18 16:13:20.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9108" for this suite. 01/18/23 16:13:20.228
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":249,"skipped":4651,"failed":0}
------------------------------
• [SLOW TEST] [14.264 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:13:05.973
    Jan 18 16:13:05.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename replicaset 01/18/23 16:13:05.975
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:13:06.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:13:06.085
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/18/23 16:13:06.092
    Jan 18 16:13:06.108: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9108" to be "running and ready"
    Jan 18 16:13:06.135: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 27.226977ms
    Jan 18 16:13:06.135: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 16:13:08.150: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041914782s
    Jan 18 16:13:08.150: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 16:13:10.141: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032676209s
    Jan 18 16:13:10.141: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 16:13:12.166: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 6.058377179s
    Jan 18 16:13:12.182: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 16:13:14.171: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 8.062563436s
    Jan 18 16:13:14.171: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 16:13:16.154: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 10.045955161s
    Jan 18 16:13:16.154: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 16:13:18.141: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 12.033353547s
    Jan 18 16:13:18.141: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jan 18 16:13:18.142: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 01/18/23 16:13:18.145
    STEP: Then the orphan pod is adopted 01/18/23 16:13:18.159
    STEP: When the matched label of one of its pods change 01/18/23 16:13:19.168
    Jan 18 16:13:19.174: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/18/23 16:13:19.195
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 18 16:13:20.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-9108" for this suite. 01/18/23 16:13:20.228
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:13:20.241
Jan 18 16:13:20.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 16:13:20.244
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:13:20.264
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:13:20.27
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 01/18/23 16:13:20.275
Jan 18 16:13:20.290: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9c2c2659-fe9e-4f65-925d-644a730796ee" in namespace "projected-3699" to be "Succeeded or Failed"
Jan 18 16:13:20.302: INFO: Pod "downwardapi-volume-9c2c2659-fe9e-4f65-925d-644a730796ee": Phase="Pending", Reason="", readiness=false. Elapsed: 12.268683ms
Jan 18 16:13:22.308: INFO: Pod "downwardapi-volume-9c2c2659-fe9e-4f65-925d-644a730796ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018731506s
Jan 18 16:13:24.311: INFO: Pod "downwardapi-volume-9c2c2659-fe9e-4f65-925d-644a730796ee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021003801s
Jan 18 16:13:26.311: INFO: Pod "downwardapi-volume-9c2c2659-fe9e-4f65-925d-644a730796ee": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020905049s
Jan 18 16:13:28.311: INFO: Pod "downwardapi-volume-9c2c2659-fe9e-4f65-925d-644a730796ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.021273652s
STEP: Saw pod success 01/18/23 16:13:28.311
Jan 18 16:13:28.311: INFO: Pod "downwardapi-volume-9c2c2659-fe9e-4f65-925d-644a730796ee" satisfied condition "Succeeded or Failed"
Jan 18 16:13:28.317: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-9c2c2659-fe9e-4f65-925d-644a730796ee container client-container: <nil>
STEP: delete the pod 01/18/23 16:13:28.33
Jan 18 16:13:28.356: INFO: Waiting for pod downwardapi-volume-9c2c2659-fe9e-4f65-925d-644a730796ee to disappear
Jan 18 16:13:28.371: INFO: Pod downwardapi-volume-9c2c2659-fe9e-4f65-925d-644a730796ee no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 16:13:28.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3699" for this suite. 01/18/23 16:13:28.386
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":250,"skipped":4669,"failed":0}
------------------------------
• [SLOW TEST] [8.156 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:13:20.241
    Jan 18 16:13:20.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 16:13:20.244
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:13:20.264
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:13:20.27
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 01/18/23 16:13:20.275
    Jan 18 16:13:20.290: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9c2c2659-fe9e-4f65-925d-644a730796ee" in namespace "projected-3699" to be "Succeeded or Failed"
    Jan 18 16:13:20.302: INFO: Pod "downwardapi-volume-9c2c2659-fe9e-4f65-925d-644a730796ee": Phase="Pending", Reason="", readiness=false. Elapsed: 12.268683ms
    Jan 18 16:13:22.308: INFO: Pod "downwardapi-volume-9c2c2659-fe9e-4f65-925d-644a730796ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018731506s
    Jan 18 16:13:24.311: INFO: Pod "downwardapi-volume-9c2c2659-fe9e-4f65-925d-644a730796ee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021003801s
    Jan 18 16:13:26.311: INFO: Pod "downwardapi-volume-9c2c2659-fe9e-4f65-925d-644a730796ee": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020905049s
    Jan 18 16:13:28.311: INFO: Pod "downwardapi-volume-9c2c2659-fe9e-4f65-925d-644a730796ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.021273652s
    STEP: Saw pod success 01/18/23 16:13:28.311
    Jan 18 16:13:28.311: INFO: Pod "downwardapi-volume-9c2c2659-fe9e-4f65-925d-644a730796ee" satisfied condition "Succeeded or Failed"
    Jan 18 16:13:28.317: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-9c2c2659-fe9e-4f65-925d-644a730796ee container client-container: <nil>
    STEP: delete the pod 01/18/23 16:13:28.33
    Jan 18 16:13:28.356: INFO: Waiting for pod downwardapi-volume-9c2c2659-fe9e-4f65-925d-644a730796ee to disappear
    Jan 18 16:13:28.371: INFO: Pod downwardapi-volume-9c2c2659-fe9e-4f65-925d-644a730796ee no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 16:13:28.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3699" for this suite. 01/18/23 16:13:28.386
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:13:28.405
Jan 18 16:13:28.405: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename deployment 01/18/23 16:13:28.409
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:13:28.426
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:13:28.431
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jan 18 16:13:28.449: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 18 16:13:33.456: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 16:13:33.456
Jan 18 16:13:33.457: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 18 16:13:35.462: INFO: Creating deployment "test-rollover-deployment"
Jan 18 16:13:35.479: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 18 16:13:37.490: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 18 16:13:37.500: INFO: Ensure that both replica sets have 1 created replica
Jan 18 16:13:37.508: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 18 16:13:37.523: INFO: Updating deployment test-rollover-deployment
Jan 18 16:13:37.523: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 18 16:13:39.543: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 18 16:13:39.579: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 18 16:13:39.601: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 16:13:39.602: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 13, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 16:13:41.614: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 16:13:41.614: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 13, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 16:13:43.615: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 16:13:43.616: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 13, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 16:13:45.617: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 16:13:45.617: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 13, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 16:13:47.613: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 16:13:47.613: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 13, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 16:13:49.614: INFO: 
Jan 18 16:13:49.614: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 16:13:49.629: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-7935  915a1549-b7a1-40f7-8ea1-f3bbc1de3def 58874 2 2023-01-18 16:13:35 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 16:13:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 16:13:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000afb438 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 16:13:35 +0000 UTC,LastTransitionTime:2023-01-18 16:13:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-01-18 16:13:49 +0000 UTC,LastTransitionTime:2023-01-18 16:13:35 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 18 16:13:49.638: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-7935  ba097b70-066b-4f57-b41e-cd9f445a1aa6 58864 2 2023-01-18 16:13:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 915a1549-b7a1-40f7-8ea1-f3bbc1de3def 0xc00122c187 0xc00122c188}] [] [{kube-controller-manager Update apps/v1 2023-01-18 16:13:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"915a1549-b7a1-40f7-8ea1-f3bbc1de3def\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 16:13:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00122c378 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 16:13:49.638: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 18 16:13:49.638: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7935  5ab55004-4f91-4179-a210-7cc8f1189a3a 58873 2 2023-01-18 16:13:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 915a1549-b7a1-40f7-8ea1-f3bbc1de3def 0xc000afbab7 0xc000afbab8}] [] [{e2e.test Update apps/v1 2023-01-18 16:13:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 16:13:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"915a1549-b7a1-40f7-8ea1-f3bbc1de3def\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-18 16:13:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000afbe28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 16:13:49.639: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-7935  47439e90-2b3a-4f0d-8a2e-3a70e5a45c23 58800 2 2023-01-18 16:13:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 915a1549-b7a1-40f7-8ea1-f3bbc1de3def 0xc00122c067 0xc00122c068}] [] [{kube-controller-manager Update apps/v1 2023-01-18 16:13:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"915a1549-b7a1-40f7-8ea1-f3bbc1de3def\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 16:13:37 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00122c118 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 16:13:49.651: INFO: Pod "test-rollover-deployment-6d45fd857b-qv497" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-qv497 test-rollover-deployment-6d45fd857b- deployment-7935  699adfc6-a404-4cf9-a653-02076275b54b 58822 0 2023-01-18 16:13:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:ef8b812856b7e03a9d349480d7f04ad714e528cbcc496aee1eb3449d19014bca cni.projectcalico.org/podIP:10.233.68.138/32 cni.projectcalico.org/podIPs:10.233.68.138/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b ba097b70-066b-4f57-b41e-cd9f445a1aa6 0xc00122d407 0xc00122d408}] [] [{kube-controller-manager Update v1 2023-01-18 16:13:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba097b70-066b-4f57-b41e-cd9f445a1aa6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 16:13:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 16:13:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.138\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w2bhq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w2bhq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:13:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:13:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:13:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:13:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.138,StartTime:2023-01-18 16:13:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 16:13:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://2377ea7d5c1a5eeef46f3bcbe08775fa061f03dbfb4740982519c472a5642708,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.138,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 18 16:13:49.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7935" for this suite. 01/18/23 16:13:49.657
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":251,"skipped":4675,"failed":0}
------------------------------
• [SLOW TEST] [21.268 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:13:28.405
    Jan 18 16:13:28.405: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename deployment 01/18/23 16:13:28.409
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:13:28.426
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:13:28.431
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jan 18 16:13:28.449: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jan 18 16:13:33.456: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 16:13:33.456
    Jan 18 16:13:33.457: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jan 18 16:13:35.462: INFO: Creating deployment "test-rollover-deployment"
    Jan 18 16:13:35.479: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jan 18 16:13:37.490: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jan 18 16:13:37.500: INFO: Ensure that both replica sets have 1 created replica
    Jan 18 16:13:37.508: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jan 18 16:13:37.523: INFO: Updating deployment test-rollover-deployment
    Jan 18 16:13:37.523: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jan 18 16:13:39.543: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jan 18 16:13:39.579: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jan 18 16:13:39.601: INFO: all replica sets need to contain the pod-template-hash label
    Jan 18 16:13:39.602: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 13, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 16:13:41.614: INFO: all replica sets need to contain the pod-template-hash label
    Jan 18 16:13:41.614: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 13, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 16:13:43.615: INFO: all replica sets need to contain the pod-template-hash label
    Jan 18 16:13:43.616: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 13, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 16:13:45.617: INFO: all replica sets need to contain the pod-template-hash label
    Jan 18 16:13:45.617: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 13, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 16:13:47.613: INFO: all replica sets need to contain the pod-template-hash label
    Jan 18 16:13:47.613: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 13, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 13, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 16:13:49.614: INFO: 
    Jan 18 16:13:49.614: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 16:13:49.629: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-7935  915a1549-b7a1-40f7-8ea1-f3bbc1de3def 58874 2 2023-01-18 16:13:35 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 16:13:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 16:13:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000afb438 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 16:13:35 +0000 UTC,LastTransitionTime:2023-01-18 16:13:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-01-18 16:13:49 +0000 UTC,LastTransitionTime:2023-01-18 16:13:35 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 18 16:13:49.638: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-7935  ba097b70-066b-4f57-b41e-cd9f445a1aa6 58864 2 2023-01-18 16:13:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 915a1549-b7a1-40f7-8ea1-f3bbc1de3def 0xc00122c187 0xc00122c188}] [] [{kube-controller-manager Update apps/v1 2023-01-18 16:13:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"915a1549-b7a1-40f7-8ea1-f3bbc1de3def\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 16:13:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00122c378 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 16:13:49.638: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jan 18 16:13:49.638: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7935  5ab55004-4f91-4179-a210-7cc8f1189a3a 58873 2 2023-01-18 16:13:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 915a1549-b7a1-40f7-8ea1-f3bbc1de3def 0xc000afbab7 0xc000afbab8}] [] [{e2e.test Update apps/v1 2023-01-18 16:13:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 16:13:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"915a1549-b7a1-40f7-8ea1-f3bbc1de3def\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-18 16:13:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000afbe28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 16:13:49.639: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-7935  47439e90-2b3a-4f0d-8a2e-3a70e5a45c23 58800 2 2023-01-18 16:13:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 915a1549-b7a1-40f7-8ea1-f3bbc1de3def 0xc00122c067 0xc00122c068}] [] [{kube-controller-manager Update apps/v1 2023-01-18 16:13:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"915a1549-b7a1-40f7-8ea1-f3bbc1de3def\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 16:13:37 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00122c118 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 16:13:49.651: INFO: Pod "test-rollover-deployment-6d45fd857b-qv497" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-qv497 test-rollover-deployment-6d45fd857b- deployment-7935  699adfc6-a404-4cf9-a653-02076275b54b 58822 0 2023-01-18 16:13:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:ef8b812856b7e03a9d349480d7f04ad714e528cbcc496aee1eb3449d19014bca cni.projectcalico.org/podIP:10.233.68.138/32 cni.projectcalico.org/podIPs:10.233.68.138/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b ba097b70-066b-4f57-b41e-cd9f445a1aa6 0xc00122d407 0xc00122d408}] [] [{kube-controller-manager Update v1 2023-01-18 16:13:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba097b70-066b-4f57-b41e-cd9f445a1aa6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 16:13:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 16:13:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.138\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w2bhq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w2bhq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:13:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:13:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:13:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:13:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.138,StartTime:2023-01-18 16:13:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 16:13:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://2377ea7d5c1a5eeef46f3bcbe08775fa061f03dbfb4740982519c472a5642708,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.138,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 18 16:13:49.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7935" for this suite. 01/18/23 16:13:49.657
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:13:49.674
Jan 18 16:13:49.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename configmap 01/18/23 16:13:49.678
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:13:49.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:13:49.716
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-1cf67907-9e8c-4f0f-aea1-3c74e943ef74 01/18/23 16:13:49.723
STEP: Creating a pod to test consume configMaps 01/18/23 16:13:49.731
Jan 18 16:13:49.741: INFO: Waiting up to 5m0s for pod "pod-configmaps-ac87cab2-3a21-4f9c-a7df-d7c334865e97" in namespace "configmap-7235" to be "Succeeded or Failed"
Jan 18 16:13:49.779: INFO: Pod "pod-configmaps-ac87cab2-3a21-4f9c-a7df-d7c334865e97": Phase="Pending", Reason="", readiness=false. Elapsed: 38.035679ms
Jan 18 16:13:51.785: INFO: Pod "pod-configmaps-ac87cab2-3a21-4f9c-a7df-d7c334865e97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043617857s
Jan 18 16:13:53.785: INFO: Pod "pod-configmaps-ac87cab2-3a21-4f9c-a7df-d7c334865e97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044126433s
STEP: Saw pod success 01/18/23 16:13:53.785
Jan 18 16:13:53.785: INFO: Pod "pod-configmaps-ac87cab2-3a21-4f9c-a7df-d7c334865e97" satisfied condition "Succeeded or Failed"
Jan 18 16:13:53.790: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-ac87cab2-3a21-4f9c-a7df-d7c334865e97 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 16:13:53.814
Jan 18 16:13:53.855: INFO: Waiting for pod pod-configmaps-ac87cab2-3a21-4f9c-a7df-d7c334865e97 to disappear
Jan 18 16:13:53.859: INFO: Pod pod-configmaps-ac87cab2-3a21-4f9c-a7df-d7c334865e97 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 16:13:53.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7235" for this suite. 01/18/23 16:13:53.865
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":252,"skipped":4675,"failed":0}
------------------------------
• [4.198 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:13:49.674
    Jan 18 16:13:49.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename configmap 01/18/23 16:13:49.678
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:13:49.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:13:49.716
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-1cf67907-9e8c-4f0f-aea1-3c74e943ef74 01/18/23 16:13:49.723
    STEP: Creating a pod to test consume configMaps 01/18/23 16:13:49.731
    Jan 18 16:13:49.741: INFO: Waiting up to 5m0s for pod "pod-configmaps-ac87cab2-3a21-4f9c-a7df-d7c334865e97" in namespace "configmap-7235" to be "Succeeded or Failed"
    Jan 18 16:13:49.779: INFO: Pod "pod-configmaps-ac87cab2-3a21-4f9c-a7df-d7c334865e97": Phase="Pending", Reason="", readiness=false. Elapsed: 38.035679ms
    Jan 18 16:13:51.785: INFO: Pod "pod-configmaps-ac87cab2-3a21-4f9c-a7df-d7c334865e97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043617857s
    Jan 18 16:13:53.785: INFO: Pod "pod-configmaps-ac87cab2-3a21-4f9c-a7df-d7c334865e97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044126433s
    STEP: Saw pod success 01/18/23 16:13:53.785
    Jan 18 16:13:53.785: INFO: Pod "pod-configmaps-ac87cab2-3a21-4f9c-a7df-d7c334865e97" satisfied condition "Succeeded or Failed"
    Jan 18 16:13:53.790: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-ac87cab2-3a21-4f9c-a7df-d7c334865e97 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 16:13:53.814
    Jan 18 16:13:53.855: INFO: Waiting for pod pod-configmaps-ac87cab2-3a21-4f9c-a7df-d7c334865e97 to disappear
    Jan 18 16:13:53.859: INFO: Pod pod-configmaps-ac87cab2-3a21-4f9c-a7df-d7c334865e97 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 16:13:53.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7235" for this suite. 01/18/23 16:13:53.865
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:13:53.876
Jan 18 16:13:53.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename webhook 01/18/23 16:13:53.881
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:13:53.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:13:53.927
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 16:13:53.96
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 16:13:54.388
STEP: Deploying the webhook pod 01/18/23 16:13:54.396
STEP: Wait for the deployment to be ready 01/18/23 16:13:54.406
Jan 18 16:13:54.452: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 16:13:56.471
STEP: Verifying the service has paired with the endpoint 01/18/23 16:13:56.509
Jan 18 16:13:57.509: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/18/23 16:13:57.514
STEP: create a namespace for the webhook 01/18/23 16:13:57.533
STEP: create a configmap should be unconditionally rejected by the webhook 01/18/23 16:13:57.551
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 16:13:57.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1008" for this suite. 01/18/23 16:13:57.582
STEP: Destroying namespace "webhook-1008-markers" for this suite. 01/18/23 16:13:57.59
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":253,"skipped":4681,"failed":0}
------------------------------
• [3.779 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:13:53.876
    Jan 18 16:13:53.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename webhook 01/18/23 16:13:53.881
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:13:53.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:13:53.927
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 16:13:53.96
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 16:13:54.388
    STEP: Deploying the webhook pod 01/18/23 16:13:54.396
    STEP: Wait for the deployment to be ready 01/18/23 16:13:54.406
    Jan 18 16:13:54.452: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 16:13:56.471
    STEP: Verifying the service has paired with the endpoint 01/18/23 16:13:56.509
    Jan 18 16:13:57.509: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/18/23 16:13:57.514
    STEP: create a namespace for the webhook 01/18/23 16:13:57.533
    STEP: create a configmap should be unconditionally rejected by the webhook 01/18/23 16:13:57.551
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 16:13:57.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1008" for this suite. 01/18/23 16:13:57.582
    STEP: Destroying namespace "webhook-1008-markers" for this suite. 01/18/23 16:13:57.59
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:13:57.661
Jan 18 16:13:57.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename gc 01/18/23 16:13:57.665
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:13:57.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:13:57.718
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 01/18/23 16:13:57.727
STEP: Wait for the Deployment to create new ReplicaSet 01/18/23 16:13:57.735
STEP: delete the deployment 01/18/23 16:13:58.264
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/18/23 16:13:58.271
STEP: Gathering metrics 01/18/23 16:13:58.822
Jan 18 16:13:58.855: INFO: Waiting up to 5m0s for pod "kube-controller-manager-v1-25-1-18760-m" in namespace "kube-system" to be "running and ready"
Jan 18 16:13:58.859: INFO: Pod "kube-controller-manager-v1-25-1-18760-m": Phase="Running", Reason="", readiness=true. Elapsed: 3.928289ms
Jan 18 16:13:58.859: INFO: The phase of Pod kube-controller-manager-v1-25-1-18760-m is Running (Ready = true)
Jan 18 16:13:58.859: INFO: Pod "kube-controller-manager-v1-25-1-18760-m" satisfied condition "running and ready"
Jan 18 16:13:59.046: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 18 16:13:59.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2773" for this suite. 01/18/23 16:13:59.061
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":254,"skipped":4705,"failed":0}
------------------------------
• [1.416 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:13:57.661
    Jan 18 16:13:57.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename gc 01/18/23 16:13:57.665
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:13:57.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:13:57.718
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 01/18/23 16:13:57.727
    STEP: Wait for the Deployment to create new ReplicaSet 01/18/23 16:13:57.735
    STEP: delete the deployment 01/18/23 16:13:58.264
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/18/23 16:13:58.271
    STEP: Gathering metrics 01/18/23 16:13:58.822
    Jan 18 16:13:58.855: INFO: Waiting up to 5m0s for pod "kube-controller-manager-v1-25-1-18760-m" in namespace "kube-system" to be "running and ready"
    Jan 18 16:13:58.859: INFO: Pod "kube-controller-manager-v1-25-1-18760-m": Phase="Running", Reason="", readiness=true. Elapsed: 3.928289ms
    Jan 18 16:13:58.859: INFO: The phase of Pod kube-controller-manager-v1-25-1-18760-m is Running (Ready = true)
    Jan 18 16:13:58.859: INFO: Pod "kube-controller-manager-v1-25-1-18760-m" satisfied condition "running and ready"
    Jan 18 16:13:59.046: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 18 16:13:59.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2773" for this suite. 01/18/23 16:13:59.061
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:13:59.079
Jan 18 16:13:59.079: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename subpath 01/18/23 16:13:59.08
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:13:59.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:13:59.121
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/18/23 16:13:59.132
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-pt9p 01/18/23 16:13:59.157
STEP: Creating a pod to test atomic-volume-subpath 01/18/23 16:13:59.157
Jan 18 16:13:59.169: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pt9p" in namespace "subpath-6731" to be "Succeeded or Failed"
Jan 18 16:13:59.180: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Pending", Reason="", readiness=false. Elapsed: 11.37983ms
Jan 18 16:14:01.187: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017787862s
Jan 18 16:14:03.185: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Running", Reason="", readiness=true. Elapsed: 4.015558015s
Jan 18 16:14:05.189: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Running", Reason="", readiness=true. Elapsed: 6.019954741s
Jan 18 16:14:07.189: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Running", Reason="", readiness=true. Elapsed: 8.020328513s
Jan 18 16:14:09.187: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Running", Reason="", readiness=true. Elapsed: 10.017610077s
Jan 18 16:14:11.186: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Running", Reason="", readiness=true. Elapsed: 12.016980794s
Jan 18 16:14:13.186: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Running", Reason="", readiness=true. Elapsed: 14.016652383s
Jan 18 16:14:15.189: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Running", Reason="", readiness=true. Elapsed: 16.02030962s
Jan 18 16:14:17.186: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Running", Reason="", readiness=true. Elapsed: 18.017298333s
Jan 18 16:14:19.187: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Running", Reason="", readiness=true. Elapsed: 20.017728523s
Jan 18 16:14:21.187: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Running", Reason="", readiness=true. Elapsed: 22.01792854s
Jan 18 16:14:23.189: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Running", Reason="", readiness=false. Elapsed: 24.020088513s
Jan 18 16:14:25.194: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.024431004s
STEP: Saw pod success 01/18/23 16:14:25.194
Jan 18 16:14:25.194: INFO: Pod "pod-subpath-test-configmap-pt9p" satisfied condition "Succeeded or Failed"
Jan 18 16:14:25.216: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-subpath-test-configmap-pt9p container test-container-subpath-configmap-pt9p: <nil>
STEP: delete the pod 01/18/23 16:14:25.243
Jan 18 16:14:25.267: INFO: Waiting for pod pod-subpath-test-configmap-pt9p to disappear
Jan 18 16:14:25.275: INFO: Pod pod-subpath-test-configmap-pt9p no longer exists
STEP: Deleting pod pod-subpath-test-configmap-pt9p 01/18/23 16:14:25.275
Jan 18 16:14:25.276: INFO: Deleting pod "pod-subpath-test-configmap-pt9p" in namespace "subpath-6731"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 18 16:14:25.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6731" for this suite. 01/18/23 16:14:25.287
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":255,"skipped":4712,"failed":0}
------------------------------
• [SLOW TEST] [26.219 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:13:59.079
    Jan 18 16:13:59.079: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename subpath 01/18/23 16:13:59.08
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:13:59.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:13:59.121
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/18/23 16:13:59.132
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-pt9p 01/18/23 16:13:59.157
    STEP: Creating a pod to test atomic-volume-subpath 01/18/23 16:13:59.157
    Jan 18 16:13:59.169: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pt9p" in namespace "subpath-6731" to be "Succeeded or Failed"
    Jan 18 16:13:59.180: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Pending", Reason="", readiness=false. Elapsed: 11.37983ms
    Jan 18 16:14:01.187: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017787862s
    Jan 18 16:14:03.185: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Running", Reason="", readiness=true. Elapsed: 4.015558015s
    Jan 18 16:14:05.189: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Running", Reason="", readiness=true. Elapsed: 6.019954741s
    Jan 18 16:14:07.189: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Running", Reason="", readiness=true. Elapsed: 8.020328513s
    Jan 18 16:14:09.187: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Running", Reason="", readiness=true. Elapsed: 10.017610077s
    Jan 18 16:14:11.186: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Running", Reason="", readiness=true. Elapsed: 12.016980794s
    Jan 18 16:14:13.186: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Running", Reason="", readiness=true. Elapsed: 14.016652383s
    Jan 18 16:14:15.189: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Running", Reason="", readiness=true. Elapsed: 16.02030962s
    Jan 18 16:14:17.186: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Running", Reason="", readiness=true. Elapsed: 18.017298333s
    Jan 18 16:14:19.187: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Running", Reason="", readiness=true. Elapsed: 20.017728523s
    Jan 18 16:14:21.187: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Running", Reason="", readiness=true. Elapsed: 22.01792854s
    Jan 18 16:14:23.189: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Running", Reason="", readiness=false. Elapsed: 24.020088513s
    Jan 18 16:14:25.194: INFO: Pod "pod-subpath-test-configmap-pt9p": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.024431004s
    STEP: Saw pod success 01/18/23 16:14:25.194
    Jan 18 16:14:25.194: INFO: Pod "pod-subpath-test-configmap-pt9p" satisfied condition "Succeeded or Failed"
    Jan 18 16:14:25.216: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-subpath-test-configmap-pt9p container test-container-subpath-configmap-pt9p: <nil>
    STEP: delete the pod 01/18/23 16:14:25.243
    Jan 18 16:14:25.267: INFO: Waiting for pod pod-subpath-test-configmap-pt9p to disappear
    Jan 18 16:14:25.275: INFO: Pod pod-subpath-test-configmap-pt9p no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-pt9p 01/18/23 16:14:25.275
    Jan 18 16:14:25.276: INFO: Deleting pod "pod-subpath-test-configmap-pt9p" in namespace "subpath-6731"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 18 16:14:25.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-6731" for this suite. 01/18/23 16:14:25.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:14:25.3
Jan 18 16:14:25.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename downward-api 01/18/23 16:14:25.303
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:14:25.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:14:25.353
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 01/18/23 16:14:25.358
Jan 18 16:14:25.372: INFO: Waiting up to 5m0s for pod "labelsupdatecb5c070b-4f6f-4abf-866b-a07dc33d6e78" in namespace "downward-api-4738" to be "running and ready"
Jan 18 16:14:25.400: INFO: Pod "labelsupdatecb5c070b-4f6f-4abf-866b-a07dc33d6e78": Phase="Pending", Reason="", readiness=false. Elapsed: 26.978599ms
Jan 18 16:14:25.400: INFO: The phase of Pod labelsupdatecb5c070b-4f6f-4abf-866b-a07dc33d6e78 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 16:14:27.406: INFO: Pod "labelsupdatecb5c070b-4f6f-4abf-866b-a07dc33d6e78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033141275s
Jan 18 16:14:27.406: INFO: The phase of Pod labelsupdatecb5c070b-4f6f-4abf-866b-a07dc33d6e78 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 16:14:29.410: INFO: Pod "labelsupdatecb5c070b-4f6f-4abf-866b-a07dc33d6e78": Phase="Running", Reason="", readiness=true. Elapsed: 4.037753466s
Jan 18 16:14:29.411: INFO: The phase of Pod labelsupdatecb5c070b-4f6f-4abf-866b-a07dc33d6e78 is Running (Ready = true)
Jan 18 16:14:29.413: INFO: Pod "labelsupdatecb5c070b-4f6f-4abf-866b-a07dc33d6e78" satisfied condition "running and ready"
Jan 18 16:14:29.988: INFO: Successfully updated pod "labelsupdatecb5c070b-4f6f-4abf-866b-a07dc33d6e78"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 16:14:32.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4738" for this suite. 01/18/23 16:14:32.019
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":256,"skipped":4737,"failed":0}
------------------------------
• [SLOW TEST] [6.726 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:14:25.3
    Jan 18 16:14:25.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename downward-api 01/18/23 16:14:25.303
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:14:25.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:14:25.353
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 01/18/23 16:14:25.358
    Jan 18 16:14:25.372: INFO: Waiting up to 5m0s for pod "labelsupdatecb5c070b-4f6f-4abf-866b-a07dc33d6e78" in namespace "downward-api-4738" to be "running and ready"
    Jan 18 16:14:25.400: INFO: Pod "labelsupdatecb5c070b-4f6f-4abf-866b-a07dc33d6e78": Phase="Pending", Reason="", readiness=false. Elapsed: 26.978599ms
    Jan 18 16:14:25.400: INFO: The phase of Pod labelsupdatecb5c070b-4f6f-4abf-866b-a07dc33d6e78 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 16:14:27.406: INFO: Pod "labelsupdatecb5c070b-4f6f-4abf-866b-a07dc33d6e78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033141275s
    Jan 18 16:14:27.406: INFO: The phase of Pod labelsupdatecb5c070b-4f6f-4abf-866b-a07dc33d6e78 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 16:14:29.410: INFO: Pod "labelsupdatecb5c070b-4f6f-4abf-866b-a07dc33d6e78": Phase="Running", Reason="", readiness=true. Elapsed: 4.037753466s
    Jan 18 16:14:29.411: INFO: The phase of Pod labelsupdatecb5c070b-4f6f-4abf-866b-a07dc33d6e78 is Running (Ready = true)
    Jan 18 16:14:29.413: INFO: Pod "labelsupdatecb5c070b-4f6f-4abf-866b-a07dc33d6e78" satisfied condition "running and ready"
    Jan 18 16:14:29.988: INFO: Successfully updated pod "labelsupdatecb5c070b-4f6f-4abf-866b-a07dc33d6e78"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 16:14:32.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4738" for this suite. 01/18/23 16:14:32.019
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:14:32.029
Jan 18 16:14:32.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename job 01/18/23 16:14:32.031
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:14:32.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:14:32.053
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 01/18/23 16:14:32.058
STEP: Ensuring active pods == parallelism 01/18/23 16:14:32.07
STEP: delete a job 01/18/23 16:14:36.076
STEP: deleting Job.batch foo in namespace job-4223, will wait for the garbage collector to delete the pods 01/18/23 16:14:36.076
Jan 18 16:14:36.148: INFO: Deleting Job.batch foo took: 16.007006ms
Jan 18 16:14:36.248: INFO: Terminating Job.batch foo pods took: 100.588086ms
STEP: Ensuring job was deleted 01/18/23 16:15:07.749
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 18 16:15:07.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4223" for this suite. 01/18/23 16:15:07.762
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":257,"skipped":4743,"failed":0}
------------------------------
• [SLOW TEST] [35.742 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:14:32.029
    Jan 18 16:14:32.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename job 01/18/23 16:14:32.031
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:14:32.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:14:32.053
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 01/18/23 16:14:32.058
    STEP: Ensuring active pods == parallelism 01/18/23 16:14:32.07
    STEP: delete a job 01/18/23 16:14:36.076
    STEP: deleting Job.batch foo in namespace job-4223, will wait for the garbage collector to delete the pods 01/18/23 16:14:36.076
    Jan 18 16:14:36.148: INFO: Deleting Job.batch foo took: 16.007006ms
    Jan 18 16:14:36.248: INFO: Terminating Job.batch foo pods took: 100.588086ms
    STEP: Ensuring job was deleted 01/18/23 16:15:07.749
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 18 16:15:07.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-4223" for this suite. 01/18/23 16:15:07.762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:15:07.774
Jan 18 16:15:07.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename services 01/18/23 16:15:07.777
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:15:07.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:15:07.832
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-3547 01/18/23 16:15:07.841
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3547 to expose endpoints map[] 01/18/23 16:15:07.909
Jan 18 16:15:07.950: INFO: successfully validated that service endpoint-test2 in namespace services-3547 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3547 01/18/23 16:15:07.95
Jan 18 16:15:07.960: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3547" to be "running and ready"
Jan 18 16:15:07.967: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.497554ms
Jan 18 16:15:07.967: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 16:15:09.972: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011801784s
Jan 18 16:15:09.972: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 18 16:15:09.972: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3547 to expose endpoints map[pod1:[80]] 01/18/23 16:15:09.979
Jan 18 16:15:09.993: INFO: successfully validated that service endpoint-test2 in namespace services-3547 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 01/18/23 16:15:09.993
Jan 18 16:15:09.994: INFO: Creating new exec pod
Jan 18 16:15:10.001: INFO: Waiting up to 5m0s for pod "execpod99ndx" in namespace "services-3547" to be "running"
Jan 18 16:15:10.013: INFO: Pod "execpod99ndx": Phase="Pending", Reason="", readiness=false. Elapsed: 11.924791ms
Jan 18 16:15:12.021: INFO: Pod "execpod99ndx": Phase="Running", Reason="", readiness=true. Elapsed: 2.019665545s
Jan 18 16:15:12.021: INFO: Pod "execpod99ndx" satisfied condition "running"
Jan 18 16:15:13.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-3547 exec execpod99ndx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 18 16:15:13.298: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 18 16:15:13.298: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 16:15:13.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-3547 exec execpod99ndx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.25.164 80'
Jan 18 16:15:13.496: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.25.164 80\nConnection to 10.233.25.164 80 port [tcp/http] succeeded!\n"
Jan 18 16:15:13.496: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-3547 01/18/23 16:15:13.497
Jan 18 16:15:13.507: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3547" to be "running and ready"
Jan 18 16:15:13.514: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.602775ms
Jan 18 16:15:13.514: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 16:15:15.520: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013629657s
Jan 18 16:15:15.520: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 18 16:15:15.520: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3547 to expose endpoints map[pod1:[80] pod2:[80]] 01/18/23 16:15:15.525
Jan 18 16:15:15.543: INFO: successfully validated that service endpoint-test2 in namespace services-3547 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 01/18/23 16:15:15.543
Jan 18 16:15:16.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-3547 exec execpod99ndx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 18 16:15:16.770: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 18 16:15:16.770: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 16:15:16.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-3547 exec execpod99ndx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.25.164 80'
Jan 18 16:15:17.027: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.25.164 80\nConnection to 10.233.25.164 80 port [tcp/http] succeeded!\n"
Jan 18 16:15:17.027: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-3547 01/18/23 16:15:17.027
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3547 to expose endpoints map[pod2:[80]] 01/18/23 16:15:17.058
Jan 18 16:15:17.103: INFO: successfully validated that service endpoint-test2 in namespace services-3547 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 01/18/23 16:15:17.103
Jan 18 16:15:18.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-3547 exec execpod99ndx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 18 16:15:18.299: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 18 16:15:18.299: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 16:15:18.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-3547 exec execpod99ndx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.25.164 80'
Jan 18 16:15:18.493: INFO: stderr: "+ + nc -v -t -w 2 10.233.25.164echo 80\n hostName\nConnection to 10.233.25.164 80 port [tcp/http] succeeded!\n"
Jan 18 16:15:18.493: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-3547 01/18/23 16:15:18.493
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3547 to expose endpoints map[] 01/18/23 16:15:18.504
Jan 18 16:15:19.522: INFO: successfully validated that service endpoint-test2 in namespace services-3547 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 16:15:19.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3547" for this suite. 01/18/23 16:15:19.573
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":258,"skipped":4745,"failed":0}
------------------------------
• [SLOW TEST] [11.815 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:15:07.774
    Jan 18 16:15:07.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename services 01/18/23 16:15:07.777
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:15:07.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:15:07.832
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-3547 01/18/23 16:15:07.841
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3547 to expose endpoints map[] 01/18/23 16:15:07.909
    Jan 18 16:15:07.950: INFO: successfully validated that service endpoint-test2 in namespace services-3547 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-3547 01/18/23 16:15:07.95
    Jan 18 16:15:07.960: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3547" to be "running and ready"
    Jan 18 16:15:07.967: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.497554ms
    Jan 18 16:15:07.967: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 16:15:09.972: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011801784s
    Jan 18 16:15:09.972: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 18 16:15:09.972: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3547 to expose endpoints map[pod1:[80]] 01/18/23 16:15:09.979
    Jan 18 16:15:09.993: INFO: successfully validated that service endpoint-test2 in namespace services-3547 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 01/18/23 16:15:09.993
    Jan 18 16:15:09.994: INFO: Creating new exec pod
    Jan 18 16:15:10.001: INFO: Waiting up to 5m0s for pod "execpod99ndx" in namespace "services-3547" to be "running"
    Jan 18 16:15:10.013: INFO: Pod "execpod99ndx": Phase="Pending", Reason="", readiness=false. Elapsed: 11.924791ms
    Jan 18 16:15:12.021: INFO: Pod "execpod99ndx": Phase="Running", Reason="", readiness=true. Elapsed: 2.019665545s
    Jan 18 16:15:12.021: INFO: Pod "execpod99ndx" satisfied condition "running"
    Jan 18 16:15:13.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-3547 exec execpod99ndx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan 18 16:15:13.298: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 18 16:15:13.298: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 16:15:13.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-3547 exec execpod99ndx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.25.164 80'
    Jan 18 16:15:13.496: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.25.164 80\nConnection to 10.233.25.164 80 port [tcp/http] succeeded!\n"
    Jan 18 16:15:13.496: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-3547 01/18/23 16:15:13.497
    Jan 18 16:15:13.507: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3547" to be "running and ready"
    Jan 18 16:15:13.514: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.602775ms
    Jan 18 16:15:13.514: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 16:15:15.520: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013629657s
    Jan 18 16:15:15.520: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 18 16:15:15.520: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3547 to expose endpoints map[pod1:[80] pod2:[80]] 01/18/23 16:15:15.525
    Jan 18 16:15:15.543: INFO: successfully validated that service endpoint-test2 in namespace services-3547 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 01/18/23 16:15:15.543
    Jan 18 16:15:16.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-3547 exec execpod99ndx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan 18 16:15:16.770: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 18 16:15:16.770: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 16:15:16.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-3547 exec execpod99ndx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.25.164 80'
    Jan 18 16:15:17.027: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.25.164 80\nConnection to 10.233.25.164 80 port [tcp/http] succeeded!\n"
    Jan 18 16:15:17.027: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-3547 01/18/23 16:15:17.027
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3547 to expose endpoints map[pod2:[80]] 01/18/23 16:15:17.058
    Jan 18 16:15:17.103: INFO: successfully validated that service endpoint-test2 in namespace services-3547 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 01/18/23 16:15:17.103
    Jan 18 16:15:18.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-3547 exec execpod99ndx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan 18 16:15:18.299: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 18 16:15:18.299: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 16:15:18.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-3547 exec execpod99ndx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.25.164 80'
    Jan 18 16:15:18.493: INFO: stderr: "+ + nc -v -t -w 2 10.233.25.164echo 80\n hostName\nConnection to 10.233.25.164 80 port [tcp/http] succeeded!\n"
    Jan 18 16:15:18.493: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-3547 01/18/23 16:15:18.493
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3547 to expose endpoints map[] 01/18/23 16:15:18.504
    Jan 18 16:15:19.522: INFO: successfully validated that service endpoint-test2 in namespace services-3547 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 16:15:19.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3547" for this suite. 01/18/23 16:15:19.573
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:15:19.598
Jan 18 16:15:19.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename job 01/18/23 16:15:19.6
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:15:19.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:15:19.635
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 01/18/23 16:15:19.641
STEP: Ensure pods equal to paralellism count is attached to the job 01/18/23 16:15:19.648
STEP: patching /status 01/18/23 16:15:23.657
STEP: updating /status 01/18/23 16:15:23.668
STEP: get /status 01/18/23 16:15:23.679
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 18 16:15:23.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1265" for this suite. 01/18/23 16:15:23.703
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":259,"skipped":4764,"failed":0}
------------------------------
• [4.112 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:15:19.598
    Jan 18 16:15:19.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename job 01/18/23 16:15:19.6
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:15:19.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:15:19.635
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 01/18/23 16:15:19.641
    STEP: Ensure pods equal to paralellism count is attached to the job 01/18/23 16:15:19.648
    STEP: patching /status 01/18/23 16:15:23.657
    STEP: updating /status 01/18/23 16:15:23.668
    STEP: get /status 01/18/23 16:15:23.679
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 18 16:15:23.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-1265" for this suite. 01/18/23 16:15:23.703
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:15:23.712
Jan 18 16:15:23.712: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename tables 01/18/23 16:15:23.715
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:15:23.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:15:23.742
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Jan 18 16:15:23.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7778" for this suite. 01/18/23 16:15:23.759
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":260,"skipped":4768,"failed":0}
------------------------------
• [0.058 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:15:23.712
    Jan 18 16:15:23.712: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename tables 01/18/23 16:15:23.715
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:15:23.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:15:23.742
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Jan 18 16:15:23.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-7778" for this suite. 01/18/23 16:15:23.759
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:15:23.779
Jan 18 16:15:23.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename pods 01/18/23 16:15:23.781
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:15:23.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:15:23.959
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 01/18/23 16:15:24
STEP: submitting the pod to kubernetes 01/18/23 16:15:24.001
STEP: verifying QOS class is set on the pod 01/18/23 16:15:24.041
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Jan 18 16:15:24.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5094" for this suite. 01/18/23 16:15:24.064
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":261,"skipped":4802,"failed":0}
------------------------------
• [0.300 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:15:23.779
    Jan 18 16:15:23.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename pods 01/18/23 16:15:23.781
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:15:23.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:15:23.959
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 01/18/23 16:15:24
    STEP: submitting the pod to kubernetes 01/18/23 16:15:24.001
    STEP: verifying QOS class is set on the pod 01/18/23 16:15:24.041
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Jan 18 16:15:24.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5094" for this suite. 01/18/23 16:15:24.064
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:15:24.08
Jan 18 16:15:24.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename endpointslice 01/18/23 16:15:24.082
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:15:24.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:15:24.113
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Jan 18 16:15:24.132: INFO: Endpoints addresses: [192.168.101.240] , ports: [6443]
Jan 18 16:15:24.133: INFO: EndpointSlices addresses: [192.168.101.240] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 18 16:15:24.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7405" for this suite. 01/18/23 16:15:24.138
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":262,"skipped":4812,"failed":0}
------------------------------
• [0.066 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:15:24.08
    Jan 18 16:15:24.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename endpointslice 01/18/23 16:15:24.082
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:15:24.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:15:24.113
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Jan 18 16:15:24.132: INFO: Endpoints addresses: [192.168.101.240] , ports: [6443]
    Jan 18 16:15:24.133: INFO: EndpointSlices addresses: [192.168.101.240] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 18 16:15:24.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-7405" for this suite. 01/18/23 16:15:24.138
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:15:24.152
Jan 18 16:15:24.152: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename init-container 01/18/23 16:15:24.153
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:15:24.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:15:24.178
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 01/18/23 16:15:24.218
Jan 18 16:15:24.219: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 18 16:15:29.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5210" for this suite. 01/18/23 16:15:29.81
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":263,"skipped":4834,"failed":0}
------------------------------
• [SLOW TEST] [5.670 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:15:24.152
    Jan 18 16:15:24.152: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename init-container 01/18/23 16:15:24.153
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:15:24.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:15:24.178
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 01/18/23 16:15:24.218
    Jan 18 16:15:24.219: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 18 16:15:29.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-5210" for this suite. 01/18/23 16:15:29.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:15:29.832
Jan 18 16:15:29.833: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename deployment 01/18/23 16:15:29.834
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:15:29.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:15:29.899
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jan 18 16:15:29.922: INFO: Creating simple deployment test-new-deployment
Jan 18 16:15:29.941: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 01/18/23 16:15:31.964
STEP: updating a scale subresource 01/18/23 16:15:31.969
STEP: verifying the deployment Spec.Replicas was modified 01/18/23 16:15:31.976
STEP: Patch a scale subresource 01/18/23 16:15:31.988
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 16:15:32.040: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-6308  e8586b43-ccc3-468a-8d0b-ba2dc4d4632d 59824 3 2023-01-18 16:15:29 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-18 16:15:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 16:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004434dd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-01-18 16:15:31 +0000 UTC,LastTransitionTime:2023-01-18 16:15:29 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-18 16:15:31 +0000 UTC,LastTransitionTime:2023-01-18 16:15:31 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 18 16:15:32.059: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-6308  c66e0e5a-3983-4ba9-acbd-b04bf90a5755 59830 3 2023-01-18 16:15:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment e8586b43-ccc3-468a-8d0b-ba2dc4d4632d 0xc004bb6e07 0xc004bb6e08}] [] [{kube-controller-manager Update apps/v1 2023-01-18 16:15:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e8586b43-ccc3-468a-8d0b-ba2dc4d4632d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 16:15:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004bb6e98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 16:15:32.099: INFO: Pod "test-new-deployment-845c8977d9-7jh98" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-7jh98 test-new-deployment-845c8977d9- deployment-6308  4a00c5d2-4a4c-4c3a-8527-642ed28a86c8 59835 0 2023-01-18 16:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 c66e0e5a-3983-4ba9-acbd-b04bf90a5755 0xc004bb72d7 0xc004bb72d8}] [] [{kube-controller-manager Update v1 2023-01-18 16:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c66e0e5a-3983-4ba9-acbd-b04bf90a5755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 16:15:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jv5g5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jv5g5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:15:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:15:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:15:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:15:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:,StartTime:2023-01-18 16:15:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 16:15:32.100: INFO: Pod "test-new-deployment-845c8977d9-fm57m" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-fm57m test-new-deployment-845c8977d9- deployment-6308  4d124693-7bf8-45e8-a0b4-c6752b2b333e 59834 0 2023-01-18 16:15:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 c66e0e5a-3983-4ba9-acbd-b04bf90a5755 0xc004bb74c7 0xc004bb74c8}] [] [{kube-controller-manager Update v1 2023-01-18 16:15:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c66e0e5a-3983-4ba9-acbd-b04bf90a5755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mm68r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mm68r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:15:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 16:15:32.101: INFO: Pod "test-new-deployment-845c8977d9-nxgbk" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-nxgbk test-new-deployment-845c8977d9- deployment-6308  b8cd5d6e-c476-4222-bb4b-621046d4d69e 59816 0 2023-01-18 16:15:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:303e7aa1529d04ed191d5df776117a7ceedcaaf398bb84297767fd01cdbe2d84 cni.projectcalico.org/podIP:10.233.68.152/32 cni.projectcalico.org/podIPs:10.233.68.152/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 c66e0e5a-3983-4ba9-acbd-b04bf90a5755 0xc004bb7680 0xc004bb7681}] [] [{kube-controller-manager Update v1 2023-01-18 16:15:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c66e0e5a-3983-4ba9-acbd-b04bf90a5755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 16:15:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 16:15:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.152\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cwxvp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cwxvp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:15:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:15:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.152,StartTime:2023-01-18 16:15:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 16:15:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://bd24fc3b739fc8cf15d74c511091212dcd2291789de4faf76a2c5cbd026b2e7f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.152,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 16:15:32.102: INFO: Pod "test-new-deployment-845c8977d9-pgkcw" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-pgkcw test-new-deployment-845c8977d9- deployment-6308  03fc72d4-0784-49c2-947c-6b786c916a48 59838 0 2023-01-18 16:15:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 c66e0e5a-3983-4ba9-acbd-b04bf90a5755 0xc004bb7887 0xc004bb7888}] [] [{kube-controller-manager Update v1 2023-01-18 16:15:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c66e0e5a-3983-4ba9-acbd-b04bf90a5755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mdh4n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mdh4n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 18 16:15:32.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6308" for this suite. 01/18/23 16:15:32.12
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":264,"skipped":4865,"failed":0}
------------------------------
• [2.305 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:15:29.832
    Jan 18 16:15:29.833: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename deployment 01/18/23 16:15:29.834
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:15:29.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:15:29.899
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jan 18 16:15:29.922: INFO: Creating simple deployment test-new-deployment
    Jan 18 16:15:29.941: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 01/18/23 16:15:31.964
    STEP: updating a scale subresource 01/18/23 16:15:31.969
    STEP: verifying the deployment Spec.Replicas was modified 01/18/23 16:15:31.976
    STEP: Patch a scale subresource 01/18/23 16:15:31.988
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 16:15:32.040: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-6308  e8586b43-ccc3-468a-8d0b-ba2dc4d4632d 59824 3 2023-01-18 16:15:29 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-18 16:15:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 16:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004434dd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-01-18 16:15:31 +0000 UTC,LastTransitionTime:2023-01-18 16:15:29 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-18 16:15:31 +0000 UTC,LastTransitionTime:2023-01-18 16:15:31 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 18 16:15:32.059: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-6308  c66e0e5a-3983-4ba9-acbd-b04bf90a5755 59830 3 2023-01-18 16:15:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment e8586b43-ccc3-468a-8d0b-ba2dc4d4632d 0xc004bb6e07 0xc004bb6e08}] [] [{kube-controller-manager Update apps/v1 2023-01-18 16:15:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e8586b43-ccc3-468a-8d0b-ba2dc4d4632d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 16:15:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004bb6e98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 16:15:32.099: INFO: Pod "test-new-deployment-845c8977d9-7jh98" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-7jh98 test-new-deployment-845c8977d9- deployment-6308  4a00c5d2-4a4c-4c3a-8527-642ed28a86c8 59835 0 2023-01-18 16:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 c66e0e5a-3983-4ba9-acbd-b04bf90a5755 0xc004bb72d7 0xc004bb72d8}] [] [{kube-controller-manager Update v1 2023-01-18 16:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c66e0e5a-3983-4ba9-acbd-b04bf90a5755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 16:15:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jv5g5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jv5g5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:15:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:15:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:15:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:15:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.168,PodIP:,StartTime:2023-01-18 16:15:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 16:15:32.100: INFO: Pod "test-new-deployment-845c8977d9-fm57m" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-fm57m test-new-deployment-845c8977d9- deployment-6308  4d124693-7bf8-45e8-a0b4-c6752b2b333e 59834 0 2023-01-18 16:15:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 c66e0e5a-3983-4ba9-acbd-b04bf90a5755 0xc004bb74c7 0xc004bb74c8}] [] [{kube-controller-manager Update v1 2023-01-18 16:15:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c66e0e5a-3983-4ba9-acbd-b04bf90a5755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mm68r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mm68r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:15:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 16:15:32.101: INFO: Pod "test-new-deployment-845c8977d9-nxgbk" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-nxgbk test-new-deployment-845c8977d9- deployment-6308  b8cd5d6e-c476-4222-bb4b-621046d4d69e 59816 0 2023-01-18 16:15:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:303e7aa1529d04ed191d5df776117a7ceedcaaf398bb84297767fd01cdbe2d84 cni.projectcalico.org/podIP:10.233.68.152/32 cni.projectcalico.org/podIPs:10.233.68.152/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 c66e0e5a-3983-4ba9-acbd-b04bf90a5755 0xc004bb7680 0xc004bb7681}] [] [{kube-controller-manager Update v1 2023-01-18 16:15:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c66e0e5a-3983-4ba9-acbd-b04bf90a5755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 16:15:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 16:15:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.152\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cwxvp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cwxvp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:15:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:15:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.152,StartTime:2023-01-18 16:15:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 16:15:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://bd24fc3b739fc8cf15d74c511091212dcd2291789de4faf76a2c5cbd026b2e7f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.152,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 16:15:32.102: INFO: Pod "test-new-deployment-845c8977d9-pgkcw" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-pgkcw test-new-deployment-845c8977d9- deployment-6308  03fc72d4-0784-49c2-947c-6b786c916a48 59838 0 2023-01-18 16:15:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 c66e0e5a-3983-4ba9-acbd-b04bf90a5755 0xc004bb7887 0xc004bb7888}] [] [{kube-controller-manager Update v1 2023-01-18 16:15:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c66e0e5a-3983-4ba9-acbd-b04bf90a5755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mdh4n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mdh4n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 18 16:15:32.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-6308" for this suite. 01/18/23 16:15:32.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:15:32.143
Jan 18 16:15:32.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename var-expansion 01/18/23 16:15:32.146
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:15:32.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:15:32.183
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Jan 18 16:15:32.198: INFO: Waiting up to 2m0s for pod "var-expansion-ffe73f2d-fae6-403b-b0f9-235c7828b349" in namespace "var-expansion-5549" to be "container 0 failed with reason CreateContainerConfigError"
Jan 18 16:15:32.204: INFO: Pod "var-expansion-ffe73f2d-fae6-403b-b0f9-235c7828b349": Phase="Pending", Reason="", readiness=false. Elapsed: 6.842976ms
Jan 18 16:15:34.210: INFO: Pod "var-expansion-ffe73f2d-fae6-403b-b0f9-235c7828b349": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012170885s
Jan 18 16:15:34.210: INFO: Pod "var-expansion-ffe73f2d-fae6-403b-b0f9-235c7828b349" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 18 16:15:34.210: INFO: Deleting pod "var-expansion-ffe73f2d-fae6-403b-b0f9-235c7828b349" in namespace "var-expansion-5549"
Jan 18 16:15:34.223: INFO: Wait up to 5m0s for pod "var-expansion-ffe73f2d-fae6-403b-b0f9-235c7828b349" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 18 16:15:38.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5549" for this suite. 01/18/23 16:15:38.239
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":265,"skipped":4875,"failed":0}
------------------------------
• [SLOW TEST] [6.119 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:15:32.143
    Jan 18 16:15:32.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename var-expansion 01/18/23 16:15:32.146
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:15:32.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:15:32.183
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Jan 18 16:15:32.198: INFO: Waiting up to 2m0s for pod "var-expansion-ffe73f2d-fae6-403b-b0f9-235c7828b349" in namespace "var-expansion-5549" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 18 16:15:32.204: INFO: Pod "var-expansion-ffe73f2d-fae6-403b-b0f9-235c7828b349": Phase="Pending", Reason="", readiness=false. Elapsed: 6.842976ms
    Jan 18 16:15:34.210: INFO: Pod "var-expansion-ffe73f2d-fae6-403b-b0f9-235c7828b349": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012170885s
    Jan 18 16:15:34.210: INFO: Pod "var-expansion-ffe73f2d-fae6-403b-b0f9-235c7828b349" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 18 16:15:34.210: INFO: Deleting pod "var-expansion-ffe73f2d-fae6-403b-b0f9-235c7828b349" in namespace "var-expansion-5549"
    Jan 18 16:15:34.223: INFO: Wait up to 5m0s for pod "var-expansion-ffe73f2d-fae6-403b-b0f9-235c7828b349" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 18 16:15:38.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5549" for this suite. 01/18/23 16:15:38.239
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:15:38.265
Jan 18 16:15:38.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename statefulset 01/18/23 16:15:38.267
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:15:38.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:15:38.293
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2258 01/18/23 16:15:38.298
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-2258 01/18/23 16:15:38.327
Jan 18 16:15:38.340: INFO: Found 0 stateful pods, waiting for 1
Jan 18 16:15:48.351: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 01/18/23 16:15:48.372
STEP: Getting /status 01/18/23 16:15:48.396
Jan 18 16:15:48.405: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 01/18/23 16:15:48.406
Jan 18 16:15:48.423: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 01/18/23 16:15:48.423
Jan 18 16:15:48.426: INFO: Observed &StatefulSet event: ADDED
Jan 18 16:15:48.427: INFO: Found Statefulset ss in namespace statefulset-2258 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 18 16:15:48.427: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 01/18/23 16:15:48.427
Jan 18 16:15:48.427: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 18 16:15:48.437: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 01/18/23 16:15:48.437
Jan 18 16:15:48.441: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 16:15:48.442: INFO: Deleting all statefulset in ns statefulset-2258
Jan 18 16:15:48.447: INFO: Scaling statefulset ss to 0
Jan 18 16:15:58.476: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 16:15:58.480: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 18 16:15:58.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2258" for this suite. 01/18/23 16:15:58.528
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":266,"skipped":4878,"failed":0}
------------------------------
• [SLOW TEST] [20.272 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:15:38.265
    Jan 18 16:15:38.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename statefulset 01/18/23 16:15:38.267
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:15:38.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:15:38.293
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2258 01/18/23 16:15:38.298
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-2258 01/18/23 16:15:38.327
    Jan 18 16:15:38.340: INFO: Found 0 stateful pods, waiting for 1
    Jan 18 16:15:48.351: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 01/18/23 16:15:48.372
    STEP: Getting /status 01/18/23 16:15:48.396
    Jan 18 16:15:48.405: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 01/18/23 16:15:48.406
    Jan 18 16:15:48.423: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 01/18/23 16:15:48.423
    Jan 18 16:15:48.426: INFO: Observed &StatefulSet event: ADDED
    Jan 18 16:15:48.427: INFO: Found Statefulset ss in namespace statefulset-2258 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 18 16:15:48.427: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 01/18/23 16:15:48.427
    Jan 18 16:15:48.427: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 18 16:15:48.437: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 01/18/23 16:15:48.437
    Jan 18 16:15:48.441: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 18 16:15:48.442: INFO: Deleting all statefulset in ns statefulset-2258
    Jan 18 16:15:48.447: INFO: Scaling statefulset ss to 0
    Jan 18 16:15:58.476: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 16:15:58.480: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 18 16:15:58.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2258" for this suite. 01/18/23 16:15:58.528
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:15:58.547
Jan 18 16:15:58.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename containers 01/18/23 16:15:58.548
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:15:58.575
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:15:58.58
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 01/18/23 16:15:58.588
Jan 18 16:15:58.601: INFO: Waiting up to 5m0s for pod "client-containers-6cc80abd-cc4d-43f4-9702-5f00642da5d4" in namespace "containers-860" to be "Succeeded or Failed"
Jan 18 16:15:58.605: INFO: Pod "client-containers-6cc80abd-cc4d-43f4-9702-5f00642da5d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.806037ms
Jan 18 16:16:00.617: INFO: Pod "client-containers-6cc80abd-cc4d-43f4-9702-5f00642da5d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015787782s
Jan 18 16:16:02.611: INFO: Pod "client-containers-6cc80abd-cc4d-43f4-9702-5f00642da5d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010105748s
Jan 18 16:16:04.611: INFO: Pod "client-containers-6cc80abd-cc4d-43f4-9702-5f00642da5d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010017393s
STEP: Saw pod success 01/18/23 16:16:04.611
Jan 18 16:16:04.611: INFO: Pod "client-containers-6cc80abd-cc4d-43f4-9702-5f00642da5d4" satisfied condition "Succeeded or Failed"
Jan 18 16:16:04.618: INFO: Trying to get logs from node v1-25-1-18760-w2 pod client-containers-6cc80abd-cc4d-43f4-9702-5f00642da5d4 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 16:16:04.638
Jan 18 16:16:04.653: INFO: Waiting for pod client-containers-6cc80abd-cc4d-43f4-9702-5f00642da5d4 to disappear
Jan 18 16:16:04.657: INFO: Pod client-containers-6cc80abd-cc4d-43f4-9702-5f00642da5d4 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 18 16:16:04.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-860" for this suite. 01/18/23 16:16:04.663
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":267,"skipped":4892,"failed":0}
------------------------------
• [SLOW TEST] [6.122 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:15:58.547
    Jan 18 16:15:58.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename containers 01/18/23 16:15:58.548
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:15:58.575
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:15:58.58
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 01/18/23 16:15:58.588
    Jan 18 16:15:58.601: INFO: Waiting up to 5m0s for pod "client-containers-6cc80abd-cc4d-43f4-9702-5f00642da5d4" in namespace "containers-860" to be "Succeeded or Failed"
    Jan 18 16:15:58.605: INFO: Pod "client-containers-6cc80abd-cc4d-43f4-9702-5f00642da5d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.806037ms
    Jan 18 16:16:00.617: INFO: Pod "client-containers-6cc80abd-cc4d-43f4-9702-5f00642da5d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015787782s
    Jan 18 16:16:02.611: INFO: Pod "client-containers-6cc80abd-cc4d-43f4-9702-5f00642da5d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010105748s
    Jan 18 16:16:04.611: INFO: Pod "client-containers-6cc80abd-cc4d-43f4-9702-5f00642da5d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010017393s
    STEP: Saw pod success 01/18/23 16:16:04.611
    Jan 18 16:16:04.611: INFO: Pod "client-containers-6cc80abd-cc4d-43f4-9702-5f00642da5d4" satisfied condition "Succeeded or Failed"
    Jan 18 16:16:04.618: INFO: Trying to get logs from node v1-25-1-18760-w2 pod client-containers-6cc80abd-cc4d-43f4-9702-5f00642da5d4 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 16:16:04.638
    Jan 18 16:16:04.653: INFO: Waiting for pod client-containers-6cc80abd-cc4d-43f4-9702-5f00642da5d4 to disappear
    Jan 18 16:16:04.657: INFO: Pod client-containers-6cc80abd-cc4d-43f4-9702-5f00642da5d4 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 18 16:16:04.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-860" for this suite. 01/18/23 16:16:04.663
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:16:04.683
Jan 18 16:16:04.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename services 01/18/23 16:16:04.684
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:16:04.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:16:04.713
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-4860 01/18/23 16:16:04.717
STEP: creating service affinity-nodeport in namespace services-4860 01/18/23 16:16:04.717
STEP: creating replication controller affinity-nodeport in namespace services-4860 01/18/23 16:16:04.738
I0118 16:16:04.749949      19 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-4860, replica count: 3
I0118 16:16:07.802951      19 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 16:16:07.842: INFO: Creating new exec pod
Jan 18 16:16:07.853: INFO: Waiting up to 5m0s for pod "execpod-affinitydhhw9" in namespace "services-4860" to be "running"
Jan 18 16:16:07.865: INFO: Pod "execpod-affinitydhhw9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.050994ms
Jan 18 16:16:09.872: INFO: Pod "execpod-affinitydhhw9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018721834s
Jan 18 16:16:11.870: INFO: Pod "execpod-affinitydhhw9": Phase="Running", Reason="", readiness=true. Elapsed: 4.017104416s
Jan 18 16:16:11.870: INFO: Pod "execpod-affinitydhhw9" satisfied condition "running"
Jan 18 16:16:12.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4860 exec execpod-affinitydhhw9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jan 18 16:16:13.119: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan 18 16:16:13.119: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 16:16:13.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4860 exec execpod-affinitydhhw9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.47.176 80'
Jan 18 16:16:13.357: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.47.176 80\nConnection to 10.233.47.176 80 port [tcp/http] succeeded!\n"
Jan 18 16:16:13.357: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 16:16:13.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4860 exec execpod-affinitydhhw9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.168 30459'
Jan 18 16:16:13.566: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.168 30459\nConnection to 192.168.101.168 30459 port [tcp/*] succeeded!\n"
Jan 18 16:16:13.566: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 16:16:13.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4860 exec execpod-affinitydhhw9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 30459'
Jan 18 16:16:13.782: INFO: stderr: "+ + nc -v -t -w 2 192.168.101.216 30459\necho hostName\nConnection to 192.168.101.216 30459 port [tcp/*] succeeded!\n"
Jan 18 16:16:13.782: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 16:16:13.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4860 exec execpod-affinitydhhw9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.101.168:30459/ ; done'
Jan 18 16:16:14.155: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n"
Jan 18 16:16:14.155: INFO: stdout: "\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p"
Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
Jan 18 16:16:14.155: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-4860, will wait for the garbage collector to delete the pods 01/18/23 16:16:14.171
Jan 18 16:16:14.236: INFO: Deleting ReplicationController affinity-nodeport took: 6.141002ms
Jan 18 16:16:14.336: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.553094ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 16:16:17.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4860" for this suite. 01/18/23 16:16:17.382
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":268,"skipped":4943,"failed":0}
------------------------------
• [SLOW TEST] [12.710 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:16:04.683
    Jan 18 16:16:04.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename services 01/18/23 16:16:04.684
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:16:04.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:16:04.713
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-4860 01/18/23 16:16:04.717
    STEP: creating service affinity-nodeport in namespace services-4860 01/18/23 16:16:04.717
    STEP: creating replication controller affinity-nodeport in namespace services-4860 01/18/23 16:16:04.738
    I0118 16:16:04.749949      19 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-4860, replica count: 3
    I0118 16:16:07.802951      19 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 16:16:07.842: INFO: Creating new exec pod
    Jan 18 16:16:07.853: INFO: Waiting up to 5m0s for pod "execpod-affinitydhhw9" in namespace "services-4860" to be "running"
    Jan 18 16:16:07.865: INFO: Pod "execpod-affinitydhhw9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.050994ms
    Jan 18 16:16:09.872: INFO: Pod "execpod-affinitydhhw9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018721834s
    Jan 18 16:16:11.870: INFO: Pod "execpod-affinitydhhw9": Phase="Running", Reason="", readiness=true. Elapsed: 4.017104416s
    Jan 18 16:16:11.870: INFO: Pod "execpod-affinitydhhw9" satisfied condition "running"
    Jan 18 16:16:12.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4860 exec execpod-affinitydhhw9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Jan 18 16:16:13.119: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jan 18 16:16:13.119: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 16:16:13.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4860 exec execpod-affinitydhhw9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.47.176 80'
    Jan 18 16:16:13.357: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.47.176 80\nConnection to 10.233.47.176 80 port [tcp/http] succeeded!\n"
    Jan 18 16:16:13.357: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 16:16:13.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4860 exec execpod-affinitydhhw9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.168 30459'
    Jan 18 16:16:13.566: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.101.168 30459\nConnection to 192.168.101.168 30459 port [tcp/*] succeeded!\n"
    Jan 18 16:16:13.566: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 16:16:13.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4860 exec execpod-affinitydhhw9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.101.216 30459'
    Jan 18 16:16:13.782: INFO: stderr: "+ + nc -v -t -w 2 192.168.101.216 30459\necho hostName\nConnection to 192.168.101.216 30459 port [tcp/*] succeeded!\n"
    Jan 18 16:16:13.782: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 16:16:13.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4860 exec execpod-affinitydhhw9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.101.168:30459/ ; done'
    Jan 18 16:16:14.155: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.168:30459/\n"
    Jan 18 16:16:14.155: INFO: stdout: "\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p\naffinity-nodeport-nzw7p"
    Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
    Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
    Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
    Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
    Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
    Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
    Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
    Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
    Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
    Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
    Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
    Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
    Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
    Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
    Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
    Jan 18 16:16:14.155: INFO: Received response from host: affinity-nodeport-nzw7p
    Jan 18 16:16:14.155: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-4860, will wait for the garbage collector to delete the pods 01/18/23 16:16:14.171
    Jan 18 16:16:14.236: INFO: Deleting ReplicationController affinity-nodeport took: 6.141002ms
    Jan 18 16:16:14.336: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.553094ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 16:16:17.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4860" for this suite. 01/18/23 16:16:17.382
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:16:17.393
Jan 18 16:16:17.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 16:16:17.395
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:16:17.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:16:17.484
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-e4fbf4fc-099a-4f32-8372-5be57d01af67 01/18/23 16:16:17.49
STEP: Creating a pod to test consume configMaps 01/18/23 16:16:17.497
Jan 18 16:16:17.514: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1afdcef1-4f58-4b56-badd-3f8e7f341803" in namespace "projected-5280" to be "Succeeded or Failed"
Jan 18 16:16:17.529: INFO: Pod "pod-projected-configmaps-1afdcef1-4f58-4b56-badd-3f8e7f341803": Phase="Pending", Reason="", readiness=false. Elapsed: 15.071043ms
Jan 18 16:16:19.535: INFO: Pod "pod-projected-configmaps-1afdcef1-4f58-4b56-badd-3f8e7f341803": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021518467s
Jan 18 16:16:21.534: INFO: Pod "pod-projected-configmaps-1afdcef1-4f58-4b56-badd-3f8e7f341803": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019868365s
Jan 18 16:16:23.536: INFO: Pod "pod-projected-configmaps-1afdcef1-4f58-4b56-badd-3f8e7f341803": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021973206s
STEP: Saw pod success 01/18/23 16:16:23.536
Jan 18 16:16:23.537: INFO: Pod "pod-projected-configmaps-1afdcef1-4f58-4b56-badd-3f8e7f341803" satisfied condition "Succeeded or Failed"
Jan 18 16:16:23.543: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-1afdcef1-4f58-4b56-badd-3f8e7f341803 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 16:16:23.554
Jan 18 16:16:23.570: INFO: Waiting for pod pod-projected-configmaps-1afdcef1-4f58-4b56-badd-3f8e7f341803 to disappear
Jan 18 16:16:23.574: INFO: Pod pod-projected-configmaps-1afdcef1-4f58-4b56-badd-3f8e7f341803 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 18 16:16:23.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5280" for this suite. 01/18/23 16:16:23.581
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":269,"skipped":4946,"failed":0}
------------------------------
• [SLOW TEST] [6.198 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:16:17.393
    Jan 18 16:16:17.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 16:16:17.395
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:16:17.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:16:17.484
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-e4fbf4fc-099a-4f32-8372-5be57d01af67 01/18/23 16:16:17.49
    STEP: Creating a pod to test consume configMaps 01/18/23 16:16:17.497
    Jan 18 16:16:17.514: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1afdcef1-4f58-4b56-badd-3f8e7f341803" in namespace "projected-5280" to be "Succeeded or Failed"
    Jan 18 16:16:17.529: INFO: Pod "pod-projected-configmaps-1afdcef1-4f58-4b56-badd-3f8e7f341803": Phase="Pending", Reason="", readiness=false. Elapsed: 15.071043ms
    Jan 18 16:16:19.535: INFO: Pod "pod-projected-configmaps-1afdcef1-4f58-4b56-badd-3f8e7f341803": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021518467s
    Jan 18 16:16:21.534: INFO: Pod "pod-projected-configmaps-1afdcef1-4f58-4b56-badd-3f8e7f341803": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019868365s
    Jan 18 16:16:23.536: INFO: Pod "pod-projected-configmaps-1afdcef1-4f58-4b56-badd-3f8e7f341803": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021973206s
    STEP: Saw pod success 01/18/23 16:16:23.536
    Jan 18 16:16:23.537: INFO: Pod "pod-projected-configmaps-1afdcef1-4f58-4b56-badd-3f8e7f341803" satisfied condition "Succeeded or Failed"
    Jan 18 16:16:23.543: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-1afdcef1-4f58-4b56-badd-3f8e7f341803 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 16:16:23.554
    Jan 18 16:16:23.570: INFO: Waiting for pod pod-projected-configmaps-1afdcef1-4f58-4b56-badd-3f8e7f341803 to disappear
    Jan 18 16:16:23.574: INFO: Pod pod-projected-configmaps-1afdcef1-4f58-4b56-badd-3f8e7f341803 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 18 16:16:23.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5280" for this suite. 01/18/23 16:16:23.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:16:23.594
Jan 18 16:16:23.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename webhook 01/18/23 16:16:23.598
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:16:23.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:16:23.625
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 16:16:23.652
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 16:16:24.218
STEP: Deploying the webhook pod 01/18/23 16:16:24.225
STEP: Wait for the deployment to be ready 01/18/23 16:16:24.264
Jan 18 16:16:24.279: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 16:16:26.301
STEP: Verifying the service has paired with the endpoint 01/18/23 16:16:26.314
Jan 18 16:16:27.315: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 01/18/23 16:16:27.32
STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 16:16:27.345
STEP: Updating a validating webhook configuration's rules to not include the create operation 01/18/23 16:16:27.358
STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 16:16:27.373
STEP: Patching a validating webhook configuration's rules to include the create operation 01/18/23 16:16:27.392
STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 16:16:27.409
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 16:16:27.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1824" for this suite. 01/18/23 16:16:27.433
STEP: Destroying namespace "webhook-1824-markers" for this suite. 01/18/23 16:16:27.449
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":270,"skipped":4954,"failed":0}
------------------------------
• [3.939 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:16:23.594
    Jan 18 16:16:23.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename webhook 01/18/23 16:16:23.598
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:16:23.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:16:23.625
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 16:16:23.652
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 16:16:24.218
    STEP: Deploying the webhook pod 01/18/23 16:16:24.225
    STEP: Wait for the deployment to be ready 01/18/23 16:16:24.264
    Jan 18 16:16:24.279: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 16:16:26.301
    STEP: Verifying the service has paired with the endpoint 01/18/23 16:16:26.314
    Jan 18 16:16:27.315: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 01/18/23 16:16:27.32
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 16:16:27.345
    STEP: Updating a validating webhook configuration's rules to not include the create operation 01/18/23 16:16:27.358
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 16:16:27.373
    STEP: Patching a validating webhook configuration's rules to include the create operation 01/18/23 16:16:27.392
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 16:16:27.409
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 16:16:27.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1824" for this suite. 01/18/23 16:16:27.433
    STEP: Destroying namespace "webhook-1824-markers" for this suite. 01/18/23 16:16:27.449
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:16:27.536
Jan 18 16:16:27.536: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 16:16:27.539
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:16:27.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:16:27.713
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-04bdd4e5-2caa-4d1e-9f44-9102daa24cd0 01/18/23 16:16:27.726
STEP: Creating a pod to test consume secrets 01/18/23 16:16:27.734
Jan 18 16:16:27.754: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-75bac770-30a1-4e4e-845d-0a1ec9a12b11" in namespace "projected-4917" to be "Succeeded or Failed"
Jan 18 16:16:27.783: INFO: Pod "pod-projected-secrets-75bac770-30a1-4e4e-845d-0a1ec9a12b11": Phase="Pending", Reason="", readiness=false. Elapsed: 28.967902ms
Jan 18 16:16:29.789: INFO: Pod "pod-projected-secrets-75bac770-30a1-4e4e-845d-0a1ec9a12b11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034900066s
Jan 18 16:16:31.789: INFO: Pod "pod-projected-secrets-75bac770-30a1-4e4e-845d-0a1ec9a12b11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034973396s
STEP: Saw pod success 01/18/23 16:16:31.789
Jan 18 16:16:31.790: INFO: Pod "pod-projected-secrets-75bac770-30a1-4e4e-845d-0a1ec9a12b11" satisfied condition "Succeeded or Failed"
Jan 18 16:16:31.795: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-secrets-75bac770-30a1-4e4e-845d-0a1ec9a12b11 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/18/23 16:16:31.805
Jan 18 16:16:31.822: INFO: Waiting for pod pod-projected-secrets-75bac770-30a1-4e4e-845d-0a1ec9a12b11 to disappear
Jan 18 16:16:31.827: INFO: Pod pod-projected-secrets-75bac770-30a1-4e4e-845d-0a1ec9a12b11 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 18 16:16:31.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4917" for this suite. 01/18/23 16:16:31.835
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":271,"skipped":4958,"failed":0}
------------------------------
• [4.312 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:16:27.536
    Jan 18 16:16:27.536: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 16:16:27.539
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:16:27.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:16:27.713
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-04bdd4e5-2caa-4d1e-9f44-9102daa24cd0 01/18/23 16:16:27.726
    STEP: Creating a pod to test consume secrets 01/18/23 16:16:27.734
    Jan 18 16:16:27.754: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-75bac770-30a1-4e4e-845d-0a1ec9a12b11" in namespace "projected-4917" to be "Succeeded or Failed"
    Jan 18 16:16:27.783: INFO: Pod "pod-projected-secrets-75bac770-30a1-4e4e-845d-0a1ec9a12b11": Phase="Pending", Reason="", readiness=false. Elapsed: 28.967902ms
    Jan 18 16:16:29.789: INFO: Pod "pod-projected-secrets-75bac770-30a1-4e4e-845d-0a1ec9a12b11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034900066s
    Jan 18 16:16:31.789: INFO: Pod "pod-projected-secrets-75bac770-30a1-4e4e-845d-0a1ec9a12b11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034973396s
    STEP: Saw pod success 01/18/23 16:16:31.789
    Jan 18 16:16:31.790: INFO: Pod "pod-projected-secrets-75bac770-30a1-4e4e-845d-0a1ec9a12b11" satisfied condition "Succeeded or Failed"
    Jan 18 16:16:31.795: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-secrets-75bac770-30a1-4e4e-845d-0a1ec9a12b11 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 16:16:31.805
    Jan 18 16:16:31.822: INFO: Waiting for pod pod-projected-secrets-75bac770-30a1-4e4e-845d-0a1ec9a12b11 to disappear
    Jan 18 16:16:31.827: INFO: Pod pod-projected-secrets-75bac770-30a1-4e4e-845d-0a1ec9a12b11 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 18 16:16:31.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4917" for this suite. 01/18/23 16:16:31.835
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:16:31.852
Jan 18 16:16:31.852: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename controllerrevisions 01/18/23 16:16:31.853
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:16:31.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:16:31.886
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-lz2mk-daemon-set" 01/18/23 16:16:31.908
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 16:16:31.914
Jan 18 16:16:31.927: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 16:16:31.938: INFO: Number of nodes with available pods controlled by daemonset e2e-lz2mk-daemon-set: 0
Jan 18 16:16:31.938: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 16:16:32.955: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 16:16:33.020: INFO: Number of nodes with available pods controlled by daemonset e2e-lz2mk-daemon-set: 0
Jan 18 16:16:33.020: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 16:16:33.945: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 16:16:33.951: INFO: Number of nodes with available pods controlled by daemonset e2e-lz2mk-daemon-set: 2
Jan 18 16:16:33.951: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-lz2mk-daemon-set
STEP: Confirm DaemonSet "e2e-lz2mk-daemon-set" successfully created with "daemonset-name=e2e-lz2mk-daemon-set" label 01/18/23 16:16:33.956
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-lz2mk-daemon-set" 01/18/23 16:16:33.965
Jan 18 16:16:33.973: INFO: Located ControllerRevision: "e2e-lz2mk-daemon-set-54dc76fbc9"
STEP: Patching ControllerRevision "e2e-lz2mk-daemon-set-54dc76fbc9" 01/18/23 16:16:33.976
Jan 18 16:16:33.984: INFO: e2e-lz2mk-daemon-set-54dc76fbc9 has been patched
STEP: Create a new ControllerRevision 01/18/23 16:16:33.984
Jan 18 16:16:34.027: INFO: Created ControllerRevision: e2e-lz2mk-daemon-set-6f565d4f4c
STEP: Confirm that there are two ControllerRevisions 01/18/23 16:16:34.027
Jan 18 16:16:34.028: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 18 16:16:34.032: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-lz2mk-daemon-set-54dc76fbc9" 01/18/23 16:16:34.032
STEP: Confirm that there is only one ControllerRevision 01/18/23 16:16:34.039
Jan 18 16:16:34.040: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 18 16:16:34.043: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-lz2mk-daemon-set-6f565d4f4c" 01/18/23 16:16:34.047
Jan 18 16:16:34.064: INFO: e2e-lz2mk-daemon-set-6f565d4f4c has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 01/18/23 16:16:34.064
W0118 16:16:34.073145      19 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 01/18/23 16:16:34.073
Jan 18 16:16:34.073: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 18 16:16:35.077: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 18 16:16:35.083: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-lz2mk-daemon-set-6f565d4f4c=updated" 01/18/23 16:16:35.083
STEP: Confirm that there is only one ControllerRevision 01/18/23 16:16:35.092
Jan 18 16:16:35.092: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 18 16:16:35.097: INFO: Found 1 ControllerRevisions
Jan 18 16:16:35.102: INFO: ControllerRevision "e2e-lz2mk-daemon-set-675d765574" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-lz2mk-daemon-set" 01/18/23 16:16:35.111
STEP: deleting DaemonSet.extensions e2e-lz2mk-daemon-set in namespace controllerrevisions-6278, will wait for the garbage collector to delete the pods 01/18/23 16:16:35.111
Jan 18 16:16:35.173: INFO: Deleting DaemonSet.extensions e2e-lz2mk-daemon-set took: 7.000304ms
Jan 18 16:16:35.279: INFO: Terminating DaemonSet.extensions e2e-lz2mk-daemon-set pods took: 105.258829ms
Jan 18 16:16:36.683: INFO: Number of nodes with available pods controlled by daemonset e2e-lz2mk-daemon-set: 0
Jan 18 16:16:36.684: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-lz2mk-daemon-set
Jan 18 16:16:36.688: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"60615"},"items":null}

Jan 18 16:16:36.692: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"60615"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Jan 18 16:16:36.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-6278" for this suite. 01/18/23 16:16:36.717
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":272,"skipped":4961,"failed":0}
------------------------------
• [4.873 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:16:31.852
    Jan 18 16:16:31.852: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename controllerrevisions 01/18/23 16:16:31.853
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:16:31.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:16:31.886
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-lz2mk-daemon-set" 01/18/23 16:16:31.908
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 16:16:31.914
    Jan 18 16:16:31.927: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 16:16:31.938: INFO: Number of nodes with available pods controlled by daemonset e2e-lz2mk-daemon-set: 0
    Jan 18 16:16:31.938: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 16:16:32.955: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 16:16:33.020: INFO: Number of nodes with available pods controlled by daemonset e2e-lz2mk-daemon-set: 0
    Jan 18 16:16:33.020: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 16:16:33.945: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 16:16:33.951: INFO: Number of nodes with available pods controlled by daemonset e2e-lz2mk-daemon-set: 2
    Jan 18 16:16:33.951: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-lz2mk-daemon-set
    STEP: Confirm DaemonSet "e2e-lz2mk-daemon-set" successfully created with "daemonset-name=e2e-lz2mk-daemon-set" label 01/18/23 16:16:33.956
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-lz2mk-daemon-set" 01/18/23 16:16:33.965
    Jan 18 16:16:33.973: INFO: Located ControllerRevision: "e2e-lz2mk-daemon-set-54dc76fbc9"
    STEP: Patching ControllerRevision "e2e-lz2mk-daemon-set-54dc76fbc9" 01/18/23 16:16:33.976
    Jan 18 16:16:33.984: INFO: e2e-lz2mk-daemon-set-54dc76fbc9 has been patched
    STEP: Create a new ControllerRevision 01/18/23 16:16:33.984
    Jan 18 16:16:34.027: INFO: Created ControllerRevision: e2e-lz2mk-daemon-set-6f565d4f4c
    STEP: Confirm that there are two ControllerRevisions 01/18/23 16:16:34.027
    Jan 18 16:16:34.028: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 18 16:16:34.032: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-lz2mk-daemon-set-54dc76fbc9" 01/18/23 16:16:34.032
    STEP: Confirm that there is only one ControllerRevision 01/18/23 16:16:34.039
    Jan 18 16:16:34.040: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 18 16:16:34.043: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-lz2mk-daemon-set-6f565d4f4c" 01/18/23 16:16:34.047
    Jan 18 16:16:34.064: INFO: e2e-lz2mk-daemon-set-6f565d4f4c has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 01/18/23 16:16:34.064
    W0118 16:16:34.073145      19 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 01/18/23 16:16:34.073
    Jan 18 16:16:34.073: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 18 16:16:35.077: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 18 16:16:35.083: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-lz2mk-daemon-set-6f565d4f4c=updated" 01/18/23 16:16:35.083
    STEP: Confirm that there is only one ControllerRevision 01/18/23 16:16:35.092
    Jan 18 16:16:35.092: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 18 16:16:35.097: INFO: Found 1 ControllerRevisions
    Jan 18 16:16:35.102: INFO: ControllerRevision "e2e-lz2mk-daemon-set-675d765574" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-lz2mk-daemon-set" 01/18/23 16:16:35.111
    STEP: deleting DaemonSet.extensions e2e-lz2mk-daemon-set in namespace controllerrevisions-6278, will wait for the garbage collector to delete the pods 01/18/23 16:16:35.111
    Jan 18 16:16:35.173: INFO: Deleting DaemonSet.extensions e2e-lz2mk-daemon-set took: 7.000304ms
    Jan 18 16:16:35.279: INFO: Terminating DaemonSet.extensions e2e-lz2mk-daemon-set pods took: 105.258829ms
    Jan 18 16:16:36.683: INFO: Number of nodes with available pods controlled by daemonset e2e-lz2mk-daemon-set: 0
    Jan 18 16:16:36.684: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-lz2mk-daemon-set
    Jan 18 16:16:36.688: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"60615"},"items":null}

    Jan 18 16:16:36.692: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"60615"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 16:16:36.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-6278" for this suite. 01/18/23 16:16:36.717
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:16:36.727
Jan 18 16:16:36.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename downward-api 01/18/23 16:16:36.728
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:16:36.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:16:36.759
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 01/18/23 16:16:36.764
Jan 18 16:16:36.775: INFO: Waiting up to 5m0s for pod "downward-api-bc358edb-a534-44cc-9202-bd8f4abed6c3" in namespace "downward-api-6635" to be "Succeeded or Failed"
Jan 18 16:16:36.793: INFO: Pod "downward-api-bc358edb-a534-44cc-9202-bd8f4abed6c3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.427158ms
Jan 18 16:16:38.799: INFO: Pod "downward-api-bc358edb-a534-44cc-9202-bd8f4abed6c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024125236s
Jan 18 16:16:40.800: INFO: Pod "downward-api-bc358edb-a534-44cc-9202-bd8f4abed6c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025206382s
STEP: Saw pod success 01/18/23 16:16:40.8
Jan 18 16:16:40.800: INFO: Pod "downward-api-bc358edb-a534-44cc-9202-bd8f4abed6c3" satisfied condition "Succeeded or Failed"
Jan 18 16:16:40.805: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downward-api-bc358edb-a534-44cc-9202-bd8f4abed6c3 container dapi-container: <nil>
STEP: delete the pod 01/18/23 16:16:40.813
Jan 18 16:16:40.829: INFO: Waiting for pod downward-api-bc358edb-a534-44cc-9202-bd8f4abed6c3 to disappear
Jan 18 16:16:40.841: INFO: Pod downward-api-bc358edb-a534-44cc-9202-bd8f4abed6c3 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 18 16:16:40.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6635" for this suite. 01/18/23 16:16:40.848
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":273,"skipped":4964,"failed":0}
------------------------------
• [4.128 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:16:36.727
    Jan 18 16:16:36.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename downward-api 01/18/23 16:16:36.728
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:16:36.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:16:36.759
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 01/18/23 16:16:36.764
    Jan 18 16:16:36.775: INFO: Waiting up to 5m0s for pod "downward-api-bc358edb-a534-44cc-9202-bd8f4abed6c3" in namespace "downward-api-6635" to be "Succeeded or Failed"
    Jan 18 16:16:36.793: INFO: Pod "downward-api-bc358edb-a534-44cc-9202-bd8f4abed6c3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.427158ms
    Jan 18 16:16:38.799: INFO: Pod "downward-api-bc358edb-a534-44cc-9202-bd8f4abed6c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024125236s
    Jan 18 16:16:40.800: INFO: Pod "downward-api-bc358edb-a534-44cc-9202-bd8f4abed6c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025206382s
    STEP: Saw pod success 01/18/23 16:16:40.8
    Jan 18 16:16:40.800: INFO: Pod "downward-api-bc358edb-a534-44cc-9202-bd8f4abed6c3" satisfied condition "Succeeded or Failed"
    Jan 18 16:16:40.805: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downward-api-bc358edb-a534-44cc-9202-bd8f4abed6c3 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 16:16:40.813
    Jan 18 16:16:40.829: INFO: Waiting for pod downward-api-bc358edb-a534-44cc-9202-bd8f4abed6c3 to disappear
    Jan 18 16:16:40.841: INFO: Pod downward-api-bc358edb-a534-44cc-9202-bd8f4abed6c3 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 18 16:16:40.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6635" for this suite. 01/18/23 16:16:40.848
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:16:40.868
Jan 18 16:16:40.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename statefulset 01/18/23 16:16:40.87
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:16:40.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:16:40.904
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5125 01/18/23 16:16:40.909
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 01/18/23 16:16:40.93
STEP: Creating pod with conflicting port in namespace statefulset-5125 01/18/23 16:16:40.956
STEP: Waiting until pod test-pod will start running in namespace statefulset-5125 01/18/23 16:16:40.987
Jan 18 16:16:40.987: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-5125" to be "running"
Jan 18 16:16:40.993: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.63008ms
Jan 18 16:16:42.998: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010771065s
Jan 18 16:16:45.002: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014625237s
Jan 18 16:16:45.002: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-5125 01/18/23 16:16:45.002
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5125 01/18/23 16:16:45.009
Jan 18 16:16:45.045: INFO: Observed stateful pod in namespace: statefulset-5125, name: ss-0, uid: 496aa953-3298-4c8b-adf9-9d2472a23893, status phase: Pending. Waiting for statefulset controller to delete.
Jan 18 16:16:45.067: INFO: Observed stateful pod in namespace: statefulset-5125, name: ss-0, uid: 496aa953-3298-4c8b-adf9-9d2472a23893, status phase: Failed. Waiting for statefulset controller to delete.
Jan 18 16:16:45.093: INFO: Observed stateful pod in namespace: statefulset-5125, name: ss-0, uid: 496aa953-3298-4c8b-adf9-9d2472a23893, status phase: Failed. Waiting for statefulset controller to delete.
Jan 18 16:16:45.093: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5125
STEP: Removing pod with conflicting port in namespace statefulset-5125 01/18/23 16:16:45.094
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5125 and will be in running state 01/18/23 16:16:45.113
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 16:16:49.146: INFO: Deleting all statefulset in ns statefulset-5125
Jan 18 16:16:49.151: INFO: Scaling statefulset ss to 0
Jan 18 16:16:59.180: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 16:16:59.185: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 18 16:16:59.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5125" for this suite. 01/18/23 16:16:59.216
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":274,"skipped":4982,"failed":0}
------------------------------
• [SLOW TEST] [18.359 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:16:40.868
    Jan 18 16:16:40.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename statefulset 01/18/23 16:16:40.87
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:16:40.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:16:40.904
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5125 01/18/23 16:16:40.909
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 01/18/23 16:16:40.93
    STEP: Creating pod with conflicting port in namespace statefulset-5125 01/18/23 16:16:40.956
    STEP: Waiting until pod test-pod will start running in namespace statefulset-5125 01/18/23 16:16:40.987
    Jan 18 16:16:40.987: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-5125" to be "running"
    Jan 18 16:16:40.993: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.63008ms
    Jan 18 16:16:42.998: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010771065s
    Jan 18 16:16:45.002: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014625237s
    Jan 18 16:16:45.002: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-5125 01/18/23 16:16:45.002
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5125 01/18/23 16:16:45.009
    Jan 18 16:16:45.045: INFO: Observed stateful pod in namespace: statefulset-5125, name: ss-0, uid: 496aa953-3298-4c8b-adf9-9d2472a23893, status phase: Pending. Waiting for statefulset controller to delete.
    Jan 18 16:16:45.067: INFO: Observed stateful pod in namespace: statefulset-5125, name: ss-0, uid: 496aa953-3298-4c8b-adf9-9d2472a23893, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 18 16:16:45.093: INFO: Observed stateful pod in namespace: statefulset-5125, name: ss-0, uid: 496aa953-3298-4c8b-adf9-9d2472a23893, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 18 16:16:45.093: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5125
    STEP: Removing pod with conflicting port in namespace statefulset-5125 01/18/23 16:16:45.094
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5125 and will be in running state 01/18/23 16:16:45.113
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 18 16:16:49.146: INFO: Deleting all statefulset in ns statefulset-5125
    Jan 18 16:16:49.151: INFO: Scaling statefulset ss to 0
    Jan 18 16:16:59.180: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 16:16:59.185: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 18 16:16:59.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5125" for this suite. 01/18/23 16:16:59.216
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:16:59.228
Jan 18 16:16:59.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename crd-watch 01/18/23 16:16:59.231
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:16:59.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:16:59.255
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jan 18 16:16:59.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Creating first CR  01/18/23 16:17:01.854
Jan 18 16:17:01.859: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T16:17:01Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T16:17:01Z]] name:name1 resourceVersion:60852 uid:02ed4544-ca6e-423b-84b3-d0ad09a75641] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 01/18/23 16:17:11.86
Jan 18 16:17:11.870: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T16:17:11Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T16:17:11Z]] name:name2 resourceVersion:60908 uid:468ab789-ea18-464e-a525-ff16b0aeb793] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 01/18/23 16:17:21.871
Jan 18 16:17:21.883: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T16:17:01Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T16:17:21Z]] name:name1 resourceVersion:60944 uid:02ed4544-ca6e-423b-84b3-d0ad09a75641] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 01/18/23 16:17:31.884
Jan 18 16:17:31.894: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T16:17:11Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T16:17:31Z]] name:name2 resourceVersion:60983 uid:468ab789-ea18-464e-a525-ff16b0aeb793] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 01/18/23 16:17:41.897
Jan 18 16:17:41.911: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T16:17:01Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T16:17:21Z]] name:name1 resourceVersion:61022 uid:02ed4544-ca6e-423b-84b3-d0ad09a75641] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 01/18/23 16:17:51.912
Jan 18 16:17:51.921: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T16:17:11Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T16:17:31Z]] name:name2 resourceVersion:61059 uid:468ab789-ea18-464e-a525-ff16b0aeb793] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 16:18:02.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6910" for this suite. 01/18/23 16:18:02.448
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":275,"skipped":4983,"failed":0}
------------------------------
• [SLOW TEST] [63.227 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:16:59.228
    Jan 18 16:16:59.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename crd-watch 01/18/23 16:16:59.231
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:16:59.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:16:59.255
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jan 18 16:16:59.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Creating first CR  01/18/23 16:17:01.854
    Jan 18 16:17:01.859: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T16:17:01Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T16:17:01Z]] name:name1 resourceVersion:60852 uid:02ed4544-ca6e-423b-84b3-d0ad09a75641] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 01/18/23 16:17:11.86
    Jan 18 16:17:11.870: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T16:17:11Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T16:17:11Z]] name:name2 resourceVersion:60908 uid:468ab789-ea18-464e-a525-ff16b0aeb793] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 01/18/23 16:17:21.871
    Jan 18 16:17:21.883: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T16:17:01Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T16:17:21Z]] name:name1 resourceVersion:60944 uid:02ed4544-ca6e-423b-84b3-d0ad09a75641] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 01/18/23 16:17:31.884
    Jan 18 16:17:31.894: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T16:17:11Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T16:17:31Z]] name:name2 resourceVersion:60983 uid:468ab789-ea18-464e-a525-ff16b0aeb793] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 01/18/23 16:17:41.897
    Jan 18 16:17:41.911: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T16:17:01Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T16:17:21Z]] name:name1 resourceVersion:61022 uid:02ed4544-ca6e-423b-84b3-d0ad09a75641] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 01/18/23 16:17:51.912
    Jan 18 16:17:51.921: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T16:17:11Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T16:17:31Z]] name:name2 resourceVersion:61059 uid:468ab789-ea18-464e-a525-ff16b0aeb793] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 16:18:02.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-6910" for this suite. 01/18/23 16:18:02.448
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:18:02.458
Jan 18 16:18:02.458: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename container-probe 01/18/23 16:18:02.46
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:18:02.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:18:02.483
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-c74e4fa3-5f17-4da7-a926-2f2eead06c69 in namespace container-probe-9931 01/18/23 16:18:02.488
Jan 18 16:18:02.497: INFO: Waiting up to 5m0s for pod "busybox-c74e4fa3-5f17-4da7-a926-2f2eead06c69" in namespace "container-probe-9931" to be "not pending"
Jan 18 16:18:02.516: INFO: Pod "busybox-c74e4fa3-5f17-4da7-a926-2f2eead06c69": Phase="Pending", Reason="", readiness=false. Elapsed: 19.112054ms
Jan 18 16:18:04.532: INFO: Pod "busybox-c74e4fa3-5f17-4da7-a926-2f2eead06c69": Phase="Running", Reason="", readiness=true. Elapsed: 2.034749656s
Jan 18 16:18:04.532: INFO: Pod "busybox-c74e4fa3-5f17-4da7-a926-2f2eead06c69" satisfied condition "not pending"
Jan 18 16:18:04.532: INFO: Started pod busybox-c74e4fa3-5f17-4da7-a926-2f2eead06c69 in namespace container-probe-9931
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 16:18:04.532
Jan 18 16:18:04.538: INFO: Initial restart count of pod busybox-c74e4fa3-5f17-4da7-a926-2f2eead06c69 is 0
STEP: deleting the pod 01/18/23 16:22:05.394
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 18 16:22:05.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9931" for this suite. 01/18/23 16:22:05.427
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":276,"skipped":4989,"failed":0}
------------------------------
• [SLOW TEST] [242.981 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:18:02.458
    Jan 18 16:18:02.458: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename container-probe 01/18/23 16:18:02.46
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:18:02.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:18:02.483
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-c74e4fa3-5f17-4da7-a926-2f2eead06c69 in namespace container-probe-9931 01/18/23 16:18:02.488
    Jan 18 16:18:02.497: INFO: Waiting up to 5m0s for pod "busybox-c74e4fa3-5f17-4da7-a926-2f2eead06c69" in namespace "container-probe-9931" to be "not pending"
    Jan 18 16:18:02.516: INFO: Pod "busybox-c74e4fa3-5f17-4da7-a926-2f2eead06c69": Phase="Pending", Reason="", readiness=false. Elapsed: 19.112054ms
    Jan 18 16:18:04.532: INFO: Pod "busybox-c74e4fa3-5f17-4da7-a926-2f2eead06c69": Phase="Running", Reason="", readiness=true. Elapsed: 2.034749656s
    Jan 18 16:18:04.532: INFO: Pod "busybox-c74e4fa3-5f17-4da7-a926-2f2eead06c69" satisfied condition "not pending"
    Jan 18 16:18:04.532: INFO: Started pod busybox-c74e4fa3-5f17-4da7-a926-2f2eead06c69 in namespace container-probe-9931
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 16:18:04.532
    Jan 18 16:18:04.538: INFO: Initial restart count of pod busybox-c74e4fa3-5f17-4da7-a926-2f2eead06c69 is 0
    STEP: deleting the pod 01/18/23 16:22:05.394
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 18 16:22:05.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9931" for this suite. 01/18/23 16:22:05.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:22:05.44
Jan 18 16:22:05.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename statefulset 01/18/23 16:22:05.442
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:22:05.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:22:05.464
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4089 01/18/23 16:22:05.469
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-4089 01/18/23 16:22:05.477
Jan 18 16:22:05.491: INFO: Found 0 stateful pods, waiting for 1
Jan 18 16:22:15.497: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 01/18/23 16:22:15.505
STEP: updating a scale subresource 01/18/23 16:22:15.509
STEP: verifying the statefulset Spec.Replicas was modified 01/18/23 16:22:15.519
STEP: Patch a scale subresource 01/18/23 16:22:15.556
STEP: verifying the statefulset Spec.Replicas was modified 01/18/23 16:22:15.592
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 16:22:15.606: INFO: Deleting all statefulset in ns statefulset-4089
Jan 18 16:22:15.629: INFO: Scaling statefulset ss to 0
Jan 18 16:22:25.656: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 16:22:25.665: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 18 16:22:25.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4089" for this suite. 01/18/23 16:22:25.697
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":277,"skipped":5001,"failed":0}
------------------------------
• [SLOW TEST] [20.267 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:22:05.44
    Jan 18 16:22:05.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename statefulset 01/18/23 16:22:05.442
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:22:05.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:22:05.464
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4089 01/18/23 16:22:05.469
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-4089 01/18/23 16:22:05.477
    Jan 18 16:22:05.491: INFO: Found 0 stateful pods, waiting for 1
    Jan 18 16:22:15.497: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 01/18/23 16:22:15.505
    STEP: updating a scale subresource 01/18/23 16:22:15.509
    STEP: verifying the statefulset Spec.Replicas was modified 01/18/23 16:22:15.519
    STEP: Patch a scale subresource 01/18/23 16:22:15.556
    STEP: verifying the statefulset Spec.Replicas was modified 01/18/23 16:22:15.592
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 18 16:22:15.606: INFO: Deleting all statefulset in ns statefulset-4089
    Jan 18 16:22:15.629: INFO: Scaling statefulset ss to 0
    Jan 18 16:22:25.656: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 16:22:25.665: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 18 16:22:25.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4089" for this suite. 01/18/23 16:22:25.697
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:22:25.709
Jan 18 16:22:25.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename resourcequota 01/18/23 16:22:25.712
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:22:25.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:22:25.734
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 01/18/23 16:22:25.74
STEP: Creating a ResourceQuota 01/18/23 16:22:30.746
STEP: Ensuring resource quota status is calculated 01/18/23 16:22:30.754
STEP: Creating a Pod that fits quota 01/18/23 16:22:32.76
STEP: Ensuring ResourceQuota status captures the pod usage 01/18/23 16:22:32.777
STEP: Not allowing a pod to be created that exceeds remaining quota 01/18/23 16:22:34.784
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/18/23 16:22:34.789
STEP: Ensuring a pod cannot update its resource requirements 01/18/23 16:22:34.794
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/18/23 16:22:34.799
STEP: Deleting the pod 01/18/23 16:22:36.807
STEP: Ensuring resource quota status released the pod usage 01/18/23 16:22:36.82
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 16:22:38.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4456" for this suite. 01/18/23 16:22:38.837
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":278,"skipped":5010,"failed":0}
------------------------------
• [SLOW TEST] [13.135 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:22:25.709
    Jan 18 16:22:25.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename resourcequota 01/18/23 16:22:25.712
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:22:25.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:22:25.734
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 01/18/23 16:22:25.74
    STEP: Creating a ResourceQuota 01/18/23 16:22:30.746
    STEP: Ensuring resource quota status is calculated 01/18/23 16:22:30.754
    STEP: Creating a Pod that fits quota 01/18/23 16:22:32.76
    STEP: Ensuring ResourceQuota status captures the pod usage 01/18/23 16:22:32.777
    STEP: Not allowing a pod to be created that exceeds remaining quota 01/18/23 16:22:34.784
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/18/23 16:22:34.789
    STEP: Ensuring a pod cannot update its resource requirements 01/18/23 16:22:34.794
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/18/23 16:22:34.799
    STEP: Deleting the pod 01/18/23 16:22:36.807
    STEP: Ensuring resource quota status released the pod usage 01/18/23 16:22:36.82
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 16:22:38.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4456" for this suite. 01/18/23 16:22:38.837
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:22:38.849
Jan 18 16:22:38.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename downward-api 01/18/23 16:22:38.851
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:22:38.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:22:38.877
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 01/18/23 16:22:38.882
Jan 18 16:22:38.892: INFO: Waiting up to 5m0s for pod "downward-api-2eba556a-f69c-40e8-ad8b-5d075cb95339" in namespace "downward-api-8465" to be "Succeeded or Failed"
Jan 18 16:22:38.899: INFO: Pod "downward-api-2eba556a-f69c-40e8-ad8b-5d075cb95339": Phase="Pending", Reason="", readiness=false. Elapsed: 6.690481ms
Jan 18 16:22:40.906: INFO: Pod "downward-api-2eba556a-f69c-40e8-ad8b-5d075cb95339": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013621281s
Jan 18 16:22:42.904: INFO: Pod "downward-api-2eba556a-f69c-40e8-ad8b-5d075cb95339": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012230545s
STEP: Saw pod success 01/18/23 16:22:42.905
Jan 18 16:22:42.905: INFO: Pod "downward-api-2eba556a-f69c-40e8-ad8b-5d075cb95339" satisfied condition "Succeeded or Failed"
Jan 18 16:22:42.910: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downward-api-2eba556a-f69c-40e8-ad8b-5d075cb95339 container dapi-container: <nil>
STEP: delete the pod 01/18/23 16:22:42.932
Jan 18 16:22:42.946: INFO: Waiting for pod downward-api-2eba556a-f69c-40e8-ad8b-5d075cb95339 to disappear
Jan 18 16:22:42.949: INFO: Pod downward-api-2eba556a-f69c-40e8-ad8b-5d075cb95339 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 18 16:22:42.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8465" for this suite. 01/18/23 16:22:42.955
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":279,"skipped":5014,"failed":0}
------------------------------
• [4.114 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:22:38.849
    Jan 18 16:22:38.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename downward-api 01/18/23 16:22:38.851
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:22:38.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:22:38.877
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 01/18/23 16:22:38.882
    Jan 18 16:22:38.892: INFO: Waiting up to 5m0s for pod "downward-api-2eba556a-f69c-40e8-ad8b-5d075cb95339" in namespace "downward-api-8465" to be "Succeeded or Failed"
    Jan 18 16:22:38.899: INFO: Pod "downward-api-2eba556a-f69c-40e8-ad8b-5d075cb95339": Phase="Pending", Reason="", readiness=false. Elapsed: 6.690481ms
    Jan 18 16:22:40.906: INFO: Pod "downward-api-2eba556a-f69c-40e8-ad8b-5d075cb95339": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013621281s
    Jan 18 16:22:42.904: INFO: Pod "downward-api-2eba556a-f69c-40e8-ad8b-5d075cb95339": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012230545s
    STEP: Saw pod success 01/18/23 16:22:42.905
    Jan 18 16:22:42.905: INFO: Pod "downward-api-2eba556a-f69c-40e8-ad8b-5d075cb95339" satisfied condition "Succeeded or Failed"
    Jan 18 16:22:42.910: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downward-api-2eba556a-f69c-40e8-ad8b-5d075cb95339 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 16:22:42.932
    Jan 18 16:22:42.946: INFO: Waiting for pod downward-api-2eba556a-f69c-40e8-ad8b-5d075cb95339 to disappear
    Jan 18 16:22:42.949: INFO: Pod downward-api-2eba556a-f69c-40e8-ad8b-5d075cb95339 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 18 16:22:42.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8465" for this suite. 01/18/23 16:22:42.955
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:22:42.972
Jan 18 16:22:42.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename services 01/18/23 16:22:42.973
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:22:42.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:22:42.999
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-4651 01/18/23 16:22:43.004
STEP: creating service affinity-clusterip in namespace services-4651 01/18/23 16:22:43.004
STEP: creating replication controller affinity-clusterip in namespace services-4651 01/18/23 16:22:43.017
I0118 16:22:43.027081      19 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-4651, replica count: 3
I0118 16:22:46.083482      19 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 16:22:46.092: INFO: Creating new exec pod
Jan 18 16:22:46.098: INFO: Waiting up to 5m0s for pod "execpod-affinitygb8j2" in namespace "services-4651" to be "running"
Jan 18 16:22:46.107: INFO: Pod "execpod-affinitygb8j2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.710144ms
Jan 18 16:22:48.111: INFO: Pod "execpod-affinitygb8j2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013429334s
Jan 18 16:22:48.111: INFO: Pod "execpod-affinitygb8j2" satisfied condition "running"
Jan 18 16:22:49.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4651 exec execpod-affinitygb8j2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jan 18 16:22:49.404: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan 18 16:22:49.404: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 16:22:49.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4651 exec execpod-affinitygb8j2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.27.234 80'
Jan 18 16:22:49.586: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.27.234 80\nConnection to 10.233.27.234 80 port [tcp/http] succeeded!\n"
Jan 18 16:22:49.586: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 16:22:49.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4651 exec execpod-affinitygb8j2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.27.234:80/ ; done'
Jan 18 16:22:49.963: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n"
Jan 18 16:22:49.963: INFO: stdout: "\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws"
Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
Jan 18 16:22:49.963: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-4651, will wait for the garbage collector to delete the pods 01/18/23 16:22:49.985
Jan 18 16:22:50.048: INFO: Deleting ReplicationController affinity-clusterip took: 8.337439ms
Jan 18 16:22:50.150: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.547313ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 16:22:52.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4651" for this suite. 01/18/23 16:22:52.693
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":280,"skipped":5026,"failed":0}
------------------------------
• [SLOW TEST] [9.736 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:22:42.972
    Jan 18 16:22:42.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename services 01/18/23 16:22:42.973
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:22:42.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:22:42.999
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-4651 01/18/23 16:22:43.004
    STEP: creating service affinity-clusterip in namespace services-4651 01/18/23 16:22:43.004
    STEP: creating replication controller affinity-clusterip in namespace services-4651 01/18/23 16:22:43.017
    I0118 16:22:43.027081      19 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-4651, replica count: 3
    I0118 16:22:46.083482      19 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 16:22:46.092: INFO: Creating new exec pod
    Jan 18 16:22:46.098: INFO: Waiting up to 5m0s for pod "execpod-affinitygb8j2" in namespace "services-4651" to be "running"
    Jan 18 16:22:46.107: INFO: Pod "execpod-affinitygb8j2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.710144ms
    Jan 18 16:22:48.111: INFO: Pod "execpod-affinitygb8j2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013429334s
    Jan 18 16:22:48.111: INFO: Pod "execpod-affinitygb8j2" satisfied condition "running"
    Jan 18 16:22:49.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4651 exec execpod-affinitygb8j2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Jan 18 16:22:49.404: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jan 18 16:22:49.404: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 16:22:49.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4651 exec execpod-affinitygb8j2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.27.234 80'
    Jan 18 16:22:49.586: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.27.234 80\nConnection to 10.233.27.234 80 port [tcp/http] succeeded!\n"
    Jan 18 16:22:49.586: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 16:22:49.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-4651 exec execpod-affinitygb8j2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.27.234:80/ ; done'
    Jan 18 16:22:49.963: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.234:80/\n"
    Jan 18 16:22:49.963: INFO: stdout: "\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws\naffinity-clusterip-l2mws"
    Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
    Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
    Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
    Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
    Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
    Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
    Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
    Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
    Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
    Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
    Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
    Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
    Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
    Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
    Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
    Jan 18 16:22:49.963: INFO: Received response from host: affinity-clusterip-l2mws
    Jan 18 16:22:49.963: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-4651, will wait for the garbage collector to delete the pods 01/18/23 16:22:49.985
    Jan 18 16:22:50.048: INFO: Deleting ReplicationController affinity-clusterip took: 8.337439ms
    Jan 18 16:22:50.150: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.547313ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 16:22:52.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4651" for this suite. 01/18/23 16:22:52.693
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:22:52.712
Jan 18 16:22:52.713: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename kubectl 01/18/23 16:22:52.717
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:22:52.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:22:52.755
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 01/18/23 16:22:52.766
Jan 18 16:22:52.766: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan 18 16:22:52.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3526 create -f -'
Jan 18 16:22:54.393: INFO: stderr: ""
Jan 18 16:22:54.393: INFO: stdout: "service/agnhost-replica created\n"
Jan 18 16:22:54.393: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan 18 16:22:54.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3526 create -f -'
Jan 18 16:22:54.845: INFO: stderr: ""
Jan 18 16:22:54.845: INFO: stdout: "service/agnhost-primary created\n"
Jan 18 16:22:54.846: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 18 16:22:54.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3526 create -f -'
Jan 18 16:22:55.240: INFO: stderr: ""
Jan 18 16:22:55.240: INFO: stdout: "service/frontend created\n"
Jan 18 16:22:55.240: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 18 16:22:55.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3526 create -f -'
Jan 18 16:22:55.613: INFO: stderr: ""
Jan 18 16:22:55.613: INFO: stdout: "deployment.apps/frontend created\n"
Jan 18 16:22:55.613: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 18 16:22:55.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3526 create -f -'
Jan 18 16:22:55.998: INFO: stderr: ""
Jan 18 16:22:55.998: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan 18 16:22:55.998: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 18 16:22:55.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3526 create -f -'
Jan 18 16:22:56.526: INFO: stderr: ""
Jan 18 16:22:56.526: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 01/18/23 16:22:56.526
Jan 18 16:22:56.526: INFO: Waiting for all frontend pods to be Running.
Jan 18 16:23:01.578: INFO: Waiting for frontend to serve content.
Jan 18 16:23:01.591: INFO: Trying to add a new entry to the guestbook.
Jan 18 16:23:01.605: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 01/18/23 16:23:01.616
Jan 18 16:23:01.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3526 delete --grace-period=0 --force -f -'
Jan 18 16:23:01.752: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 16:23:01.752: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 01/18/23 16:23:01.752
Jan 18 16:23:01.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3526 delete --grace-period=0 --force -f -'
Jan 18 16:23:01.901: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 16:23:01.901: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/18/23 16:23:01.901
Jan 18 16:23:01.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3526 delete --grace-period=0 --force -f -'
Jan 18 16:23:02.075: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 16:23:02.075: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/18/23 16:23:02.075
Jan 18 16:23:02.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3526 delete --grace-period=0 --force -f -'
Jan 18 16:23:02.206: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 16:23:02.206: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/18/23 16:23:02.206
Jan 18 16:23:02.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3526 delete --grace-period=0 --force -f -'
Jan 18 16:23:02.438: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 16:23:02.438: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/18/23 16:23:02.438
Jan 18 16:23:02.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3526 delete --grace-period=0 --force -f -'
Jan 18 16:23:02.733: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 16:23:02.733: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 16:23:02.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3526" for this suite. 01/18/23 16:23:02.743
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":281,"skipped":5028,"failed":0}
------------------------------
• [SLOW TEST] [10.047 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:22:52.712
    Jan 18 16:22:52.713: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename kubectl 01/18/23 16:22:52.717
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:22:52.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:22:52.755
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 01/18/23 16:22:52.766
    Jan 18 16:22:52.766: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jan 18 16:22:52.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3526 create -f -'
    Jan 18 16:22:54.393: INFO: stderr: ""
    Jan 18 16:22:54.393: INFO: stdout: "service/agnhost-replica created\n"
    Jan 18 16:22:54.393: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jan 18 16:22:54.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3526 create -f -'
    Jan 18 16:22:54.845: INFO: stderr: ""
    Jan 18 16:22:54.845: INFO: stdout: "service/agnhost-primary created\n"
    Jan 18 16:22:54.846: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jan 18 16:22:54.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3526 create -f -'
    Jan 18 16:22:55.240: INFO: stderr: ""
    Jan 18 16:22:55.240: INFO: stdout: "service/frontend created\n"
    Jan 18 16:22:55.240: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jan 18 16:22:55.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3526 create -f -'
    Jan 18 16:22:55.613: INFO: stderr: ""
    Jan 18 16:22:55.613: INFO: stdout: "deployment.apps/frontend created\n"
    Jan 18 16:22:55.613: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 18 16:22:55.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3526 create -f -'
    Jan 18 16:22:55.998: INFO: stderr: ""
    Jan 18 16:22:55.998: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jan 18 16:22:55.998: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 18 16:22:55.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3526 create -f -'
    Jan 18 16:22:56.526: INFO: stderr: ""
    Jan 18 16:22:56.526: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 01/18/23 16:22:56.526
    Jan 18 16:22:56.526: INFO: Waiting for all frontend pods to be Running.
    Jan 18 16:23:01.578: INFO: Waiting for frontend to serve content.
    Jan 18 16:23:01.591: INFO: Trying to add a new entry to the guestbook.
    Jan 18 16:23:01.605: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 01/18/23 16:23:01.616
    Jan 18 16:23:01.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3526 delete --grace-period=0 --force -f -'
    Jan 18 16:23:01.752: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 16:23:01.752: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 01/18/23 16:23:01.752
    Jan 18 16:23:01.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3526 delete --grace-period=0 --force -f -'
    Jan 18 16:23:01.901: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 16:23:01.901: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/18/23 16:23:01.901
    Jan 18 16:23:01.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3526 delete --grace-period=0 --force -f -'
    Jan 18 16:23:02.075: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 16:23:02.075: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/18/23 16:23:02.075
    Jan 18 16:23:02.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3526 delete --grace-period=0 --force -f -'
    Jan 18 16:23:02.206: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 16:23:02.206: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/18/23 16:23:02.206
    Jan 18 16:23:02.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3526 delete --grace-period=0 --force -f -'
    Jan 18 16:23:02.438: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 16:23:02.438: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/18/23 16:23:02.438
    Jan 18 16:23:02.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3526 delete --grace-period=0 --force -f -'
    Jan 18 16:23:02.733: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 16:23:02.733: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 16:23:02.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3526" for this suite. 01/18/23 16:23:02.743
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:23:02.785
Jan 18 16:23:02.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename configmap 01/18/23 16:23:02.787
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:23:02.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:23:02.819
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-1e3a894f-4578-40aa-b33c-58a709f08b73 01/18/23 16:23:02.844
STEP: Creating a pod to test consume configMaps 01/18/23 16:23:02.867
Jan 18 16:23:02.879: INFO: Waiting up to 5m0s for pod "pod-configmaps-a4dcd721-5486-4a21-81c0-abd4dc5f2d7f" in namespace "configmap-7759" to be "Succeeded or Failed"
Jan 18 16:23:02.890: INFO: Pod "pod-configmaps-a4dcd721-5486-4a21-81c0-abd4dc5f2d7f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.773003ms
Jan 18 16:23:04.895: INFO: Pod "pod-configmaps-a4dcd721-5486-4a21-81c0-abd4dc5f2d7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015880245s
Jan 18 16:23:06.895: INFO: Pod "pod-configmaps-a4dcd721-5486-4a21-81c0-abd4dc5f2d7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016377173s
STEP: Saw pod success 01/18/23 16:23:06.896
Jan 18 16:23:06.896: INFO: Pod "pod-configmaps-a4dcd721-5486-4a21-81c0-abd4dc5f2d7f" satisfied condition "Succeeded or Failed"
Jan 18 16:23:06.901: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-a4dcd721-5486-4a21-81c0-abd4dc5f2d7f container agnhost-container: <nil>
STEP: delete the pod 01/18/23 16:23:06.91
Jan 18 16:23:06.928: INFO: Waiting for pod pod-configmaps-a4dcd721-5486-4a21-81c0-abd4dc5f2d7f to disappear
Jan 18 16:23:06.934: INFO: Pod pod-configmaps-a4dcd721-5486-4a21-81c0-abd4dc5f2d7f no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 16:23:06.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7759" for this suite. 01/18/23 16:23:06.939
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":282,"skipped":5081,"failed":0}
------------------------------
• [4.168 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:23:02.785
    Jan 18 16:23:02.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename configmap 01/18/23 16:23:02.787
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:23:02.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:23:02.819
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-1e3a894f-4578-40aa-b33c-58a709f08b73 01/18/23 16:23:02.844
    STEP: Creating a pod to test consume configMaps 01/18/23 16:23:02.867
    Jan 18 16:23:02.879: INFO: Waiting up to 5m0s for pod "pod-configmaps-a4dcd721-5486-4a21-81c0-abd4dc5f2d7f" in namespace "configmap-7759" to be "Succeeded or Failed"
    Jan 18 16:23:02.890: INFO: Pod "pod-configmaps-a4dcd721-5486-4a21-81c0-abd4dc5f2d7f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.773003ms
    Jan 18 16:23:04.895: INFO: Pod "pod-configmaps-a4dcd721-5486-4a21-81c0-abd4dc5f2d7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015880245s
    Jan 18 16:23:06.895: INFO: Pod "pod-configmaps-a4dcd721-5486-4a21-81c0-abd4dc5f2d7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016377173s
    STEP: Saw pod success 01/18/23 16:23:06.896
    Jan 18 16:23:06.896: INFO: Pod "pod-configmaps-a4dcd721-5486-4a21-81c0-abd4dc5f2d7f" satisfied condition "Succeeded or Failed"
    Jan 18 16:23:06.901: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-a4dcd721-5486-4a21-81c0-abd4dc5f2d7f container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 16:23:06.91
    Jan 18 16:23:06.928: INFO: Waiting for pod pod-configmaps-a4dcd721-5486-4a21-81c0-abd4dc5f2d7f to disappear
    Jan 18 16:23:06.934: INFO: Pod pod-configmaps-a4dcd721-5486-4a21-81c0-abd4dc5f2d7f no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 16:23:06.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7759" for this suite. 01/18/23 16:23:06.939
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:23:06.96
Jan 18 16:23:06.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename replication-controller 01/18/23 16:23:06.962
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:23:06.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:23:06.993
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 01/18/23 16:23:07.002
STEP: waiting for RC to be added 01/18/23 16:23:07.012
STEP: waiting for available Replicas 01/18/23 16:23:07.014
STEP: patching ReplicationController 01/18/23 16:23:08.742
STEP: waiting for RC to be modified 01/18/23 16:23:08.755
STEP: patching ReplicationController status 01/18/23 16:23:08.756
STEP: waiting for RC to be modified 01/18/23 16:23:08.766
STEP: waiting for available Replicas 01/18/23 16:23:08.766
STEP: fetching ReplicationController status 01/18/23 16:23:08.771
STEP: patching ReplicationController scale 01/18/23 16:23:08.777
STEP: waiting for RC to be modified 01/18/23 16:23:08.79
STEP: waiting for ReplicationController's scale to be the max amount 01/18/23 16:23:08.791
STEP: fetching ReplicationController; ensuring that it's patched 01/18/23 16:23:10.75
STEP: updating ReplicationController status 01/18/23 16:23:10.758
STEP: waiting for RC to be modified 01/18/23 16:23:10.766
STEP: listing all ReplicationControllers 01/18/23 16:23:10.767
STEP: checking that ReplicationController has expected values 01/18/23 16:23:10.772
STEP: deleting ReplicationControllers by collection 01/18/23 16:23:10.772
STEP: waiting for ReplicationController to have a DELETED watchEvent 01/18/23 16:23:10.787
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 18 16:23:11.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1596" for this suite. 01/18/23 16:23:11.008
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":283,"skipped":5108,"failed":0}
------------------------------
• [4.091 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:23:06.96
    Jan 18 16:23:06.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename replication-controller 01/18/23 16:23:06.962
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:23:06.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:23:06.993
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 01/18/23 16:23:07.002
    STEP: waiting for RC to be added 01/18/23 16:23:07.012
    STEP: waiting for available Replicas 01/18/23 16:23:07.014
    STEP: patching ReplicationController 01/18/23 16:23:08.742
    STEP: waiting for RC to be modified 01/18/23 16:23:08.755
    STEP: patching ReplicationController status 01/18/23 16:23:08.756
    STEP: waiting for RC to be modified 01/18/23 16:23:08.766
    STEP: waiting for available Replicas 01/18/23 16:23:08.766
    STEP: fetching ReplicationController status 01/18/23 16:23:08.771
    STEP: patching ReplicationController scale 01/18/23 16:23:08.777
    STEP: waiting for RC to be modified 01/18/23 16:23:08.79
    STEP: waiting for ReplicationController's scale to be the max amount 01/18/23 16:23:08.791
    STEP: fetching ReplicationController; ensuring that it's patched 01/18/23 16:23:10.75
    STEP: updating ReplicationController status 01/18/23 16:23:10.758
    STEP: waiting for RC to be modified 01/18/23 16:23:10.766
    STEP: listing all ReplicationControllers 01/18/23 16:23:10.767
    STEP: checking that ReplicationController has expected values 01/18/23 16:23:10.772
    STEP: deleting ReplicationControllers by collection 01/18/23 16:23:10.772
    STEP: waiting for ReplicationController to have a DELETED watchEvent 01/18/23 16:23:10.787
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 18 16:23:11.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-1596" for this suite. 01/18/23 16:23:11.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:23:11.056
Jan 18 16:23:11.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 16:23:11.059
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:23:11.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:23:11.135
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jan 18 16:23:11.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 16:23:11.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6967" for this suite. 01/18/23 16:23:11.72
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":284,"skipped":5116,"failed":0}
------------------------------
• [0.671 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:23:11.056
    Jan 18 16:23:11.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 16:23:11.059
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:23:11.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:23:11.135
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jan 18 16:23:11.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 16:23:11.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-6967" for this suite. 01/18/23 16:23:11.72
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:23:11.732
Jan 18 16:23:11.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename cronjob 01/18/23 16:23:11.735
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:23:11.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:23:11.761
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 01/18/23 16:23:11.766
STEP: Ensuring more than one job is running at a time 01/18/23 16:23:11.775
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/18/23 16:25:01.781
STEP: Removing cronjob 01/18/23 16:25:01.787
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 18 16:25:01.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4398" for this suite. 01/18/23 16:25:01.808
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":285,"skipped":5124,"failed":0}
------------------------------
• [SLOW TEST] [110.086 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:23:11.732
    Jan 18 16:23:11.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename cronjob 01/18/23 16:23:11.735
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:23:11.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:23:11.761
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 01/18/23 16:23:11.766
    STEP: Ensuring more than one job is running at a time 01/18/23 16:23:11.775
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/18/23 16:25:01.781
    STEP: Removing cronjob 01/18/23 16:25:01.787
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 18 16:25:01.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-4398" for this suite. 01/18/23 16:25:01.808
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:25:01.841
Jan 18 16:25:01.842: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename statefulset 01/18/23 16:25:01.844
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:25:01.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:25:01.896
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5835 01/18/23 16:25:01.915
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Jan 18 16:25:01.939: INFO: Found 0 stateful pods, waiting for 1
Jan 18 16:25:11.949: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 01/18/23 16:25:11.96
W0118 16:25:11.972483      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 18 16:25:11.990: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 16:25:11.990: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
Jan 18 16:25:21.999: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 16:25:21.999: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 01/18/23 16:25:22.013
STEP: Delete all of the StatefulSets 01/18/23 16:25:22.016
STEP: Verify that StatefulSets have been deleted 01/18/23 16:25:22.031
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 16:25:22.035: INFO: Deleting all statefulset in ns statefulset-5835
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 18 16:25:22.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5835" for this suite. 01/18/23 16:25:22.081
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":286,"skipped":5252,"failed":0}
------------------------------
• [SLOW TEST] [20.254 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:25:01.841
    Jan 18 16:25:01.842: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename statefulset 01/18/23 16:25:01.844
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:25:01.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:25:01.896
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5835 01/18/23 16:25:01.915
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Jan 18 16:25:01.939: INFO: Found 0 stateful pods, waiting for 1
    Jan 18 16:25:11.949: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 01/18/23 16:25:11.96
    W0118 16:25:11.972483      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 18 16:25:11.990: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 16:25:11.990: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
    Jan 18 16:25:21.999: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 16:25:21.999: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 01/18/23 16:25:22.013
    STEP: Delete all of the StatefulSets 01/18/23 16:25:22.016
    STEP: Verify that StatefulSets have been deleted 01/18/23 16:25:22.031
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 18 16:25:22.035: INFO: Deleting all statefulset in ns statefulset-5835
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 18 16:25:22.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5835" for this suite. 01/18/23 16:25:22.081
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:25:22.127
Jan 18 16:25:22.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename container-runtime 01/18/23 16:25:22.128
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:25:22.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:25:22.162
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/18/23 16:25:22.178
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/18/23 16:25:40.293
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/18/23 16:25:40.298
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/18/23 16:25:40.305
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/18/23 16:25:40.306
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/18/23 16:25:40.348
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/18/23 16:25:43.377
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/18/23 16:25:45.413
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/18/23 16:25:45.425
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/18/23 16:25:45.426
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/18/23 16:25:45.447
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/18/23 16:25:46.467
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/18/23 16:25:49.491
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/18/23 16:25:49.504
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/18/23 16:25:49.504
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 18 16:25:49.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2427" for this suite. 01/18/23 16:25:49.554
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":287,"skipped":5307,"failed":0}
------------------------------
• [SLOW TEST] [27.446 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:25:22.127
    Jan 18 16:25:22.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename container-runtime 01/18/23 16:25:22.128
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:25:22.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:25:22.162
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/18/23 16:25:22.178
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/18/23 16:25:40.293
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/18/23 16:25:40.298
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/18/23 16:25:40.305
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/18/23 16:25:40.306
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/18/23 16:25:40.348
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/18/23 16:25:43.377
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/18/23 16:25:45.413
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/18/23 16:25:45.425
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/18/23 16:25:45.426
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/18/23 16:25:45.447
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/18/23 16:25:46.467
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/18/23 16:25:49.491
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/18/23 16:25:49.504
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/18/23 16:25:49.504
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 18 16:25:49.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-2427" for this suite. 01/18/23 16:25:49.554
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:25:49.588
Jan 18 16:25:49.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename pods 01/18/23 16:25:49.59
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:25:49.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:25:49.616
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 01/18/23 16:25:49.622
Jan 18 16:25:49.632: INFO: Waiting up to 5m0s for pod "pod-hbgwf" in namespace "pods-4515" to be "running"
Jan 18 16:25:49.643: INFO: Pod "pod-hbgwf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.362323ms
Jan 18 16:25:51.649: INFO: Pod "pod-hbgwf": Phase="Running", Reason="", readiness=true. Elapsed: 2.016354409s
Jan 18 16:25:51.649: INFO: Pod "pod-hbgwf" satisfied condition "running"
STEP: patching /status 01/18/23 16:25:51.649
Jan 18 16:25:51.659: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 16:25:51.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4515" for this suite. 01/18/23 16:25:51.674
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":288,"skipped":5361,"failed":0}
------------------------------
• [2.095 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:25:49.588
    Jan 18 16:25:49.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename pods 01/18/23 16:25:49.59
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:25:49.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:25:49.616
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 01/18/23 16:25:49.622
    Jan 18 16:25:49.632: INFO: Waiting up to 5m0s for pod "pod-hbgwf" in namespace "pods-4515" to be "running"
    Jan 18 16:25:49.643: INFO: Pod "pod-hbgwf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.362323ms
    Jan 18 16:25:51.649: INFO: Pod "pod-hbgwf": Phase="Running", Reason="", readiness=true. Elapsed: 2.016354409s
    Jan 18 16:25:51.649: INFO: Pod "pod-hbgwf" satisfied condition "running"
    STEP: patching /status 01/18/23 16:25:51.649
    Jan 18 16:25:51.659: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 16:25:51.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4515" for this suite. 01/18/23 16:25:51.674
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:25:51.696
Jan 18 16:25:51.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename watch 01/18/23 16:25:51.697
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:25:51.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:25:51.725
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 01/18/23 16:25:51.731
STEP: creating a new configmap 01/18/23 16:25:51.734
STEP: modifying the configmap once 01/18/23 16:25:51.739
STEP: changing the label value of the configmap 01/18/23 16:25:51.748
STEP: Expecting to observe a delete notification for the watched object 01/18/23 16:25:51.759
Jan 18 16:25:51.759: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9554  02c8902e-1667-4a4a-875e-6e2e78049ace 63803 0 2023-01-18 16:25:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 16:25:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 16:25:51.760: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9554  02c8902e-1667-4a4a-875e-6e2e78049ace 63804 0 2023-01-18 16:25:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 16:25:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 16:25:51.760: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9554  02c8902e-1667-4a4a-875e-6e2e78049ace 63805 0 2023-01-18 16:25:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 16:25:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 01/18/23 16:25:51.76
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/18/23 16:25:51.77
STEP: changing the label value of the configmap back 01/18/23 16:26:01.771
STEP: modifying the configmap a third time 01/18/23 16:26:01.783
STEP: deleting the configmap 01/18/23 16:26:01.795
STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/18/23 16:26:01.803
Jan 18 16:26:01.803: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9554  02c8902e-1667-4a4a-875e-6e2e78049ace 63879 0 2023-01-18 16:25:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 16:26:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 16:26:01.804: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9554  02c8902e-1667-4a4a-875e-6e2e78049ace 63880 0 2023-01-18 16:25:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 16:26:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 16:26:01.804: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9554  02c8902e-1667-4a4a-875e-6e2e78049ace 63881 0 2023-01-18 16:25:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 16:26:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 18 16:26:01.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9554" for this suite. 01/18/23 16:26:01.813
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":289,"skipped":5438,"failed":0}
------------------------------
• [SLOW TEST] [10.129 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:25:51.696
    Jan 18 16:25:51.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename watch 01/18/23 16:25:51.697
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:25:51.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:25:51.725
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 01/18/23 16:25:51.731
    STEP: creating a new configmap 01/18/23 16:25:51.734
    STEP: modifying the configmap once 01/18/23 16:25:51.739
    STEP: changing the label value of the configmap 01/18/23 16:25:51.748
    STEP: Expecting to observe a delete notification for the watched object 01/18/23 16:25:51.759
    Jan 18 16:25:51.759: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9554  02c8902e-1667-4a4a-875e-6e2e78049ace 63803 0 2023-01-18 16:25:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 16:25:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 16:25:51.760: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9554  02c8902e-1667-4a4a-875e-6e2e78049ace 63804 0 2023-01-18 16:25:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 16:25:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 16:25:51.760: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9554  02c8902e-1667-4a4a-875e-6e2e78049ace 63805 0 2023-01-18 16:25:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 16:25:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 01/18/23 16:25:51.76
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/18/23 16:25:51.77
    STEP: changing the label value of the configmap back 01/18/23 16:26:01.771
    STEP: modifying the configmap a third time 01/18/23 16:26:01.783
    STEP: deleting the configmap 01/18/23 16:26:01.795
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/18/23 16:26:01.803
    Jan 18 16:26:01.803: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9554  02c8902e-1667-4a4a-875e-6e2e78049ace 63879 0 2023-01-18 16:25:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 16:26:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 16:26:01.804: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9554  02c8902e-1667-4a4a-875e-6e2e78049ace 63880 0 2023-01-18 16:25:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 16:26:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 16:26:01.804: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9554  02c8902e-1667-4a4a-875e-6e2e78049ace 63881 0 2023-01-18 16:25:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 16:26:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 18 16:26:01.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-9554" for this suite. 01/18/23 16:26:01.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:26:01.831
Jan 18 16:26:01.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename services 01/18/23 16:26:01.833
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:26:01.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:26:01.863
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 01/18/23 16:26:01.877
STEP: waiting for available Endpoint 01/18/23 16:26:01.881
STEP: listing all Endpoints 01/18/23 16:26:01.884
STEP: updating the Endpoint 01/18/23 16:26:01.888
STEP: fetching the Endpoint 01/18/23 16:26:01.898
STEP: patching the Endpoint 01/18/23 16:26:01.902
STEP: fetching the Endpoint 01/18/23 16:26:01.916
STEP: deleting the Endpoint by Collection 01/18/23 16:26:01.92
STEP: waiting for Endpoint deletion 01/18/23 16:26:01.928
STEP: fetching the Endpoint 01/18/23 16:26:01.934
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 16:26:01.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9825" for this suite. 01/18/23 16:26:01.944
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":290,"skipped":5451,"failed":0}
------------------------------
• [0.121 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:26:01.831
    Jan 18 16:26:01.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename services 01/18/23 16:26:01.833
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:26:01.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:26:01.863
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 01/18/23 16:26:01.877
    STEP: waiting for available Endpoint 01/18/23 16:26:01.881
    STEP: listing all Endpoints 01/18/23 16:26:01.884
    STEP: updating the Endpoint 01/18/23 16:26:01.888
    STEP: fetching the Endpoint 01/18/23 16:26:01.898
    STEP: patching the Endpoint 01/18/23 16:26:01.902
    STEP: fetching the Endpoint 01/18/23 16:26:01.916
    STEP: deleting the Endpoint by Collection 01/18/23 16:26:01.92
    STEP: waiting for Endpoint deletion 01/18/23 16:26:01.928
    STEP: fetching the Endpoint 01/18/23 16:26:01.934
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 16:26:01.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9825" for this suite. 01/18/23 16:26:01.944
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:26:01.961
Jan 18 16:26:01.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename replicaset 01/18/23 16:26:01.963
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:26:02.036
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:26:02.041
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 01/18/23 16:26:02.045
STEP: Verify that the required pods have come up 01/18/23 16:26:02.051
Jan 18 16:26:02.058: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan 18 16:26:07.075: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 01/18/23 16:26:07.075
Jan 18 16:26:07.088: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 01/18/23 16:26:07.088
STEP: DeleteCollection of the ReplicaSets 01/18/23 16:26:07.095
STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/18/23 16:26:07.109
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 18 16:26:07.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-356" for this suite. 01/18/23 16:26:07.136
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":291,"skipped":5490,"failed":0}
------------------------------
• [SLOW TEST] [5.208 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:26:01.961
    Jan 18 16:26:01.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename replicaset 01/18/23 16:26:01.963
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:26:02.036
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:26:02.041
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 01/18/23 16:26:02.045
    STEP: Verify that the required pods have come up 01/18/23 16:26:02.051
    Jan 18 16:26:02.058: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jan 18 16:26:07.075: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 01/18/23 16:26:07.075
    Jan 18 16:26:07.088: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 01/18/23 16:26:07.088
    STEP: DeleteCollection of the ReplicaSets 01/18/23 16:26:07.095
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/18/23 16:26:07.109
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 18 16:26:07.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-356" for this suite. 01/18/23 16:26:07.136
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:26:07.171
Jan 18 16:26:07.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename job 01/18/23 16:26:07.173
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:26:07.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:26:07.284
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 01/18/23 16:26:07.293
STEP: Ensuring job reaches completions 01/18/23 16:26:07.316
STEP: Ensuring pods with index for job exist 01/18/23 16:26:17.324
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 18 16:26:17.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-242" for this suite. 01/18/23 16:26:17.338
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":292,"skipped":5503,"failed":0}
------------------------------
• [SLOW TEST] [10.175 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:26:07.171
    Jan 18 16:26:07.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename job 01/18/23 16:26:07.173
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:26:07.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:26:07.284
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 01/18/23 16:26:07.293
    STEP: Ensuring job reaches completions 01/18/23 16:26:07.316
    STEP: Ensuring pods with index for job exist 01/18/23 16:26:17.324
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 18 16:26:17.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-242" for this suite. 01/18/23 16:26:17.338
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:26:17.356
Jan 18 16:26:17.356: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename sched-pred 01/18/23 16:26:17.358
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:26:17.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:26:17.388
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 18 16:26:17.392: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 18 16:26:17.403: INFO: Waiting for terminating namespaces to be deleted...
Jan 18 16:26:17.406: INFO: 
Logging pods the apiserver thinks is on node v1-25-1-18760-w before test
Jan 18 16:26:17.417: INFO: helm-controller-578bd944df-dzr2c from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 16:26:17.417: INFO: 	Container manager ready: true, restart count 0
Jan 18 16:26:17.417: INFO: notification-controller-6459696d4f-gdmj8 from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 16:26:17.417: INFO: 	Container manager ready: true, restart count 0
Jan 18 16:26:17.417: INFO: source-controller-84f6bcbfb8-ljjjm from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 16:26:17.417: INFO: 	Container manager ready: true, restart count 0
Jan 18 16:26:17.417: INFO: calico-node-59tts from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
Jan 18 16:26:17.417: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 16:26:17.417: INFO: coredns-coredns-5f9b955d9-pjqxb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 16:26:17.417: INFO: 	Container coredns ready: true, restart count 0
Jan 18 16:26:17.417: INFO: coredns-coredns-autoscaler-7d98f7496d-7zbwb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 16:26:17.417: INFO: 	Container autoscaler ready: true, restart count 0
Jan 18 16:26:17.417: INFO: kube-proxy-rzflq from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
Jan 18 16:26:17.417: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 16:26:17.417: INFO: metrics-server-58498b9c56-q88cp from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 16:26:17.417: INFO: 	Container metrics-server ready: true, restart count 0
Jan 18 16:26:17.417: INFO: nginx-proxy-v1-25-1-18760-w from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
Jan 18 16:26:17.417: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 18 16:26:17.417: INFO: nodelocaldns-skdxm from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
Jan 18 16:26:17.417: INFO: 	Container node-cache ready: true, restart count 0
Jan 18 16:26:17.417: INFO: openstack-cinder-csi-controllerplugin-55d57dd799-6496v from kube-system started at 2023-01-18 14:39:48 +0000 UTC (6 container statuses recorded)
Jan 18 16:26:17.417: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 18 16:26:17.417: INFO: 	Container csi-attacher ready: true, restart count 0
Jan 18 16:26:17.417: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan 18 16:26:17.417: INFO: 	Container csi-resizer ready: true, restart count 0
Jan 18 16:26:17.417: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan 18 16:26:17.417: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 18 16:26:17.417: INFO: openstack-cinder-csi-nodeplugin-w7sxz from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
Jan 18 16:26:17.417: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 18 16:26:17.417: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 18 16:26:17.417: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 18 16:26:17.417: INFO: sonobuoy-systemd-logs-daemon-set-e45d8d00b64540c2-j5v5z from sonobuoy started at 2023-01-18 15:05:41 +0000 UTC (2 container statuses recorded)
Jan 18 16:26:17.417: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 16:26:17.417: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 16:26:17.417: INFO: 
Logging pods the apiserver thinks is on node v1-25-1-18760-w2 before test
Jan 18 16:26:17.429: INFO: indexed-job-0-n8ktm from job-242 started at 2023-01-18 16:26:07 +0000 UTC (1 container statuses recorded)
Jan 18 16:26:17.429: INFO: 	Container c ready: false, restart count 0
Jan 18 16:26:17.429: INFO: indexed-job-1-qd257 from job-242 started at 2023-01-18 16:26:07 +0000 UTC (1 container statuses recorded)
Jan 18 16:26:17.429: INFO: 	Container c ready: false, restart count 0
Jan 18 16:26:17.429: INFO: indexed-job-2-jnspt from job-242 started at 2023-01-18 16:26:12 +0000 UTC (1 container statuses recorded)
Jan 18 16:26:17.429: INFO: 	Container c ready: false, restart count 0
Jan 18 16:26:17.429: INFO: indexed-job-3-j5lsz from job-242 started at 2023-01-18 16:26:12 +0000 UTC (1 container statuses recorded)
Jan 18 16:26:17.429: INFO: 	Container c ready: false, restart count 0
Jan 18 16:26:17.429: INFO: calico-node-qx4f9 from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
Jan 18 16:26:17.430: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 16:26:17.430: INFO: coredns-coredns-5f9b955d9-qggng from kube-system started at 2023-01-18 14:40:34 +0000 UTC (1 container statuses recorded)
Jan 18 16:26:17.430: INFO: 	Container coredns ready: true, restart count 0
Jan 18 16:26:17.430: INFO: kube-proxy-8r2pt from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
Jan 18 16:26:17.430: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 16:26:17.430: INFO: nginx-proxy-v1-25-1-18760-w2 from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
Jan 18 16:26:17.430: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 18 16:26:17.430: INFO: nodelocaldns-dzwbj from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
Jan 18 16:26:17.430: INFO: 	Container node-cache ready: true, restart count 0
Jan 18 16:26:17.430: INFO: openstack-cinder-csi-nodeplugin-qm575 from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
Jan 18 16:26:17.431: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 18 16:26:17.431: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 18 16:26:17.431: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 18 16:26:17.431: INFO: sonobuoy from sonobuoy started at 2023-01-18 15:05:32 +0000 UTC (1 container statuses recorded)
Jan 18 16:26:17.431: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 18 16:26:17.431: INFO: sonobuoy-e2e-job-97bb2aa868f3486c from sonobuoy started at 2023-01-18 15:05:41 +0000 UTC (2 container statuses recorded)
Jan 18 16:26:17.431: INFO: 	Container e2e ready: true, restart count 0
Jan 18 16:26:17.431: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 16:26:17.431: INFO: sonobuoy-systemd-logs-daemon-set-e45d8d00b64540c2-488l9 from sonobuoy started at 2023-01-18 15:05:41 +0000 UTC (2 container statuses recorded)
Jan 18 16:26:17.431: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 16:26:17.431: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 16:26:17.432
Jan 18 16:26:17.441: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2874" to be "running"
Jan 18 16:26:17.446: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.618103ms
Jan 18 16:26:19.453: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.011647722s
Jan 18 16:26:19.453: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 16:26:19.459
STEP: Trying to apply a random label on the found node. 01/18/23 16:26:19.475
STEP: verifying the node has the label kubernetes.io/e2e-80009752-77cd-4395-a50f-6f422a8d44df 42 01/18/23 16:26:19.507
STEP: Trying to relaunch the pod, now with labels. 01/18/23 16:26:19.522
Jan 18 16:26:19.529: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-2874" to be "not pending"
Jan 18 16:26:19.537: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 7.892097ms
Jan 18 16:26:21.544: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.014469514s
Jan 18 16:26:21.544: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-80009752-77cd-4395-a50f-6f422a8d44df off the node v1-25-1-18760-w2 01/18/23 16:26:21.549
STEP: verifying the node doesn't have the label kubernetes.io/e2e-80009752-77cd-4395-a50f-6f422a8d44df 01/18/23 16:26:21.569
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 18 16:26:21.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2874" for this suite. 01/18/23 16:26:21.589
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":293,"skipped":5503,"failed":0}
------------------------------
• [4.240 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:26:17.356
    Jan 18 16:26:17.356: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename sched-pred 01/18/23 16:26:17.358
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:26:17.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:26:17.388
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 18 16:26:17.392: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 18 16:26:17.403: INFO: Waiting for terminating namespaces to be deleted...
    Jan 18 16:26:17.406: INFO: 
    Logging pods the apiserver thinks is on node v1-25-1-18760-w before test
    Jan 18 16:26:17.417: INFO: helm-controller-578bd944df-dzr2c from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 16:26:17.417: INFO: 	Container manager ready: true, restart count 0
    Jan 18 16:26:17.417: INFO: notification-controller-6459696d4f-gdmj8 from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 16:26:17.417: INFO: 	Container manager ready: true, restart count 0
    Jan 18 16:26:17.417: INFO: source-controller-84f6bcbfb8-ljjjm from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 16:26:17.417: INFO: 	Container manager ready: true, restart count 0
    Jan 18 16:26:17.417: INFO: calico-node-59tts from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
    Jan 18 16:26:17.417: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 16:26:17.417: INFO: coredns-coredns-5f9b955d9-pjqxb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 16:26:17.417: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 16:26:17.417: INFO: coredns-coredns-autoscaler-7d98f7496d-7zbwb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 16:26:17.417: INFO: 	Container autoscaler ready: true, restart count 0
    Jan 18 16:26:17.417: INFO: kube-proxy-rzflq from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
    Jan 18 16:26:17.417: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 16:26:17.417: INFO: metrics-server-58498b9c56-q88cp from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 16:26:17.417: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 18 16:26:17.417: INFO: nginx-proxy-v1-25-1-18760-w from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
    Jan 18 16:26:17.417: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 18 16:26:17.417: INFO: nodelocaldns-skdxm from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
    Jan 18 16:26:17.417: INFO: 	Container node-cache ready: true, restart count 0
    Jan 18 16:26:17.417: INFO: openstack-cinder-csi-controllerplugin-55d57dd799-6496v from kube-system started at 2023-01-18 14:39:48 +0000 UTC (6 container statuses recorded)
    Jan 18 16:26:17.417: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 18 16:26:17.417: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan 18 16:26:17.417: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan 18 16:26:17.417: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan 18 16:26:17.417: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan 18 16:26:17.417: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 18 16:26:17.417: INFO: openstack-cinder-csi-nodeplugin-w7sxz from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
    Jan 18 16:26:17.417: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 18 16:26:17.417: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 18 16:26:17.417: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 18 16:26:17.417: INFO: sonobuoy-systemd-logs-daemon-set-e45d8d00b64540c2-j5v5z from sonobuoy started at 2023-01-18 15:05:41 +0000 UTC (2 container statuses recorded)
    Jan 18 16:26:17.417: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 16:26:17.417: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 18 16:26:17.417: INFO: 
    Logging pods the apiserver thinks is on node v1-25-1-18760-w2 before test
    Jan 18 16:26:17.429: INFO: indexed-job-0-n8ktm from job-242 started at 2023-01-18 16:26:07 +0000 UTC (1 container statuses recorded)
    Jan 18 16:26:17.429: INFO: 	Container c ready: false, restart count 0
    Jan 18 16:26:17.429: INFO: indexed-job-1-qd257 from job-242 started at 2023-01-18 16:26:07 +0000 UTC (1 container statuses recorded)
    Jan 18 16:26:17.429: INFO: 	Container c ready: false, restart count 0
    Jan 18 16:26:17.429: INFO: indexed-job-2-jnspt from job-242 started at 2023-01-18 16:26:12 +0000 UTC (1 container statuses recorded)
    Jan 18 16:26:17.429: INFO: 	Container c ready: false, restart count 0
    Jan 18 16:26:17.429: INFO: indexed-job-3-j5lsz from job-242 started at 2023-01-18 16:26:12 +0000 UTC (1 container statuses recorded)
    Jan 18 16:26:17.429: INFO: 	Container c ready: false, restart count 0
    Jan 18 16:26:17.429: INFO: calico-node-qx4f9 from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
    Jan 18 16:26:17.430: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 16:26:17.430: INFO: coredns-coredns-5f9b955d9-qggng from kube-system started at 2023-01-18 14:40:34 +0000 UTC (1 container statuses recorded)
    Jan 18 16:26:17.430: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 16:26:17.430: INFO: kube-proxy-8r2pt from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
    Jan 18 16:26:17.430: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 16:26:17.430: INFO: nginx-proxy-v1-25-1-18760-w2 from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
    Jan 18 16:26:17.430: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 18 16:26:17.430: INFO: nodelocaldns-dzwbj from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
    Jan 18 16:26:17.430: INFO: 	Container node-cache ready: true, restart count 0
    Jan 18 16:26:17.430: INFO: openstack-cinder-csi-nodeplugin-qm575 from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
    Jan 18 16:26:17.431: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 18 16:26:17.431: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 18 16:26:17.431: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 18 16:26:17.431: INFO: sonobuoy from sonobuoy started at 2023-01-18 15:05:32 +0000 UTC (1 container statuses recorded)
    Jan 18 16:26:17.431: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 18 16:26:17.431: INFO: sonobuoy-e2e-job-97bb2aa868f3486c from sonobuoy started at 2023-01-18 15:05:41 +0000 UTC (2 container statuses recorded)
    Jan 18 16:26:17.431: INFO: 	Container e2e ready: true, restart count 0
    Jan 18 16:26:17.431: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 16:26:17.431: INFO: sonobuoy-systemd-logs-daemon-set-e45d8d00b64540c2-488l9 from sonobuoy started at 2023-01-18 15:05:41 +0000 UTC (2 container statuses recorded)
    Jan 18 16:26:17.431: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 16:26:17.431: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 16:26:17.432
    Jan 18 16:26:17.441: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2874" to be "running"
    Jan 18 16:26:17.446: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.618103ms
    Jan 18 16:26:19.453: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.011647722s
    Jan 18 16:26:19.453: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 16:26:19.459
    STEP: Trying to apply a random label on the found node. 01/18/23 16:26:19.475
    STEP: verifying the node has the label kubernetes.io/e2e-80009752-77cd-4395-a50f-6f422a8d44df 42 01/18/23 16:26:19.507
    STEP: Trying to relaunch the pod, now with labels. 01/18/23 16:26:19.522
    Jan 18 16:26:19.529: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-2874" to be "not pending"
    Jan 18 16:26:19.537: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 7.892097ms
    Jan 18 16:26:21.544: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.014469514s
    Jan 18 16:26:21.544: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-80009752-77cd-4395-a50f-6f422a8d44df off the node v1-25-1-18760-w2 01/18/23 16:26:21.549
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-80009752-77cd-4395-a50f-6f422a8d44df 01/18/23 16:26:21.569
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 16:26:21.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-2874" for this suite. 01/18/23 16:26:21.589
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:26:21.598
Jan 18 16:26:21.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 16:26:21.602
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:26:21.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:26:21.638
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-67b159b4-2ffb-4646-8bb1-29f5c08dbb11 01/18/23 16:26:21.643
STEP: Creating a pod to test consume configMaps 01/18/23 16:26:21.649
Jan 18 16:26:21.669: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-52e49b4b-ecb2-4f3e-9834-fab0e6f6568b" in namespace "projected-8866" to be "Succeeded or Failed"
Jan 18 16:26:21.676: INFO: Pod "pod-projected-configmaps-52e49b4b-ecb2-4f3e-9834-fab0e6f6568b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.736916ms
Jan 18 16:26:23.683: INFO: Pod "pod-projected-configmaps-52e49b4b-ecb2-4f3e-9834-fab0e6f6568b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013256354s
Jan 18 16:26:25.687: INFO: Pod "pod-projected-configmaps-52e49b4b-ecb2-4f3e-9834-fab0e6f6568b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017088926s
STEP: Saw pod success 01/18/23 16:26:25.687
Jan 18 16:26:25.687: INFO: Pod "pod-projected-configmaps-52e49b4b-ecb2-4f3e-9834-fab0e6f6568b" satisfied condition "Succeeded or Failed"
Jan 18 16:26:25.692: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-52e49b4b-ecb2-4f3e-9834-fab0e6f6568b container agnhost-container: <nil>
STEP: delete the pod 01/18/23 16:26:25.719
Jan 18 16:26:25.743: INFO: Waiting for pod pod-projected-configmaps-52e49b4b-ecb2-4f3e-9834-fab0e6f6568b to disappear
Jan 18 16:26:25.750: INFO: Pod pod-projected-configmaps-52e49b4b-ecb2-4f3e-9834-fab0e6f6568b no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 18 16:26:25.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8866" for this suite. 01/18/23 16:26:25.757
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":294,"skipped":5516,"failed":0}
------------------------------
• [4.166 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:26:21.598
    Jan 18 16:26:21.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 16:26:21.602
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:26:21.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:26:21.638
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-67b159b4-2ffb-4646-8bb1-29f5c08dbb11 01/18/23 16:26:21.643
    STEP: Creating a pod to test consume configMaps 01/18/23 16:26:21.649
    Jan 18 16:26:21.669: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-52e49b4b-ecb2-4f3e-9834-fab0e6f6568b" in namespace "projected-8866" to be "Succeeded or Failed"
    Jan 18 16:26:21.676: INFO: Pod "pod-projected-configmaps-52e49b4b-ecb2-4f3e-9834-fab0e6f6568b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.736916ms
    Jan 18 16:26:23.683: INFO: Pod "pod-projected-configmaps-52e49b4b-ecb2-4f3e-9834-fab0e6f6568b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013256354s
    Jan 18 16:26:25.687: INFO: Pod "pod-projected-configmaps-52e49b4b-ecb2-4f3e-9834-fab0e6f6568b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017088926s
    STEP: Saw pod success 01/18/23 16:26:25.687
    Jan 18 16:26:25.687: INFO: Pod "pod-projected-configmaps-52e49b4b-ecb2-4f3e-9834-fab0e6f6568b" satisfied condition "Succeeded or Failed"
    Jan 18 16:26:25.692: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-configmaps-52e49b4b-ecb2-4f3e-9834-fab0e6f6568b container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 16:26:25.719
    Jan 18 16:26:25.743: INFO: Waiting for pod pod-projected-configmaps-52e49b4b-ecb2-4f3e-9834-fab0e6f6568b to disappear
    Jan 18 16:26:25.750: INFO: Pod pod-projected-configmaps-52e49b4b-ecb2-4f3e-9834-fab0e6f6568b no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 18 16:26:25.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8866" for this suite. 01/18/23 16:26:25.757
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:26:25.768
Jan 18 16:26:25.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename kubectl 01/18/23 16:26:25.769
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:26:25.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:26:25.794
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/18/23 16:26:25.8
Jan 18 16:26:25.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1842 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 18 16:26:25.926: INFO: stderr: ""
Jan 18 16:26:25.926: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 01/18/23 16:26:25.926
STEP: verifying the pod e2e-test-httpd-pod was created 01/18/23 16:26:30.978
Jan 18 16:26:30.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1842 get pod e2e-test-httpd-pod -o json'
Jan 18 16:26:31.122: INFO: stderr: ""
Jan 18 16:26:31.123: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"2c2f3af59389bff93ac36e93dfac9bca0a6e4c227f8cd83a0ef15344a6d62cd9\",\n            \"cni.projectcalico.org/podIP\": \"10.233.68.196/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.233.68.196/32\"\n        },\n        \"creationTimestamp\": \"2023-01-18T16:26:25Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1842\",\n        \"resourceVersion\": \"64280\",\n        \"uid\": \"3479b879-411f-45a5-b6b8-7400d88cf862\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-f7w4d\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"v1-25-1-18760-w2\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-f7w4d\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T16:26:25Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T16:26:27Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T16:26:27Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T16:26:25Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://df8b2c0a642743ebbe72c0f466b2c54f85cc29999751e80cccdd2e5463ee48e9\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-18T16:26:27Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.101.216\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.68.196\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.68.196\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-18T16:26:25Z\"\n    }\n}\n"
STEP: replace the image in the pod 01/18/23 16:26:31.123
Jan 18 16:26:31.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1842 replace -f -'
Jan 18 16:26:31.523: INFO: stderr: ""
Jan 18 16:26:31.523: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 01/18/23 16:26:31.523
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Jan 18 16:26:31.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1842 delete pods e2e-test-httpd-pod'
Jan 18 16:26:33.757: INFO: stderr: ""
Jan 18 16:26:33.757: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 16:26:33.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1842" for this suite. 01/18/23 16:26:33.765
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":295,"skipped":5542,"failed":0}
------------------------------
• [SLOW TEST] [8.006 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:26:25.768
    Jan 18 16:26:25.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename kubectl 01/18/23 16:26:25.769
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:26:25.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:26:25.794
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/18/23 16:26:25.8
    Jan 18 16:26:25.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1842 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 18 16:26:25.926: INFO: stderr: ""
    Jan 18 16:26:25.926: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 01/18/23 16:26:25.926
    STEP: verifying the pod e2e-test-httpd-pod was created 01/18/23 16:26:30.978
    Jan 18 16:26:30.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1842 get pod e2e-test-httpd-pod -o json'
    Jan 18 16:26:31.122: INFO: stderr: ""
    Jan 18 16:26:31.123: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"2c2f3af59389bff93ac36e93dfac9bca0a6e4c227f8cd83a0ef15344a6d62cd9\",\n            \"cni.projectcalico.org/podIP\": \"10.233.68.196/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.233.68.196/32\"\n        },\n        \"creationTimestamp\": \"2023-01-18T16:26:25Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1842\",\n        \"resourceVersion\": \"64280\",\n        \"uid\": \"3479b879-411f-45a5-b6b8-7400d88cf862\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-f7w4d\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"v1-25-1-18760-w2\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-f7w4d\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T16:26:25Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T16:26:27Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T16:26:27Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T16:26:25Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://df8b2c0a642743ebbe72c0f466b2c54f85cc29999751e80cccdd2e5463ee48e9\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-18T16:26:27Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.101.216\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.68.196\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.68.196\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-18T16:26:25Z\"\n    }\n}\n"
    STEP: replace the image in the pod 01/18/23 16:26:31.123
    Jan 18 16:26:31.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1842 replace -f -'
    Jan 18 16:26:31.523: INFO: stderr: ""
    Jan 18 16:26:31.523: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 01/18/23 16:26:31.523
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Jan 18 16:26:31.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-1842 delete pods e2e-test-httpd-pod'
    Jan 18 16:26:33.757: INFO: stderr: ""
    Jan 18 16:26:33.757: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 16:26:33.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1842" for this suite. 01/18/23 16:26:33.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:26:33.775
Jan 18 16:26:33.775: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename events 01/18/23 16:26:33.776
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:26:33.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:26:33.806
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 01/18/23 16:26:33.817
STEP: get a list of Events with a label in the current namespace 01/18/23 16:26:33.839
STEP: delete a list of events 01/18/23 16:26:33.844
Jan 18 16:26:33.844: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/18/23 16:26:33.877
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jan 18 16:26:33.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3673" for this suite. 01/18/23 16:26:33.887
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":296,"skipped":5551,"failed":0}
------------------------------
• [0.120 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:26:33.775
    Jan 18 16:26:33.775: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename events 01/18/23 16:26:33.776
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:26:33.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:26:33.806
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 01/18/23 16:26:33.817
    STEP: get a list of Events with a label in the current namespace 01/18/23 16:26:33.839
    STEP: delete a list of events 01/18/23 16:26:33.844
    Jan 18 16:26:33.844: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/18/23 16:26:33.877
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jan 18 16:26:33.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-3673" for this suite. 01/18/23 16:26:33.887
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:26:33.895
Jan 18 16:26:33.896: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename pods 01/18/23 16:26:33.897
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:26:33.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:26:33.937
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Jan 18 16:26:33.941: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: creating the pod 01/18/23 16:26:33.943
STEP: submitting the pod to kubernetes 01/18/23 16:26:33.943
Jan 18 16:26:33.957: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-b7852015-28d7-40cd-be9b-412cfd94f74a" in namespace "pods-2038" to be "running and ready"
Jan 18 16:26:33.964: INFO: Pod "pod-logs-websocket-b7852015-28d7-40cd-be9b-412cfd94f74a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.838464ms
Jan 18 16:26:33.964: INFO: The phase of Pod pod-logs-websocket-b7852015-28d7-40cd-be9b-412cfd94f74a is Pending, waiting for it to be Running (with Ready = true)
Jan 18 16:26:35.971: INFO: Pod "pod-logs-websocket-b7852015-28d7-40cd-be9b-412cfd94f74a": Phase="Running", Reason="", readiness=true. Elapsed: 2.013867168s
Jan 18 16:26:35.971: INFO: The phase of Pod pod-logs-websocket-b7852015-28d7-40cd-be9b-412cfd94f74a is Running (Ready = true)
Jan 18 16:26:35.971: INFO: Pod "pod-logs-websocket-b7852015-28d7-40cd-be9b-412cfd94f74a" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 16:26:35.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2038" for this suite. 01/18/23 16:26:35.995
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":297,"skipped":5554,"failed":0}
------------------------------
• [2.106 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:26:33.895
    Jan 18 16:26:33.896: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename pods 01/18/23 16:26:33.897
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:26:33.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:26:33.937
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Jan 18 16:26:33.941: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: creating the pod 01/18/23 16:26:33.943
    STEP: submitting the pod to kubernetes 01/18/23 16:26:33.943
    Jan 18 16:26:33.957: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-b7852015-28d7-40cd-be9b-412cfd94f74a" in namespace "pods-2038" to be "running and ready"
    Jan 18 16:26:33.964: INFO: Pod "pod-logs-websocket-b7852015-28d7-40cd-be9b-412cfd94f74a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.838464ms
    Jan 18 16:26:33.964: INFO: The phase of Pod pod-logs-websocket-b7852015-28d7-40cd-be9b-412cfd94f74a is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 16:26:35.971: INFO: Pod "pod-logs-websocket-b7852015-28d7-40cd-be9b-412cfd94f74a": Phase="Running", Reason="", readiness=true. Elapsed: 2.013867168s
    Jan 18 16:26:35.971: INFO: The phase of Pod pod-logs-websocket-b7852015-28d7-40cd-be9b-412cfd94f74a is Running (Ready = true)
    Jan 18 16:26:35.971: INFO: Pod "pod-logs-websocket-b7852015-28d7-40cd-be9b-412cfd94f74a" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 16:26:35.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2038" for this suite. 01/18/23 16:26:35.995
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:26:36.005
Jan 18 16:26:36.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename emptydir 01/18/23 16:26:36.006
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:26:36.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:26:36.03
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 01/18/23 16:26:36.035
Jan 18 16:26:36.047: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-c1d1c5c7-d82b-4588-845f-b397adfbd965" in namespace "emptydir-1458" to be "running"
Jan 18 16:26:36.052: INFO: Pod "pod-sharedvolume-c1d1c5c7-d82b-4588-845f-b397adfbd965": Phase="Pending", Reason="", readiness=false. Elapsed: 4.931393ms
Jan 18 16:26:38.057: INFO: Pod "pod-sharedvolume-c1d1c5c7-d82b-4588-845f-b397adfbd965": Phase="Running", Reason="", readiness=false. Elapsed: 2.010074729s
Jan 18 16:26:38.057: INFO: Pod "pod-sharedvolume-c1d1c5c7-d82b-4588-845f-b397adfbd965" satisfied condition "running"
STEP: Reading file content from the nginx-container 01/18/23 16:26:38.057
Jan 18 16:26:38.058: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1458 PodName:pod-sharedvolume-c1d1c5c7-d82b-4588-845f-b397adfbd965 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 16:26:38.058: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 16:26:38.059: INFO: ExecWithOptions: Clientset creation
Jan 18 16:26:38.060: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-1458/pods/pod-sharedvolume-c1d1c5c7-d82b-4588-845f-b397adfbd965/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan 18 16:26:38.150: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 16:26:38.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1458" for this suite. 01/18/23 16:26:38.163
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":298,"skipped":5574,"failed":0}
------------------------------
• [2.164 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:26:36.005
    Jan 18 16:26:36.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename emptydir 01/18/23 16:26:36.006
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:26:36.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:26:36.03
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 01/18/23 16:26:36.035
    Jan 18 16:26:36.047: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-c1d1c5c7-d82b-4588-845f-b397adfbd965" in namespace "emptydir-1458" to be "running"
    Jan 18 16:26:36.052: INFO: Pod "pod-sharedvolume-c1d1c5c7-d82b-4588-845f-b397adfbd965": Phase="Pending", Reason="", readiness=false. Elapsed: 4.931393ms
    Jan 18 16:26:38.057: INFO: Pod "pod-sharedvolume-c1d1c5c7-d82b-4588-845f-b397adfbd965": Phase="Running", Reason="", readiness=false. Elapsed: 2.010074729s
    Jan 18 16:26:38.057: INFO: Pod "pod-sharedvolume-c1d1c5c7-d82b-4588-845f-b397adfbd965" satisfied condition "running"
    STEP: Reading file content from the nginx-container 01/18/23 16:26:38.057
    Jan 18 16:26:38.058: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1458 PodName:pod-sharedvolume-c1d1c5c7-d82b-4588-845f-b397adfbd965 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 16:26:38.058: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 16:26:38.059: INFO: ExecWithOptions: Clientset creation
    Jan 18 16:26:38.060: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-1458/pods/pod-sharedvolume-c1d1c5c7-d82b-4588-845f-b397adfbd965/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jan 18 16:26:38.150: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 16:26:38.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1458" for this suite. 01/18/23 16:26:38.163
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:26:38.175
Jan 18 16:26:38.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename cronjob 01/18/23 16:26:38.178
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:26:38.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:26:38.206
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 01/18/23 16:26:38.21
STEP: Ensuring a job is scheduled 01/18/23 16:26:38.218
STEP: Ensuring exactly one is scheduled 01/18/23 16:27:00.223
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/18/23 16:27:00.228
STEP: Ensuring no more jobs are scheduled 01/18/23 16:27:00.231
STEP: Removing cronjob 01/18/23 16:32:00.242
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 18 16:32:00.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2723" for this suite. 01/18/23 16:32:00.257
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":299,"skipped":5576,"failed":0}
------------------------------
• [SLOW TEST] [322.097 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:26:38.175
    Jan 18 16:26:38.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename cronjob 01/18/23 16:26:38.178
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:26:38.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:26:38.206
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 01/18/23 16:26:38.21
    STEP: Ensuring a job is scheduled 01/18/23 16:26:38.218
    STEP: Ensuring exactly one is scheduled 01/18/23 16:27:00.223
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/18/23 16:27:00.228
    STEP: Ensuring no more jobs are scheduled 01/18/23 16:27:00.231
    STEP: Removing cronjob 01/18/23 16:32:00.242
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 18 16:32:00.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-2723" for this suite. 01/18/23 16:32:00.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:32:00.275
Jan 18 16:32:00.275: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename pod-network-test 01/18/23 16:32:00.278
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:32:00.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:32:00.341
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-3802 01/18/23 16:32:00.346
STEP: creating a selector 01/18/23 16:32:00.346
STEP: Creating the service pods in kubernetes 01/18/23 16:32:00.347
Jan 18 16:32:00.347: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 18 16:32:00.376: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3802" to be "running and ready"
Jan 18 16:32:00.387: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.545475ms
Jan 18 16:32:00.388: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 16:32:02.393: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.017546505s
Jan 18 16:32:02.394: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 16:32:04.393: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.017403623s
Jan 18 16:32:04.393: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 16:32:06.393: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017465802s
Jan 18 16:32:06.393: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 16:32:08.393: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.017300609s
Jan 18 16:32:08.393: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 16:32:10.402: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.026073265s
Jan 18 16:32:10.402: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 16:32:12.397: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.02100066s
Jan 18 16:32:12.397: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 16:32:14.395: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.018938283s
Jan 18 16:32:14.395: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 16:32:16.393: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.016801929s
Jan 18 16:32:16.393: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 16:32:18.397: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.020786638s
Jan 18 16:32:18.397: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 16:32:20.396: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.020054587s
Jan 18 16:32:20.397: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 16:32:22.393: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.017274167s
Jan 18 16:32:22.394: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 18 16:32:22.394: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 18 16:32:22.398: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3802" to be "running and ready"
Jan 18 16:32:22.403: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.194756ms
Jan 18 16:32:22.403: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 18 16:32:22.403: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/18/23 16:32:22.409
Jan 18 16:32:22.417: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3802" to be "running"
Jan 18 16:32:22.453: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 36.721093ms
Jan 18 16:32:24.460: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.043359492s
Jan 18 16:32:24.460: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 18 16:32:24.465: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 18 16:32:24.465: INFO: Breadth first check of 10.233.78.177 on host 192.168.101.168...
Jan 18 16:32:24.470: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.68.199:9080/dial?request=hostname&protocol=udp&host=10.233.78.177&port=8081&tries=1'] Namespace:pod-network-test-3802 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 16:32:24.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 16:32:24.473: INFO: ExecWithOptions: Clientset creation
Jan 18 16:32:24.474: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3802/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.68.199%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.78.177%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 16:32:24.590: INFO: Waiting for responses: map[]
Jan 18 16:32:24.590: INFO: reached 10.233.78.177 after 0/1 tries
Jan 18 16:32:24.590: INFO: Breadth first check of 10.233.68.201 on host 192.168.101.216...
Jan 18 16:32:24.770: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.68.199:9080/dial?request=hostname&protocol=udp&host=10.233.68.201&port=8081&tries=1'] Namespace:pod-network-test-3802 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 16:32:24.770: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 16:32:24.771: INFO: ExecWithOptions: Clientset creation
Jan 18 16:32:24.771: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3802/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.68.199%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.68.201%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 16:32:24.885: INFO: Waiting for responses: map[]
Jan 18 16:32:24.885: INFO: reached 10.233.68.201 after 0/1 tries
Jan 18 16:32:24.885: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 18 16:32:24.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3802" for this suite. 01/18/23 16:32:24.893
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":300,"skipped":5582,"failed":0}
------------------------------
• [SLOW TEST] [24.635 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:32:00.275
    Jan 18 16:32:00.275: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename pod-network-test 01/18/23 16:32:00.278
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:32:00.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:32:00.341
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-3802 01/18/23 16:32:00.346
    STEP: creating a selector 01/18/23 16:32:00.346
    STEP: Creating the service pods in kubernetes 01/18/23 16:32:00.347
    Jan 18 16:32:00.347: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 18 16:32:00.376: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3802" to be "running and ready"
    Jan 18 16:32:00.387: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.545475ms
    Jan 18 16:32:00.388: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 16:32:02.393: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.017546505s
    Jan 18 16:32:02.394: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 16:32:04.393: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.017403623s
    Jan 18 16:32:04.393: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 16:32:06.393: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017465802s
    Jan 18 16:32:06.393: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 16:32:08.393: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.017300609s
    Jan 18 16:32:08.393: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 16:32:10.402: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.026073265s
    Jan 18 16:32:10.402: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 16:32:12.397: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.02100066s
    Jan 18 16:32:12.397: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 16:32:14.395: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.018938283s
    Jan 18 16:32:14.395: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 16:32:16.393: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.016801929s
    Jan 18 16:32:16.393: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 16:32:18.397: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.020786638s
    Jan 18 16:32:18.397: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 16:32:20.396: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.020054587s
    Jan 18 16:32:20.397: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 16:32:22.393: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.017274167s
    Jan 18 16:32:22.394: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 18 16:32:22.394: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 18 16:32:22.398: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3802" to be "running and ready"
    Jan 18 16:32:22.403: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.194756ms
    Jan 18 16:32:22.403: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 18 16:32:22.403: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/18/23 16:32:22.409
    Jan 18 16:32:22.417: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3802" to be "running"
    Jan 18 16:32:22.453: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 36.721093ms
    Jan 18 16:32:24.460: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.043359492s
    Jan 18 16:32:24.460: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 18 16:32:24.465: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 18 16:32:24.465: INFO: Breadth first check of 10.233.78.177 on host 192.168.101.168...
    Jan 18 16:32:24.470: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.68.199:9080/dial?request=hostname&protocol=udp&host=10.233.78.177&port=8081&tries=1'] Namespace:pod-network-test-3802 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 16:32:24.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 16:32:24.473: INFO: ExecWithOptions: Clientset creation
    Jan 18 16:32:24.474: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3802/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.68.199%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.78.177%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 18 16:32:24.590: INFO: Waiting for responses: map[]
    Jan 18 16:32:24.590: INFO: reached 10.233.78.177 after 0/1 tries
    Jan 18 16:32:24.590: INFO: Breadth first check of 10.233.68.201 on host 192.168.101.216...
    Jan 18 16:32:24.770: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.68.199:9080/dial?request=hostname&protocol=udp&host=10.233.68.201&port=8081&tries=1'] Namespace:pod-network-test-3802 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 16:32:24.770: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 16:32:24.771: INFO: ExecWithOptions: Clientset creation
    Jan 18 16:32:24.771: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3802/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.68.199%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.68.201%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 18 16:32:24.885: INFO: Waiting for responses: map[]
    Jan 18 16:32:24.885: INFO: reached 10.233.68.201 after 0/1 tries
    Jan 18 16:32:24.885: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 18 16:32:24.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-3802" for this suite. 01/18/23 16:32:24.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:32:24.912
Jan 18 16:32:24.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename deployment 01/18/23 16:32:24.914
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:32:24.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:32:24.946
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 01/18/23 16:32:24.961
Jan 18 16:32:24.962: INFO: Creating simple deployment test-deployment-pcprh
Jan 18 16:32:25.017: INFO: deployment "test-deployment-pcprh" doesn't have the required revision set
STEP: Getting /status 01/18/23 16:32:27.053
Jan 18 16:32:27.060: INFO: Deployment test-deployment-pcprh has Conditions: [{Available True 2023-01-18 16:32:27 +0000 UTC 2023-01-18 16:32:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-18 16:32:27 +0000 UTC 2023-01-18 16:32:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pcprh-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 01/18/23 16:32:27.06
Jan 18 16:32:27.077: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 32, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 32, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 32, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 32, 24, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-pcprh-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 01/18/23 16:32:27.077
Jan 18 16:32:27.082: INFO: Observed &Deployment event: ADDED
Jan 18 16:32:27.082: INFO: Observed Deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 16:32:24 +0000 UTC 2023-01-18 16:32:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pcprh-777898ffcc"}
Jan 18 16:32:27.083: INFO: Observed &Deployment event: MODIFIED
Jan 18 16:32:27.083: INFO: Observed Deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 16:32:24 +0000 UTC 2023-01-18 16:32:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pcprh-777898ffcc"}
Jan 18 16:32:27.083: INFO: Observed Deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 16:32:25 +0000 UTC 2023-01-18 16:32:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 18 16:32:27.084: INFO: Observed &Deployment event: MODIFIED
Jan 18 16:32:27.084: INFO: Observed Deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 16:32:25 +0000 UTC 2023-01-18 16:32:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 18 16:32:27.084: INFO: Observed Deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 16:32:25 +0000 UTC 2023-01-18 16:32:24 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-pcprh-777898ffcc" is progressing.}
Jan 18 16:32:27.085: INFO: Observed &Deployment event: MODIFIED
Jan 18 16:32:27.085: INFO: Observed Deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 16:32:27 +0000 UTC 2023-01-18 16:32:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 18 16:32:27.085: INFO: Observed Deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 16:32:27 +0000 UTC 2023-01-18 16:32:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pcprh-777898ffcc" has successfully progressed.}
Jan 18 16:32:27.085: INFO: Observed &Deployment event: MODIFIED
Jan 18 16:32:27.086: INFO: Observed Deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 16:32:27 +0000 UTC 2023-01-18 16:32:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 18 16:32:27.086: INFO: Observed Deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 16:32:27 +0000 UTC 2023-01-18 16:32:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pcprh-777898ffcc" has successfully progressed.}
Jan 18 16:32:27.086: INFO: Found Deployment test-deployment-pcprh in namespace deployment-8754 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 18 16:32:27.086: INFO: Deployment test-deployment-pcprh has an updated status
STEP: patching the Statefulset Status 01/18/23 16:32:27.086
Jan 18 16:32:27.087: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 18 16:32:27.096: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 01/18/23 16:32:27.096
Jan 18 16:32:27.101: INFO: Observed &Deployment event: ADDED
Jan 18 16:32:27.101: INFO: Observed deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 16:32:24 +0000 UTC 2023-01-18 16:32:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pcprh-777898ffcc"}
Jan 18 16:32:27.101: INFO: Observed &Deployment event: MODIFIED
Jan 18 16:32:27.102: INFO: Observed deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 16:32:24 +0000 UTC 2023-01-18 16:32:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pcprh-777898ffcc"}
Jan 18 16:32:27.102: INFO: Observed deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 16:32:25 +0000 UTC 2023-01-18 16:32:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 18 16:32:27.102: INFO: Observed &Deployment event: MODIFIED
Jan 18 16:32:27.102: INFO: Observed deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 16:32:25 +0000 UTC 2023-01-18 16:32:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 18 16:32:27.103: INFO: Observed deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 16:32:25 +0000 UTC 2023-01-18 16:32:24 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-pcprh-777898ffcc" is progressing.}
Jan 18 16:32:27.103: INFO: Observed &Deployment event: MODIFIED
Jan 18 16:32:27.103: INFO: Observed deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 16:32:27 +0000 UTC 2023-01-18 16:32:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 18 16:32:27.103: INFO: Observed deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 16:32:27 +0000 UTC 2023-01-18 16:32:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pcprh-777898ffcc" has successfully progressed.}
Jan 18 16:32:27.104: INFO: Observed &Deployment event: MODIFIED
Jan 18 16:32:27.104: INFO: Observed deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 16:32:27 +0000 UTC 2023-01-18 16:32:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 18 16:32:27.104: INFO: Observed deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 16:32:27 +0000 UTC 2023-01-18 16:32:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pcprh-777898ffcc" has successfully progressed.}
Jan 18 16:32:27.104: INFO: Observed deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 18 16:32:27.105: INFO: Observed &Deployment event: MODIFIED
Jan 18 16:32:27.105: INFO: Found deployment test-deployment-pcprh in namespace deployment-8754 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan 18 16:32:27.105: INFO: Deployment test-deployment-pcprh has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 16:32:27.112: INFO: Deployment "test-deployment-pcprh":
&Deployment{ObjectMeta:{test-deployment-pcprh  deployment-8754  1ff93e99-dd64-4fcd-b11d-63302d5e1b25 65803 1 2023-01-18 16:32:24 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-18 16:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-18 16:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-18 16:32:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b11b08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-pcprh-777898ffcc",LastUpdateTime:2023-01-18 16:32:27 +0000 UTC,LastTransitionTime:2023-01-18 16:32:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 18 16:32:27.123: INFO: New ReplicaSet "test-deployment-pcprh-777898ffcc" of Deployment "test-deployment-pcprh":
&ReplicaSet{ObjectMeta:{test-deployment-pcprh-777898ffcc  deployment-8754  b6cb198b-e6c8-4f39-8fcf-dcc95d23e09f 65799 1 2023-01-18 16:32:24 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-pcprh 1ff93e99-dd64-4fcd-b11d-63302d5e1b25 0xc004b11f10 0xc004b11f11}] [] [{kube-controller-manager Update apps/v1 2023-01-18 16:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1ff93e99-dd64-4fcd-b11d-63302d5e1b25\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 16:32:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b11fb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 16:32:27.129: INFO: Pod "test-deployment-pcprh-777898ffcc-glzzs" is available:
&Pod{ObjectMeta:{test-deployment-pcprh-777898ffcc-glzzs test-deployment-pcprh-777898ffcc- deployment-8754  d5d37aa4-46e0-4039-aafb-6eadbd7b52de 65798 0 2023-01-18 16:32:24 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:b623657e2da648c5c427e26b6e915accfa70920d9b0e3249d3cb6da06c43b671 cni.projectcalico.org/podIP:10.233.68.200/32 cni.projectcalico.org/podIPs:10.233.68.200/32] [{apps/v1 ReplicaSet test-deployment-pcprh-777898ffcc b6cb198b-e6c8-4f39-8fcf-dcc95d23e09f 0xc003ae81b0 0xc003ae81b1}] [] [{kube-controller-manager Update v1 2023-01-18 16:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6cb198b-e6c8-4f39-8fcf-dcc95d23e09f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 16:32:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 16:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.200\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pcbq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pcbq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:32:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.200,StartTime:2023-01-18 16:32:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 16:32:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f5824c4931a0072ebc5f0464c79a97b10ce69cf3f1e0229cea0e8d4b178d4cf4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.200,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 18 16:32:27.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8754" for this suite. 01/18/23 16:32:27.136
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":301,"skipped":5590,"failed":0}
------------------------------
• [2.229 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:32:24.912
    Jan 18 16:32:24.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename deployment 01/18/23 16:32:24.914
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:32:24.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:32:24.946
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 01/18/23 16:32:24.961
    Jan 18 16:32:24.962: INFO: Creating simple deployment test-deployment-pcprh
    Jan 18 16:32:25.017: INFO: deployment "test-deployment-pcprh" doesn't have the required revision set
    STEP: Getting /status 01/18/23 16:32:27.053
    Jan 18 16:32:27.060: INFO: Deployment test-deployment-pcprh has Conditions: [{Available True 2023-01-18 16:32:27 +0000 UTC 2023-01-18 16:32:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-18 16:32:27 +0000 UTC 2023-01-18 16:32:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pcprh-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 01/18/23 16:32:27.06
    Jan 18 16:32:27.077: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 32, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 32, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 32, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 32, 24, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-pcprh-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 01/18/23 16:32:27.077
    Jan 18 16:32:27.082: INFO: Observed &Deployment event: ADDED
    Jan 18 16:32:27.082: INFO: Observed Deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 16:32:24 +0000 UTC 2023-01-18 16:32:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pcprh-777898ffcc"}
    Jan 18 16:32:27.083: INFO: Observed &Deployment event: MODIFIED
    Jan 18 16:32:27.083: INFO: Observed Deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 16:32:24 +0000 UTC 2023-01-18 16:32:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pcprh-777898ffcc"}
    Jan 18 16:32:27.083: INFO: Observed Deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 16:32:25 +0000 UTC 2023-01-18 16:32:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 18 16:32:27.084: INFO: Observed &Deployment event: MODIFIED
    Jan 18 16:32:27.084: INFO: Observed Deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 16:32:25 +0000 UTC 2023-01-18 16:32:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 18 16:32:27.084: INFO: Observed Deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 16:32:25 +0000 UTC 2023-01-18 16:32:24 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-pcprh-777898ffcc" is progressing.}
    Jan 18 16:32:27.085: INFO: Observed &Deployment event: MODIFIED
    Jan 18 16:32:27.085: INFO: Observed Deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 16:32:27 +0000 UTC 2023-01-18 16:32:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 18 16:32:27.085: INFO: Observed Deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 16:32:27 +0000 UTC 2023-01-18 16:32:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pcprh-777898ffcc" has successfully progressed.}
    Jan 18 16:32:27.085: INFO: Observed &Deployment event: MODIFIED
    Jan 18 16:32:27.086: INFO: Observed Deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 16:32:27 +0000 UTC 2023-01-18 16:32:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 18 16:32:27.086: INFO: Observed Deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 16:32:27 +0000 UTC 2023-01-18 16:32:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pcprh-777898ffcc" has successfully progressed.}
    Jan 18 16:32:27.086: INFO: Found Deployment test-deployment-pcprh in namespace deployment-8754 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 18 16:32:27.086: INFO: Deployment test-deployment-pcprh has an updated status
    STEP: patching the Statefulset Status 01/18/23 16:32:27.086
    Jan 18 16:32:27.087: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 18 16:32:27.096: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 01/18/23 16:32:27.096
    Jan 18 16:32:27.101: INFO: Observed &Deployment event: ADDED
    Jan 18 16:32:27.101: INFO: Observed deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 16:32:24 +0000 UTC 2023-01-18 16:32:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pcprh-777898ffcc"}
    Jan 18 16:32:27.101: INFO: Observed &Deployment event: MODIFIED
    Jan 18 16:32:27.102: INFO: Observed deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 16:32:24 +0000 UTC 2023-01-18 16:32:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pcprh-777898ffcc"}
    Jan 18 16:32:27.102: INFO: Observed deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 16:32:25 +0000 UTC 2023-01-18 16:32:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 18 16:32:27.102: INFO: Observed &Deployment event: MODIFIED
    Jan 18 16:32:27.102: INFO: Observed deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 16:32:25 +0000 UTC 2023-01-18 16:32:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 18 16:32:27.103: INFO: Observed deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 16:32:25 +0000 UTC 2023-01-18 16:32:24 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-pcprh-777898ffcc" is progressing.}
    Jan 18 16:32:27.103: INFO: Observed &Deployment event: MODIFIED
    Jan 18 16:32:27.103: INFO: Observed deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 16:32:27 +0000 UTC 2023-01-18 16:32:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 18 16:32:27.103: INFO: Observed deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 16:32:27 +0000 UTC 2023-01-18 16:32:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pcprh-777898ffcc" has successfully progressed.}
    Jan 18 16:32:27.104: INFO: Observed &Deployment event: MODIFIED
    Jan 18 16:32:27.104: INFO: Observed deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 16:32:27 +0000 UTC 2023-01-18 16:32:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 18 16:32:27.104: INFO: Observed deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 16:32:27 +0000 UTC 2023-01-18 16:32:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pcprh-777898ffcc" has successfully progressed.}
    Jan 18 16:32:27.104: INFO: Observed deployment test-deployment-pcprh in namespace deployment-8754 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 18 16:32:27.105: INFO: Observed &Deployment event: MODIFIED
    Jan 18 16:32:27.105: INFO: Found deployment test-deployment-pcprh in namespace deployment-8754 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jan 18 16:32:27.105: INFO: Deployment test-deployment-pcprh has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 16:32:27.112: INFO: Deployment "test-deployment-pcprh":
    &Deployment{ObjectMeta:{test-deployment-pcprh  deployment-8754  1ff93e99-dd64-4fcd-b11d-63302d5e1b25 65803 1 2023-01-18 16:32:24 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-18 16:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-18 16:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-18 16:32:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b11b08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-pcprh-777898ffcc",LastUpdateTime:2023-01-18 16:32:27 +0000 UTC,LastTransitionTime:2023-01-18 16:32:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 18 16:32:27.123: INFO: New ReplicaSet "test-deployment-pcprh-777898ffcc" of Deployment "test-deployment-pcprh":
    &ReplicaSet{ObjectMeta:{test-deployment-pcprh-777898ffcc  deployment-8754  b6cb198b-e6c8-4f39-8fcf-dcc95d23e09f 65799 1 2023-01-18 16:32:24 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-pcprh 1ff93e99-dd64-4fcd-b11d-63302d5e1b25 0xc004b11f10 0xc004b11f11}] [] [{kube-controller-manager Update apps/v1 2023-01-18 16:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1ff93e99-dd64-4fcd-b11d-63302d5e1b25\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 16:32:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b11fb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 16:32:27.129: INFO: Pod "test-deployment-pcprh-777898ffcc-glzzs" is available:
    &Pod{ObjectMeta:{test-deployment-pcprh-777898ffcc-glzzs test-deployment-pcprh-777898ffcc- deployment-8754  d5d37aa4-46e0-4039-aafb-6eadbd7b52de 65798 0 2023-01-18 16:32:24 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:b623657e2da648c5c427e26b6e915accfa70920d9b0e3249d3cb6da06c43b671 cni.projectcalico.org/podIP:10.233.68.200/32 cni.projectcalico.org/podIPs:10.233.68.200/32] [{apps/v1 ReplicaSet test-deployment-pcprh-777898ffcc b6cb198b-e6c8-4f39-8fcf-dcc95d23e09f 0xc003ae81b0 0xc003ae81b1}] [] [{kube-controller-manager Update v1 2023-01-18 16:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6cb198b-e6c8-4f39-8fcf-dcc95d23e09f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 16:32:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 16:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.200\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pcbq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pcbq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-25-1-18760-w2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:32:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.216,PodIP:10.233.68.200,StartTime:2023-01-18 16:32:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 16:32:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f5824c4931a0072ebc5f0464c79a97b10ce69cf3f1e0229cea0e8d4b178d4cf4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.200,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 18 16:32:27.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-8754" for this suite. 01/18/23 16:32:27.136
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:32:27.147
Jan 18 16:32:27.147: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename container-probe 01/18/23 16:32:27.149
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:32:27.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:32:27.193
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-3f49b307-591a-4679-9053-e76d211bb261 in namespace container-probe-6186 01/18/23 16:32:27.2
Jan 18 16:32:27.214: INFO: Waiting up to 5m0s for pod "liveness-3f49b307-591a-4679-9053-e76d211bb261" in namespace "container-probe-6186" to be "not pending"
Jan 18 16:32:27.221: INFO: Pod "liveness-3f49b307-591a-4679-9053-e76d211bb261": Phase="Pending", Reason="", readiness=false. Elapsed: 6.151873ms
Jan 18 16:32:29.226: INFO: Pod "liveness-3f49b307-591a-4679-9053-e76d211bb261": Phase="Running", Reason="", readiness=true. Elapsed: 2.011935012s
Jan 18 16:32:29.227: INFO: Pod "liveness-3f49b307-591a-4679-9053-e76d211bb261" satisfied condition "not pending"
Jan 18 16:32:29.227: INFO: Started pod liveness-3f49b307-591a-4679-9053-e76d211bb261 in namespace container-probe-6186
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 16:32:29.227
Jan 18 16:32:29.231: INFO: Initial restart count of pod liveness-3f49b307-591a-4679-9053-e76d211bb261 is 0
Jan 18 16:32:49.294: INFO: Restart count of pod container-probe-6186/liveness-3f49b307-591a-4679-9053-e76d211bb261 is now 1 (20.063013032s elapsed)
STEP: deleting the pod 01/18/23 16:32:49.294
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 18 16:32:49.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6186" for this suite. 01/18/23 16:32:49.333
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":302,"skipped":5590,"failed":0}
------------------------------
• [SLOW TEST] [22.195 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:32:27.147
    Jan 18 16:32:27.147: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename container-probe 01/18/23 16:32:27.149
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:32:27.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:32:27.193
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-3f49b307-591a-4679-9053-e76d211bb261 in namespace container-probe-6186 01/18/23 16:32:27.2
    Jan 18 16:32:27.214: INFO: Waiting up to 5m0s for pod "liveness-3f49b307-591a-4679-9053-e76d211bb261" in namespace "container-probe-6186" to be "not pending"
    Jan 18 16:32:27.221: INFO: Pod "liveness-3f49b307-591a-4679-9053-e76d211bb261": Phase="Pending", Reason="", readiness=false. Elapsed: 6.151873ms
    Jan 18 16:32:29.226: INFO: Pod "liveness-3f49b307-591a-4679-9053-e76d211bb261": Phase="Running", Reason="", readiness=true. Elapsed: 2.011935012s
    Jan 18 16:32:29.227: INFO: Pod "liveness-3f49b307-591a-4679-9053-e76d211bb261" satisfied condition "not pending"
    Jan 18 16:32:29.227: INFO: Started pod liveness-3f49b307-591a-4679-9053-e76d211bb261 in namespace container-probe-6186
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 16:32:29.227
    Jan 18 16:32:29.231: INFO: Initial restart count of pod liveness-3f49b307-591a-4679-9053-e76d211bb261 is 0
    Jan 18 16:32:49.294: INFO: Restart count of pod container-probe-6186/liveness-3f49b307-591a-4679-9053-e76d211bb261 is now 1 (20.063013032s elapsed)
    STEP: deleting the pod 01/18/23 16:32:49.294
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 18 16:32:49.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6186" for this suite. 01/18/23 16:32:49.333
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:32:49.352
Jan 18 16:32:49.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 16:32:49.354
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:32:49.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:32:49.42
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-15886548-e58b-4a61-8d4c-af9be229e815 01/18/23 16:32:49.424
STEP: Creating a pod to test consume secrets 01/18/23 16:32:49.429
Jan 18 16:32:49.436: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fd0208d9-c20d-49df-b57e-cee0e32a6254" in namespace "projected-6405" to be "Succeeded or Failed"
Jan 18 16:32:49.449: INFO: Pod "pod-projected-secrets-fd0208d9-c20d-49df-b57e-cee0e32a6254": Phase="Pending", Reason="", readiness=false. Elapsed: 13.260581ms
Jan 18 16:32:51.457: INFO: Pod "pod-projected-secrets-fd0208d9-c20d-49df-b57e-cee0e32a6254": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021515796s
Jan 18 16:32:53.456: INFO: Pod "pod-projected-secrets-fd0208d9-c20d-49df-b57e-cee0e32a6254": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020061318s
STEP: Saw pod success 01/18/23 16:32:53.456
Jan 18 16:32:53.456: INFO: Pod "pod-projected-secrets-fd0208d9-c20d-49df-b57e-cee0e32a6254" satisfied condition "Succeeded or Failed"
Jan 18 16:32:53.462: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-secrets-fd0208d9-c20d-49df-b57e-cee0e32a6254 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 16:32:53.484
Jan 18 16:32:53.500: INFO: Waiting for pod pod-projected-secrets-fd0208d9-c20d-49df-b57e-cee0e32a6254 to disappear
Jan 18 16:32:53.504: INFO: Pod pod-projected-secrets-fd0208d9-c20d-49df-b57e-cee0e32a6254 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 18 16:32:53.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6405" for this suite. 01/18/23 16:32:53.514
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":303,"skipped":5630,"failed":0}
------------------------------
• [4.170 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:32:49.352
    Jan 18 16:32:49.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 16:32:49.354
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:32:49.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:32:49.42
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-15886548-e58b-4a61-8d4c-af9be229e815 01/18/23 16:32:49.424
    STEP: Creating a pod to test consume secrets 01/18/23 16:32:49.429
    Jan 18 16:32:49.436: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fd0208d9-c20d-49df-b57e-cee0e32a6254" in namespace "projected-6405" to be "Succeeded or Failed"
    Jan 18 16:32:49.449: INFO: Pod "pod-projected-secrets-fd0208d9-c20d-49df-b57e-cee0e32a6254": Phase="Pending", Reason="", readiness=false. Elapsed: 13.260581ms
    Jan 18 16:32:51.457: INFO: Pod "pod-projected-secrets-fd0208d9-c20d-49df-b57e-cee0e32a6254": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021515796s
    Jan 18 16:32:53.456: INFO: Pod "pod-projected-secrets-fd0208d9-c20d-49df-b57e-cee0e32a6254": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020061318s
    STEP: Saw pod success 01/18/23 16:32:53.456
    Jan 18 16:32:53.456: INFO: Pod "pod-projected-secrets-fd0208d9-c20d-49df-b57e-cee0e32a6254" satisfied condition "Succeeded or Failed"
    Jan 18 16:32:53.462: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-projected-secrets-fd0208d9-c20d-49df-b57e-cee0e32a6254 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 16:32:53.484
    Jan 18 16:32:53.500: INFO: Waiting for pod pod-projected-secrets-fd0208d9-c20d-49df-b57e-cee0e32a6254 to disappear
    Jan 18 16:32:53.504: INFO: Pod pod-projected-secrets-fd0208d9-c20d-49df-b57e-cee0e32a6254 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 18 16:32:53.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6405" for this suite. 01/18/23 16:32:53.514
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:32:53.525
Jan 18 16:32:53.525: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename emptydir-wrapper 01/18/23 16:32:53.527
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:32:53.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:32:53.557
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 01/18/23 16:32:53.565
STEP: Creating RC which spawns configmap-volume pods 01/18/23 16:32:53.829
Jan 18 16:32:53.959: INFO: Pod name wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950: Found 3 pods out of 5
Jan 18 16:32:58.969: INFO: Pod name wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/18/23 16:32:58.969
Jan 18 16:32:58.969: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-7g9hl" in namespace "emptydir-wrapper-1446" to be "running"
Jan 18 16:32:58.973: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-7g9hl": Phase="Pending", Reason="", readiness=false. Elapsed: 3.673495ms
Jan 18 16:33:00.983: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-7g9hl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013978648s
Jan 18 16:33:02.995: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-7g9hl": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026095568s
Jan 18 16:33:04.997: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-7g9hl": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027454448s
Jan 18 16:33:06.984: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-7g9hl": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014537966s
Jan 18 16:33:08.980: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-7g9hl": Phase="Running", Reason="", readiness=true. Elapsed: 10.010609977s
Jan 18 16:33:08.980: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-7g9hl" satisfied condition "running"
Jan 18 16:33:08.980: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-8gw68" in namespace "emptydir-wrapper-1446" to be "running"
Jan 18 16:33:08.986: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-8gw68": Phase="Running", Reason="", readiness=true. Elapsed: 5.535398ms
Jan 18 16:33:08.986: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-8gw68" satisfied condition "running"
Jan 18 16:33:08.986: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-cjsp4" in namespace "emptydir-wrapper-1446" to be "running"
Jan 18 16:33:08.998: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-cjsp4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.915696ms
Jan 18 16:33:11.007: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-cjsp4": Phase="Running", Reason="", readiness=true. Elapsed: 2.021705201s
Jan 18 16:33:11.008: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-cjsp4" satisfied condition "running"
Jan 18 16:33:11.008: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-cwd4s" in namespace "emptydir-wrapper-1446" to be "running"
Jan 18 16:33:11.020: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-cwd4s": Phase="Running", Reason="", readiness=true. Elapsed: 12.180351ms
Jan 18 16:33:11.020: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-cwd4s" satisfied condition "running"
Jan 18 16:33:11.020: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-nf6zp" in namespace "emptydir-wrapper-1446" to be "running"
Jan 18 16:33:11.035: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-nf6zp": Phase="Running", Reason="", readiness=true. Elapsed: 14.078687ms
Jan 18 16:33:11.035: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-nf6zp" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950 in namespace emptydir-wrapper-1446, will wait for the garbage collector to delete the pods 01/18/23 16:33:11.035
Jan 18 16:33:11.103: INFO: Deleting ReplicationController wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950 took: 9.830952ms
Jan 18 16:33:11.305: INFO: Terminating ReplicationController wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950 pods took: 201.768571ms
STEP: Creating RC which spawns configmap-volume pods 01/18/23 16:33:14.612
Jan 18 16:33:14.635: INFO: Pod name wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b: Found 0 pods out of 5
Jan 18 16:33:19.644: INFO: Pod name wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/18/23 16:33:19.645
Jan 18 16:33:19.645: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-5kzpk" in namespace "emptydir-wrapper-1446" to be "running"
Jan 18 16:33:19.651: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-5kzpk": Phase="Pending", Reason="", readiness=false. Elapsed: 5.448927ms
Jan 18 16:33:21.658: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-5kzpk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013275281s
Jan 18 16:33:23.660: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-5kzpk": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01431981s
Jan 18 16:33:25.657: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-5kzpk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011571136s
Jan 18 16:33:27.657: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-5kzpk": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012035656s
Jan 18 16:33:29.662: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-5kzpk": Phase="Pending", Reason="", readiness=false. Elapsed: 10.016463224s
Jan 18 16:33:31.660: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-5kzpk": Phase="Running", Reason="", readiness=true. Elapsed: 12.01488206s
Jan 18 16:33:31.660: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-5kzpk" satisfied condition "running"
Jan 18 16:33:31.660: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-98hsm" in namespace "emptydir-wrapper-1446" to be "running"
Jan 18 16:33:31.666: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-98hsm": Phase="Running", Reason="", readiness=true. Elapsed: 6.067117ms
Jan 18 16:33:31.666: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-98hsm" satisfied condition "running"
Jan 18 16:33:31.666: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-gwh58" in namespace "emptydir-wrapper-1446" to be "running"
Jan 18 16:33:31.673: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-gwh58": Phase="Running", Reason="", readiness=true. Elapsed: 6.937143ms
Jan 18 16:33:31.674: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-gwh58" satisfied condition "running"
Jan 18 16:33:31.674: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-q4lm4" in namespace "emptydir-wrapper-1446" to be "running"
Jan 18 16:33:31.680: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-q4lm4": Phase="Running", Reason="", readiness=true. Elapsed: 6.340117ms
Jan 18 16:33:31.680: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-q4lm4" satisfied condition "running"
Jan 18 16:33:31.680: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-z4xpd" in namespace "emptydir-wrapper-1446" to be "running"
Jan 18 16:33:31.685: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-z4xpd": Phase="Running", Reason="", readiness=true. Elapsed: 5.30501ms
Jan 18 16:33:31.685: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-z4xpd" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b in namespace emptydir-wrapper-1446, will wait for the garbage collector to delete the pods 01/18/23 16:33:31.685
Jan 18 16:33:31.753: INFO: Deleting ReplicationController wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b took: 8.285179ms
Jan 18 16:33:31.955: INFO: Terminating ReplicationController wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b pods took: 201.248515ms
STEP: Creating RC which spawns configmap-volume pods 01/18/23 16:33:34.866
Jan 18 16:33:34.888: INFO: Pod name wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c: Found 0 pods out of 5
Jan 18 16:33:39.900: INFO: Pod name wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/18/23 16:33:39.9
Jan 18 16:33:39.901: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-66s4z" in namespace "emptydir-wrapper-1446" to be "running"
Jan 18 16:33:39.907: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-66s4z": Phase="Pending", Reason="", readiness=false. Elapsed: 6.164736ms
Jan 18 16:33:41.916: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-66s4z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014575245s
Jan 18 16:33:43.915: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-66s4z": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01439775s
Jan 18 16:33:45.916: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-66s4z": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014973905s
Jan 18 16:33:47.915: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-66s4z": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013683219s
Jan 18 16:33:49.915: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-66s4z": Phase="Running", Reason="", readiness=true. Elapsed: 10.013896199s
Jan 18 16:33:49.915: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-66s4z" satisfied condition "running"
Jan 18 16:33:49.915: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-6drr7" in namespace "emptydir-wrapper-1446" to be "running"
Jan 18 16:33:49.920: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-6drr7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.559478ms
Jan 18 16:33:51.935: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-6drr7": Phase="Running", Reason="", readiness=true. Elapsed: 2.02000641s
Jan 18 16:33:51.935: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-6drr7" satisfied condition "running"
Jan 18 16:33:51.935: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-dr9p2" in namespace "emptydir-wrapper-1446" to be "running"
Jan 18 16:33:51.941: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-dr9p2": Phase="Running", Reason="", readiness=true. Elapsed: 5.799446ms
Jan 18 16:33:51.941: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-dr9p2" satisfied condition "running"
Jan 18 16:33:51.941: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-gtl9b" in namespace "emptydir-wrapper-1446" to be "running"
Jan 18 16:33:51.950: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-gtl9b": Phase="Running", Reason="", readiness=true. Elapsed: 8.680292ms
Jan 18 16:33:51.950: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-gtl9b" satisfied condition "running"
Jan 18 16:33:51.950: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-xb9xt" in namespace "emptydir-wrapper-1446" to be "running"
Jan 18 16:33:51.955: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-xb9xt": Phase="Running", Reason="", readiness=true. Elapsed: 5.259528ms
Jan 18 16:33:51.955: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-xb9xt" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c in namespace emptydir-wrapper-1446, will wait for the garbage collector to delete the pods 01/18/23 16:33:51.956
Jan 18 16:33:52.026: INFO: Deleting ReplicationController wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c took: 14.412964ms
Jan 18 16:33:52.130: INFO: Terminating ReplicationController wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c pods took: 103.761531ms
STEP: Cleaning up the configMaps 01/18/23 16:33:55.231
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jan 18 16:33:55.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1446" for this suite. 01/18/23 16:33:55.595
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":304,"skipped":5632,"failed":0}
------------------------------
• [SLOW TEST] [62.079 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:32:53.525
    Jan 18 16:32:53.525: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename emptydir-wrapper 01/18/23 16:32:53.527
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:32:53.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:32:53.557
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 01/18/23 16:32:53.565
    STEP: Creating RC which spawns configmap-volume pods 01/18/23 16:32:53.829
    Jan 18 16:32:53.959: INFO: Pod name wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950: Found 3 pods out of 5
    Jan 18 16:32:58.969: INFO: Pod name wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/18/23 16:32:58.969
    Jan 18 16:32:58.969: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-7g9hl" in namespace "emptydir-wrapper-1446" to be "running"
    Jan 18 16:32:58.973: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-7g9hl": Phase="Pending", Reason="", readiness=false. Elapsed: 3.673495ms
    Jan 18 16:33:00.983: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-7g9hl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013978648s
    Jan 18 16:33:02.995: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-7g9hl": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026095568s
    Jan 18 16:33:04.997: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-7g9hl": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027454448s
    Jan 18 16:33:06.984: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-7g9hl": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014537966s
    Jan 18 16:33:08.980: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-7g9hl": Phase="Running", Reason="", readiness=true. Elapsed: 10.010609977s
    Jan 18 16:33:08.980: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-7g9hl" satisfied condition "running"
    Jan 18 16:33:08.980: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-8gw68" in namespace "emptydir-wrapper-1446" to be "running"
    Jan 18 16:33:08.986: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-8gw68": Phase="Running", Reason="", readiness=true. Elapsed: 5.535398ms
    Jan 18 16:33:08.986: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-8gw68" satisfied condition "running"
    Jan 18 16:33:08.986: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-cjsp4" in namespace "emptydir-wrapper-1446" to be "running"
    Jan 18 16:33:08.998: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-cjsp4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.915696ms
    Jan 18 16:33:11.007: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-cjsp4": Phase="Running", Reason="", readiness=true. Elapsed: 2.021705201s
    Jan 18 16:33:11.008: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-cjsp4" satisfied condition "running"
    Jan 18 16:33:11.008: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-cwd4s" in namespace "emptydir-wrapper-1446" to be "running"
    Jan 18 16:33:11.020: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-cwd4s": Phase="Running", Reason="", readiness=true. Elapsed: 12.180351ms
    Jan 18 16:33:11.020: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-cwd4s" satisfied condition "running"
    Jan 18 16:33:11.020: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-nf6zp" in namespace "emptydir-wrapper-1446" to be "running"
    Jan 18 16:33:11.035: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-nf6zp": Phase="Running", Reason="", readiness=true. Elapsed: 14.078687ms
    Jan 18 16:33:11.035: INFO: Pod "wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950-nf6zp" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950 in namespace emptydir-wrapper-1446, will wait for the garbage collector to delete the pods 01/18/23 16:33:11.035
    Jan 18 16:33:11.103: INFO: Deleting ReplicationController wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950 took: 9.830952ms
    Jan 18 16:33:11.305: INFO: Terminating ReplicationController wrapped-volume-race-0b4965c0-2db7-4e8a-a4cd-d274209ff950 pods took: 201.768571ms
    STEP: Creating RC which spawns configmap-volume pods 01/18/23 16:33:14.612
    Jan 18 16:33:14.635: INFO: Pod name wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b: Found 0 pods out of 5
    Jan 18 16:33:19.644: INFO: Pod name wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/18/23 16:33:19.645
    Jan 18 16:33:19.645: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-5kzpk" in namespace "emptydir-wrapper-1446" to be "running"
    Jan 18 16:33:19.651: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-5kzpk": Phase="Pending", Reason="", readiness=false. Elapsed: 5.448927ms
    Jan 18 16:33:21.658: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-5kzpk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013275281s
    Jan 18 16:33:23.660: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-5kzpk": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01431981s
    Jan 18 16:33:25.657: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-5kzpk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011571136s
    Jan 18 16:33:27.657: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-5kzpk": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012035656s
    Jan 18 16:33:29.662: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-5kzpk": Phase="Pending", Reason="", readiness=false. Elapsed: 10.016463224s
    Jan 18 16:33:31.660: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-5kzpk": Phase="Running", Reason="", readiness=true. Elapsed: 12.01488206s
    Jan 18 16:33:31.660: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-5kzpk" satisfied condition "running"
    Jan 18 16:33:31.660: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-98hsm" in namespace "emptydir-wrapper-1446" to be "running"
    Jan 18 16:33:31.666: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-98hsm": Phase="Running", Reason="", readiness=true. Elapsed: 6.067117ms
    Jan 18 16:33:31.666: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-98hsm" satisfied condition "running"
    Jan 18 16:33:31.666: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-gwh58" in namespace "emptydir-wrapper-1446" to be "running"
    Jan 18 16:33:31.673: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-gwh58": Phase="Running", Reason="", readiness=true. Elapsed: 6.937143ms
    Jan 18 16:33:31.674: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-gwh58" satisfied condition "running"
    Jan 18 16:33:31.674: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-q4lm4" in namespace "emptydir-wrapper-1446" to be "running"
    Jan 18 16:33:31.680: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-q4lm4": Phase="Running", Reason="", readiness=true. Elapsed: 6.340117ms
    Jan 18 16:33:31.680: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-q4lm4" satisfied condition "running"
    Jan 18 16:33:31.680: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-z4xpd" in namespace "emptydir-wrapper-1446" to be "running"
    Jan 18 16:33:31.685: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-z4xpd": Phase="Running", Reason="", readiness=true. Elapsed: 5.30501ms
    Jan 18 16:33:31.685: INFO: Pod "wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b-z4xpd" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b in namespace emptydir-wrapper-1446, will wait for the garbage collector to delete the pods 01/18/23 16:33:31.685
    Jan 18 16:33:31.753: INFO: Deleting ReplicationController wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b took: 8.285179ms
    Jan 18 16:33:31.955: INFO: Terminating ReplicationController wrapped-volume-race-5562945d-3e42-40e5-8c29-ff016353b96b pods took: 201.248515ms
    STEP: Creating RC which spawns configmap-volume pods 01/18/23 16:33:34.866
    Jan 18 16:33:34.888: INFO: Pod name wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c: Found 0 pods out of 5
    Jan 18 16:33:39.900: INFO: Pod name wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/18/23 16:33:39.9
    Jan 18 16:33:39.901: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-66s4z" in namespace "emptydir-wrapper-1446" to be "running"
    Jan 18 16:33:39.907: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-66s4z": Phase="Pending", Reason="", readiness=false. Elapsed: 6.164736ms
    Jan 18 16:33:41.916: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-66s4z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014575245s
    Jan 18 16:33:43.915: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-66s4z": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01439775s
    Jan 18 16:33:45.916: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-66s4z": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014973905s
    Jan 18 16:33:47.915: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-66s4z": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013683219s
    Jan 18 16:33:49.915: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-66s4z": Phase="Running", Reason="", readiness=true. Elapsed: 10.013896199s
    Jan 18 16:33:49.915: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-66s4z" satisfied condition "running"
    Jan 18 16:33:49.915: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-6drr7" in namespace "emptydir-wrapper-1446" to be "running"
    Jan 18 16:33:49.920: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-6drr7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.559478ms
    Jan 18 16:33:51.935: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-6drr7": Phase="Running", Reason="", readiness=true. Elapsed: 2.02000641s
    Jan 18 16:33:51.935: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-6drr7" satisfied condition "running"
    Jan 18 16:33:51.935: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-dr9p2" in namespace "emptydir-wrapper-1446" to be "running"
    Jan 18 16:33:51.941: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-dr9p2": Phase="Running", Reason="", readiness=true. Elapsed: 5.799446ms
    Jan 18 16:33:51.941: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-dr9p2" satisfied condition "running"
    Jan 18 16:33:51.941: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-gtl9b" in namespace "emptydir-wrapper-1446" to be "running"
    Jan 18 16:33:51.950: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-gtl9b": Phase="Running", Reason="", readiness=true. Elapsed: 8.680292ms
    Jan 18 16:33:51.950: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-gtl9b" satisfied condition "running"
    Jan 18 16:33:51.950: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-xb9xt" in namespace "emptydir-wrapper-1446" to be "running"
    Jan 18 16:33:51.955: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-xb9xt": Phase="Running", Reason="", readiness=true. Elapsed: 5.259528ms
    Jan 18 16:33:51.955: INFO: Pod "wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c-xb9xt" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c in namespace emptydir-wrapper-1446, will wait for the garbage collector to delete the pods 01/18/23 16:33:51.956
    Jan 18 16:33:52.026: INFO: Deleting ReplicationController wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c took: 14.412964ms
    Jan 18 16:33:52.130: INFO: Terminating ReplicationController wrapped-volume-race-74e0f47d-d05e-494c-9da0-568f3a83fd1c pods took: 103.761531ms
    STEP: Cleaning up the configMaps 01/18/23 16:33:55.231
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jan 18 16:33:55.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-1446" for this suite. 01/18/23 16:33:55.595
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:33:55.607
Jan 18 16:33:55.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 16:33:55.609
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:33:55.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:33:55.641
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 01/18/23 16:33:55.646
Jan 18 16:33:55.656: INFO: Waiting up to 5m0s for pod "downwardapi-volume-53f0ead0-bb04-4e44-9511-53bd395803e4" in namespace "projected-5766" to be "Succeeded or Failed"
Jan 18 16:33:55.897: INFO: Pod "downwardapi-volume-53f0ead0-bb04-4e44-9511-53bd395803e4": Phase="Pending", Reason="", readiness=false. Elapsed: 240.774972ms
Jan 18 16:33:57.907: INFO: Pod "downwardapi-volume-53f0ead0-bb04-4e44-9511-53bd395803e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.250623115s
Jan 18 16:33:59.905: INFO: Pod "downwardapi-volume-53f0ead0-bb04-4e44-9511-53bd395803e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.248731903s
STEP: Saw pod success 01/18/23 16:33:59.905
Jan 18 16:33:59.906: INFO: Pod "downwardapi-volume-53f0ead0-bb04-4e44-9511-53bd395803e4" satisfied condition "Succeeded or Failed"
Jan 18 16:33:59.911: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-53f0ead0-bb04-4e44-9511-53bd395803e4 container client-container: <nil>
STEP: delete the pod 01/18/23 16:33:59.923
Jan 18 16:33:59.939: INFO: Waiting for pod downwardapi-volume-53f0ead0-bb04-4e44-9511-53bd395803e4 to disappear
Jan 18 16:33:59.943: INFO: Pod downwardapi-volume-53f0ead0-bb04-4e44-9511-53bd395803e4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 16:33:59.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5766" for this suite. 01/18/23 16:33:59.949
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":305,"skipped":5644,"failed":0}
------------------------------
• [4.349 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:33:55.607
    Jan 18 16:33:55.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 16:33:55.609
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:33:55.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:33:55.641
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 01/18/23 16:33:55.646
    Jan 18 16:33:55.656: INFO: Waiting up to 5m0s for pod "downwardapi-volume-53f0ead0-bb04-4e44-9511-53bd395803e4" in namespace "projected-5766" to be "Succeeded or Failed"
    Jan 18 16:33:55.897: INFO: Pod "downwardapi-volume-53f0ead0-bb04-4e44-9511-53bd395803e4": Phase="Pending", Reason="", readiness=false. Elapsed: 240.774972ms
    Jan 18 16:33:57.907: INFO: Pod "downwardapi-volume-53f0ead0-bb04-4e44-9511-53bd395803e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.250623115s
    Jan 18 16:33:59.905: INFO: Pod "downwardapi-volume-53f0ead0-bb04-4e44-9511-53bd395803e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.248731903s
    STEP: Saw pod success 01/18/23 16:33:59.905
    Jan 18 16:33:59.906: INFO: Pod "downwardapi-volume-53f0ead0-bb04-4e44-9511-53bd395803e4" satisfied condition "Succeeded or Failed"
    Jan 18 16:33:59.911: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-53f0ead0-bb04-4e44-9511-53bd395803e4 container client-container: <nil>
    STEP: delete the pod 01/18/23 16:33:59.923
    Jan 18 16:33:59.939: INFO: Waiting for pod downwardapi-volume-53f0ead0-bb04-4e44-9511-53bd395803e4 to disappear
    Jan 18 16:33:59.943: INFO: Pod downwardapi-volume-53f0ead0-bb04-4e44-9511-53bd395803e4 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 16:33:59.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5766" for this suite. 01/18/23 16:33:59.949
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:33:59.957
Jan 18 16:33:59.957: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename disruption 01/18/23 16:33:59.959
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:33:59.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:33:59.998
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 01/18/23 16:34:00.027
STEP: Waiting for all pods to be running 01/18/23 16:34:02.097
Jan 18 16:34:02.125: INFO: running pods: 0 < 3
Jan 18 16:34:04.132: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 18 16:34:06.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-359" for this suite. 01/18/23 16:34:06.14
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":306,"skipped":5645,"failed":0}
------------------------------
• [SLOW TEST] [6.191 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:33:59.957
    Jan 18 16:33:59.957: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename disruption 01/18/23 16:33:59.959
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:33:59.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:33:59.998
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 01/18/23 16:34:00.027
    STEP: Waiting for all pods to be running 01/18/23 16:34:02.097
    Jan 18 16:34:02.125: INFO: running pods: 0 < 3
    Jan 18 16:34:04.132: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 18 16:34:06.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-359" for this suite. 01/18/23 16:34:06.14
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:34:06.149
Jan 18 16:34:06.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename services 01/18/23 16:34:06.151
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:34:06.168
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:34:06.182
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-6540 01/18/23 16:34:06.187
Jan 18 16:34:06.206: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-6540" to be "running and ready"
Jan 18 16:34:06.215: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 9.759058ms
Jan 18 16:34:06.217: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 18 16:34:08.228: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.022193552s
Jan 18 16:34:08.228: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 18 16:34:08.229: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Jan 18 16:34:08.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6540 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 18 16:34:08.474: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 18 16:34:08.474: INFO: stdout: "ipvs"
Jan 18 16:34:08.474: INFO: proxyMode: ipvs
Jan 18 16:34:08.492: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 18 16:34:08.496: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-6540 01/18/23 16:34:08.496
STEP: creating replication controller affinity-clusterip-timeout in namespace services-6540 01/18/23 16:34:08.52
I0118 16:34:08.533060      19 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-6540, replica count: 3
I0118 16:34:11.585107      19 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 16:34:11.596: INFO: Creating new exec pod
Jan 18 16:34:11.610: INFO: Waiting up to 5m0s for pod "execpod-affinitytrgsz" in namespace "services-6540" to be "running"
Jan 18 16:34:11.630: INFO: Pod "execpod-affinitytrgsz": Phase="Pending", Reason="", readiness=false. Elapsed: 20.057452ms
Jan 18 16:34:13.637: INFO: Pod "execpod-affinitytrgsz": Phase="Running", Reason="", readiness=true. Elapsed: 2.026366473s
Jan 18 16:34:13.637: INFO: Pod "execpod-affinitytrgsz" satisfied condition "running"
Jan 18 16:34:14.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6540 exec execpod-affinitytrgsz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jan 18 16:34:14.841: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jan 18 16:34:14.841: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 16:34:14.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6540 exec execpod-affinitytrgsz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.5.105 80'
Jan 18 16:34:15.070: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.5.105 80\nConnection to 10.233.5.105 80 port [tcp/http] succeeded!\n"
Jan 18 16:34:15.070: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 16:34:15.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6540 exec execpod-affinitytrgsz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.5.105:80/ ; done'
Jan 18 16:34:15.385: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n"
Jan 18 16:34:15.386: INFO: stdout: "\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4"
Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
Jan 18 16:34:15.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6540 exec execpod-affinitytrgsz -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.5.105:80/'
Jan 18 16:34:15.586: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n"
Jan 18 16:34:15.586: INFO: stdout: "affinity-clusterip-timeout-z55z4"
Jan 18 16:36:25.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6540 exec execpod-affinitytrgsz -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.5.105:80/'
Jan 18 16:36:25.812: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n"
Jan 18 16:36:25.812: INFO: stdout: "affinity-clusterip-timeout-s27cf"
Jan 18 16:36:25.812: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-6540, will wait for the garbage collector to delete the pods 01/18/23 16:36:25.838
Jan 18 16:36:25.918: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 11.889842ms
Jan 18 16:36:26.021: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 102.646305ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 16:36:29.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6540" for this suite. 01/18/23 16:36:29.166
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":307,"skipped":5661,"failed":0}
------------------------------
• [SLOW TEST] [143.024 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:34:06.149
    Jan 18 16:34:06.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename services 01/18/23 16:34:06.151
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:34:06.168
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:34:06.182
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-6540 01/18/23 16:34:06.187
    Jan 18 16:34:06.206: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-6540" to be "running and ready"
    Jan 18 16:34:06.215: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 9.759058ms
    Jan 18 16:34:06.217: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 16:34:08.228: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.022193552s
    Jan 18 16:34:08.228: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Jan 18 16:34:08.229: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Jan 18 16:34:08.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6540 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Jan 18 16:34:08.474: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Jan 18 16:34:08.474: INFO: stdout: "ipvs"
    Jan 18 16:34:08.474: INFO: proxyMode: ipvs
    Jan 18 16:34:08.492: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Jan 18 16:34:08.496: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-6540 01/18/23 16:34:08.496
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-6540 01/18/23 16:34:08.52
    I0118 16:34:08.533060      19 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-6540, replica count: 3
    I0118 16:34:11.585107      19 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 16:34:11.596: INFO: Creating new exec pod
    Jan 18 16:34:11.610: INFO: Waiting up to 5m0s for pod "execpod-affinitytrgsz" in namespace "services-6540" to be "running"
    Jan 18 16:34:11.630: INFO: Pod "execpod-affinitytrgsz": Phase="Pending", Reason="", readiness=false. Elapsed: 20.057452ms
    Jan 18 16:34:13.637: INFO: Pod "execpod-affinitytrgsz": Phase="Running", Reason="", readiness=true. Elapsed: 2.026366473s
    Jan 18 16:34:13.637: INFO: Pod "execpod-affinitytrgsz" satisfied condition "running"
    Jan 18 16:34:14.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6540 exec execpod-affinitytrgsz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Jan 18 16:34:14.841: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Jan 18 16:34:14.841: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 16:34:14.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6540 exec execpod-affinitytrgsz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.5.105 80'
    Jan 18 16:34:15.070: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.5.105 80\nConnection to 10.233.5.105 80 port [tcp/http] succeeded!\n"
    Jan 18 16:34:15.070: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 16:34:15.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6540 exec execpod-affinitytrgsz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.5.105:80/ ; done'
    Jan 18 16:34:15.385: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n"
    Jan 18 16:34:15.386: INFO: stdout: "\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4\naffinity-clusterip-timeout-z55z4"
    Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
    Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
    Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
    Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
    Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
    Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
    Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
    Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
    Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
    Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
    Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
    Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
    Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
    Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
    Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
    Jan 18 16:34:15.386: INFO: Received response from host: affinity-clusterip-timeout-z55z4
    Jan 18 16:34:15.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6540 exec execpod-affinitytrgsz -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.5.105:80/'
    Jan 18 16:34:15.586: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n"
    Jan 18 16:34:15.586: INFO: stdout: "affinity-clusterip-timeout-z55z4"
    Jan 18 16:36:25.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-6540 exec execpod-affinitytrgsz -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.5.105:80/'
    Jan 18 16:36:25.812: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.5.105:80/\n"
    Jan 18 16:36:25.812: INFO: stdout: "affinity-clusterip-timeout-s27cf"
    Jan 18 16:36:25.812: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-6540, will wait for the garbage collector to delete the pods 01/18/23 16:36:25.838
    Jan 18 16:36:25.918: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 11.889842ms
    Jan 18 16:36:26.021: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 102.646305ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 16:36:29.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6540" for this suite. 01/18/23 16:36:29.166
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:36:29.179
Jan 18 16:36:29.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename kubelet-test 01/18/23 16:36:29.187
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:36:29.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:36:29.225
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 01/18/23 16:36:29.245
Jan 18 16:36:29.246: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases4ea4b1f5-2b42-4cf2-a861-cd1ecc5a9ba6" in namespace "kubelet-test-5128" to be "completed"
Jan 18 16:36:29.251: INFO: Pod "agnhost-host-aliases4ea4b1f5-2b42-4cf2-a861-cd1ecc5a9ba6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.872625ms
Jan 18 16:36:31.256: INFO: Pod "agnhost-host-aliases4ea4b1f5-2b42-4cf2-a861-cd1ecc5a9ba6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009585035s
Jan 18 16:36:33.257: INFO: Pod "agnhost-host-aliases4ea4b1f5-2b42-4cf2-a861-cd1ecc5a9ba6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011129705s
Jan 18 16:36:33.258: INFO: Pod "agnhost-host-aliases4ea4b1f5-2b42-4cf2-a861-cd1ecc5a9ba6" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 18 16:36:33.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5128" for this suite. 01/18/23 16:36:33.285
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":308,"skipped":5669,"failed":0}
------------------------------
• [4.118 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:36:29.179
    Jan 18 16:36:29.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename kubelet-test 01/18/23 16:36:29.187
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:36:29.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:36:29.225
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 01/18/23 16:36:29.245
    Jan 18 16:36:29.246: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases4ea4b1f5-2b42-4cf2-a861-cd1ecc5a9ba6" in namespace "kubelet-test-5128" to be "completed"
    Jan 18 16:36:29.251: INFO: Pod "agnhost-host-aliases4ea4b1f5-2b42-4cf2-a861-cd1ecc5a9ba6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.872625ms
    Jan 18 16:36:31.256: INFO: Pod "agnhost-host-aliases4ea4b1f5-2b42-4cf2-a861-cd1ecc5a9ba6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009585035s
    Jan 18 16:36:33.257: INFO: Pod "agnhost-host-aliases4ea4b1f5-2b42-4cf2-a861-cd1ecc5a9ba6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011129705s
    Jan 18 16:36:33.258: INFO: Pod "agnhost-host-aliases4ea4b1f5-2b42-4cf2-a861-cd1ecc5a9ba6" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 18 16:36:33.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-5128" for this suite. 01/18/23 16:36:33.285
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:36:33.3
Jan 18 16:36:33.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 16:36:33.31
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:36:33.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:36:33.342
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/18/23 16:36:33.347
Jan 18 16:36:33.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 16:36:38.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 16:36:55.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2254" for this suite. 01/18/23 16:36:55.316
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":309,"skipped":5696,"failed":0}
------------------------------
• [SLOW TEST] [22.022 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:36:33.3
    Jan 18 16:36:33.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 16:36:33.31
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:36:33.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:36:33.342
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/18/23 16:36:33.347
    Jan 18 16:36:33.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 16:36:38.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 16:36:55.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2254" for this suite. 01/18/23 16:36:55.316
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:36:55.33
Jan 18 16:36:55.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 16:36:55.333
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:36:55.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:36:55.366
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 01/18/23 16:36:55.375
Jan 18 16:36:55.385: INFO: Waiting up to 5m0s for pod "annotationupdate20506477-536c-47db-b59a-c8b498e9e3b6" in namespace "projected-4350" to be "running and ready"
Jan 18 16:36:55.390: INFO: Pod "annotationupdate20506477-536c-47db-b59a-c8b498e9e3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.805265ms
Jan 18 16:36:55.390: INFO: The phase of Pod annotationupdate20506477-536c-47db-b59a-c8b498e9e3b6 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 16:36:57.404: INFO: Pod "annotationupdate20506477-536c-47db-b59a-c8b498e9e3b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.019276023s
Jan 18 16:36:57.405: INFO: The phase of Pod annotationupdate20506477-536c-47db-b59a-c8b498e9e3b6 is Running (Ready = true)
Jan 18 16:36:57.405: INFO: Pod "annotationupdate20506477-536c-47db-b59a-c8b498e9e3b6" satisfied condition "running and ready"
Jan 18 16:36:57.942: INFO: Successfully updated pod "annotationupdate20506477-536c-47db-b59a-c8b498e9e3b6"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 16:36:59.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4350" for this suite. 01/18/23 16:36:59.965
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":310,"skipped":5708,"failed":0}
------------------------------
• [4.642 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:36:55.33
    Jan 18 16:36:55.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 16:36:55.333
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:36:55.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:36:55.366
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 01/18/23 16:36:55.375
    Jan 18 16:36:55.385: INFO: Waiting up to 5m0s for pod "annotationupdate20506477-536c-47db-b59a-c8b498e9e3b6" in namespace "projected-4350" to be "running and ready"
    Jan 18 16:36:55.390: INFO: Pod "annotationupdate20506477-536c-47db-b59a-c8b498e9e3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.805265ms
    Jan 18 16:36:55.390: INFO: The phase of Pod annotationupdate20506477-536c-47db-b59a-c8b498e9e3b6 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 16:36:57.404: INFO: Pod "annotationupdate20506477-536c-47db-b59a-c8b498e9e3b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.019276023s
    Jan 18 16:36:57.405: INFO: The phase of Pod annotationupdate20506477-536c-47db-b59a-c8b498e9e3b6 is Running (Ready = true)
    Jan 18 16:36:57.405: INFO: Pod "annotationupdate20506477-536c-47db-b59a-c8b498e9e3b6" satisfied condition "running and ready"
    Jan 18 16:36:57.942: INFO: Successfully updated pod "annotationupdate20506477-536c-47db-b59a-c8b498e9e3b6"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 16:36:59.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4350" for this suite. 01/18/23 16:36:59.965
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:36:59.973
Jan 18 16:36:59.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename downward-api 01/18/23 16:36:59.975
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:36:59.995
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:37:00
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 01/18/23 16:37:00.005
Jan 18 16:37:00.021: INFO: Waiting up to 5m0s for pod "annotationupdate4928044d-c8f5-4a18-b8c2-3787a6c11718" in namespace "downward-api-2180" to be "running and ready"
Jan 18 16:37:00.027: INFO: Pod "annotationupdate4928044d-c8f5-4a18-b8c2-3787a6c11718": Phase="Pending", Reason="", readiness=false. Elapsed: 5.266354ms
Jan 18 16:37:00.027: INFO: The phase of Pod annotationupdate4928044d-c8f5-4a18-b8c2-3787a6c11718 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 16:37:02.035: INFO: Pod "annotationupdate4928044d-c8f5-4a18-b8c2-3787a6c11718": Phase="Running", Reason="", readiness=true. Elapsed: 2.013668499s
Jan 18 16:37:02.035: INFO: The phase of Pod annotationupdate4928044d-c8f5-4a18-b8c2-3787a6c11718 is Running (Ready = true)
Jan 18 16:37:02.035: INFO: Pod "annotationupdate4928044d-c8f5-4a18-b8c2-3787a6c11718" satisfied condition "running and ready"
Jan 18 16:37:02.569: INFO: Successfully updated pod "annotationupdate4928044d-c8f5-4a18-b8c2-3787a6c11718"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 16:37:06.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2180" for this suite. 01/18/23 16:37:06.609
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":311,"skipped":5709,"failed":0}
------------------------------
• [SLOW TEST] [6.644 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:36:59.973
    Jan 18 16:36:59.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename downward-api 01/18/23 16:36:59.975
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:36:59.995
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:37:00
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 01/18/23 16:37:00.005
    Jan 18 16:37:00.021: INFO: Waiting up to 5m0s for pod "annotationupdate4928044d-c8f5-4a18-b8c2-3787a6c11718" in namespace "downward-api-2180" to be "running and ready"
    Jan 18 16:37:00.027: INFO: Pod "annotationupdate4928044d-c8f5-4a18-b8c2-3787a6c11718": Phase="Pending", Reason="", readiness=false. Elapsed: 5.266354ms
    Jan 18 16:37:00.027: INFO: The phase of Pod annotationupdate4928044d-c8f5-4a18-b8c2-3787a6c11718 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 16:37:02.035: INFO: Pod "annotationupdate4928044d-c8f5-4a18-b8c2-3787a6c11718": Phase="Running", Reason="", readiness=true. Elapsed: 2.013668499s
    Jan 18 16:37:02.035: INFO: The phase of Pod annotationupdate4928044d-c8f5-4a18-b8c2-3787a6c11718 is Running (Ready = true)
    Jan 18 16:37:02.035: INFO: Pod "annotationupdate4928044d-c8f5-4a18-b8c2-3787a6c11718" satisfied condition "running and ready"
    Jan 18 16:37:02.569: INFO: Successfully updated pod "annotationupdate4928044d-c8f5-4a18-b8c2-3787a6c11718"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 16:37:06.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2180" for this suite. 01/18/23 16:37:06.609
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:37:06.618
Jan 18 16:37:06.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename webhook 01/18/23 16:37:06.619
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:37:06.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:37:06.648
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 16:37:06.676
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 16:37:07.423
STEP: Deploying the webhook pod 01/18/23 16:37:07.432
STEP: Wait for the deployment to be ready 01/18/23 16:37:07.444
Jan 18 16:37:07.464: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 16:37:09.484
STEP: Verifying the service has paired with the endpoint 01/18/23 16:37:09.494
Jan 18 16:37:10.494: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 01/18/23 16:37:10.498
STEP: create a pod 01/18/23 16:37:10.526
Jan 18 16:37:10.535: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-8378" to be "running"
Jan 18 16:37:10.542: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.913157ms
Jan 18 16:37:12.551: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01531219s
Jan 18 16:37:12.551: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 01/18/23 16:37:12.551
Jan 18 16:37:12.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=webhook-8378 attach --namespace=webhook-8378 to-be-attached-pod -i -c=container1'
Jan 18 16:37:12.683: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 16:37:12.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8378" for this suite. 01/18/23 16:37:12.694
STEP: Destroying namespace "webhook-8378-markers" for this suite. 01/18/23 16:37:12.701
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":312,"skipped":5710,"failed":0}
------------------------------
• [SLOW TEST] [6.153 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:37:06.618
    Jan 18 16:37:06.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename webhook 01/18/23 16:37:06.619
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:37:06.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:37:06.648
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 16:37:06.676
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 16:37:07.423
    STEP: Deploying the webhook pod 01/18/23 16:37:07.432
    STEP: Wait for the deployment to be ready 01/18/23 16:37:07.444
    Jan 18 16:37:07.464: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 16:37:09.484
    STEP: Verifying the service has paired with the endpoint 01/18/23 16:37:09.494
    Jan 18 16:37:10.494: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 01/18/23 16:37:10.498
    STEP: create a pod 01/18/23 16:37:10.526
    Jan 18 16:37:10.535: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-8378" to be "running"
    Jan 18 16:37:10.542: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.913157ms
    Jan 18 16:37:12.551: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01531219s
    Jan 18 16:37:12.551: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 01/18/23 16:37:12.551
    Jan 18 16:37:12.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=webhook-8378 attach --namespace=webhook-8378 to-be-attached-pod -i -c=container1'
    Jan 18 16:37:12.683: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 16:37:12.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8378" for this suite. 01/18/23 16:37:12.694
    STEP: Destroying namespace "webhook-8378-markers" for this suite. 01/18/23 16:37:12.701
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:37:12.773
Jan 18 16:37:12.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename kubectl 01/18/23 16:37:12.776
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:37:12.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:37:12.856
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Jan 18 16:37:12.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3680 create -f -'
Jan 18 16:37:14.191: INFO: stderr: ""
Jan 18 16:37:14.191: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan 18 16:37:14.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3680 create -f -'
Jan 18 16:37:14.586: INFO: stderr: ""
Jan 18 16:37:14.586: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/18/23 16:37:14.586
Jan 18 16:37:15.595: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 16:37:15.595: INFO: Found 0 / 1
Jan 18 16:37:16.596: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 16:37:16.596: INFO: Found 1 / 1
Jan 18 16:37:16.596: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 18 16:37:16.601: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 16:37:16.601: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 18 16:37:16.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3680 describe pod agnhost-primary-c2bw5'
Jan 18 16:37:16.730: INFO: stderr: ""
Jan 18 16:37:16.730: INFO: stdout: "Name:             agnhost-primary-c2bw5\nNamespace:        kubectl-3680\nPriority:         0\nService Account:  default\nNode:             v1-25-1-18760-w2/192.168.101.216\nStart Time:       Wed, 18 Jan 2023 16:37:14 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 219306290d8234d77e6ba67ffd3dfa1d5e3f2acf673d4f13857638bd9d587cf5\n                  cni.projectcalico.org/podIP: 10.233.68.231/32\n                  cni.projectcalico.org/podIPs: 10.233.68.231/32\nStatus:           Running\nIP:               10.233.68.231\nIPs:\n  IP:           10.233.68.231\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://277d1c553ea7e9d0f5cad1d07e9a51dcdf2cde5c95188447a4ae19ddca30494d\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 18 Jan 2023 16:37:15 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9hdbl (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-9hdbl:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-3680/agnhost-primary-c2bw5 to v1-25-1-18760-w2\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Jan 18 16:37:16.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3680 describe rc agnhost-primary'
Jan 18 16:37:16.864: INFO: stderr: ""
Jan 18 16:37:16.864: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3680\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-c2bw5\n"
Jan 18 16:37:16.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3680 describe service agnhost-primary'
Jan 18 16:37:16.985: INFO: stderr: ""
Jan 18 16:37:16.985: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3680\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.58.129\nIPs:               10.233.58.129\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.68.231:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 18 16:37:16.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3680 describe node v1-25-1-18760-m'
Jan 18 16:37:17.172: INFO: stderr: ""
Jan 18 16:37:17.172: INFO: stdout: "Name:               v1-25-1-18760-m\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m1.medium\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=RegionOne\n                    failure-domain.beta.kubernetes.io/zone=pod5\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=v1-25-1-18760-m\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=m1.medium\n                    taikun.cloud/org-id=1\n                    taikun.cloud/project-id=18760\n                    taikun.cloud/server-id=37767\n                    topology.cinder.csi.openstack.org/zone=pod5\n                    topology.kubernetes.io/region=RegionOne\n                    topology.kubernetes.io/zone=pod5\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 192.168.101.240\n                    csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"31b97858-80a8-441e-947f-0678d1396043\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.101.240/24\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 18 Jan 2023 11:44:54 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  v1-25-1-18760-m\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 18 Jan 2023 16:37:15 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 18 Jan 2023 11:47:05 +0000   Wed, 18 Jan 2023 11:47:05 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 18 Jan 2023 16:37:14 +0000   Wed, 18 Jan 2023 11:44:51 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 18 Jan 2023 16:37:14 +0000   Wed, 18 Jan 2023 11:44:51 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 18 Jan 2023 16:37:14 +0000   Wed, 18 Jan 2023 11:44:51 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 18 Jan 2023 16:37:14 +0000   Wed, 18 Jan 2023 11:47:04 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.101.240\n  Hostname:    v1-25-1-18760-m\nCapacity:\n  cpu:                2\n  ephemeral-storage:  30308240Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             4030132Ki\n  pods:               110\nAllocatable:\n  cpu:                1800m\n  ephemeral-storage:  27932073938\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3403444Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 bfd6d08d7bc4496a9514c7cc576c35f2\n  System UUID:                31b97858-80a8-441e-947f-0678d1396043\n  Boot ID:                    82a98be4-5790-4809-b102-a84938ce2ec6\n  Kernel Version:             5.8.0-43-generic\n  OS Image:                   Ubuntu 20.04.1 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.14\n  Kubelet Version:            v1.25.4\n  Kube-Proxy Version:         v1.25.4\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nProviderID:                   openstack:///31b97858-80a8-441e-947f-0678d1396043\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-575656fd47-bgrr8                   30m (1%)      1 (55%)     64M (1%)         256M (7%)      4h49m\n  kube-system                 calico-node-25zdl                                          150m (8%)     300m (16%)  64M (1%)         500M (14%)     4h50m\n  kube-system                 kube-apiserver-v1-25-1-18760-m                             250m (13%)    0 (0%)      0 (0%)           0 (0%)         4h52m\n  kube-system                 kube-controller-manager-v1-25-1-18760-m                    200m (11%)    0 (0%)      0 (0%)           0 (0%)         4h52m\n  kube-system                 kube-proxy-fnlsm                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h50m\n  kube-system                 kube-scheduler-v1-25-1-18760-m                             100m (5%)     0 (0%)      0 (0%)           0 (0%)         4h52m\n  kube-system                 nodelocaldns-pfhmz                                         100m (5%)     0 (0%)      70Mi (2%)        200Mi (6%)     4h49m\n  kube-system                 openstack-cinder-csi-nodeplugin-8wfrw                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         117m\n  kube-system                 openstack-cloud-controller-manager-n7b8m                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         117m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-e45d8d00b64540c2-bfjjg    0 (0%)        0 (0%)      0 (0%)           0 (0%)         91m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                830m (46%)      1300m (72%)\n  memory             201400320 (5%)  965715200 (27%)\n  ephemeral-storage  0 (0%)          0 (0%)\n  hugepages-1Gi      0 (0%)          0 (0%)\n  hugepages-2Mi      0 (0%)          0 (0%)\nEvents:              <none>\n"
Jan 18 16:37:17.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3680 describe namespace kubectl-3680'
Jan 18 16:37:17.310: INFO: stderr: ""
Jan 18 16:37:17.310: INFO: stdout: "Name:         kubectl-3680\nLabels:       e2e-framework=kubectl\n              e2e-run=2f2ef4af-b7a6-4619-9e15-db1bd869d321\n              kubernetes.io/metadata.name=kubectl-3680\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 16:37:17.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3680" for this suite. 01/18/23 16:37:17.317
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":313,"skipped":5724,"failed":0}
------------------------------
• [4.556 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:37:12.773
    Jan 18 16:37:12.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename kubectl 01/18/23 16:37:12.776
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:37:12.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:37:12.856
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Jan 18 16:37:12.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3680 create -f -'
    Jan 18 16:37:14.191: INFO: stderr: ""
    Jan 18 16:37:14.191: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jan 18 16:37:14.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3680 create -f -'
    Jan 18 16:37:14.586: INFO: stderr: ""
    Jan 18 16:37:14.586: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/18/23 16:37:14.586
    Jan 18 16:37:15.595: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 16:37:15.595: INFO: Found 0 / 1
    Jan 18 16:37:16.596: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 16:37:16.596: INFO: Found 1 / 1
    Jan 18 16:37:16.596: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 18 16:37:16.601: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 16:37:16.601: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 18 16:37:16.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3680 describe pod agnhost-primary-c2bw5'
    Jan 18 16:37:16.730: INFO: stderr: ""
    Jan 18 16:37:16.730: INFO: stdout: "Name:             agnhost-primary-c2bw5\nNamespace:        kubectl-3680\nPriority:         0\nService Account:  default\nNode:             v1-25-1-18760-w2/192.168.101.216\nStart Time:       Wed, 18 Jan 2023 16:37:14 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 219306290d8234d77e6ba67ffd3dfa1d5e3f2acf673d4f13857638bd9d587cf5\n                  cni.projectcalico.org/podIP: 10.233.68.231/32\n                  cni.projectcalico.org/podIPs: 10.233.68.231/32\nStatus:           Running\nIP:               10.233.68.231\nIPs:\n  IP:           10.233.68.231\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://277d1c553ea7e9d0f5cad1d07e9a51dcdf2cde5c95188447a4ae19ddca30494d\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 18 Jan 2023 16:37:15 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9hdbl (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-9hdbl:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-3680/agnhost-primary-c2bw5 to v1-25-1-18760-w2\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Jan 18 16:37:16.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3680 describe rc agnhost-primary'
    Jan 18 16:37:16.864: INFO: stderr: ""
    Jan 18 16:37:16.864: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3680\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-c2bw5\n"
    Jan 18 16:37:16.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3680 describe service agnhost-primary'
    Jan 18 16:37:16.985: INFO: stderr: ""
    Jan 18 16:37:16.985: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3680\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.58.129\nIPs:               10.233.58.129\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.68.231:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jan 18 16:37:16.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3680 describe node v1-25-1-18760-m'
    Jan 18 16:37:17.172: INFO: stderr: ""
    Jan 18 16:37:17.172: INFO: stdout: "Name:               v1-25-1-18760-m\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m1.medium\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=RegionOne\n                    failure-domain.beta.kubernetes.io/zone=pod5\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=v1-25-1-18760-m\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=m1.medium\n                    taikun.cloud/org-id=1\n                    taikun.cloud/project-id=18760\n                    taikun.cloud/server-id=37767\n                    topology.cinder.csi.openstack.org/zone=pod5\n                    topology.kubernetes.io/region=RegionOne\n                    topology.kubernetes.io/zone=pod5\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 192.168.101.240\n                    csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"31b97858-80a8-441e-947f-0678d1396043\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.101.240/24\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 18 Jan 2023 11:44:54 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  v1-25-1-18760-m\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 18 Jan 2023 16:37:15 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 18 Jan 2023 11:47:05 +0000   Wed, 18 Jan 2023 11:47:05 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 18 Jan 2023 16:37:14 +0000   Wed, 18 Jan 2023 11:44:51 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 18 Jan 2023 16:37:14 +0000   Wed, 18 Jan 2023 11:44:51 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 18 Jan 2023 16:37:14 +0000   Wed, 18 Jan 2023 11:44:51 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 18 Jan 2023 16:37:14 +0000   Wed, 18 Jan 2023 11:47:04 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.101.240\n  Hostname:    v1-25-1-18760-m\nCapacity:\n  cpu:                2\n  ephemeral-storage:  30308240Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             4030132Ki\n  pods:               110\nAllocatable:\n  cpu:                1800m\n  ephemeral-storage:  27932073938\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3403444Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 bfd6d08d7bc4496a9514c7cc576c35f2\n  System UUID:                31b97858-80a8-441e-947f-0678d1396043\n  Boot ID:                    82a98be4-5790-4809-b102-a84938ce2ec6\n  Kernel Version:             5.8.0-43-generic\n  OS Image:                   Ubuntu 20.04.1 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.14\n  Kubelet Version:            v1.25.4\n  Kube-Proxy Version:         v1.25.4\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nProviderID:                   openstack:///31b97858-80a8-441e-947f-0678d1396043\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-575656fd47-bgrr8                   30m (1%)      1 (55%)     64M (1%)         256M (7%)      4h49m\n  kube-system                 calico-node-25zdl                                          150m (8%)     300m (16%)  64M (1%)         500M (14%)     4h50m\n  kube-system                 kube-apiserver-v1-25-1-18760-m                             250m (13%)    0 (0%)      0 (0%)           0 (0%)         4h52m\n  kube-system                 kube-controller-manager-v1-25-1-18760-m                    200m (11%)    0 (0%)      0 (0%)           0 (0%)         4h52m\n  kube-system                 kube-proxy-fnlsm                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h50m\n  kube-system                 kube-scheduler-v1-25-1-18760-m                             100m (5%)     0 (0%)      0 (0%)           0 (0%)         4h52m\n  kube-system                 nodelocaldns-pfhmz                                         100m (5%)     0 (0%)      70Mi (2%)        200Mi (6%)     4h49m\n  kube-system                 openstack-cinder-csi-nodeplugin-8wfrw                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         117m\n  kube-system                 openstack-cloud-controller-manager-n7b8m                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         117m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-e45d8d00b64540c2-bfjjg    0 (0%)        0 (0%)      0 (0%)           0 (0%)         91m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                830m (46%)      1300m (72%)\n  memory             201400320 (5%)  965715200 (27%)\n  ephemeral-storage  0 (0%)          0 (0%)\n  hugepages-1Gi      0 (0%)          0 (0%)\n  hugepages-2Mi      0 (0%)          0 (0%)\nEvents:              <none>\n"
    Jan 18 16:37:17.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-3680 describe namespace kubectl-3680'
    Jan 18 16:37:17.310: INFO: stderr: ""
    Jan 18 16:37:17.310: INFO: stdout: "Name:         kubectl-3680\nLabels:       e2e-framework=kubectl\n              e2e-run=2f2ef4af-b7a6-4619-9e15-db1bd869d321\n              kubernetes.io/metadata.name=kubectl-3680\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 16:37:17.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3680" for this suite. 01/18/23 16:37:17.317
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:37:17.33
Jan 18 16:37:17.331: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename webhook 01/18/23 16:37:17.332
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:37:17.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:37:17.358
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 16:37:17.379
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 16:37:18.145
STEP: Deploying the webhook pod 01/18/23 16:37:18.16
STEP: Wait for the deployment to be ready 01/18/23 16:37:18.198
Jan 18 16:37:18.242: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 18 16:37:20.253: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 16, 37, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 37, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 37, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 37, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/18/23 16:37:22.261
STEP: Verifying the service has paired with the endpoint 01/18/23 16:37:22.275
Jan 18 16:37:23.276: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/18/23 16:37:23.282
STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 16:37:23.282
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/18/23 16:37:23.329
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/18/23 16:37:24.343
STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 16:37:24.344
STEP: Having no error when timeout is longer than webhook latency 01/18/23 16:37:25.379
STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 16:37:25.38
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/18/23 16:37:30.423
STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 16:37:30.423
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 16:37:35.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2603" for this suite. 01/18/23 16:37:35.479
STEP: Destroying namespace "webhook-2603-markers" for this suite. 01/18/23 16:37:35.491
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":314,"skipped":5733,"failed":0}
------------------------------
• [SLOW TEST] [18.229 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:37:17.33
    Jan 18 16:37:17.331: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename webhook 01/18/23 16:37:17.332
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:37:17.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:37:17.358
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 16:37:17.379
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 16:37:18.145
    STEP: Deploying the webhook pod 01/18/23 16:37:18.16
    STEP: Wait for the deployment to be ready 01/18/23 16:37:18.198
    Jan 18 16:37:18.242: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 18 16:37:20.253: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 16, 37, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 37, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 37, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 37, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/18/23 16:37:22.261
    STEP: Verifying the service has paired with the endpoint 01/18/23 16:37:22.275
    Jan 18 16:37:23.276: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/18/23 16:37:23.282
    STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 16:37:23.282
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/18/23 16:37:23.329
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/18/23 16:37:24.343
    STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 16:37:24.344
    STEP: Having no error when timeout is longer than webhook latency 01/18/23 16:37:25.379
    STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 16:37:25.38
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/18/23 16:37:30.423
    STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 16:37:30.423
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 16:37:35.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2603" for this suite. 01/18/23 16:37:35.479
    STEP: Destroying namespace "webhook-2603-markers" for this suite. 01/18/23 16:37:35.491
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:37:35.56
Jan 18 16:37:35.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename downward-api 01/18/23 16:37:35.564
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:37:35.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:37:35.603
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 01/18/23 16:37:35.617
Jan 18 16:37:35.649: INFO: Waiting up to 5m0s for pod "downwardapi-volume-53633c2c-b53e-4e92-8fb6-6276db93a1a3" in namespace "downward-api-5924" to be "Succeeded or Failed"
Jan 18 16:37:35.652: INFO: Pod "downwardapi-volume-53633c2c-b53e-4e92-8fb6-6276db93a1a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.97042ms
Jan 18 16:37:37.657: INFO: Pod "downwardapi-volume-53633c2c-b53e-4e92-8fb6-6276db93a1a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007769332s
Jan 18 16:37:39.658: INFO: Pod "downwardapi-volume-53633c2c-b53e-4e92-8fb6-6276db93a1a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009308805s
STEP: Saw pod success 01/18/23 16:37:39.659
Jan 18 16:37:39.659: INFO: Pod "downwardapi-volume-53633c2c-b53e-4e92-8fb6-6276db93a1a3" satisfied condition "Succeeded or Failed"
Jan 18 16:37:39.666: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-53633c2c-b53e-4e92-8fb6-6276db93a1a3 container client-container: <nil>
STEP: delete the pod 01/18/23 16:37:39.674
Jan 18 16:37:39.689: INFO: Waiting for pod downwardapi-volume-53633c2c-b53e-4e92-8fb6-6276db93a1a3 to disappear
Jan 18 16:37:39.693: INFO: Pod downwardapi-volume-53633c2c-b53e-4e92-8fb6-6276db93a1a3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 16:37:39.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5924" for this suite. 01/18/23 16:37:39.699
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":315,"skipped":5734,"failed":0}
------------------------------
• [4.147 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:37:35.56
    Jan 18 16:37:35.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename downward-api 01/18/23 16:37:35.564
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:37:35.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:37:35.603
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 01/18/23 16:37:35.617
    Jan 18 16:37:35.649: INFO: Waiting up to 5m0s for pod "downwardapi-volume-53633c2c-b53e-4e92-8fb6-6276db93a1a3" in namespace "downward-api-5924" to be "Succeeded or Failed"
    Jan 18 16:37:35.652: INFO: Pod "downwardapi-volume-53633c2c-b53e-4e92-8fb6-6276db93a1a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.97042ms
    Jan 18 16:37:37.657: INFO: Pod "downwardapi-volume-53633c2c-b53e-4e92-8fb6-6276db93a1a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007769332s
    Jan 18 16:37:39.658: INFO: Pod "downwardapi-volume-53633c2c-b53e-4e92-8fb6-6276db93a1a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009308805s
    STEP: Saw pod success 01/18/23 16:37:39.659
    Jan 18 16:37:39.659: INFO: Pod "downwardapi-volume-53633c2c-b53e-4e92-8fb6-6276db93a1a3" satisfied condition "Succeeded or Failed"
    Jan 18 16:37:39.666: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-53633c2c-b53e-4e92-8fb6-6276db93a1a3 container client-container: <nil>
    STEP: delete the pod 01/18/23 16:37:39.674
    Jan 18 16:37:39.689: INFO: Waiting for pod downwardapi-volume-53633c2c-b53e-4e92-8fb6-6276db93a1a3 to disappear
    Jan 18 16:37:39.693: INFO: Pod downwardapi-volume-53633c2c-b53e-4e92-8fb6-6276db93a1a3 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 16:37:39.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5924" for this suite. 01/18/23 16:37:39.699
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:37:39.715
Jan 18 16:37:39.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename namespaces 01/18/23 16:37:39.717
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:37:39.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:37:39.739
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 01/18/23 16:37:39.744
Jan 18 16:37:39.749: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 01/18/23 16:37:39.749
Jan 18 16:37:39.757: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 01/18/23 16:37:39.758
Jan 18 16:37:39.774: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 18 16:37:39.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5047" for this suite. 01/18/23 16:37:39.781
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":316,"skipped":5757,"failed":0}
------------------------------
• [0.075 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:37:39.715
    Jan 18 16:37:39.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename namespaces 01/18/23 16:37:39.717
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:37:39.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:37:39.739
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 01/18/23 16:37:39.744
    Jan 18 16:37:39.749: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 01/18/23 16:37:39.749
    Jan 18 16:37:39.757: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 01/18/23 16:37:39.758
    Jan 18 16:37:39.774: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 16:37:39.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-5047" for this suite. 01/18/23 16:37:39.781
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:37:39.798
Jan 18 16:37:39.798: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename resourcequota 01/18/23 16:37:39.8
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:37:39.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:37:39.829
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 01/18/23 16:37:39.833
STEP: Ensuring ResourceQuota status is calculated 01/18/23 16:37:39.837
STEP: Creating a ResourceQuota with not best effort scope 01/18/23 16:37:41.841
STEP: Ensuring ResourceQuota status is calculated 01/18/23 16:37:41.846
STEP: Creating a best-effort pod 01/18/23 16:37:43.852
STEP: Ensuring resource quota with best effort scope captures the pod usage 01/18/23 16:37:43.87
STEP: Ensuring resource quota with not best effort ignored the pod usage 01/18/23 16:37:45.875
STEP: Deleting the pod 01/18/23 16:37:47.881
STEP: Ensuring resource quota status released the pod usage 01/18/23 16:37:47.977
STEP: Creating a not best-effort pod 01/18/23 16:37:49.986
STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/18/23 16:37:50
STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/18/23 16:37:52.005
STEP: Deleting the pod 01/18/23 16:37:54.012
STEP: Ensuring resource quota status released the pod usage 01/18/23 16:37:54.03
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 16:37:56.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4340" for this suite. 01/18/23 16:37:56.04
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":317,"skipped":5781,"failed":0}
------------------------------
• [SLOW TEST] [16.249 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:37:39.798
    Jan 18 16:37:39.798: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename resourcequota 01/18/23 16:37:39.8
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:37:39.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:37:39.829
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 01/18/23 16:37:39.833
    STEP: Ensuring ResourceQuota status is calculated 01/18/23 16:37:39.837
    STEP: Creating a ResourceQuota with not best effort scope 01/18/23 16:37:41.841
    STEP: Ensuring ResourceQuota status is calculated 01/18/23 16:37:41.846
    STEP: Creating a best-effort pod 01/18/23 16:37:43.852
    STEP: Ensuring resource quota with best effort scope captures the pod usage 01/18/23 16:37:43.87
    STEP: Ensuring resource quota with not best effort ignored the pod usage 01/18/23 16:37:45.875
    STEP: Deleting the pod 01/18/23 16:37:47.881
    STEP: Ensuring resource quota status released the pod usage 01/18/23 16:37:47.977
    STEP: Creating a not best-effort pod 01/18/23 16:37:49.986
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/18/23 16:37:50
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/18/23 16:37:52.005
    STEP: Deleting the pod 01/18/23 16:37:54.012
    STEP: Ensuring resource quota status released the pod usage 01/18/23 16:37:54.03
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 16:37:56.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4340" for this suite. 01/18/23 16:37:56.04
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:37:56.057
Jan 18 16:37:56.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename endpointslicemirroring 01/18/23 16:37:56.059
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:37:56.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:37:56.093
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 01/18/23 16:37:56.121
Jan 18 16:37:56.130: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 01/18/23 16:37:58.136
Jan 18 16:37:58.147: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 01/18/23 16:38:00.153
Jan 18 16:38:00.172: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Jan 18 16:38:02.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-5060" for this suite. 01/18/23 16:38:02.186
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":318,"skipped":5799,"failed":0}
------------------------------
• [SLOW TEST] [6.142 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:37:56.057
    Jan 18 16:37:56.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename endpointslicemirroring 01/18/23 16:37:56.059
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:37:56.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:37:56.093
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 01/18/23 16:37:56.121
    Jan 18 16:37:56.130: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 01/18/23 16:37:58.136
    Jan 18 16:37:58.147: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 01/18/23 16:38:00.153
    Jan 18 16:38:00.172: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Jan 18 16:38:02.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-5060" for this suite. 01/18/23 16:38:02.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:38:02.21
Jan 18 16:38:02.211: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename configmap 01/18/23 16:38:02.213
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:38:02.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:38:02.235
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-e4666b5f-7d64-48d8-a3af-9002829dd68c 01/18/23 16:38:02.242
STEP: Creating a pod to test consume configMaps 01/18/23 16:38:02.248
Jan 18 16:38:02.263: INFO: Waiting up to 5m0s for pod "pod-configmaps-aa0e3d65-b2d2-4589-adb1-898ec20011f2" in namespace "configmap-9186" to be "Succeeded or Failed"
Jan 18 16:38:02.273: INFO: Pod "pod-configmaps-aa0e3d65-b2d2-4589-adb1-898ec20011f2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.364375ms
Jan 18 16:38:04.280: INFO: Pod "pod-configmaps-aa0e3d65-b2d2-4589-adb1-898ec20011f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016498056s
Jan 18 16:38:06.280: INFO: Pod "pod-configmaps-aa0e3d65-b2d2-4589-adb1-898ec20011f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017311837s
Jan 18 16:38:08.281: INFO: Pod "pod-configmaps-aa0e3d65-b2d2-4589-adb1-898ec20011f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017793402s
STEP: Saw pod success 01/18/23 16:38:08.281
Jan 18 16:38:08.281: INFO: Pod "pod-configmaps-aa0e3d65-b2d2-4589-adb1-898ec20011f2" satisfied condition "Succeeded or Failed"
Jan 18 16:38:08.286: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-aa0e3d65-b2d2-4589-adb1-898ec20011f2 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 16:38:08.298
Jan 18 16:38:08.331: INFO: Waiting for pod pod-configmaps-aa0e3d65-b2d2-4589-adb1-898ec20011f2 to disappear
Jan 18 16:38:08.338: INFO: Pod pod-configmaps-aa0e3d65-b2d2-4589-adb1-898ec20011f2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 16:38:08.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9186" for this suite. 01/18/23 16:38:08.357
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":319,"skipped":5820,"failed":0}
------------------------------
• [SLOW TEST] [6.164 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:38:02.21
    Jan 18 16:38:02.211: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename configmap 01/18/23 16:38:02.213
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:38:02.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:38:02.235
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-e4666b5f-7d64-48d8-a3af-9002829dd68c 01/18/23 16:38:02.242
    STEP: Creating a pod to test consume configMaps 01/18/23 16:38:02.248
    Jan 18 16:38:02.263: INFO: Waiting up to 5m0s for pod "pod-configmaps-aa0e3d65-b2d2-4589-adb1-898ec20011f2" in namespace "configmap-9186" to be "Succeeded or Failed"
    Jan 18 16:38:02.273: INFO: Pod "pod-configmaps-aa0e3d65-b2d2-4589-adb1-898ec20011f2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.364375ms
    Jan 18 16:38:04.280: INFO: Pod "pod-configmaps-aa0e3d65-b2d2-4589-adb1-898ec20011f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016498056s
    Jan 18 16:38:06.280: INFO: Pod "pod-configmaps-aa0e3d65-b2d2-4589-adb1-898ec20011f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017311837s
    Jan 18 16:38:08.281: INFO: Pod "pod-configmaps-aa0e3d65-b2d2-4589-adb1-898ec20011f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017793402s
    STEP: Saw pod success 01/18/23 16:38:08.281
    Jan 18 16:38:08.281: INFO: Pod "pod-configmaps-aa0e3d65-b2d2-4589-adb1-898ec20011f2" satisfied condition "Succeeded or Failed"
    Jan 18 16:38:08.286: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-aa0e3d65-b2d2-4589-adb1-898ec20011f2 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 16:38:08.298
    Jan 18 16:38:08.331: INFO: Waiting for pod pod-configmaps-aa0e3d65-b2d2-4589-adb1-898ec20011f2 to disappear
    Jan 18 16:38:08.338: INFO: Pod pod-configmaps-aa0e3d65-b2d2-4589-adb1-898ec20011f2 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 16:38:08.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9186" for this suite. 01/18/23 16:38:08.357
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:38:08.377
Jan 18 16:38:08.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename kubectl 01/18/23 16:38:08.38
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:38:08.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:38:08.416
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 01/18/23 16:38:08.425
Jan 18 16:38:08.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-4160 cluster-info'
Jan 18 16:38:08.571: INFO: stderr: ""
Jan 18 16:38:08.571: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 16:38:08.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4160" for this suite. 01/18/23 16:38:08.579
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":320,"skipped":5824,"failed":0}
------------------------------
• [0.210 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:38:08.377
    Jan 18 16:38:08.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename kubectl 01/18/23 16:38:08.38
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:38:08.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:38:08.416
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 01/18/23 16:38:08.425
    Jan 18 16:38:08.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-4160 cluster-info'
    Jan 18 16:38:08.571: INFO: stderr: ""
    Jan 18 16:38:08.571: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 16:38:08.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4160" for this suite. 01/18/23 16:38:08.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:38:08.59
Jan 18 16:38:08.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename disruption 01/18/23 16:38:08.591
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:38:08.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:38:08.619
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 01/18/23 16:38:08.624
STEP: Waiting for the pdb to be processed 01/18/23 16:38:08.631
STEP: updating the pdb 01/18/23 16:38:08.646
STEP: Waiting for the pdb to be processed 01/18/23 16:38:08.662
STEP: patching the pdb 01/18/23 16:38:08.682
STEP: Waiting for the pdb to be processed 01/18/23 16:38:08.695
STEP: Waiting for the pdb to be deleted 01/18/23 16:38:08.723
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 18 16:38:08.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5460" for this suite. 01/18/23 16:38:08.737
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":321,"skipped":5844,"failed":0}
------------------------------
• [0.165 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:38:08.59
    Jan 18 16:38:08.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename disruption 01/18/23 16:38:08.591
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:38:08.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:38:08.619
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 01/18/23 16:38:08.624
    STEP: Waiting for the pdb to be processed 01/18/23 16:38:08.631
    STEP: updating the pdb 01/18/23 16:38:08.646
    STEP: Waiting for the pdb to be processed 01/18/23 16:38:08.662
    STEP: patching the pdb 01/18/23 16:38:08.682
    STEP: Waiting for the pdb to be processed 01/18/23 16:38:08.695
    STEP: Waiting for the pdb to be deleted 01/18/23 16:38:08.723
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 18 16:38:08.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-5460" for this suite. 01/18/23 16:38:08.737
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:38:08.756
Jan 18 16:38:08.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename sched-pred 01/18/23 16:38:08.758
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:38:08.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:38:08.795
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 18 16:38:08.801: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 18 16:38:08.813: INFO: Waiting for terminating namespaces to be deleted...
Jan 18 16:38:08.818: INFO: 
Logging pods the apiserver thinks is on node v1-25-1-18760-w before test
Jan 18 16:38:08.834: INFO: helm-controller-578bd944df-dzr2c from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 16:38:08.834: INFO: 	Container manager ready: true, restart count 0
Jan 18 16:38:08.834: INFO: notification-controller-6459696d4f-gdmj8 from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 16:38:08.834: INFO: 	Container manager ready: true, restart count 0
Jan 18 16:38:08.834: INFO: source-controller-84f6bcbfb8-ljjjm from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 16:38:08.834: INFO: 	Container manager ready: true, restart count 0
Jan 18 16:38:08.834: INFO: calico-node-59tts from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
Jan 18 16:38:08.834: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 16:38:08.834: INFO: coredns-coredns-5f9b955d9-pjqxb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 16:38:08.834: INFO: 	Container coredns ready: true, restart count 0
Jan 18 16:38:08.834: INFO: coredns-coredns-autoscaler-7d98f7496d-7zbwb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 16:38:08.834: INFO: 	Container autoscaler ready: true, restart count 0
Jan 18 16:38:08.834: INFO: kube-proxy-rzflq from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
Jan 18 16:38:08.834: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 16:38:08.834: INFO: metrics-server-58498b9c56-q88cp from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
Jan 18 16:38:08.834: INFO: 	Container metrics-server ready: true, restart count 0
Jan 18 16:38:08.834: INFO: nginx-proxy-v1-25-1-18760-w from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
Jan 18 16:38:08.834: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 18 16:38:08.834: INFO: nodelocaldns-skdxm from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
Jan 18 16:38:08.834: INFO: 	Container node-cache ready: true, restart count 0
Jan 18 16:38:08.834: INFO: openstack-cinder-csi-controllerplugin-55d57dd799-6496v from kube-system started at 2023-01-18 14:39:48 +0000 UTC (6 container statuses recorded)
Jan 18 16:38:08.834: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 18 16:38:08.834: INFO: 	Container csi-attacher ready: true, restart count 0
Jan 18 16:38:08.834: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan 18 16:38:08.835: INFO: 	Container csi-resizer ready: true, restart count 0
Jan 18 16:38:08.835: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan 18 16:38:08.835: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 18 16:38:08.835: INFO: openstack-cinder-csi-nodeplugin-w7sxz from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
Jan 18 16:38:08.835: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 18 16:38:08.835: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 18 16:38:08.835: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 18 16:38:08.835: INFO: sonobuoy-systemd-logs-daemon-set-e45d8d00b64540c2-j5v5z from sonobuoy started at 2023-01-18 15:05:41 +0000 UTC (2 container statuses recorded)
Jan 18 16:38:08.835: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 16:38:08.835: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 16:38:08.835: INFO: 
Logging pods the apiserver thinks is on node v1-25-1-18760-w2 before test
Jan 18 16:38:08.844: INFO: calico-node-qx4f9 from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
Jan 18 16:38:08.844: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 16:38:08.844: INFO: coredns-coredns-5f9b955d9-qggng from kube-system started at 2023-01-18 14:40:34 +0000 UTC (1 container statuses recorded)
Jan 18 16:38:08.844: INFO: 	Container coredns ready: true, restart count 0
Jan 18 16:38:08.844: INFO: kube-proxy-8r2pt from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
Jan 18 16:38:08.845: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 16:38:08.845: INFO: nginx-proxy-v1-25-1-18760-w2 from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
Jan 18 16:38:08.845: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 18 16:38:08.845: INFO: nodelocaldns-dzwbj from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
Jan 18 16:38:08.845: INFO: 	Container node-cache ready: true, restart count 0
Jan 18 16:38:08.845: INFO: openstack-cinder-csi-nodeplugin-qm575 from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
Jan 18 16:38:08.845: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 18 16:38:08.845: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 18 16:38:08.845: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 18 16:38:08.845: INFO: sonobuoy from sonobuoy started at 2023-01-18 15:05:32 +0000 UTC (1 container statuses recorded)
Jan 18 16:38:08.845: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 18 16:38:08.845: INFO: sonobuoy-e2e-job-97bb2aa868f3486c from sonobuoy started at 2023-01-18 15:05:41 +0000 UTC (2 container statuses recorded)
Jan 18 16:38:08.846: INFO: 	Container e2e ready: true, restart count 0
Jan 18 16:38:08.846: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 16:38:08.846: INFO: sonobuoy-systemd-logs-daemon-set-e45d8d00b64540c2-488l9 from sonobuoy started at 2023-01-18 15:05:41 +0000 UTC (2 container statuses recorded)
Jan 18 16:38:08.846: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 16:38:08.846: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node v1-25-1-18760-w 01/18/23 16:38:08.892
STEP: verifying the node has the label node v1-25-1-18760-w2 01/18/23 16:38:08.917
Jan 18 16:38:08.955: INFO: Pod helm-controller-578bd944df-dzr2c requesting resource cpu=100m on Node v1-25-1-18760-w
Jan 18 16:38:08.955: INFO: Pod notification-controller-6459696d4f-gdmj8 requesting resource cpu=100m on Node v1-25-1-18760-w
Jan 18 16:38:08.955: INFO: Pod source-controller-84f6bcbfb8-ljjjm requesting resource cpu=50m on Node v1-25-1-18760-w
Jan 18 16:38:08.955: INFO: Pod calico-node-59tts requesting resource cpu=150m on Node v1-25-1-18760-w
Jan 18 16:38:08.956: INFO: Pod calico-node-qx4f9 requesting resource cpu=150m on Node v1-25-1-18760-w2
Jan 18 16:38:08.956: INFO: Pod coredns-coredns-5f9b955d9-pjqxb requesting resource cpu=100m on Node v1-25-1-18760-w
Jan 18 16:38:08.956: INFO: Pod coredns-coredns-5f9b955d9-qggng requesting resource cpu=100m on Node v1-25-1-18760-w2
Jan 18 16:38:08.956: INFO: Pod coredns-coredns-autoscaler-7d98f7496d-7zbwb requesting resource cpu=20m on Node v1-25-1-18760-w
Jan 18 16:38:08.956: INFO: Pod kube-proxy-8r2pt requesting resource cpu=0m on Node v1-25-1-18760-w2
Jan 18 16:38:08.956: INFO: Pod kube-proxy-rzflq requesting resource cpu=0m on Node v1-25-1-18760-w
Jan 18 16:38:08.956: INFO: Pod metrics-server-58498b9c56-q88cp requesting resource cpu=0m on Node v1-25-1-18760-w
Jan 18 16:38:08.956: INFO: Pod nginx-proxy-v1-25-1-18760-w requesting resource cpu=25m on Node v1-25-1-18760-w
Jan 18 16:38:08.956: INFO: Pod nginx-proxy-v1-25-1-18760-w2 requesting resource cpu=25m on Node v1-25-1-18760-w2
Jan 18 16:38:08.956: INFO: Pod nodelocaldns-dzwbj requesting resource cpu=100m on Node v1-25-1-18760-w2
Jan 18 16:38:08.956: INFO: Pod nodelocaldns-skdxm requesting resource cpu=100m on Node v1-25-1-18760-w
Jan 18 16:38:08.956: INFO: Pod openstack-cinder-csi-controllerplugin-55d57dd799-6496v requesting resource cpu=0m on Node v1-25-1-18760-w
Jan 18 16:38:08.956: INFO: Pod openstack-cinder-csi-nodeplugin-qm575 requesting resource cpu=0m on Node v1-25-1-18760-w2
Jan 18 16:38:08.957: INFO: Pod openstack-cinder-csi-nodeplugin-w7sxz requesting resource cpu=0m on Node v1-25-1-18760-w
Jan 18 16:38:08.957: INFO: Pod sonobuoy requesting resource cpu=0m on Node v1-25-1-18760-w2
Jan 18 16:38:08.957: INFO: Pod sonobuoy-e2e-job-97bb2aa868f3486c requesting resource cpu=0m on Node v1-25-1-18760-w2
Jan 18 16:38:08.957: INFO: Pod sonobuoy-systemd-logs-daemon-set-e45d8d00b64540c2-488l9 requesting resource cpu=0m on Node v1-25-1-18760-w2
Jan 18 16:38:08.959: INFO: Pod sonobuoy-systemd-logs-daemon-set-e45d8d00b64540c2-j5v5z requesting resource cpu=0m on Node v1-25-1-18760-w
STEP: Starting Pods to consume most of the cluster CPU. 01/18/23 16:38:08.959
Jan 18 16:38:08.960: INFO: Creating a pod which consumes cpu=878m on Node v1-25-1-18760-w
Jan 18 16:38:08.981: INFO: Creating a pod which consumes cpu=1067m on Node v1-25-1-18760-w2
Jan 18 16:38:08.995: INFO: Waiting up to 5m0s for pod "filler-pod-7a385259-4693-4e10-914f-fb33661fa865" in namespace "sched-pred-7866" to be "running"
Jan 18 16:38:09.017: INFO: Pod "filler-pod-7a385259-4693-4e10-914f-fb33661fa865": Phase="Pending", Reason="", readiness=false. Elapsed: 21.424035ms
Jan 18 16:38:11.027: INFO: Pod "filler-pod-7a385259-4693-4e10-914f-fb33661fa865": Phase="Running", Reason="", readiness=true. Elapsed: 2.031878077s
Jan 18 16:38:11.028: INFO: Pod "filler-pod-7a385259-4693-4e10-914f-fb33661fa865" satisfied condition "running"
Jan 18 16:38:11.028: INFO: Waiting up to 5m0s for pod "filler-pod-f17fb911-875d-4d20-b574-fc54c4e99d5c" in namespace "sched-pred-7866" to be "running"
Jan 18 16:38:11.032: INFO: Pod "filler-pod-f17fb911-875d-4d20-b574-fc54c4e99d5c": Phase="Running", Reason="", readiness=true. Elapsed: 4.572393ms
Jan 18 16:38:11.032: INFO: Pod "filler-pod-f17fb911-875d-4d20-b574-fc54c4e99d5c" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 01/18/23 16:38:11.032
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7a385259-4693-4e10-914f-fb33661fa865.173b7499780c7db8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7866/filler-pod-7a385259-4693-4e10-914f-fb33661fa865 to v1-25-1-18760-w] 01/18/23 16:38:11.039
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7a385259-4693-4e10-914f-fb33661fa865.173b7499ae7e8a43], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/18/23 16:38:11.039
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7a385259-4693-4e10-914f-fb33661fa865.173b7499b1dd0c28], Reason = [Created], Message = [Created container filler-pod-7a385259-4693-4e10-914f-fb33661fa865] 01/18/23 16:38:11.039
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7a385259-4693-4e10-914f-fb33661fa865.173b7499ba1c554e], Reason = [Started], Message = [Started container filler-pod-7a385259-4693-4e10-914f-fb33661fa865] 01/18/23 16:38:11.039
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f17fb911-875d-4d20-b574-fc54c4e99d5c.173b74997934412a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7866/filler-pod-f17fb911-875d-4d20-b574-fc54c4e99d5c to v1-25-1-18760-w2] 01/18/23 16:38:11.039
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f17fb911-875d-4d20-b574-fc54c4e99d5c.173b7499af68cfe2], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/18/23 16:38:11.039
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f17fb911-875d-4d20-b574-fc54c4e99d5c.173b7499b30f40d1], Reason = [Created], Message = [Created container filler-pod-f17fb911-875d-4d20-b574-fc54c4e99d5c] 01/18/23 16:38:11.039
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f17fb911-875d-4d20-b574-fc54c4e99d5c.173b7499bbe1798d], Reason = [Started], Message = [Started container filler-pod-f17fb911-875d-4d20-b574-fc54c4e99d5c] 01/18/23 16:38:11.039
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.173b7499f2d99b03], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.] 01/18/23 16:38:11.07
STEP: removing the label node off the node v1-25-1-18760-w 01/18/23 16:38:12.066
STEP: verifying the node doesn't have the label node 01/18/23 16:38:12.091
STEP: removing the label node off the node v1-25-1-18760-w2 01/18/23 16:38:12.098
STEP: verifying the node doesn't have the label node 01/18/23 16:38:12.127
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 18 16:38:12.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7866" for this suite. 01/18/23 16:38:12.149
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":322,"skipped":5847,"failed":0}
------------------------------
• [3.402 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:38:08.756
    Jan 18 16:38:08.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename sched-pred 01/18/23 16:38:08.758
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:38:08.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:38:08.795
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 18 16:38:08.801: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 18 16:38:08.813: INFO: Waiting for terminating namespaces to be deleted...
    Jan 18 16:38:08.818: INFO: 
    Logging pods the apiserver thinks is on node v1-25-1-18760-w before test
    Jan 18 16:38:08.834: INFO: helm-controller-578bd944df-dzr2c from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 16:38:08.834: INFO: 	Container manager ready: true, restart count 0
    Jan 18 16:38:08.834: INFO: notification-controller-6459696d4f-gdmj8 from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 16:38:08.834: INFO: 	Container manager ready: true, restart count 0
    Jan 18 16:38:08.834: INFO: source-controller-84f6bcbfb8-ljjjm from flux-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 16:38:08.834: INFO: 	Container manager ready: true, restart count 0
    Jan 18 16:38:08.834: INFO: calico-node-59tts from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
    Jan 18 16:38:08.834: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 16:38:08.834: INFO: coredns-coredns-5f9b955d9-pjqxb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 16:38:08.834: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 16:38:08.834: INFO: coredns-coredns-autoscaler-7d98f7496d-7zbwb from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 16:38:08.834: INFO: 	Container autoscaler ready: true, restart count 0
    Jan 18 16:38:08.834: INFO: kube-proxy-rzflq from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
    Jan 18 16:38:08.834: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 16:38:08.834: INFO: metrics-server-58498b9c56-q88cp from kube-system started at 2023-01-18 14:39:48 +0000 UTC (1 container statuses recorded)
    Jan 18 16:38:08.834: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 18 16:38:08.834: INFO: nginx-proxy-v1-25-1-18760-w from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
    Jan 18 16:38:08.834: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 18 16:38:08.834: INFO: nodelocaldns-skdxm from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
    Jan 18 16:38:08.834: INFO: 	Container node-cache ready: true, restart count 0
    Jan 18 16:38:08.834: INFO: openstack-cinder-csi-controllerplugin-55d57dd799-6496v from kube-system started at 2023-01-18 14:39:48 +0000 UTC (6 container statuses recorded)
    Jan 18 16:38:08.834: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 18 16:38:08.834: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan 18 16:38:08.834: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan 18 16:38:08.835: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan 18 16:38:08.835: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan 18 16:38:08.835: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 18 16:38:08.835: INFO: openstack-cinder-csi-nodeplugin-w7sxz from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
    Jan 18 16:38:08.835: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 18 16:38:08.835: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 18 16:38:08.835: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 18 16:38:08.835: INFO: sonobuoy-systemd-logs-daemon-set-e45d8d00b64540c2-j5v5z from sonobuoy started at 2023-01-18 15:05:41 +0000 UTC (2 container statuses recorded)
    Jan 18 16:38:08.835: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 16:38:08.835: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 18 16:38:08.835: INFO: 
    Logging pods the apiserver thinks is on node v1-25-1-18760-w2 before test
    Jan 18 16:38:08.844: INFO: calico-node-qx4f9 from kube-system started at 2023-01-18 11:46:57 +0000 UTC (1 container statuses recorded)
    Jan 18 16:38:08.844: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 16:38:08.844: INFO: coredns-coredns-5f9b955d9-qggng from kube-system started at 2023-01-18 14:40:34 +0000 UTC (1 container statuses recorded)
    Jan 18 16:38:08.844: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 16:38:08.844: INFO: kube-proxy-8r2pt from kube-system started at 2023-01-18 11:46:27 +0000 UTC (1 container statuses recorded)
    Jan 18 16:38:08.845: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 16:38:08.845: INFO: nginx-proxy-v1-25-1-18760-w2 from kube-system started at 2023-01-18 11:47:06 +0000 UTC (1 container statuses recorded)
    Jan 18 16:38:08.845: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 18 16:38:08.845: INFO: nodelocaldns-dzwbj from kube-system started at 2023-01-18 11:47:34 +0000 UTC (1 container statuses recorded)
    Jan 18 16:38:08.845: INFO: 	Container node-cache ready: true, restart count 0
    Jan 18 16:38:08.845: INFO: openstack-cinder-csi-nodeplugin-qm575 from kube-system started at 2023-01-18 14:39:36 +0000 UTC (3 container statuses recorded)
    Jan 18 16:38:08.845: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 18 16:38:08.845: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 18 16:38:08.845: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 18 16:38:08.845: INFO: sonobuoy from sonobuoy started at 2023-01-18 15:05:32 +0000 UTC (1 container statuses recorded)
    Jan 18 16:38:08.845: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 18 16:38:08.845: INFO: sonobuoy-e2e-job-97bb2aa868f3486c from sonobuoy started at 2023-01-18 15:05:41 +0000 UTC (2 container statuses recorded)
    Jan 18 16:38:08.846: INFO: 	Container e2e ready: true, restart count 0
    Jan 18 16:38:08.846: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 16:38:08.846: INFO: sonobuoy-systemd-logs-daemon-set-e45d8d00b64540c2-488l9 from sonobuoy started at 2023-01-18 15:05:41 +0000 UTC (2 container statuses recorded)
    Jan 18 16:38:08.846: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 16:38:08.846: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node v1-25-1-18760-w 01/18/23 16:38:08.892
    STEP: verifying the node has the label node v1-25-1-18760-w2 01/18/23 16:38:08.917
    Jan 18 16:38:08.955: INFO: Pod helm-controller-578bd944df-dzr2c requesting resource cpu=100m on Node v1-25-1-18760-w
    Jan 18 16:38:08.955: INFO: Pod notification-controller-6459696d4f-gdmj8 requesting resource cpu=100m on Node v1-25-1-18760-w
    Jan 18 16:38:08.955: INFO: Pod source-controller-84f6bcbfb8-ljjjm requesting resource cpu=50m on Node v1-25-1-18760-w
    Jan 18 16:38:08.955: INFO: Pod calico-node-59tts requesting resource cpu=150m on Node v1-25-1-18760-w
    Jan 18 16:38:08.956: INFO: Pod calico-node-qx4f9 requesting resource cpu=150m on Node v1-25-1-18760-w2
    Jan 18 16:38:08.956: INFO: Pod coredns-coredns-5f9b955d9-pjqxb requesting resource cpu=100m on Node v1-25-1-18760-w
    Jan 18 16:38:08.956: INFO: Pod coredns-coredns-5f9b955d9-qggng requesting resource cpu=100m on Node v1-25-1-18760-w2
    Jan 18 16:38:08.956: INFO: Pod coredns-coredns-autoscaler-7d98f7496d-7zbwb requesting resource cpu=20m on Node v1-25-1-18760-w
    Jan 18 16:38:08.956: INFO: Pod kube-proxy-8r2pt requesting resource cpu=0m on Node v1-25-1-18760-w2
    Jan 18 16:38:08.956: INFO: Pod kube-proxy-rzflq requesting resource cpu=0m on Node v1-25-1-18760-w
    Jan 18 16:38:08.956: INFO: Pod metrics-server-58498b9c56-q88cp requesting resource cpu=0m on Node v1-25-1-18760-w
    Jan 18 16:38:08.956: INFO: Pod nginx-proxy-v1-25-1-18760-w requesting resource cpu=25m on Node v1-25-1-18760-w
    Jan 18 16:38:08.956: INFO: Pod nginx-proxy-v1-25-1-18760-w2 requesting resource cpu=25m on Node v1-25-1-18760-w2
    Jan 18 16:38:08.956: INFO: Pod nodelocaldns-dzwbj requesting resource cpu=100m on Node v1-25-1-18760-w2
    Jan 18 16:38:08.956: INFO: Pod nodelocaldns-skdxm requesting resource cpu=100m on Node v1-25-1-18760-w
    Jan 18 16:38:08.956: INFO: Pod openstack-cinder-csi-controllerplugin-55d57dd799-6496v requesting resource cpu=0m on Node v1-25-1-18760-w
    Jan 18 16:38:08.956: INFO: Pod openstack-cinder-csi-nodeplugin-qm575 requesting resource cpu=0m on Node v1-25-1-18760-w2
    Jan 18 16:38:08.957: INFO: Pod openstack-cinder-csi-nodeplugin-w7sxz requesting resource cpu=0m on Node v1-25-1-18760-w
    Jan 18 16:38:08.957: INFO: Pod sonobuoy requesting resource cpu=0m on Node v1-25-1-18760-w2
    Jan 18 16:38:08.957: INFO: Pod sonobuoy-e2e-job-97bb2aa868f3486c requesting resource cpu=0m on Node v1-25-1-18760-w2
    Jan 18 16:38:08.957: INFO: Pod sonobuoy-systemd-logs-daemon-set-e45d8d00b64540c2-488l9 requesting resource cpu=0m on Node v1-25-1-18760-w2
    Jan 18 16:38:08.959: INFO: Pod sonobuoy-systemd-logs-daemon-set-e45d8d00b64540c2-j5v5z requesting resource cpu=0m on Node v1-25-1-18760-w
    STEP: Starting Pods to consume most of the cluster CPU. 01/18/23 16:38:08.959
    Jan 18 16:38:08.960: INFO: Creating a pod which consumes cpu=878m on Node v1-25-1-18760-w
    Jan 18 16:38:08.981: INFO: Creating a pod which consumes cpu=1067m on Node v1-25-1-18760-w2
    Jan 18 16:38:08.995: INFO: Waiting up to 5m0s for pod "filler-pod-7a385259-4693-4e10-914f-fb33661fa865" in namespace "sched-pred-7866" to be "running"
    Jan 18 16:38:09.017: INFO: Pod "filler-pod-7a385259-4693-4e10-914f-fb33661fa865": Phase="Pending", Reason="", readiness=false. Elapsed: 21.424035ms
    Jan 18 16:38:11.027: INFO: Pod "filler-pod-7a385259-4693-4e10-914f-fb33661fa865": Phase="Running", Reason="", readiness=true. Elapsed: 2.031878077s
    Jan 18 16:38:11.028: INFO: Pod "filler-pod-7a385259-4693-4e10-914f-fb33661fa865" satisfied condition "running"
    Jan 18 16:38:11.028: INFO: Waiting up to 5m0s for pod "filler-pod-f17fb911-875d-4d20-b574-fc54c4e99d5c" in namespace "sched-pred-7866" to be "running"
    Jan 18 16:38:11.032: INFO: Pod "filler-pod-f17fb911-875d-4d20-b574-fc54c4e99d5c": Phase="Running", Reason="", readiness=true. Elapsed: 4.572393ms
    Jan 18 16:38:11.032: INFO: Pod "filler-pod-f17fb911-875d-4d20-b574-fc54c4e99d5c" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 01/18/23 16:38:11.032
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7a385259-4693-4e10-914f-fb33661fa865.173b7499780c7db8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7866/filler-pod-7a385259-4693-4e10-914f-fb33661fa865 to v1-25-1-18760-w] 01/18/23 16:38:11.039
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7a385259-4693-4e10-914f-fb33661fa865.173b7499ae7e8a43], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/18/23 16:38:11.039
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7a385259-4693-4e10-914f-fb33661fa865.173b7499b1dd0c28], Reason = [Created], Message = [Created container filler-pod-7a385259-4693-4e10-914f-fb33661fa865] 01/18/23 16:38:11.039
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7a385259-4693-4e10-914f-fb33661fa865.173b7499ba1c554e], Reason = [Started], Message = [Started container filler-pod-7a385259-4693-4e10-914f-fb33661fa865] 01/18/23 16:38:11.039
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f17fb911-875d-4d20-b574-fc54c4e99d5c.173b74997934412a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7866/filler-pod-f17fb911-875d-4d20-b574-fc54c4e99d5c to v1-25-1-18760-w2] 01/18/23 16:38:11.039
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f17fb911-875d-4d20-b574-fc54c4e99d5c.173b7499af68cfe2], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/18/23 16:38:11.039
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f17fb911-875d-4d20-b574-fc54c4e99d5c.173b7499b30f40d1], Reason = [Created], Message = [Created container filler-pod-f17fb911-875d-4d20-b574-fc54c4e99d5c] 01/18/23 16:38:11.039
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f17fb911-875d-4d20-b574-fc54c4e99d5c.173b7499bbe1798d], Reason = [Started], Message = [Started container filler-pod-f17fb911-875d-4d20-b574-fc54c4e99d5c] 01/18/23 16:38:11.039
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.173b7499f2d99b03], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.] 01/18/23 16:38:11.07
    STEP: removing the label node off the node v1-25-1-18760-w 01/18/23 16:38:12.066
    STEP: verifying the node doesn't have the label node 01/18/23 16:38:12.091
    STEP: removing the label node off the node v1-25-1-18760-w2 01/18/23 16:38:12.098
    STEP: verifying the node doesn't have the label node 01/18/23 16:38:12.127
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 16:38:12.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-7866" for this suite. 01/18/23 16:38:12.149
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:38:12.176
Jan 18 16:38:12.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename emptydir 01/18/23 16:38:12.177
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:38:12.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:38:12.218
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 01/18/23 16:38:12.223
Jan 18 16:38:12.233: INFO: Waiting up to 5m0s for pod "pod-3ba47127-a62c-4aea-96bf-bb8bee35005d" in namespace "emptydir-6574" to be "Succeeded or Failed"
Jan 18 16:38:12.244: INFO: Pod "pod-3ba47127-a62c-4aea-96bf-bb8bee35005d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.874156ms
Jan 18 16:38:14.255: INFO: Pod "pod-3ba47127-a62c-4aea-96bf-bb8bee35005d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021165658s
Jan 18 16:38:16.251: INFO: Pod "pod-3ba47127-a62c-4aea-96bf-bb8bee35005d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01708528s
STEP: Saw pod success 01/18/23 16:38:16.251
Jan 18 16:38:16.251: INFO: Pod "pod-3ba47127-a62c-4aea-96bf-bb8bee35005d" satisfied condition "Succeeded or Failed"
Jan 18 16:38:16.255: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-3ba47127-a62c-4aea-96bf-bb8bee35005d container test-container: <nil>
STEP: delete the pod 01/18/23 16:38:16.264
Jan 18 16:38:16.501: INFO: Waiting for pod pod-3ba47127-a62c-4aea-96bf-bb8bee35005d to disappear
Jan 18 16:38:16.508: INFO: Pod pod-3ba47127-a62c-4aea-96bf-bb8bee35005d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 16:38:16.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6574" for this suite. 01/18/23 16:38:16.516
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":323,"skipped":5890,"failed":0}
------------------------------
• [4.349 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:38:12.176
    Jan 18 16:38:12.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename emptydir 01/18/23 16:38:12.177
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:38:12.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:38:12.218
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/18/23 16:38:12.223
    Jan 18 16:38:12.233: INFO: Waiting up to 5m0s for pod "pod-3ba47127-a62c-4aea-96bf-bb8bee35005d" in namespace "emptydir-6574" to be "Succeeded or Failed"
    Jan 18 16:38:12.244: INFO: Pod "pod-3ba47127-a62c-4aea-96bf-bb8bee35005d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.874156ms
    Jan 18 16:38:14.255: INFO: Pod "pod-3ba47127-a62c-4aea-96bf-bb8bee35005d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021165658s
    Jan 18 16:38:16.251: INFO: Pod "pod-3ba47127-a62c-4aea-96bf-bb8bee35005d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01708528s
    STEP: Saw pod success 01/18/23 16:38:16.251
    Jan 18 16:38:16.251: INFO: Pod "pod-3ba47127-a62c-4aea-96bf-bb8bee35005d" satisfied condition "Succeeded or Failed"
    Jan 18 16:38:16.255: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-3ba47127-a62c-4aea-96bf-bb8bee35005d container test-container: <nil>
    STEP: delete the pod 01/18/23 16:38:16.264
    Jan 18 16:38:16.501: INFO: Waiting for pod pod-3ba47127-a62c-4aea-96bf-bb8bee35005d to disappear
    Jan 18 16:38:16.508: INFO: Pod pod-3ba47127-a62c-4aea-96bf-bb8bee35005d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 16:38:16.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6574" for this suite. 01/18/23 16:38:16.516
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:38:16.529
Jan 18 16:38:16.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename hostport 01/18/23 16:38:16.531
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:38:16.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:38:16.562
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/18/23 16:38:16.579
Jan 18 16:38:16.601: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-1492" to be "running and ready"
Jan 18 16:38:16.610: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.032991ms
Jan 18 16:38:16.610: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 16:38:18.617: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016076908s
Jan 18 16:38:18.617: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 16:38:20.616: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.01590057s
Jan 18 16:38:20.617: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 18 16:38:20.617: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.101.216 on the node which pod1 resides and expect scheduled 01/18/23 16:38:20.617
Jan 18 16:38:20.625: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-1492" to be "running and ready"
Jan 18 16:38:20.635: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.745511ms
Jan 18 16:38:20.635: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 16:38:22.644: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.019172847s
Jan 18 16:38:22.644: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 18 16:38:22.644: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.101.216 but use UDP protocol on the node which pod2 resides 01/18/23 16:38:22.644
Jan 18 16:38:22.653: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-1492" to be "running and ready"
Jan 18 16:38:22.662: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.060663ms
Jan 18 16:38:22.662: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 16:38:24.666: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.013807873s
Jan 18 16:38:24.667: INFO: The phase of Pod pod3 is Running (Ready = true)
Jan 18 16:38:24.667: INFO: Pod "pod3" satisfied condition "running and ready"
Jan 18 16:38:24.679: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-1492" to be "running and ready"
Jan 18 16:38:24.689: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 9.351264ms
Jan 18 16:38:24.689: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 18 16:38:26.700: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.020778483s
Jan 18 16:38:26.700: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jan 18 16:38:26.700: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/18/23 16:38:26.704
Jan 18 16:38:26.705: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.101.216 http://127.0.0.1:54323/hostname] Namespace:hostport-1492 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 16:38:26.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 16:38:26.707: INFO: ExecWithOptions: Clientset creation
Jan 18 16:38:26.707: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-1492/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.101.216+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.101.216, port: 54323 01/18/23 16:38:26.823
Jan 18 16:38:26.823: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.101.216:54323/hostname] Namespace:hostport-1492 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 16:38:26.823: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 16:38:26.824: INFO: ExecWithOptions: Clientset creation
Jan 18 16:38:26.824: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-1492/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.101.216%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.101.216, port: 54323 UDP 01/18/23 16:38:26.928
Jan 18 16:38:26.928: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.101.216 54323] Namespace:hostport-1492 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 16:38:26.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
Jan 18 16:38:26.929: INFO: ExecWithOptions: Clientset creation
Jan 18 16:38:26.929: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-1492/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.101.216+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Jan 18 16:38:32.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-1492" for this suite. 01/18/23 16:38:32.024
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":324,"skipped":5895,"failed":0}
------------------------------
• [SLOW TEST] [15.503 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:38:16.529
    Jan 18 16:38:16.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename hostport 01/18/23 16:38:16.531
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:38:16.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:38:16.562
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/18/23 16:38:16.579
    Jan 18 16:38:16.601: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-1492" to be "running and ready"
    Jan 18 16:38:16.610: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.032991ms
    Jan 18 16:38:16.610: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 16:38:18.617: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016076908s
    Jan 18 16:38:18.617: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 16:38:20.616: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.01590057s
    Jan 18 16:38:20.617: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 18 16:38:20.617: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.101.216 on the node which pod1 resides and expect scheduled 01/18/23 16:38:20.617
    Jan 18 16:38:20.625: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-1492" to be "running and ready"
    Jan 18 16:38:20.635: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.745511ms
    Jan 18 16:38:20.635: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 16:38:22.644: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.019172847s
    Jan 18 16:38:22.644: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 18 16:38:22.644: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.101.216 but use UDP protocol on the node which pod2 resides 01/18/23 16:38:22.644
    Jan 18 16:38:22.653: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-1492" to be "running and ready"
    Jan 18 16:38:22.662: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.060663ms
    Jan 18 16:38:22.662: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 16:38:24.666: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.013807873s
    Jan 18 16:38:24.667: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jan 18 16:38:24.667: INFO: Pod "pod3" satisfied condition "running and ready"
    Jan 18 16:38:24.679: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-1492" to be "running and ready"
    Jan 18 16:38:24.689: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 9.351264ms
    Jan 18 16:38:24.689: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 16:38:26.700: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.020778483s
    Jan 18 16:38:26.700: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jan 18 16:38:26.700: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/18/23 16:38:26.704
    Jan 18 16:38:26.705: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.101.216 http://127.0.0.1:54323/hostname] Namespace:hostport-1492 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 16:38:26.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 16:38:26.707: INFO: ExecWithOptions: Clientset creation
    Jan 18 16:38:26.707: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-1492/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.101.216+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.101.216, port: 54323 01/18/23 16:38:26.823
    Jan 18 16:38:26.823: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.101.216:54323/hostname] Namespace:hostport-1492 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 16:38:26.823: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 16:38:26.824: INFO: ExecWithOptions: Clientset creation
    Jan 18 16:38:26.824: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-1492/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.101.216%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.101.216, port: 54323 UDP 01/18/23 16:38:26.928
    Jan 18 16:38:26.928: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.101.216 54323] Namespace:hostport-1492 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 16:38:26.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    Jan 18 16:38:26.929: INFO: ExecWithOptions: Clientset creation
    Jan 18 16:38:26.929: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-1492/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.101.216+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Jan 18 16:38:32.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-1492" for this suite. 01/18/23 16:38:32.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:38:32.033
Jan 18 16:38:32.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename pods 01/18/23 16:38:32.036
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:38:32.059
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:38:32.063
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Jan 18 16:38:32.069: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: creating the pod 01/18/23 16:38:32.07
STEP: submitting the pod to kubernetes 01/18/23 16:38:32.07
Jan 18 16:38:32.080: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-33a08d10-1ccf-48ad-b7fa-79c1bddee52d" in namespace "pods-386" to be "running and ready"
Jan 18 16:38:32.096: INFO: Pod "pod-exec-websocket-33a08d10-1ccf-48ad-b7fa-79c1bddee52d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.879607ms
Jan 18 16:38:32.096: INFO: The phase of Pod pod-exec-websocket-33a08d10-1ccf-48ad-b7fa-79c1bddee52d is Pending, waiting for it to be Running (with Ready = true)
Jan 18 16:38:34.102: INFO: Pod "pod-exec-websocket-33a08d10-1ccf-48ad-b7fa-79c1bddee52d": Phase="Running", Reason="", readiness=true. Elapsed: 2.021611238s
Jan 18 16:38:34.102: INFO: The phase of Pod pod-exec-websocket-33a08d10-1ccf-48ad-b7fa-79c1bddee52d is Running (Ready = true)
Jan 18 16:38:34.102: INFO: Pod "pod-exec-websocket-33a08d10-1ccf-48ad-b7fa-79c1bddee52d" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 16:38:34.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-386" for this suite. 01/18/23 16:38:34.263
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":325,"skipped":5901,"failed":0}
------------------------------
• [2.235 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:38:32.033
    Jan 18 16:38:32.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename pods 01/18/23 16:38:32.036
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:38:32.059
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:38:32.063
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Jan 18 16:38:32.069: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: creating the pod 01/18/23 16:38:32.07
    STEP: submitting the pod to kubernetes 01/18/23 16:38:32.07
    Jan 18 16:38:32.080: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-33a08d10-1ccf-48ad-b7fa-79c1bddee52d" in namespace "pods-386" to be "running and ready"
    Jan 18 16:38:32.096: INFO: Pod "pod-exec-websocket-33a08d10-1ccf-48ad-b7fa-79c1bddee52d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.879607ms
    Jan 18 16:38:32.096: INFO: The phase of Pod pod-exec-websocket-33a08d10-1ccf-48ad-b7fa-79c1bddee52d is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 16:38:34.102: INFO: Pod "pod-exec-websocket-33a08d10-1ccf-48ad-b7fa-79c1bddee52d": Phase="Running", Reason="", readiness=true. Elapsed: 2.021611238s
    Jan 18 16:38:34.102: INFO: The phase of Pod pod-exec-websocket-33a08d10-1ccf-48ad-b7fa-79c1bddee52d is Running (Ready = true)
    Jan 18 16:38:34.102: INFO: Pod "pod-exec-websocket-33a08d10-1ccf-48ad-b7fa-79c1bddee52d" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 16:38:34.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-386" for this suite. 01/18/23 16:38:34.263
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:38:34.27
Jan 18 16:38:34.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename endpointslice 01/18/23 16:38:34.273
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:38:34.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:38:34.3
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 18 16:38:36.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4918" for this suite. 01/18/23 16:38:36.451
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":326,"skipped":5904,"failed":0}
------------------------------
• [2.190 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:38:34.27
    Jan 18 16:38:34.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename endpointslice 01/18/23 16:38:34.273
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:38:34.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:38:34.3
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 18 16:38:36.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-4918" for this suite. 01/18/23 16:38:36.451
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:38:36.479
Jan 18 16:38:36.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename secrets 01/18/23 16:38:36.481
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:38:36.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:38:36.534
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-be56ad0e-bebd-4e37-ad88-e3d4eb61f7b6 01/18/23 16:38:36.541
STEP: Creating a pod to test consume secrets 01/18/23 16:38:36.548
Jan 18 16:38:36.566: INFO: Waiting up to 5m0s for pod "pod-secrets-3b35eccd-29df-474d-b722-4b4eb5e9c5e0" in namespace "secrets-3906" to be "Succeeded or Failed"
Jan 18 16:38:36.603: INFO: Pod "pod-secrets-3b35eccd-29df-474d-b722-4b4eb5e9c5e0": Phase="Pending", Reason="", readiness=false. Elapsed: 37.148287ms
Jan 18 16:38:38.620: INFO: Pod "pod-secrets-3b35eccd-29df-474d-b722-4b4eb5e9c5e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054098575s
Jan 18 16:38:40.611: INFO: Pod "pod-secrets-3b35eccd-29df-474d-b722-4b4eb5e9c5e0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045675185s
Jan 18 16:38:42.609: INFO: Pod "pod-secrets-3b35eccd-29df-474d-b722-4b4eb5e9c5e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043688703s
STEP: Saw pod success 01/18/23 16:38:42.609
Jan 18 16:38:42.610: INFO: Pod "pod-secrets-3b35eccd-29df-474d-b722-4b4eb5e9c5e0" satisfied condition "Succeeded or Failed"
Jan 18 16:38:42.614: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-3b35eccd-29df-474d-b722-4b4eb5e9c5e0 container secret-env-test: <nil>
STEP: delete the pod 01/18/23 16:38:42.624
Jan 18 16:38:42.756: INFO: Waiting for pod pod-secrets-3b35eccd-29df-474d-b722-4b4eb5e9c5e0 to disappear
Jan 18 16:38:42.764: INFO: Pod pod-secrets-3b35eccd-29df-474d-b722-4b4eb5e9c5e0 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 18 16:38:42.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3906" for this suite. 01/18/23 16:38:42.795
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":327,"skipped":5978,"failed":0}
------------------------------
• [SLOW TEST] [6.325 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:38:36.479
    Jan 18 16:38:36.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename secrets 01/18/23 16:38:36.481
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:38:36.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:38:36.534
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-be56ad0e-bebd-4e37-ad88-e3d4eb61f7b6 01/18/23 16:38:36.541
    STEP: Creating a pod to test consume secrets 01/18/23 16:38:36.548
    Jan 18 16:38:36.566: INFO: Waiting up to 5m0s for pod "pod-secrets-3b35eccd-29df-474d-b722-4b4eb5e9c5e0" in namespace "secrets-3906" to be "Succeeded or Failed"
    Jan 18 16:38:36.603: INFO: Pod "pod-secrets-3b35eccd-29df-474d-b722-4b4eb5e9c5e0": Phase="Pending", Reason="", readiness=false. Elapsed: 37.148287ms
    Jan 18 16:38:38.620: INFO: Pod "pod-secrets-3b35eccd-29df-474d-b722-4b4eb5e9c5e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054098575s
    Jan 18 16:38:40.611: INFO: Pod "pod-secrets-3b35eccd-29df-474d-b722-4b4eb5e9c5e0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045675185s
    Jan 18 16:38:42.609: INFO: Pod "pod-secrets-3b35eccd-29df-474d-b722-4b4eb5e9c5e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043688703s
    STEP: Saw pod success 01/18/23 16:38:42.609
    Jan 18 16:38:42.610: INFO: Pod "pod-secrets-3b35eccd-29df-474d-b722-4b4eb5e9c5e0" satisfied condition "Succeeded or Failed"
    Jan 18 16:38:42.614: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-secrets-3b35eccd-29df-474d-b722-4b4eb5e9c5e0 container secret-env-test: <nil>
    STEP: delete the pod 01/18/23 16:38:42.624
    Jan 18 16:38:42.756: INFO: Waiting for pod pod-secrets-3b35eccd-29df-474d-b722-4b4eb5e9c5e0 to disappear
    Jan 18 16:38:42.764: INFO: Pod pod-secrets-3b35eccd-29df-474d-b722-4b4eb5e9c5e0 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 16:38:42.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3906" for this suite. 01/18/23 16:38:42.795
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:38:42.809
Jan 18 16:38:42.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 16:38:42.811
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:38:42.831
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:38:42.842
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-d6742d7c-0bbb-4942-85ad-7833994a838f 01/18/23 16:38:42.853
STEP: Creating configMap with name cm-test-opt-upd-d9354fd9-35fd-4ef2-a00c-b4f315b6f855 01/18/23 16:38:42.864
STEP: Creating the pod 01/18/23 16:38:42.869
Jan 18 16:38:42.877: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-03818799-dcbc-4787-abea-df7cded87199" in namespace "projected-4727" to be "running and ready"
Jan 18 16:38:42.891: INFO: Pod "pod-projected-configmaps-03818799-dcbc-4787-abea-df7cded87199": Phase="Pending", Reason="", readiness=false. Elapsed: 12.805834ms
Jan 18 16:38:42.891: INFO: The phase of Pod pod-projected-configmaps-03818799-dcbc-4787-abea-df7cded87199 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 16:38:44.896: INFO: Pod "pod-projected-configmaps-03818799-dcbc-4787-abea-df7cded87199": Phase="Running", Reason="", readiness=true. Elapsed: 2.018210943s
Jan 18 16:38:44.896: INFO: The phase of Pod pod-projected-configmaps-03818799-dcbc-4787-abea-df7cded87199 is Running (Ready = true)
Jan 18 16:38:44.896: INFO: Pod "pod-projected-configmaps-03818799-dcbc-4787-abea-df7cded87199" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-d6742d7c-0bbb-4942-85ad-7833994a838f 01/18/23 16:38:44.93
STEP: Updating configmap cm-test-opt-upd-d9354fd9-35fd-4ef2-a00c-b4f315b6f855 01/18/23 16:38:44.938
STEP: Creating configMap with name cm-test-opt-create-4413501f-3d18-47e5-893e-aa555bb11cdb 01/18/23 16:38:44.945
STEP: waiting to observe update in volume 01/18/23 16:38:44.95
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 18 16:38:46.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4727" for this suite. 01/18/23 16:38:46.997
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":328,"skipped":5988,"failed":0}
------------------------------
• [4.195 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:38:42.809
    Jan 18 16:38:42.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 16:38:42.811
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:38:42.831
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:38:42.842
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-d6742d7c-0bbb-4942-85ad-7833994a838f 01/18/23 16:38:42.853
    STEP: Creating configMap with name cm-test-opt-upd-d9354fd9-35fd-4ef2-a00c-b4f315b6f855 01/18/23 16:38:42.864
    STEP: Creating the pod 01/18/23 16:38:42.869
    Jan 18 16:38:42.877: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-03818799-dcbc-4787-abea-df7cded87199" in namespace "projected-4727" to be "running and ready"
    Jan 18 16:38:42.891: INFO: Pod "pod-projected-configmaps-03818799-dcbc-4787-abea-df7cded87199": Phase="Pending", Reason="", readiness=false. Elapsed: 12.805834ms
    Jan 18 16:38:42.891: INFO: The phase of Pod pod-projected-configmaps-03818799-dcbc-4787-abea-df7cded87199 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 16:38:44.896: INFO: Pod "pod-projected-configmaps-03818799-dcbc-4787-abea-df7cded87199": Phase="Running", Reason="", readiness=true. Elapsed: 2.018210943s
    Jan 18 16:38:44.896: INFO: The phase of Pod pod-projected-configmaps-03818799-dcbc-4787-abea-df7cded87199 is Running (Ready = true)
    Jan 18 16:38:44.896: INFO: Pod "pod-projected-configmaps-03818799-dcbc-4787-abea-df7cded87199" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-d6742d7c-0bbb-4942-85ad-7833994a838f 01/18/23 16:38:44.93
    STEP: Updating configmap cm-test-opt-upd-d9354fd9-35fd-4ef2-a00c-b4f315b6f855 01/18/23 16:38:44.938
    STEP: Creating configMap with name cm-test-opt-create-4413501f-3d18-47e5-893e-aa555bb11cdb 01/18/23 16:38:44.945
    STEP: waiting to observe update in volume 01/18/23 16:38:44.95
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 18 16:38:46.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4727" for this suite. 01/18/23 16:38:46.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:38:47.009
Jan 18 16:38:47.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename webhook 01/18/23 16:38:47.011
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:38:47.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:38:47.034
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 16:38:47.049
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 16:38:47.789
STEP: Deploying the webhook pod 01/18/23 16:38:47.797
STEP: Wait for the deployment to be ready 01/18/23 16:38:47.814
Jan 18 16:38:47.850: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 16:38:49.862
STEP: Verifying the service has paired with the endpoint 01/18/23 16:38:49.881
Jan 18 16:38:50.881: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 01/18/23 16:38:51.001
STEP: Creating a configMap that should be mutated 01/18/23 16:38:51.027
STEP: Deleting the collection of validation webhooks 01/18/23 16:38:51.067
STEP: Creating a configMap that should not be mutated 01/18/23 16:38:51.131
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 16:38:51.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5016" for this suite. 01/18/23 16:38:51.154
STEP: Destroying namespace "webhook-5016-markers" for this suite. 01/18/23 16:38:51.162
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":329,"skipped":6004,"failed":0}
------------------------------
• [4.238 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:38:47.009
    Jan 18 16:38:47.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename webhook 01/18/23 16:38:47.011
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:38:47.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:38:47.034
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 16:38:47.049
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 16:38:47.789
    STEP: Deploying the webhook pod 01/18/23 16:38:47.797
    STEP: Wait for the deployment to be ready 01/18/23 16:38:47.814
    Jan 18 16:38:47.850: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 16:38:49.862
    STEP: Verifying the service has paired with the endpoint 01/18/23 16:38:49.881
    Jan 18 16:38:50.881: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 01/18/23 16:38:51.001
    STEP: Creating a configMap that should be mutated 01/18/23 16:38:51.027
    STEP: Deleting the collection of validation webhooks 01/18/23 16:38:51.067
    STEP: Creating a configMap that should not be mutated 01/18/23 16:38:51.131
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 16:38:51.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5016" for this suite. 01/18/23 16:38:51.154
    STEP: Destroying namespace "webhook-5016-markers" for this suite. 01/18/23 16:38:51.162
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:38:51.249
Jan 18 16:38:51.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename daemonsets 01/18/23 16:38:51.25
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:38:51.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:38:51.307
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 01/18/23 16:38:51.357
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 16:38:51.363
Jan 18 16:38:51.370: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 16:38:51.373: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 16:38:51.373: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 16:38:52.428: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 16:38:52.440: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 16:38:52.440: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 16:38:53.380: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 16:38:53.387: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 16:38:53.387: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
Jan 18 16:38:54.379: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 16:38:54.383: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 16:38:54.383: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 01/18/23 16:38:54.387
Jan 18 16:38:54.406: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 16:38:54.415: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 16:38:54.415: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
Jan 18 16:38:55.421: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 16:38:55.425: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 16:38:55.425: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
Jan 18 16:38:56.428: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 16:38:56.443: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 16:38:56.443: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
Jan 18 16:38:57.422: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 16:38:57.426: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 16:38:57.426: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
Jan 18 16:38:58.421: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 16:38:58.428: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 16:38:58.428: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
Jan 18 16:38:59.422: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 16:38:59.427: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 16:38:59.428: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/18/23 16:38:59.432
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9590, will wait for the garbage collector to delete the pods 01/18/23 16:38:59.432
Jan 18 16:38:59.517: INFO: Deleting DaemonSet.extensions daemon-set took: 30.296521ms
Jan 18 16:38:59.618: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.133357ms
Jan 18 16:39:01.924: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 16:39:01.924: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 16:39:01.928: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"69477"},"items":null}

Jan 18 16:39:01.932: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"69477"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 18 16:39:01.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9590" for this suite. 01/18/23 16:39:01.952
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":330,"skipped":6008,"failed":0}
------------------------------
• [SLOW TEST] [10.711 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:38:51.249
    Jan 18 16:38:51.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename daemonsets 01/18/23 16:38:51.25
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:38:51.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:38:51.307
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 01/18/23 16:38:51.357
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 16:38:51.363
    Jan 18 16:38:51.370: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 16:38:51.373: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 16:38:51.373: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 16:38:52.428: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 16:38:52.440: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 16:38:52.440: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 16:38:53.380: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 16:38:53.387: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 16:38:53.387: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
    Jan 18 16:38:54.379: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 16:38:54.383: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 16:38:54.383: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 01/18/23 16:38:54.387
    Jan 18 16:38:54.406: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 16:38:54.415: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 16:38:54.415: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
    Jan 18 16:38:55.421: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 16:38:55.425: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 16:38:55.425: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
    Jan 18 16:38:56.428: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 16:38:56.443: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 16:38:56.443: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
    Jan 18 16:38:57.422: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 16:38:57.426: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 16:38:57.426: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
    Jan 18 16:38:58.421: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 16:38:58.428: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 16:38:58.428: INFO: Node v1-25-1-18760-w2 is running 0 daemon pod, expected 1
    Jan 18 16:38:59.422: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 16:38:59.427: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 16:38:59.428: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 16:38:59.432
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9590, will wait for the garbage collector to delete the pods 01/18/23 16:38:59.432
    Jan 18 16:38:59.517: INFO: Deleting DaemonSet.extensions daemon-set took: 30.296521ms
    Jan 18 16:38:59.618: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.133357ms
    Jan 18 16:39:01.924: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 16:39:01.924: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 16:39:01.928: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"69477"},"items":null}

    Jan 18 16:39:01.932: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"69477"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 16:39:01.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-9590" for this suite. 01/18/23 16:39:01.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:39:01.962
Jan 18 16:39:01.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename dns 01/18/23 16:39:01.963
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:39:01.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:39:01.996
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 01/18/23 16:39:02.001
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6263.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6263.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6263.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6263.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6263.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6263.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6263.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6263.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6263.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 152.25.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.25.152_udp@PTR;check="$$(dig +tcp +noall +answer +search 152.25.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.25.152_tcp@PTR;sleep 1; done
 01/18/23 16:39:02.03
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6263.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6263.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6263.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6263.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6263.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6263.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6263.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6263.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6263.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6263.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 152.25.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.25.152_udp@PTR;check="$$(dig +tcp +noall +answer +search 152.25.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.25.152_tcp@PTR;sleep 1; done
 01/18/23 16:39:02.032
STEP: creating a pod to probe DNS 01/18/23 16:39:02.033
STEP: submitting the pod to kubernetes 01/18/23 16:39:02.033
Jan 18 16:39:02.053: INFO: Waiting up to 15m0s for pod "dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137" in namespace "dns-6263" to be "running"
Jan 18 16:39:02.069: INFO: Pod "dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137": Phase="Pending", Reason="", readiness=false. Elapsed: 15.500345ms
Jan 18 16:39:04.159: INFO: Pod "dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137": Phase="Running", Reason="", readiness=true. Elapsed: 2.105598732s
Jan 18 16:39:04.159: INFO: Pod "dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137" satisfied condition "running"
STEP: retrieving the pod 01/18/23 16:39:04.159
STEP: looking for the results for each expected name from probers 01/18/23 16:39:04.165
Jan 18 16:39:04.172: INFO: Unable to read wheezy_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:04.177: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:04.183: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:04.188: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:04.223: INFO: Unable to read jessie_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:04.227: INFO: Unable to read jessie_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:04.232: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:04.237: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:04.258: INFO: Lookups using dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137 failed for: [wheezy_udp@dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_udp@dns-test-service.dns-6263.svc.cluster.local jessie_tcp@dns-test-service.dns-6263.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local]

Jan 18 16:39:09.265: INFO: Unable to read wheezy_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:09.270: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:09.278: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:09.288: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:09.323: INFO: Unable to read jessie_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:09.328: INFO: Unable to read jessie_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:09.333: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:09.338: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:09.379: INFO: Lookups using dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137 failed for: [wheezy_udp@dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_udp@dns-test-service.dns-6263.svc.cluster.local jessie_tcp@dns-test-service.dns-6263.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local]

Jan 18 16:39:14.263: INFO: Unable to read wheezy_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:14.268: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:14.273: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:14.277: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:14.301: INFO: Unable to read jessie_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:14.306: INFO: Unable to read jessie_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:14.317: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:14.323: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:14.343: INFO: Lookups using dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137 failed for: [wheezy_udp@dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_udp@dns-test-service.dns-6263.svc.cluster.local jessie_tcp@dns-test-service.dns-6263.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local]

Jan 18 16:39:19.266: INFO: Unable to read wheezy_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:19.271: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:19.276: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:19.281: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:19.311: INFO: Unable to read jessie_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:19.317: INFO: Unable to read jessie_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:19.323: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:19.328: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:19.346: INFO: Lookups using dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137 failed for: [wheezy_udp@dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_udp@dns-test-service.dns-6263.svc.cluster.local jessie_tcp@dns-test-service.dns-6263.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local]

Jan 18 16:39:24.265: INFO: Unable to read wheezy_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:24.271: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:24.276: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:24.281: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:24.304: INFO: Unable to read jessie_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:24.309: INFO: Unable to read jessie_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:24.315: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:24.320: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:24.342: INFO: Lookups using dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137 failed for: [wheezy_udp@dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_udp@dns-test-service.dns-6263.svc.cluster.local jessie_tcp@dns-test-service.dns-6263.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local]

Jan 18 16:39:29.266: INFO: Unable to read wheezy_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:29.273: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:29.277: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:29.281: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:29.324: INFO: Unable to read jessie_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:29.329: INFO: Unable to read jessie_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:29.338: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:29.344: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:29.372: INFO: Lookups using dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137 failed for: [wheezy_udp@dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_udp@dns-test-service.dns-6263.svc.cluster.local jessie_tcp@dns-test-service.dns-6263.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local]

Jan 18 16:39:34.264: INFO: Unable to read wheezy_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:34.270: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:34.275: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:34.279: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:34.309: INFO: Unable to read jessie_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:34.315: INFO: Unable to read jessie_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:34.319: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:34.325: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
Jan 18 16:39:34.345: INFO: Lookups using dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137 failed for: [wheezy_udp@dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_udp@dns-test-service.dns-6263.svc.cluster.local jessie_tcp@dns-test-service.dns-6263.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local]

Jan 18 16:39:39.366: INFO: DNS probes using dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137 succeeded

STEP: deleting the pod 01/18/23 16:39:39.366
STEP: deleting the test service 01/18/23 16:39:39.391
STEP: deleting the test headless service 01/18/23 16:39:39.434
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 18 16:39:39.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6263" for this suite. 01/18/23 16:39:39.526
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":331,"skipped":6019,"failed":0}
------------------------------
• [SLOW TEST] [37.575 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:39:01.962
    Jan 18 16:39:01.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename dns 01/18/23 16:39:01.963
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:39:01.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:39:01.996
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 01/18/23 16:39:02.001
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6263.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6263.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6263.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6263.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6263.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6263.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6263.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6263.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6263.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 152.25.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.25.152_udp@PTR;check="$$(dig +tcp +noall +answer +search 152.25.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.25.152_tcp@PTR;sleep 1; done
     01/18/23 16:39:02.03
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6263.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6263.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6263.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6263.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6263.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6263.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6263.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6263.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6263.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6263.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 152.25.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.25.152_udp@PTR;check="$$(dig +tcp +noall +answer +search 152.25.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.25.152_tcp@PTR;sleep 1; done
     01/18/23 16:39:02.032
    STEP: creating a pod to probe DNS 01/18/23 16:39:02.033
    STEP: submitting the pod to kubernetes 01/18/23 16:39:02.033
    Jan 18 16:39:02.053: INFO: Waiting up to 15m0s for pod "dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137" in namespace "dns-6263" to be "running"
    Jan 18 16:39:02.069: INFO: Pod "dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137": Phase="Pending", Reason="", readiness=false. Elapsed: 15.500345ms
    Jan 18 16:39:04.159: INFO: Pod "dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137": Phase="Running", Reason="", readiness=true. Elapsed: 2.105598732s
    Jan 18 16:39:04.159: INFO: Pod "dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 16:39:04.159
    STEP: looking for the results for each expected name from probers 01/18/23 16:39:04.165
    Jan 18 16:39:04.172: INFO: Unable to read wheezy_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:04.177: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:04.183: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:04.188: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:04.223: INFO: Unable to read jessie_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:04.227: INFO: Unable to read jessie_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:04.232: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:04.237: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:04.258: INFO: Lookups using dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137 failed for: [wheezy_udp@dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_udp@dns-test-service.dns-6263.svc.cluster.local jessie_tcp@dns-test-service.dns-6263.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local]

    Jan 18 16:39:09.265: INFO: Unable to read wheezy_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:09.270: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:09.278: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:09.288: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:09.323: INFO: Unable to read jessie_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:09.328: INFO: Unable to read jessie_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:09.333: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:09.338: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:09.379: INFO: Lookups using dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137 failed for: [wheezy_udp@dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_udp@dns-test-service.dns-6263.svc.cluster.local jessie_tcp@dns-test-service.dns-6263.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local]

    Jan 18 16:39:14.263: INFO: Unable to read wheezy_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:14.268: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:14.273: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:14.277: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:14.301: INFO: Unable to read jessie_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:14.306: INFO: Unable to read jessie_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:14.317: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:14.323: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:14.343: INFO: Lookups using dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137 failed for: [wheezy_udp@dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_udp@dns-test-service.dns-6263.svc.cluster.local jessie_tcp@dns-test-service.dns-6263.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local]

    Jan 18 16:39:19.266: INFO: Unable to read wheezy_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:19.271: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:19.276: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:19.281: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:19.311: INFO: Unable to read jessie_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:19.317: INFO: Unable to read jessie_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:19.323: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:19.328: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:19.346: INFO: Lookups using dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137 failed for: [wheezy_udp@dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_udp@dns-test-service.dns-6263.svc.cluster.local jessie_tcp@dns-test-service.dns-6263.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local]

    Jan 18 16:39:24.265: INFO: Unable to read wheezy_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:24.271: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:24.276: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:24.281: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:24.304: INFO: Unable to read jessie_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:24.309: INFO: Unable to read jessie_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:24.315: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:24.320: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:24.342: INFO: Lookups using dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137 failed for: [wheezy_udp@dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_udp@dns-test-service.dns-6263.svc.cluster.local jessie_tcp@dns-test-service.dns-6263.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local]

    Jan 18 16:39:29.266: INFO: Unable to read wheezy_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:29.273: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:29.277: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:29.281: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:29.324: INFO: Unable to read jessie_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:29.329: INFO: Unable to read jessie_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:29.338: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:29.344: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:29.372: INFO: Lookups using dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137 failed for: [wheezy_udp@dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_udp@dns-test-service.dns-6263.svc.cluster.local jessie_tcp@dns-test-service.dns-6263.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local]

    Jan 18 16:39:34.264: INFO: Unable to read wheezy_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:34.270: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:34.275: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:34.279: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:34.309: INFO: Unable to read jessie_udp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:34.315: INFO: Unable to read jessie_tcp@dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:34.319: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:34.325: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local from pod dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137: the server could not find the requested resource (get pods dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137)
    Jan 18 16:39:34.345: INFO: Lookups using dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137 failed for: [wheezy_udp@dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@dns-test-service.dns-6263.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_udp@dns-test-service.dns-6263.svc.cluster.local jessie_tcp@dns-test-service.dns-6263.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6263.svc.cluster.local]

    Jan 18 16:39:39.366: INFO: DNS probes using dns-6263/dns-test-496cf8ff-0dbe-43c3-b78e-add0d55c4137 succeeded

    STEP: deleting the pod 01/18/23 16:39:39.366
    STEP: deleting the test service 01/18/23 16:39:39.391
    STEP: deleting the test headless service 01/18/23 16:39:39.434
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 18 16:39:39.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6263" for this suite. 01/18/23 16:39:39.526
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:39:39.549
Jan 18 16:39:39.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 16:39:39.551
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:39:39.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:39:39.586
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 01/18/23 16:39:39.594
Jan 18 16:39:39.612: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fdf2f230-db44-4c5e-a556-7120b78b0692" in namespace "projected-3679" to be "Succeeded or Failed"
Jan 18 16:39:39.616: INFO: Pod "downwardapi-volume-fdf2f230-db44-4c5e-a556-7120b78b0692": Phase="Pending", Reason="", readiness=false. Elapsed: 4.245238ms
Jan 18 16:39:41.623: INFO: Pod "downwardapi-volume-fdf2f230-db44-4c5e-a556-7120b78b0692": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010714481s
Jan 18 16:39:43.622: INFO: Pod "downwardapi-volume-fdf2f230-db44-4c5e-a556-7120b78b0692": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010084175s
Jan 18 16:39:45.621: INFO: Pod "downwardapi-volume-fdf2f230-db44-4c5e-a556-7120b78b0692": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008773115s
STEP: Saw pod success 01/18/23 16:39:45.622
Jan 18 16:39:45.622: INFO: Pod "downwardapi-volume-fdf2f230-db44-4c5e-a556-7120b78b0692" satisfied condition "Succeeded or Failed"
Jan 18 16:39:45.627: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-fdf2f230-db44-4c5e-a556-7120b78b0692 container client-container: <nil>
STEP: delete the pod 01/18/23 16:39:45.635
Jan 18 16:39:45.652: INFO: Waiting for pod downwardapi-volume-fdf2f230-db44-4c5e-a556-7120b78b0692 to disappear
Jan 18 16:39:45.658: INFO: Pod downwardapi-volume-fdf2f230-db44-4c5e-a556-7120b78b0692 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 16:39:45.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3679" for this suite. 01/18/23 16:39:45.664
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":332,"skipped":6042,"failed":0}
------------------------------
• [SLOW TEST] [6.123 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:39:39.549
    Jan 18 16:39:39.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 16:39:39.551
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:39:39.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:39:39.586
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 01/18/23 16:39:39.594
    Jan 18 16:39:39.612: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fdf2f230-db44-4c5e-a556-7120b78b0692" in namespace "projected-3679" to be "Succeeded or Failed"
    Jan 18 16:39:39.616: INFO: Pod "downwardapi-volume-fdf2f230-db44-4c5e-a556-7120b78b0692": Phase="Pending", Reason="", readiness=false. Elapsed: 4.245238ms
    Jan 18 16:39:41.623: INFO: Pod "downwardapi-volume-fdf2f230-db44-4c5e-a556-7120b78b0692": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010714481s
    Jan 18 16:39:43.622: INFO: Pod "downwardapi-volume-fdf2f230-db44-4c5e-a556-7120b78b0692": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010084175s
    Jan 18 16:39:45.621: INFO: Pod "downwardapi-volume-fdf2f230-db44-4c5e-a556-7120b78b0692": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008773115s
    STEP: Saw pod success 01/18/23 16:39:45.622
    Jan 18 16:39:45.622: INFO: Pod "downwardapi-volume-fdf2f230-db44-4c5e-a556-7120b78b0692" satisfied condition "Succeeded or Failed"
    Jan 18 16:39:45.627: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-fdf2f230-db44-4c5e-a556-7120b78b0692 container client-container: <nil>
    STEP: delete the pod 01/18/23 16:39:45.635
    Jan 18 16:39:45.652: INFO: Waiting for pod downwardapi-volume-fdf2f230-db44-4c5e-a556-7120b78b0692 to disappear
    Jan 18 16:39:45.658: INFO: Pod downwardapi-volume-fdf2f230-db44-4c5e-a556-7120b78b0692 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 16:39:45.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3679" for this suite. 01/18/23 16:39:45.664
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:39:45.675
Jan 18 16:39:45.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename services 01/18/23 16:39:45.677
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:39:45.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:39:45.711
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1130 01/18/23 16:39:45.719
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/18/23 16:39:45.744
STEP: creating service externalsvc in namespace services-1130 01/18/23 16:39:45.745
STEP: creating replication controller externalsvc in namespace services-1130 01/18/23 16:39:45.766
I0118 16:39:45.797697      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1130, replica count: 2
I0118 16:39:48.855192      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 01/18/23 16:39:48.859
Jan 18 16:39:48.883: INFO: Creating new exec pod
Jan 18 16:39:48.898: INFO: Waiting up to 5m0s for pod "execpodj7p6w" in namespace "services-1130" to be "running"
Jan 18 16:39:48.905: INFO: Pod "execpodj7p6w": Phase="Pending", Reason="", readiness=false. Elapsed: 5.675177ms
Jan 18 16:39:50.911: INFO: Pod "execpodj7p6w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012041589s
Jan 18 16:39:52.912: INFO: Pod "execpodj7p6w": Phase="Running", Reason="", readiness=true. Elapsed: 4.013560844s
Jan 18 16:39:52.913: INFO: Pod "execpodj7p6w" satisfied condition "running"
Jan 18 16:39:52.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-1130 exec execpodj7p6w -- /bin/sh -x -c nslookup clusterip-service.services-1130.svc.cluster.local'
Jan 18 16:39:53.222: INFO: stderr: "+ nslookup clusterip-service.services-1130.svc.cluster.local\n"
Jan 18 16:39:53.222: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nclusterip-service.services-1130.svc.cluster.local\tcanonical name = externalsvc.services-1130.svc.cluster.local.\nName:\texternalsvc.services-1130.svc.cluster.local\nAddress: 10.233.4.208\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1130, will wait for the garbage collector to delete the pods 01/18/23 16:39:53.222
Jan 18 16:39:53.626: INFO: Deleting ReplicationController externalsvc took: 8.462564ms
Jan 18 16:39:53.726: INFO: Terminating ReplicationController externalsvc pods took: 100.328882ms
Jan 18 16:39:56.357: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 16:39:56.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1130" for this suite. 01/18/23 16:39:56.414
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":333,"skipped":6060,"failed":0}
------------------------------
• [SLOW TEST] [10.749 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:39:45.675
    Jan 18 16:39:45.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename services 01/18/23 16:39:45.677
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:39:45.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:39:45.711
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1130 01/18/23 16:39:45.719
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/18/23 16:39:45.744
    STEP: creating service externalsvc in namespace services-1130 01/18/23 16:39:45.745
    STEP: creating replication controller externalsvc in namespace services-1130 01/18/23 16:39:45.766
    I0118 16:39:45.797697      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1130, replica count: 2
    I0118 16:39:48.855192      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 01/18/23 16:39:48.859
    Jan 18 16:39:48.883: INFO: Creating new exec pod
    Jan 18 16:39:48.898: INFO: Waiting up to 5m0s for pod "execpodj7p6w" in namespace "services-1130" to be "running"
    Jan 18 16:39:48.905: INFO: Pod "execpodj7p6w": Phase="Pending", Reason="", readiness=false. Elapsed: 5.675177ms
    Jan 18 16:39:50.911: INFO: Pod "execpodj7p6w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012041589s
    Jan 18 16:39:52.912: INFO: Pod "execpodj7p6w": Phase="Running", Reason="", readiness=true. Elapsed: 4.013560844s
    Jan 18 16:39:52.913: INFO: Pod "execpodj7p6w" satisfied condition "running"
    Jan 18 16:39:52.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=services-1130 exec execpodj7p6w -- /bin/sh -x -c nslookup clusterip-service.services-1130.svc.cluster.local'
    Jan 18 16:39:53.222: INFO: stderr: "+ nslookup clusterip-service.services-1130.svc.cluster.local\n"
    Jan 18 16:39:53.222: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nclusterip-service.services-1130.svc.cluster.local\tcanonical name = externalsvc.services-1130.svc.cluster.local.\nName:\texternalsvc.services-1130.svc.cluster.local\nAddress: 10.233.4.208\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-1130, will wait for the garbage collector to delete the pods 01/18/23 16:39:53.222
    Jan 18 16:39:53.626: INFO: Deleting ReplicationController externalsvc took: 8.462564ms
    Jan 18 16:39:53.726: INFO: Terminating ReplicationController externalsvc pods took: 100.328882ms
    Jan 18 16:39:56.357: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 16:39:56.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1130" for this suite. 01/18/23 16:39:56.414
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:39:56.43
Jan 18 16:39:56.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename emptydir 01/18/23 16:39:56.434
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:39:56.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:39:56.47
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/18/23 16:39:56.477
Jan 18 16:39:56.486: INFO: Waiting up to 5m0s for pod "pod-cf4226c7-041c-45f7-a203-c5ae70ddedff" in namespace "emptydir-6654" to be "Succeeded or Failed"
Jan 18 16:39:56.504: INFO: Pod "pod-cf4226c7-041c-45f7-a203-c5ae70ddedff": Phase="Pending", Reason="", readiness=false. Elapsed: 17.38118ms
Jan 18 16:39:58.509: INFO: Pod "pod-cf4226c7-041c-45f7-a203-c5ae70ddedff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022783928s
Jan 18 16:40:00.511: INFO: Pod "pod-cf4226c7-041c-45f7-a203-c5ae70ddedff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024077702s
STEP: Saw pod success 01/18/23 16:40:00.511
Jan 18 16:40:00.511: INFO: Pod "pod-cf4226c7-041c-45f7-a203-c5ae70ddedff" satisfied condition "Succeeded or Failed"
Jan 18 16:40:00.517: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-cf4226c7-041c-45f7-a203-c5ae70ddedff container test-container: <nil>
STEP: delete the pod 01/18/23 16:40:00.525
Jan 18 16:40:00.543: INFO: Waiting for pod pod-cf4226c7-041c-45f7-a203-c5ae70ddedff to disappear
Jan 18 16:40:00.546: INFO: Pod pod-cf4226c7-041c-45f7-a203-c5ae70ddedff no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 16:40:00.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6654" for this suite. 01/18/23 16:40:00.553
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":334,"skipped":6081,"failed":0}
------------------------------
• [4.133 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:39:56.43
    Jan 18 16:39:56.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename emptydir 01/18/23 16:39:56.434
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:39:56.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:39:56.47
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/18/23 16:39:56.477
    Jan 18 16:39:56.486: INFO: Waiting up to 5m0s for pod "pod-cf4226c7-041c-45f7-a203-c5ae70ddedff" in namespace "emptydir-6654" to be "Succeeded or Failed"
    Jan 18 16:39:56.504: INFO: Pod "pod-cf4226c7-041c-45f7-a203-c5ae70ddedff": Phase="Pending", Reason="", readiness=false. Elapsed: 17.38118ms
    Jan 18 16:39:58.509: INFO: Pod "pod-cf4226c7-041c-45f7-a203-c5ae70ddedff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022783928s
    Jan 18 16:40:00.511: INFO: Pod "pod-cf4226c7-041c-45f7-a203-c5ae70ddedff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024077702s
    STEP: Saw pod success 01/18/23 16:40:00.511
    Jan 18 16:40:00.511: INFO: Pod "pod-cf4226c7-041c-45f7-a203-c5ae70ddedff" satisfied condition "Succeeded or Failed"
    Jan 18 16:40:00.517: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-cf4226c7-041c-45f7-a203-c5ae70ddedff container test-container: <nil>
    STEP: delete the pod 01/18/23 16:40:00.525
    Jan 18 16:40:00.543: INFO: Waiting for pod pod-cf4226c7-041c-45f7-a203-c5ae70ddedff to disappear
    Jan 18 16:40:00.546: INFO: Pod pod-cf4226c7-041c-45f7-a203-c5ae70ddedff no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 16:40:00.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6654" for this suite. 01/18/23 16:40:00.553
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:40:00.572
Jan 18 16:40:00.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename configmap 01/18/23 16:40:00.574
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:40:00.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:40:00.594
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 16:40:00.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1340" for this suite. 01/18/23 16:40:00.662
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":335,"skipped":6149,"failed":0}
------------------------------
• [0.096 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:40:00.572
    Jan 18 16:40:00.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename configmap 01/18/23 16:40:00.574
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:40:00.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:40:00.594
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 16:40:00.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1340" for this suite. 01/18/23 16:40:00.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:40:00.672
Jan 18 16:40:00.672: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename daemonsets 01/18/23 16:40:00.674
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:40:00.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:40:00.729
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 01/18/23 16:40:00.786
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 16:40:00.793
Jan 18 16:40:00.798: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 16:40:00.806: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 16:40:00.806: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 16:40:01.825: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 16:40:01.843: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 16:40:01.843: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 16:40:02.814: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 16:40:02.818: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 16:40:02.818: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/18/23 16:40:02.822
Jan 18 16:40:02.848: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 16:40:02.858: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 16:40:02.858: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 16:40:03.866: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 16:40:03.871: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 16:40:03.872: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
Jan 18 16:40:04.870: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 18 16:40:04.876: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 16:40:04.877: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 01/18/23 16:40:04.877
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/18/23 16:40:04.887
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-262, will wait for the garbage collector to delete the pods 01/18/23 16:40:04.888
Jan 18 16:40:04.951: INFO: Deleting DaemonSet.extensions daemon-set took: 6.617727ms
Jan 18 16:40:05.052: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.236933ms
Jan 18 16:40:07.258: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 16:40:07.258: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 16:40:07.263: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"70092"},"items":null}

Jan 18 16:40:07.267: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"70092"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 18 16:40:07.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-262" for this suite. 01/18/23 16:40:07.287
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":336,"skipped":6177,"failed":0}
------------------------------
• [SLOW TEST] [6.747 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:40:00.672
    Jan 18 16:40:00.672: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename daemonsets 01/18/23 16:40:00.674
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:40:00.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:40:00.729
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 01/18/23 16:40:00.786
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 16:40:00.793
    Jan 18 16:40:00.798: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 16:40:00.806: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 16:40:00.806: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 16:40:01.825: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 16:40:01.843: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 16:40:01.843: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 16:40:02.814: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 16:40:02.818: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 16:40:02.818: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/18/23 16:40:02.822
    Jan 18 16:40:02.848: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 16:40:02.858: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 16:40:02.858: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 16:40:03.866: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 16:40:03.871: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 16:40:03.872: INFO: Node v1-25-1-18760-w is running 0 daemon pod, expected 1
    Jan 18 16:40:04.870: INFO: DaemonSet pods can't tolerate node v1-25-1-18760-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 18 16:40:04.876: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 16:40:04.877: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 01/18/23 16:40:04.877
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 16:40:04.887
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-262, will wait for the garbage collector to delete the pods 01/18/23 16:40:04.888
    Jan 18 16:40:04.951: INFO: Deleting DaemonSet.extensions daemon-set took: 6.617727ms
    Jan 18 16:40:05.052: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.236933ms
    Jan 18 16:40:07.258: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 16:40:07.258: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 16:40:07.263: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"70092"},"items":null}

    Jan 18 16:40:07.267: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"70092"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 16:40:07.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-262" for this suite. 01/18/23 16:40:07.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:40:07.436
Jan 18 16:40:07.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename container-runtime 01/18/23 16:40:07.438
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:40:07.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:40:07.466
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 01/18/23 16:40:07.471
STEP: wait for the container to reach Succeeded 01/18/23 16:40:07.485
STEP: get the container status 01/18/23 16:40:11.528
STEP: the container should be terminated 01/18/23 16:40:11.532
STEP: the termination message should be set 01/18/23 16:40:11.532
Jan 18 16:40:11.533: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/18/23 16:40:11.533
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 18 16:40:11.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9546" for this suite. 01/18/23 16:40:11.586
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":337,"skipped":6228,"failed":0}
------------------------------
• [4.160 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:40:07.436
    Jan 18 16:40:07.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename container-runtime 01/18/23 16:40:07.438
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:40:07.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:40:07.466
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 01/18/23 16:40:07.471
    STEP: wait for the container to reach Succeeded 01/18/23 16:40:07.485
    STEP: get the container status 01/18/23 16:40:11.528
    STEP: the container should be terminated 01/18/23 16:40:11.532
    STEP: the termination message should be set 01/18/23 16:40:11.532
    Jan 18 16:40:11.533: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/18/23 16:40:11.533
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 18 16:40:11.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-9546" for this suite. 01/18/23 16:40:11.586
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:40:11.599
Jan 18 16:40:11.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename kubectl 01/18/23 16:40:11.601
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:40:11.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:40:11.63
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/18/23 16:40:11.637
Jan 18 16:40:11.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-8492 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 18 16:40:11.777: INFO: stderr: ""
Jan 18 16:40:11.777: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 01/18/23 16:40:11.777
Jan 18 16:40:11.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-8492 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Jan 18 16:40:12.311: INFO: stderr: ""
Jan 18 16:40:12.311: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/18/23 16:40:12.311
Jan 18 16:40:12.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-8492 delete pods e2e-test-httpd-pod'
Jan 18 16:40:14.311: INFO: stderr: ""
Jan 18 16:40:14.311: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 16:40:14.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8492" for this suite. 01/18/23 16:40:14.322
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":338,"skipped":6276,"failed":0}
------------------------------
• [2.733 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:40:11.599
    Jan 18 16:40:11.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename kubectl 01/18/23 16:40:11.601
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:40:11.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:40:11.63
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/18/23 16:40:11.637
    Jan 18 16:40:11.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-8492 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 18 16:40:11.777: INFO: stderr: ""
    Jan 18 16:40:11.777: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 01/18/23 16:40:11.777
    Jan 18 16:40:11.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-8492 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Jan 18 16:40:12.311: INFO: stderr: ""
    Jan 18 16:40:12.311: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/18/23 16:40:12.311
    Jan 18 16:40:12.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3957318277 --namespace=kubectl-8492 delete pods e2e-test-httpd-pod'
    Jan 18 16:40:14.311: INFO: stderr: ""
    Jan 18 16:40:14.311: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 16:40:14.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8492" for this suite. 01/18/23 16:40:14.322
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:40:14.334
Jan 18 16:40:14.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename webhook 01/18/23 16:40:14.335
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:40:14.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:40:14.359
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 16:40:14.381
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 16:40:15.616
STEP: Deploying the webhook pod 01/18/23 16:40:15.625
STEP: Wait for the deployment to be ready 01/18/23 16:40:15.639
Jan 18 16:40:15.659: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 16:40:17.678
STEP: Verifying the service has paired with the endpoint 01/18/23 16:40:17.693
Jan 18 16:40:18.695: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 01/18/23 16:40:19
STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 16:40:19.054
STEP: Deleting the collection of validation webhooks 01/18/23 16:40:19.103
STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 16:40:19.176
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 16:40:19.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2757" for this suite. 01/18/23 16:40:19.203
STEP: Destroying namespace "webhook-2757-markers" for this suite. 01/18/23 16:40:19.224
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":339,"skipped":6297,"failed":0}
------------------------------
• [4.991 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:40:14.334
    Jan 18 16:40:14.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename webhook 01/18/23 16:40:14.335
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:40:14.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:40:14.359
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 16:40:14.381
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 16:40:15.616
    STEP: Deploying the webhook pod 01/18/23 16:40:15.625
    STEP: Wait for the deployment to be ready 01/18/23 16:40:15.639
    Jan 18 16:40:15.659: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 16:40:17.678
    STEP: Verifying the service has paired with the endpoint 01/18/23 16:40:17.693
    Jan 18 16:40:18.695: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 01/18/23 16:40:19
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 16:40:19.054
    STEP: Deleting the collection of validation webhooks 01/18/23 16:40:19.103
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 16:40:19.176
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 16:40:19.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2757" for this suite. 01/18/23 16:40:19.203
    STEP: Destroying namespace "webhook-2757-markers" for this suite. 01/18/23 16:40:19.224
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:40:19.333
Jan 18 16:40:19.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 16:40:19.336
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:40:19.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:40:19.39
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 01/18/23 16:40:19.397
Jan 18 16:40:19.416: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aa1f6d5c-3ac7-462d-a149-e559d95febd7" in namespace "projected-7091" to be "Succeeded or Failed"
Jan 18 16:40:19.427: INFO: Pod "downwardapi-volume-aa1f6d5c-3ac7-462d-a149-e559d95febd7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.168999ms
Jan 18 16:40:21.432: INFO: Pod "downwardapi-volume-aa1f6d5c-3ac7-462d-a149-e559d95febd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015936817s
Jan 18 16:40:23.434: INFO: Pod "downwardapi-volume-aa1f6d5c-3ac7-462d-a149-e559d95febd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017629786s
STEP: Saw pod success 01/18/23 16:40:23.434
Jan 18 16:40:23.435: INFO: Pod "downwardapi-volume-aa1f6d5c-3ac7-462d-a149-e559d95febd7" satisfied condition "Succeeded or Failed"
Jan 18 16:40:23.439: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-aa1f6d5c-3ac7-462d-a149-e559d95febd7 container client-container: <nil>
STEP: delete the pod 01/18/23 16:40:23.446
Jan 18 16:40:23.468: INFO: Waiting for pod downwardapi-volume-aa1f6d5c-3ac7-462d-a149-e559d95febd7 to disappear
Jan 18 16:40:23.475: INFO: Pod downwardapi-volume-aa1f6d5c-3ac7-462d-a149-e559d95febd7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 16:40:23.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7091" for this suite. 01/18/23 16:40:23.482
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":340,"skipped":6307,"failed":0}
------------------------------
• [4.158 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:40:19.333
    Jan 18 16:40:19.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 16:40:19.336
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:40:19.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:40:19.39
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 01/18/23 16:40:19.397
    Jan 18 16:40:19.416: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aa1f6d5c-3ac7-462d-a149-e559d95febd7" in namespace "projected-7091" to be "Succeeded or Failed"
    Jan 18 16:40:19.427: INFO: Pod "downwardapi-volume-aa1f6d5c-3ac7-462d-a149-e559d95febd7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.168999ms
    Jan 18 16:40:21.432: INFO: Pod "downwardapi-volume-aa1f6d5c-3ac7-462d-a149-e559d95febd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015936817s
    Jan 18 16:40:23.434: INFO: Pod "downwardapi-volume-aa1f6d5c-3ac7-462d-a149-e559d95febd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017629786s
    STEP: Saw pod success 01/18/23 16:40:23.434
    Jan 18 16:40:23.435: INFO: Pod "downwardapi-volume-aa1f6d5c-3ac7-462d-a149-e559d95febd7" satisfied condition "Succeeded or Failed"
    Jan 18 16:40:23.439: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-aa1f6d5c-3ac7-462d-a149-e559d95febd7 container client-container: <nil>
    STEP: delete the pod 01/18/23 16:40:23.446
    Jan 18 16:40:23.468: INFO: Waiting for pod downwardapi-volume-aa1f6d5c-3ac7-462d-a149-e559d95febd7 to disappear
    Jan 18 16:40:23.475: INFO: Pod downwardapi-volume-aa1f6d5c-3ac7-462d-a149-e559d95febd7 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 16:40:23.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7091" for this suite. 01/18/23 16:40:23.482
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:40:23.493
Jan 18 16:40:23.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 16:40:23.496
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:40:23.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:40:23.524
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 01/18/23 16:40:23.529
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/18/23 16:40:23.531
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/18/23 16:40:23.531
STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/18/23 16:40:23.531
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/18/23 16:40:23.533
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/18/23 16:40:23.533
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/18/23 16:40:23.535
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 16:40:23.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7286" for this suite. 01/18/23 16:40:23.54
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":341,"skipped":6309,"failed":0}
------------------------------
• [0.056 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:40:23.493
    Jan 18 16:40:23.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 16:40:23.496
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:40:23.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:40:23.524
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 01/18/23 16:40:23.529
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/18/23 16:40:23.531
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/18/23 16:40:23.531
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/18/23 16:40:23.531
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/18/23 16:40:23.533
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/18/23 16:40:23.533
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/18/23 16:40:23.535
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 16:40:23.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-7286" for this suite. 01/18/23 16:40:23.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:40:23.553
Jan 18 16:40:23.553: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename var-expansion 01/18/23 16:40:23.555
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:40:23.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:40:23.588
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 01/18/23 16:40:23.593
Jan 18 16:40:23.604: INFO: Waiting up to 5m0s for pod "var-expansion-ec9af14e-1f63-4c60-b2a5-3e3ce4de6882" in namespace "var-expansion-5834" to be "Succeeded or Failed"
Jan 18 16:40:23.608: INFO: Pod "var-expansion-ec9af14e-1f63-4c60-b2a5-3e3ce4de6882": Phase="Pending", Reason="", readiness=false. Elapsed: 3.97743ms
Jan 18 16:40:25.616: INFO: Pod "var-expansion-ec9af14e-1f63-4c60-b2a5-3e3ce4de6882": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011879233s
Jan 18 16:40:27.616: INFO: Pod "var-expansion-ec9af14e-1f63-4c60-b2a5-3e3ce4de6882": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011609456s
STEP: Saw pod success 01/18/23 16:40:27.616
Jan 18 16:40:27.617: INFO: Pod "var-expansion-ec9af14e-1f63-4c60-b2a5-3e3ce4de6882" satisfied condition "Succeeded or Failed"
Jan 18 16:40:27.624: INFO: Trying to get logs from node v1-25-1-18760-w2 pod var-expansion-ec9af14e-1f63-4c60-b2a5-3e3ce4de6882 container dapi-container: <nil>
STEP: delete the pod 01/18/23 16:40:27.633
Jan 18 16:40:27.650: INFO: Waiting for pod var-expansion-ec9af14e-1f63-4c60-b2a5-3e3ce4de6882 to disappear
Jan 18 16:40:27.656: INFO: Pod var-expansion-ec9af14e-1f63-4c60-b2a5-3e3ce4de6882 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 18 16:40:27.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5834" for this suite. 01/18/23 16:40:27.662
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":342,"skipped":6347,"failed":0}
------------------------------
• [4.118 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:40:23.553
    Jan 18 16:40:23.553: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename var-expansion 01/18/23 16:40:23.555
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:40:23.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:40:23.588
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 01/18/23 16:40:23.593
    Jan 18 16:40:23.604: INFO: Waiting up to 5m0s for pod "var-expansion-ec9af14e-1f63-4c60-b2a5-3e3ce4de6882" in namespace "var-expansion-5834" to be "Succeeded or Failed"
    Jan 18 16:40:23.608: INFO: Pod "var-expansion-ec9af14e-1f63-4c60-b2a5-3e3ce4de6882": Phase="Pending", Reason="", readiness=false. Elapsed: 3.97743ms
    Jan 18 16:40:25.616: INFO: Pod "var-expansion-ec9af14e-1f63-4c60-b2a5-3e3ce4de6882": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011879233s
    Jan 18 16:40:27.616: INFO: Pod "var-expansion-ec9af14e-1f63-4c60-b2a5-3e3ce4de6882": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011609456s
    STEP: Saw pod success 01/18/23 16:40:27.616
    Jan 18 16:40:27.617: INFO: Pod "var-expansion-ec9af14e-1f63-4c60-b2a5-3e3ce4de6882" satisfied condition "Succeeded or Failed"
    Jan 18 16:40:27.624: INFO: Trying to get logs from node v1-25-1-18760-w2 pod var-expansion-ec9af14e-1f63-4c60-b2a5-3e3ce4de6882 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 16:40:27.633
    Jan 18 16:40:27.650: INFO: Waiting for pod var-expansion-ec9af14e-1f63-4c60-b2a5-3e3ce4de6882 to disappear
    Jan 18 16:40:27.656: INFO: Pod var-expansion-ec9af14e-1f63-4c60-b2a5-3e3ce4de6882 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 18 16:40:27.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5834" for this suite. 01/18/23 16:40:27.662
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:40:27.673
Jan 18 16:40:27.674: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename kubelet-test 01/18/23 16:40:27.677
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:40:27.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:40:27.707
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 18 16:40:27.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4124" for this suite. 01/18/23 16:40:27.765
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":343,"skipped":6349,"failed":0}
------------------------------
• [0.100 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:40:27.673
    Jan 18 16:40:27.674: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename kubelet-test 01/18/23 16:40:27.677
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:40:27.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:40:27.707
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 18 16:40:27.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-4124" for this suite. 01/18/23 16:40:27.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:40:27.776
Jan 18 16:40:27.776: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename crd-webhook 01/18/23 16:40:27.779
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:40:27.804
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:40:27.822
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/18/23 16:40:27.832
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/18/23 16:40:28.189
STEP: Deploying the custom resource conversion webhook pod 01/18/23 16:40:28.195
STEP: Wait for the deployment to be ready 01/18/23 16:40:28.215
Jan 18 16:40:28.283: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jan 18 16:40:30.304: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 16, 40, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 40, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 40, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 40, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/18/23 16:40:32.316
STEP: Verifying the service has paired with the endpoint 01/18/23 16:40:32.328
Jan 18 16:40:33.330: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jan 18 16:40:33.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Creating a v1 custom resource 01/18/23 16:40:35.969
STEP: Create a v2 custom resource 01/18/23 16:40:35.991
STEP: List CRs in v1 01/18/23 16:40:35.998
STEP: List CRs in v2 01/18/23 16:40:36.069
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 16:40:36.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6292" for this suite. 01/18/23 16:40:36.61
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":344,"skipped":6374,"failed":0}
------------------------------
• [SLOW TEST] [8.945 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:40:27.776
    Jan 18 16:40:27.776: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename crd-webhook 01/18/23 16:40:27.779
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:40:27.804
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:40:27.822
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/18/23 16:40:27.832
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/18/23 16:40:28.189
    STEP: Deploying the custom resource conversion webhook pod 01/18/23 16:40:28.195
    STEP: Wait for the deployment to be ready 01/18/23 16:40:28.215
    Jan 18 16:40:28.283: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Jan 18 16:40:30.304: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 16, 40, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 40, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 40, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 40, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/18/23 16:40:32.316
    STEP: Verifying the service has paired with the endpoint 01/18/23 16:40:32.328
    Jan 18 16:40:33.330: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jan 18 16:40:33.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Creating a v1 custom resource 01/18/23 16:40:35.969
    STEP: Create a v2 custom resource 01/18/23 16:40:35.991
    STEP: List CRs in v1 01/18/23 16:40:35.998
    STEP: List CRs in v2 01/18/23 16:40:36.069
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 16:40:36.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-6292" for this suite. 01/18/23 16:40:36.61
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:40:36.722
Jan 18 16:40:36.723: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename projected 01/18/23 16:40:36.725
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:40:36.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:40:36.772
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 01/18/23 16:40:36.779
Jan 18 16:40:36.822: INFO: Waiting up to 5m0s for pod "labelsupdate2a4aa20e-e9ab-43b4-a1a1-9c86c499b44c" in namespace "projected-3411" to be "running and ready"
Jan 18 16:40:36.828: INFO: Pod "labelsupdate2a4aa20e-e9ab-43b4-a1a1-9c86c499b44c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.844983ms
Jan 18 16:40:36.828: INFO: The phase of Pod labelsupdate2a4aa20e-e9ab-43b4-a1a1-9c86c499b44c is Pending, waiting for it to be Running (with Ready = true)
Jan 18 16:40:38.836: INFO: Pod "labelsupdate2a4aa20e-e9ab-43b4-a1a1-9c86c499b44c": Phase="Running", Reason="", readiness=true. Elapsed: 2.013719451s
Jan 18 16:40:38.836: INFO: The phase of Pod labelsupdate2a4aa20e-e9ab-43b4-a1a1-9c86c499b44c is Running (Ready = true)
Jan 18 16:40:38.836: INFO: Pod "labelsupdate2a4aa20e-e9ab-43b4-a1a1-9c86c499b44c" satisfied condition "running and ready"
Jan 18 16:40:39.364: INFO: Successfully updated pod "labelsupdate2a4aa20e-e9ab-43b4-a1a1-9c86c499b44c"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 16:40:41.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3411" for this suite. 01/18/23 16:40:41.395
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":345,"skipped":6374,"failed":0}
------------------------------
• [4.680 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:40:36.722
    Jan 18 16:40:36.723: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename projected 01/18/23 16:40:36.725
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:40:36.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:40:36.772
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 01/18/23 16:40:36.779
    Jan 18 16:40:36.822: INFO: Waiting up to 5m0s for pod "labelsupdate2a4aa20e-e9ab-43b4-a1a1-9c86c499b44c" in namespace "projected-3411" to be "running and ready"
    Jan 18 16:40:36.828: INFO: Pod "labelsupdate2a4aa20e-e9ab-43b4-a1a1-9c86c499b44c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.844983ms
    Jan 18 16:40:36.828: INFO: The phase of Pod labelsupdate2a4aa20e-e9ab-43b4-a1a1-9c86c499b44c is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 16:40:38.836: INFO: Pod "labelsupdate2a4aa20e-e9ab-43b4-a1a1-9c86c499b44c": Phase="Running", Reason="", readiness=true. Elapsed: 2.013719451s
    Jan 18 16:40:38.836: INFO: The phase of Pod labelsupdate2a4aa20e-e9ab-43b4-a1a1-9c86c499b44c is Running (Ready = true)
    Jan 18 16:40:38.836: INFO: Pod "labelsupdate2a4aa20e-e9ab-43b4-a1a1-9c86c499b44c" satisfied condition "running and ready"
    Jan 18 16:40:39.364: INFO: Successfully updated pod "labelsupdate2a4aa20e-e9ab-43b4-a1a1-9c86c499b44c"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 16:40:41.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3411" for this suite. 01/18/23 16:40:41.395
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:40:41.407
Jan 18 16:40:41.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename resourcequota 01/18/23 16:40:41.408
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:40:41.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:40:41.435
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 01/18/23 16:40:41.439
STEP: Counting existing ResourceQuota 01/18/23 16:40:46.453
STEP: Creating a ResourceQuota 01/18/23 16:40:51.458
STEP: Ensuring resource quota status is calculated 01/18/23 16:40:51.466
STEP: Creating a Secret 01/18/23 16:40:53.473
STEP: Ensuring resource quota status captures secret creation 01/18/23 16:40:53.491
STEP: Deleting a secret 01/18/23 16:40:55.497
STEP: Ensuring resource quota status released usage 01/18/23 16:40:55.505
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 16:40:57.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7065" for this suite. 01/18/23 16:40:57.518
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":346,"skipped":6404,"failed":0}
------------------------------
• [SLOW TEST] [16.123 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:40:41.407
    Jan 18 16:40:41.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename resourcequota 01/18/23 16:40:41.408
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:40:41.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:40:41.435
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 01/18/23 16:40:41.439
    STEP: Counting existing ResourceQuota 01/18/23 16:40:46.453
    STEP: Creating a ResourceQuota 01/18/23 16:40:51.458
    STEP: Ensuring resource quota status is calculated 01/18/23 16:40:51.466
    STEP: Creating a Secret 01/18/23 16:40:53.473
    STEP: Ensuring resource quota status captures secret creation 01/18/23 16:40:53.491
    STEP: Deleting a secret 01/18/23 16:40:55.497
    STEP: Ensuring resource quota status released usage 01/18/23 16:40:55.505
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 16:40:57.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7065" for this suite. 01/18/23 16:40:57.518
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:40:57.533
Jan 18 16:40:57.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename sched-preemption 01/18/23 16:40:57.534
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:40:57.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:40:57.555
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 18 16:40:57.586: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 16:41:57.648: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 01/18/23 16:41:57.653
Jan 18 16:41:57.685: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 18 16:41:57.703: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 18 16:41:58.339: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 18 16:41:58.800: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/18/23 16:41:58.8
Jan 18 16:41:58.800: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6978" to be "running"
Jan 18 16:41:59.165: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 364.835682ms
Jan 18 16:42:01.173: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.372890177s
Jan 18 16:42:03.172: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.371864559s
Jan 18 16:42:03.172: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 18 16:42:03.172: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6978" to be "running"
Jan 18 16:42:03.178: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.018599ms
Jan 18 16:42:03.178: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 18 16:42:03.179: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6978" to be "running"
Jan 18 16:42:03.185: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.661681ms
Jan 18 16:42:03.186: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 18 16:42:03.186: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6978" to be "running"
Jan 18 16:42:03.192: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.618012ms
Jan 18 16:42:03.192: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 01/18/23 16:42:03.192
Jan 18 16:42:03.205: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jan 18 16:42:03.209: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.636305ms
Jan 18 16:42:05.219: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013839661s
Jan 18 16:42:07.224: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.018444118s
Jan 18 16:42:07.224: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 18 16:42:07.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6978" for this suite. 01/18/23 16:42:07.269
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":347,"skipped":6433,"failed":0}
------------------------------
• [SLOW TEST] [69.819 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:40:57.533
    Jan 18 16:40:57.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename sched-preemption 01/18/23 16:40:57.534
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:40:57.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:40:57.555
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 18 16:40:57.586: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 16:41:57.648: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 01/18/23 16:41:57.653
    Jan 18 16:41:57.685: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 18 16:41:57.703: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 18 16:41:58.339: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 18 16:41:58.800: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/18/23 16:41:58.8
    Jan 18 16:41:58.800: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6978" to be "running"
    Jan 18 16:41:59.165: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 364.835682ms
    Jan 18 16:42:01.173: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.372890177s
    Jan 18 16:42:03.172: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.371864559s
    Jan 18 16:42:03.172: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 18 16:42:03.172: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6978" to be "running"
    Jan 18 16:42:03.178: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.018599ms
    Jan 18 16:42:03.178: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 18 16:42:03.179: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6978" to be "running"
    Jan 18 16:42:03.185: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.661681ms
    Jan 18 16:42:03.186: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 18 16:42:03.186: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6978" to be "running"
    Jan 18 16:42:03.192: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.618012ms
    Jan 18 16:42:03.192: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 01/18/23 16:42:03.192
    Jan 18 16:42:03.205: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jan 18 16:42:03.209: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.636305ms
    Jan 18 16:42:05.219: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013839661s
    Jan 18 16:42:07.224: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.018444118s
    Jan 18 16:42:07.224: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 16:42:07.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-6978" for this suite. 01/18/23 16:42:07.269
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:42:07.353
Jan 18 16:42:07.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename namespaces 01/18/23 16:42:07.358
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:42:07.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:42:07.419
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 01/18/23 16:42:07.423
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:42:07.451
STEP: Creating a service in the namespace 01/18/23 16:42:07.456
STEP: Deleting the namespace 01/18/23 16:42:07.479
STEP: Waiting for the namespace to be removed. 01/18/23 16:42:07.493
STEP: Recreating the namespace 01/18/23 16:42:14.497
STEP: Verifying there is no service in the namespace 01/18/23 16:42:14.53
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 18 16:42:14.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7426" for this suite. 01/18/23 16:42:14.551
STEP: Destroying namespace "nsdeletetest-8241" for this suite. 01/18/23 16:42:14.56
Jan 18 16:42:14.564: INFO: Namespace nsdeletetest-8241 was already deleted
STEP: Destroying namespace "nsdeletetest-1781" for this suite. 01/18/23 16:42:14.564
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":348,"skipped":6449,"failed":0}
------------------------------
• [SLOW TEST] [7.218 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:42:07.353
    Jan 18 16:42:07.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename namespaces 01/18/23 16:42:07.358
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:42:07.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:42:07.419
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 01/18/23 16:42:07.423
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:42:07.451
    STEP: Creating a service in the namespace 01/18/23 16:42:07.456
    STEP: Deleting the namespace 01/18/23 16:42:07.479
    STEP: Waiting for the namespace to be removed. 01/18/23 16:42:07.493
    STEP: Recreating the namespace 01/18/23 16:42:14.497
    STEP: Verifying there is no service in the namespace 01/18/23 16:42:14.53
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 16:42:14.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-7426" for this suite. 01/18/23 16:42:14.551
    STEP: Destroying namespace "nsdeletetest-8241" for this suite. 01/18/23 16:42:14.56
    Jan 18 16:42:14.564: INFO: Namespace nsdeletetest-8241 was already deleted
    STEP: Destroying namespace "nsdeletetest-1781" for this suite. 01/18/23 16:42:14.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:42:14.573
Jan 18 16:42:14.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 16:42:14.575
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:42:14.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:42:14.599
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jan 18 16:42:14.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 16:42:15.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4996" for this suite. 01/18/23 16:42:15.651
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":349,"skipped":6477,"failed":0}
------------------------------
• [1.087 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:42:14.573
    Jan 18 16:42:14.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 16:42:14.575
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:42:14.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:42:14.599
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jan 18 16:42:14.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 16:42:15.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-4996" for this suite. 01/18/23 16:42:15.651
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:42:15.673
Jan 18 16:42:15.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename emptydir 01/18/23 16:42:15.674
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:42:15.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:42:15.702
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 01/18/23 16:42:15.709
Jan 18 16:42:15.724: INFO: Waiting up to 5m0s for pod "pod-992037c9-a537-45bc-9565-301244a5212e" in namespace "emptydir-3486" to be "Succeeded or Failed"
Jan 18 16:42:15.732: INFO: Pod "pod-992037c9-a537-45bc-9565-301244a5212e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.034526ms
Jan 18 16:42:17.739: INFO: Pod "pod-992037c9-a537-45bc-9565-301244a5212e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014244726s
Jan 18 16:42:19.738: INFO: Pod "pod-992037c9-a537-45bc-9565-301244a5212e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013397131s
Jan 18 16:42:21.739: INFO: Pod "pod-992037c9-a537-45bc-9565-301244a5212e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014885989s
STEP: Saw pod success 01/18/23 16:42:21.74
Jan 18 16:42:21.741: INFO: Pod "pod-992037c9-a537-45bc-9565-301244a5212e" satisfied condition "Succeeded or Failed"
Jan 18 16:42:21.747: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-992037c9-a537-45bc-9565-301244a5212e container test-container: <nil>
STEP: delete the pod 01/18/23 16:42:21.771
Jan 18 16:42:21.787: INFO: Waiting for pod pod-992037c9-a537-45bc-9565-301244a5212e to disappear
Jan 18 16:42:21.792: INFO: Pod pod-992037c9-a537-45bc-9565-301244a5212e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 16:42:21.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3486" for this suite. 01/18/23 16:42:21.798
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":350,"skipped":6513,"failed":0}
------------------------------
• [SLOW TEST] [6.133 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:42:15.673
    Jan 18 16:42:15.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename emptydir 01/18/23 16:42:15.674
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:42:15.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:42:15.702
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 01/18/23 16:42:15.709
    Jan 18 16:42:15.724: INFO: Waiting up to 5m0s for pod "pod-992037c9-a537-45bc-9565-301244a5212e" in namespace "emptydir-3486" to be "Succeeded or Failed"
    Jan 18 16:42:15.732: INFO: Pod "pod-992037c9-a537-45bc-9565-301244a5212e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.034526ms
    Jan 18 16:42:17.739: INFO: Pod "pod-992037c9-a537-45bc-9565-301244a5212e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014244726s
    Jan 18 16:42:19.738: INFO: Pod "pod-992037c9-a537-45bc-9565-301244a5212e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013397131s
    Jan 18 16:42:21.739: INFO: Pod "pod-992037c9-a537-45bc-9565-301244a5212e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014885989s
    STEP: Saw pod success 01/18/23 16:42:21.74
    Jan 18 16:42:21.741: INFO: Pod "pod-992037c9-a537-45bc-9565-301244a5212e" satisfied condition "Succeeded or Failed"
    Jan 18 16:42:21.747: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-992037c9-a537-45bc-9565-301244a5212e container test-container: <nil>
    STEP: delete the pod 01/18/23 16:42:21.771
    Jan 18 16:42:21.787: INFO: Waiting for pod pod-992037c9-a537-45bc-9565-301244a5212e to disappear
    Jan 18 16:42:21.792: INFO: Pod pod-992037c9-a537-45bc-9565-301244a5212e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 16:42:21.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3486" for this suite. 01/18/23 16:42:21.798
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:42:21.807
Jan 18 16:42:21.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename webhook 01/18/23 16:42:21.81
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:42:21.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:42:21.843
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 16:42:21.863
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 16:42:22.623
STEP: Deploying the webhook pod 01/18/23 16:42:22.63
STEP: Wait for the deployment to be ready 01/18/23 16:42:22.65
Jan 18 16:42:22.684: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 16:42:24.698
STEP: Verifying the service has paired with the endpoint 01/18/23 16:42:24.721
Jan 18 16:42:25.724: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/18/23 16:42:25.729
STEP: create a pod that should be updated by the webhook 01/18/23 16:42:25.751
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 16:42:25.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5276" for this suite. 01/18/23 16:42:25.824
STEP: Destroying namespace "webhook-5276-markers" for this suite. 01/18/23 16:42:25.838
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":351,"skipped":6535,"failed":0}
------------------------------
• [4.097 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:42:21.807
    Jan 18 16:42:21.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename webhook 01/18/23 16:42:21.81
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:42:21.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:42:21.843
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 16:42:21.863
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 16:42:22.623
    STEP: Deploying the webhook pod 01/18/23 16:42:22.63
    STEP: Wait for the deployment to be ready 01/18/23 16:42:22.65
    Jan 18 16:42:22.684: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 16:42:24.698
    STEP: Verifying the service has paired with the endpoint 01/18/23 16:42:24.721
    Jan 18 16:42:25.724: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/18/23 16:42:25.729
    STEP: create a pod that should be updated by the webhook 01/18/23 16:42:25.751
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 16:42:25.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5276" for this suite. 01/18/23 16:42:25.824
    STEP: Destroying namespace "webhook-5276-markers" for this suite. 01/18/23 16:42:25.838
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:42:25.91
Jan 18 16:42:25.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename webhook 01/18/23 16:42:25.913
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:42:25.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:42:25.964
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 16:42:25.991
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 16:42:26.437
STEP: Deploying the webhook pod 01/18/23 16:42:26.443
STEP: Wait for the deployment to be ready 01/18/23 16:42:26.452
Jan 18 16:42:26.467: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 16:42:28.484
STEP: Verifying the service has paired with the endpoint 01/18/23 16:42:28.497
Jan 18 16:42:29.497: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 01/18/23 16:42:29.502
STEP: Creating a custom resource definition that should be denied by the webhook 01/18/23 16:42:29.531
Jan 18 16:42:29.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 16:42:29.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8271" for this suite. 01/18/23 16:42:29.567
STEP: Destroying namespace "webhook-8271-markers" for this suite. 01/18/23 16:42:29.581
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":352,"skipped":6545,"failed":0}
------------------------------
• [3.812 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:42:25.91
    Jan 18 16:42:25.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename webhook 01/18/23 16:42:25.913
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:42:25.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:42:25.964
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 16:42:25.991
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 16:42:26.437
    STEP: Deploying the webhook pod 01/18/23 16:42:26.443
    STEP: Wait for the deployment to be ready 01/18/23 16:42:26.452
    Jan 18 16:42:26.467: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 16:42:28.484
    STEP: Verifying the service has paired with the endpoint 01/18/23 16:42:28.497
    Jan 18 16:42:29.497: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 01/18/23 16:42:29.502
    STEP: Creating a custom resource definition that should be denied by the webhook 01/18/23 16:42:29.531
    Jan 18 16:42:29.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 16:42:29.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8271" for this suite. 01/18/23 16:42:29.567
    STEP: Destroying namespace "webhook-8271-markers" for this suite. 01/18/23 16:42:29.581
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:42:29.723
Jan 18 16:42:29.723: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename downward-api 01/18/23 16:42:29.724
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:42:29.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:42:29.758
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 01/18/23 16:42:29.764
Jan 18 16:42:29.783: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bb22b8f1-5850-49d6-8e48-adb2e2c8c7fa" in namespace "downward-api-417" to be "Succeeded or Failed"
Jan 18 16:42:29.791: INFO: Pod "downwardapi-volume-bb22b8f1-5850-49d6-8e48-adb2e2c8c7fa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.361827ms
Jan 18 16:42:31.797: INFO: Pod "downwardapi-volume-bb22b8f1-5850-49d6-8e48-adb2e2c8c7fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013511865s
Jan 18 16:42:33.798: INFO: Pod "downwardapi-volume-bb22b8f1-5850-49d6-8e48-adb2e2c8c7fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014665051s
Jan 18 16:42:35.798: INFO: Pod "downwardapi-volume-bb22b8f1-5850-49d6-8e48-adb2e2c8c7fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014931521s
STEP: Saw pod success 01/18/23 16:42:35.798
Jan 18 16:42:35.798: INFO: Pod "downwardapi-volume-bb22b8f1-5850-49d6-8e48-adb2e2c8c7fa" satisfied condition "Succeeded or Failed"
Jan 18 16:42:35.804: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-bb22b8f1-5850-49d6-8e48-adb2e2c8c7fa container client-container: <nil>
STEP: delete the pod 01/18/23 16:42:35.814
Jan 18 16:42:35.829: INFO: Waiting for pod downwardapi-volume-bb22b8f1-5850-49d6-8e48-adb2e2c8c7fa to disappear
Jan 18 16:42:35.836: INFO: Pod downwardapi-volume-bb22b8f1-5850-49d6-8e48-adb2e2c8c7fa no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 16:42:35.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-417" for this suite. 01/18/23 16:42:35.842
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":353,"skipped":6556,"failed":0}
------------------------------
• [SLOW TEST] [6.128 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:42:29.723
    Jan 18 16:42:29.723: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename downward-api 01/18/23 16:42:29.724
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:42:29.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:42:29.758
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 01/18/23 16:42:29.764
    Jan 18 16:42:29.783: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bb22b8f1-5850-49d6-8e48-adb2e2c8c7fa" in namespace "downward-api-417" to be "Succeeded or Failed"
    Jan 18 16:42:29.791: INFO: Pod "downwardapi-volume-bb22b8f1-5850-49d6-8e48-adb2e2c8c7fa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.361827ms
    Jan 18 16:42:31.797: INFO: Pod "downwardapi-volume-bb22b8f1-5850-49d6-8e48-adb2e2c8c7fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013511865s
    Jan 18 16:42:33.798: INFO: Pod "downwardapi-volume-bb22b8f1-5850-49d6-8e48-adb2e2c8c7fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014665051s
    Jan 18 16:42:35.798: INFO: Pod "downwardapi-volume-bb22b8f1-5850-49d6-8e48-adb2e2c8c7fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014931521s
    STEP: Saw pod success 01/18/23 16:42:35.798
    Jan 18 16:42:35.798: INFO: Pod "downwardapi-volume-bb22b8f1-5850-49d6-8e48-adb2e2c8c7fa" satisfied condition "Succeeded or Failed"
    Jan 18 16:42:35.804: INFO: Trying to get logs from node v1-25-1-18760-w2 pod downwardapi-volume-bb22b8f1-5850-49d6-8e48-adb2e2c8c7fa container client-container: <nil>
    STEP: delete the pod 01/18/23 16:42:35.814
    Jan 18 16:42:35.829: INFO: Waiting for pod downwardapi-volume-bb22b8f1-5850-49d6-8e48-adb2e2c8c7fa to disappear
    Jan 18 16:42:35.836: INFO: Pod downwardapi-volume-bb22b8f1-5850-49d6-8e48-adb2e2c8c7fa no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 16:42:35.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-417" for this suite. 01/18/23 16:42:35.842
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:42:35.858
Jan 18 16:42:35.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename var-expansion 01/18/23 16:42:35.86
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:42:35.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:42:35.882
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 01/18/23 16:42:35.888
Jan 18 16:42:35.897: INFO: Waiting up to 5m0s for pod "var-expansion-681cd47b-f5f3-42e6-a79e-6084ba70d111" in namespace "var-expansion-6701" to be "Succeeded or Failed"
Jan 18 16:42:35.911: INFO: Pod "var-expansion-681cd47b-f5f3-42e6-a79e-6084ba70d111": Phase="Pending", Reason="", readiness=false. Elapsed: 14.358162ms
Jan 18 16:42:37.929: INFO: Pod "var-expansion-681cd47b-f5f3-42e6-a79e-6084ba70d111": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032043879s
Jan 18 16:42:39.917: INFO: Pod "var-expansion-681cd47b-f5f3-42e6-a79e-6084ba70d111": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020657607s
Jan 18 16:42:41.918: INFO: Pod "var-expansion-681cd47b-f5f3-42e6-a79e-6084ba70d111": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021638194s
STEP: Saw pod success 01/18/23 16:42:41.919
Jan 18 16:42:41.919: INFO: Pod "var-expansion-681cd47b-f5f3-42e6-a79e-6084ba70d111" satisfied condition "Succeeded or Failed"
Jan 18 16:42:41.923: INFO: Trying to get logs from node v1-25-1-18760-w2 pod var-expansion-681cd47b-f5f3-42e6-a79e-6084ba70d111 container dapi-container: <nil>
STEP: delete the pod 01/18/23 16:42:41.934
Jan 18 16:42:41.951: INFO: Waiting for pod var-expansion-681cd47b-f5f3-42e6-a79e-6084ba70d111 to disappear
Jan 18 16:42:41.956: INFO: Pod var-expansion-681cd47b-f5f3-42e6-a79e-6084ba70d111 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 18 16:42:41.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6701" for this suite. 01/18/23 16:42:41.962
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":354,"skipped":6587,"failed":0}
------------------------------
• [SLOW TEST] [6.110 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:42:35.858
    Jan 18 16:42:35.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename var-expansion 01/18/23 16:42:35.86
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:42:35.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:42:35.882
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 01/18/23 16:42:35.888
    Jan 18 16:42:35.897: INFO: Waiting up to 5m0s for pod "var-expansion-681cd47b-f5f3-42e6-a79e-6084ba70d111" in namespace "var-expansion-6701" to be "Succeeded or Failed"
    Jan 18 16:42:35.911: INFO: Pod "var-expansion-681cd47b-f5f3-42e6-a79e-6084ba70d111": Phase="Pending", Reason="", readiness=false. Elapsed: 14.358162ms
    Jan 18 16:42:37.929: INFO: Pod "var-expansion-681cd47b-f5f3-42e6-a79e-6084ba70d111": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032043879s
    Jan 18 16:42:39.917: INFO: Pod "var-expansion-681cd47b-f5f3-42e6-a79e-6084ba70d111": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020657607s
    Jan 18 16:42:41.918: INFO: Pod "var-expansion-681cd47b-f5f3-42e6-a79e-6084ba70d111": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021638194s
    STEP: Saw pod success 01/18/23 16:42:41.919
    Jan 18 16:42:41.919: INFO: Pod "var-expansion-681cd47b-f5f3-42e6-a79e-6084ba70d111" satisfied condition "Succeeded or Failed"
    Jan 18 16:42:41.923: INFO: Trying to get logs from node v1-25-1-18760-w2 pod var-expansion-681cd47b-f5f3-42e6-a79e-6084ba70d111 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 16:42:41.934
    Jan 18 16:42:41.951: INFO: Waiting for pod var-expansion-681cd47b-f5f3-42e6-a79e-6084ba70d111 to disappear
    Jan 18 16:42:41.956: INFO: Pod var-expansion-681cd47b-f5f3-42e6-a79e-6084ba70d111 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 18 16:42:41.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6701" for this suite. 01/18/23 16:42:41.962
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:42:41.973
Jan 18 16:42:41.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename configmap 01/18/23 16:42:41.974
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:42:41.994
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:42:42.003
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-5197a14c-1592-4dab-843f-9d825dde58f0 01/18/23 16:42:42.009
STEP: Creating a pod to test consume configMaps 01/18/23 16:42:42.019
Jan 18 16:42:42.030: INFO: Waiting up to 5m0s for pod "pod-configmaps-59600bfe-3b6a-4060-8c56-aa29296b3805" in namespace "configmap-8488" to be "Succeeded or Failed"
Jan 18 16:42:42.035: INFO: Pod "pod-configmaps-59600bfe-3b6a-4060-8c56-aa29296b3805": Phase="Pending", Reason="", readiness=false. Elapsed: 4.716814ms
Jan 18 16:42:44.044: INFO: Pod "pod-configmaps-59600bfe-3b6a-4060-8c56-aa29296b3805": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014320616s
Jan 18 16:42:46.041: INFO: Pod "pod-configmaps-59600bfe-3b6a-4060-8c56-aa29296b3805": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010827881s
STEP: Saw pod success 01/18/23 16:42:46.041
Jan 18 16:42:46.042: INFO: Pod "pod-configmaps-59600bfe-3b6a-4060-8c56-aa29296b3805" satisfied condition "Succeeded or Failed"
Jan 18 16:42:46.047: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-59600bfe-3b6a-4060-8c56-aa29296b3805 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 16:42:46.055
Jan 18 16:42:46.072: INFO: Waiting for pod pod-configmaps-59600bfe-3b6a-4060-8c56-aa29296b3805 to disappear
Jan 18 16:42:46.078: INFO: Pod pod-configmaps-59600bfe-3b6a-4060-8c56-aa29296b3805 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 16:42:46.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8488" for this suite. 01/18/23 16:42:46.084
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":355,"skipped":6614,"failed":0}
------------------------------
• [4.120 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:42:41.973
    Jan 18 16:42:41.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename configmap 01/18/23 16:42:41.974
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:42:41.994
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:42:42.003
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-5197a14c-1592-4dab-843f-9d825dde58f0 01/18/23 16:42:42.009
    STEP: Creating a pod to test consume configMaps 01/18/23 16:42:42.019
    Jan 18 16:42:42.030: INFO: Waiting up to 5m0s for pod "pod-configmaps-59600bfe-3b6a-4060-8c56-aa29296b3805" in namespace "configmap-8488" to be "Succeeded or Failed"
    Jan 18 16:42:42.035: INFO: Pod "pod-configmaps-59600bfe-3b6a-4060-8c56-aa29296b3805": Phase="Pending", Reason="", readiness=false. Elapsed: 4.716814ms
    Jan 18 16:42:44.044: INFO: Pod "pod-configmaps-59600bfe-3b6a-4060-8c56-aa29296b3805": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014320616s
    Jan 18 16:42:46.041: INFO: Pod "pod-configmaps-59600bfe-3b6a-4060-8c56-aa29296b3805": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010827881s
    STEP: Saw pod success 01/18/23 16:42:46.041
    Jan 18 16:42:46.042: INFO: Pod "pod-configmaps-59600bfe-3b6a-4060-8c56-aa29296b3805" satisfied condition "Succeeded or Failed"
    Jan 18 16:42:46.047: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-59600bfe-3b6a-4060-8c56-aa29296b3805 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 16:42:46.055
    Jan 18 16:42:46.072: INFO: Waiting for pod pod-configmaps-59600bfe-3b6a-4060-8c56-aa29296b3805 to disappear
    Jan 18 16:42:46.078: INFO: Pod pod-configmaps-59600bfe-3b6a-4060-8c56-aa29296b3805 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 16:42:46.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8488" for this suite. 01/18/23 16:42:46.084
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:42:46.102
Jan 18 16:42:46.102: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename container-probe 01/18/23 16:42:46.103
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:42:46.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:42:46.133
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 18 16:43:46.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8804" for this suite. 01/18/23 16:43:46.159
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":356,"skipped":6630,"failed":0}
------------------------------
• [SLOW TEST] [60.064 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:42:46.102
    Jan 18 16:42:46.102: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename container-probe 01/18/23 16:42:46.103
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:42:46.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:42:46.133
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 18 16:43:46.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8804" for this suite. 01/18/23 16:43:46.159
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:43:46.167
Jan 18 16:43:46.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename configmap 01/18/23 16:43:46.171
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:43:46.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:43:46.199
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-62bef2af-6c49-4449-b759-df1dd1d93c90 01/18/23 16:43:46.213
STEP: Creating the pod 01/18/23 16:43:46.22
Jan 18 16:43:46.232: INFO: Waiting up to 5m0s for pod "pod-configmaps-12ef297b-f7c0-40fe-a0ba-29f0b1222f88" in namespace "configmap-999" to be "running"
Jan 18 16:43:46.238: INFO: Pod "pod-configmaps-12ef297b-f7c0-40fe-a0ba-29f0b1222f88": Phase="Pending", Reason="", readiness=false. Elapsed: 6.07143ms
Jan 18 16:43:48.249: INFO: Pod "pod-configmaps-12ef297b-f7c0-40fe-a0ba-29f0b1222f88": Phase="Running", Reason="", readiness=false. Elapsed: 2.016348881s
Jan 18 16:43:48.249: INFO: Pod "pod-configmaps-12ef297b-f7c0-40fe-a0ba-29f0b1222f88" satisfied condition "running"
STEP: Waiting for pod with text data 01/18/23 16:43:48.249
STEP: Waiting for pod with binary data 01/18/23 16:43:48.277
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 16:43:48.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-999" for this suite. 01/18/23 16:43:48.296
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":357,"skipped":6634,"failed":0}
------------------------------
• [2.154 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:43:46.167
    Jan 18 16:43:46.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename configmap 01/18/23 16:43:46.171
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:43:46.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:43:46.199
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-62bef2af-6c49-4449-b759-df1dd1d93c90 01/18/23 16:43:46.213
    STEP: Creating the pod 01/18/23 16:43:46.22
    Jan 18 16:43:46.232: INFO: Waiting up to 5m0s for pod "pod-configmaps-12ef297b-f7c0-40fe-a0ba-29f0b1222f88" in namespace "configmap-999" to be "running"
    Jan 18 16:43:46.238: INFO: Pod "pod-configmaps-12ef297b-f7c0-40fe-a0ba-29f0b1222f88": Phase="Pending", Reason="", readiness=false. Elapsed: 6.07143ms
    Jan 18 16:43:48.249: INFO: Pod "pod-configmaps-12ef297b-f7c0-40fe-a0ba-29f0b1222f88": Phase="Running", Reason="", readiness=false. Elapsed: 2.016348881s
    Jan 18 16:43:48.249: INFO: Pod "pod-configmaps-12ef297b-f7c0-40fe-a0ba-29f0b1222f88" satisfied condition "running"
    STEP: Waiting for pod with text data 01/18/23 16:43:48.249
    STEP: Waiting for pod with binary data 01/18/23 16:43:48.277
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 16:43:48.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-999" for this suite. 01/18/23 16:43:48.296
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:43:48.323
Jan 18 16:43:48.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename dns 01/18/23 16:43:48.324
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:43:48.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:43:48.351
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 01/18/23 16:43:48.356
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-186.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local; sleep 1; done
 01/18/23 16:43:48.381
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-186.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-186.svc.cluster.local; sleep 1; done
 01/18/23 16:43:48.382
STEP: creating a pod to probe DNS 01/18/23 16:43:48.382
STEP: submitting the pod to kubernetes 01/18/23 16:43:48.383
Jan 18 16:43:48.395: INFO: Waiting up to 15m0s for pod "dns-test-9f63b440-084b-4df1-98d6-d0bbbbf45c62" in namespace "dns-186" to be "running"
Jan 18 16:43:48.420: INFO: Pod "dns-test-9f63b440-084b-4df1-98d6-d0bbbbf45c62": Phase="Pending", Reason="", readiness=false. Elapsed: 25.868631ms
Jan 18 16:43:50.426: INFO: Pod "dns-test-9f63b440-084b-4df1-98d6-d0bbbbf45c62": Phase="Running", Reason="", readiness=true. Elapsed: 2.031799272s
Jan 18 16:43:50.427: INFO: Pod "dns-test-9f63b440-084b-4df1-98d6-d0bbbbf45c62" satisfied condition "running"
STEP: retrieving the pod 01/18/23 16:43:50.427
STEP: looking for the results for each expected name from probers 01/18/23 16:43:50.432
Jan 18 16:43:50.448: INFO: DNS probes using dns-test-9f63b440-084b-4df1-98d6-d0bbbbf45c62 succeeded

STEP: deleting the pod 01/18/23 16:43:50.448
STEP: changing the externalName to bar.example.com 01/18/23 16:43:50.467
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-186.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local; sleep 1; done
 01/18/23 16:43:50.478
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-186.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-186.svc.cluster.local; sleep 1; done
 01/18/23 16:43:50.478
STEP: creating a second pod to probe DNS 01/18/23 16:43:50.479
STEP: submitting the pod to kubernetes 01/18/23 16:43:50.479
Jan 18 16:43:50.487: INFO: Waiting up to 15m0s for pod "dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55" in namespace "dns-186" to be "running"
Jan 18 16:43:50.494: INFO: Pod "dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55": Phase="Pending", Reason="", readiness=false. Elapsed: 7.095404ms
Jan 18 16:43:52.504: INFO: Pod "dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55": Phase="Running", Reason="", readiness=true. Elapsed: 2.016930133s
Jan 18 16:43:52.504: INFO: Pod "dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55" satisfied condition "running"
STEP: retrieving the pod 01/18/23 16:43:52.504
STEP: looking for the results for each expected name from probers 01/18/23 16:43:52.51
Jan 18 16:43:52.522: INFO: File wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local from pod  dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 16:43:52.526: INFO: File jessie_udp@dns-test-service-3.dns-186.svc.cluster.local from pod  dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 16:43:52.526: INFO: Lookups using dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 failed for: [wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local jessie_udp@dns-test-service-3.dns-186.svc.cluster.local]

Jan 18 16:43:57.534: INFO: File wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local from pod  dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 16:43:57.540: INFO: File jessie_udp@dns-test-service-3.dns-186.svc.cluster.local from pod  dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 16:43:57.541: INFO: Lookups using dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 failed for: [wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local jessie_udp@dns-test-service-3.dns-186.svc.cluster.local]

Jan 18 16:44:02.537: INFO: File wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local from pod  dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 16:44:02.542: INFO: File jessie_udp@dns-test-service-3.dns-186.svc.cluster.local from pod  dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 16:44:02.542: INFO: Lookups using dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 failed for: [wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local jessie_udp@dns-test-service-3.dns-186.svc.cluster.local]

Jan 18 16:44:07.535: INFO: File wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local from pod  dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 16:44:07.540: INFO: File jessie_udp@dns-test-service-3.dns-186.svc.cluster.local from pod  dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 16:44:07.540: INFO: Lookups using dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 failed for: [wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local jessie_udp@dns-test-service-3.dns-186.svc.cluster.local]

Jan 18 16:44:12.536: INFO: File wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local from pod  dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 16:44:12.542: INFO: File jessie_udp@dns-test-service-3.dns-186.svc.cluster.local from pod  dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 16:44:12.542: INFO: Lookups using dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 failed for: [wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local jessie_udp@dns-test-service-3.dns-186.svc.cluster.local]

Jan 18 16:44:17.536: INFO: File wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local from pod  dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 16:44:17.542: INFO: File jessie_udp@dns-test-service-3.dns-186.svc.cluster.local from pod  dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 16:44:17.542: INFO: Lookups using dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 failed for: [wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local jessie_udp@dns-test-service-3.dns-186.svc.cluster.local]

Jan 18 16:44:22.542: INFO: DNS probes using dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 succeeded

STEP: deleting the pod 01/18/23 16:44:22.543
STEP: changing the service to type=ClusterIP 01/18/23 16:44:22.671
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-186.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local; sleep 1; done
 01/18/23 16:44:22.727
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-186.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-186.svc.cluster.local; sleep 1; done
 01/18/23 16:44:22.731
STEP: creating a third pod to probe DNS 01/18/23 16:44:22.732
STEP: submitting the pod to kubernetes 01/18/23 16:44:22.745
Jan 18 16:44:22.778: INFO: Waiting up to 15m0s for pod "dns-test-f56c4327-8af6-4bf8-8cee-3cd3db668956" in namespace "dns-186" to be "running"
Jan 18 16:44:22.791: INFO: Pod "dns-test-f56c4327-8af6-4bf8-8cee-3cd3db668956": Phase="Pending", Reason="", readiness=false. Elapsed: 12.521291ms
Jan 18 16:44:24.796: INFO: Pod "dns-test-f56c4327-8af6-4bf8-8cee-3cd3db668956": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018021142s
Jan 18 16:44:26.802: INFO: Pod "dns-test-f56c4327-8af6-4bf8-8cee-3cd3db668956": Phase="Running", Reason="", readiness=true. Elapsed: 4.024213805s
Jan 18 16:44:26.802: INFO: Pod "dns-test-f56c4327-8af6-4bf8-8cee-3cd3db668956" satisfied condition "running"
STEP: retrieving the pod 01/18/23 16:44:26.802
STEP: looking for the results for each expected name from probers 01/18/23 16:44:26.808
Jan 18 16:44:26.834: INFO: DNS probes using dns-test-f56c4327-8af6-4bf8-8cee-3cd3db668956 succeeded

STEP: deleting the pod 01/18/23 16:44:26.834
STEP: deleting the test externalName service 01/18/23 16:44:26.855
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 18 16:44:26.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-186" for this suite. 01/18/23 16:44:26.886
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":358,"skipped":6634,"failed":0}
------------------------------
• [SLOW TEST] [38.580 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:43:48.323
    Jan 18 16:43:48.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename dns 01/18/23 16:43:48.324
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:43:48.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:43:48.351
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 01/18/23 16:43:48.356
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-186.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local; sleep 1; done
     01/18/23 16:43:48.381
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-186.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-186.svc.cluster.local; sleep 1; done
     01/18/23 16:43:48.382
    STEP: creating a pod to probe DNS 01/18/23 16:43:48.382
    STEP: submitting the pod to kubernetes 01/18/23 16:43:48.383
    Jan 18 16:43:48.395: INFO: Waiting up to 15m0s for pod "dns-test-9f63b440-084b-4df1-98d6-d0bbbbf45c62" in namespace "dns-186" to be "running"
    Jan 18 16:43:48.420: INFO: Pod "dns-test-9f63b440-084b-4df1-98d6-d0bbbbf45c62": Phase="Pending", Reason="", readiness=false. Elapsed: 25.868631ms
    Jan 18 16:43:50.426: INFO: Pod "dns-test-9f63b440-084b-4df1-98d6-d0bbbbf45c62": Phase="Running", Reason="", readiness=true. Elapsed: 2.031799272s
    Jan 18 16:43:50.427: INFO: Pod "dns-test-9f63b440-084b-4df1-98d6-d0bbbbf45c62" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 16:43:50.427
    STEP: looking for the results for each expected name from probers 01/18/23 16:43:50.432
    Jan 18 16:43:50.448: INFO: DNS probes using dns-test-9f63b440-084b-4df1-98d6-d0bbbbf45c62 succeeded

    STEP: deleting the pod 01/18/23 16:43:50.448
    STEP: changing the externalName to bar.example.com 01/18/23 16:43:50.467
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-186.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local; sleep 1; done
     01/18/23 16:43:50.478
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-186.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-186.svc.cluster.local; sleep 1; done
     01/18/23 16:43:50.478
    STEP: creating a second pod to probe DNS 01/18/23 16:43:50.479
    STEP: submitting the pod to kubernetes 01/18/23 16:43:50.479
    Jan 18 16:43:50.487: INFO: Waiting up to 15m0s for pod "dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55" in namespace "dns-186" to be "running"
    Jan 18 16:43:50.494: INFO: Pod "dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55": Phase="Pending", Reason="", readiness=false. Elapsed: 7.095404ms
    Jan 18 16:43:52.504: INFO: Pod "dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55": Phase="Running", Reason="", readiness=true. Elapsed: 2.016930133s
    Jan 18 16:43:52.504: INFO: Pod "dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 16:43:52.504
    STEP: looking for the results for each expected name from probers 01/18/23 16:43:52.51
    Jan 18 16:43:52.522: INFO: File wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local from pod  dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 16:43:52.526: INFO: File jessie_udp@dns-test-service-3.dns-186.svc.cluster.local from pod  dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 16:43:52.526: INFO: Lookups using dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 failed for: [wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local jessie_udp@dns-test-service-3.dns-186.svc.cluster.local]

    Jan 18 16:43:57.534: INFO: File wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local from pod  dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 16:43:57.540: INFO: File jessie_udp@dns-test-service-3.dns-186.svc.cluster.local from pod  dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 16:43:57.541: INFO: Lookups using dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 failed for: [wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local jessie_udp@dns-test-service-3.dns-186.svc.cluster.local]

    Jan 18 16:44:02.537: INFO: File wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local from pod  dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 16:44:02.542: INFO: File jessie_udp@dns-test-service-3.dns-186.svc.cluster.local from pod  dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 16:44:02.542: INFO: Lookups using dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 failed for: [wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local jessie_udp@dns-test-service-3.dns-186.svc.cluster.local]

    Jan 18 16:44:07.535: INFO: File wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local from pod  dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 16:44:07.540: INFO: File jessie_udp@dns-test-service-3.dns-186.svc.cluster.local from pod  dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 16:44:07.540: INFO: Lookups using dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 failed for: [wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local jessie_udp@dns-test-service-3.dns-186.svc.cluster.local]

    Jan 18 16:44:12.536: INFO: File wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local from pod  dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 16:44:12.542: INFO: File jessie_udp@dns-test-service-3.dns-186.svc.cluster.local from pod  dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 16:44:12.542: INFO: Lookups using dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 failed for: [wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local jessie_udp@dns-test-service-3.dns-186.svc.cluster.local]

    Jan 18 16:44:17.536: INFO: File wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local from pod  dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 16:44:17.542: INFO: File jessie_udp@dns-test-service-3.dns-186.svc.cluster.local from pod  dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 16:44:17.542: INFO: Lookups using dns-186/dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 failed for: [wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local jessie_udp@dns-test-service-3.dns-186.svc.cluster.local]

    Jan 18 16:44:22.542: INFO: DNS probes using dns-test-a67e972d-ea54-4d17-813a-d8ab28cfee55 succeeded

    STEP: deleting the pod 01/18/23 16:44:22.543
    STEP: changing the service to type=ClusterIP 01/18/23 16:44:22.671
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-186.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-186.svc.cluster.local; sleep 1; done
     01/18/23 16:44:22.727
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-186.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-186.svc.cluster.local; sleep 1; done
     01/18/23 16:44:22.731
    STEP: creating a third pod to probe DNS 01/18/23 16:44:22.732
    STEP: submitting the pod to kubernetes 01/18/23 16:44:22.745
    Jan 18 16:44:22.778: INFO: Waiting up to 15m0s for pod "dns-test-f56c4327-8af6-4bf8-8cee-3cd3db668956" in namespace "dns-186" to be "running"
    Jan 18 16:44:22.791: INFO: Pod "dns-test-f56c4327-8af6-4bf8-8cee-3cd3db668956": Phase="Pending", Reason="", readiness=false. Elapsed: 12.521291ms
    Jan 18 16:44:24.796: INFO: Pod "dns-test-f56c4327-8af6-4bf8-8cee-3cd3db668956": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018021142s
    Jan 18 16:44:26.802: INFO: Pod "dns-test-f56c4327-8af6-4bf8-8cee-3cd3db668956": Phase="Running", Reason="", readiness=true. Elapsed: 4.024213805s
    Jan 18 16:44:26.802: INFO: Pod "dns-test-f56c4327-8af6-4bf8-8cee-3cd3db668956" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 16:44:26.802
    STEP: looking for the results for each expected name from probers 01/18/23 16:44:26.808
    Jan 18 16:44:26.834: INFO: DNS probes using dns-test-f56c4327-8af6-4bf8-8cee-3cd3db668956 succeeded

    STEP: deleting the pod 01/18/23 16:44:26.834
    STEP: deleting the test externalName service 01/18/23 16:44:26.855
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 18 16:44:26.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-186" for this suite. 01/18/23 16:44:26.886
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:44:26.906
Jan 18 16:44:26.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename container-runtime 01/18/23 16:44:26.919
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:44:26.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:44:26.966
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 01/18/23 16:44:26.975
STEP: wait for the container to reach Succeeded 01/18/23 16:44:26.989
STEP: get the container status 01/18/23 16:44:31.029
STEP: the container should be terminated 01/18/23 16:44:31.033
STEP: the termination message should be set 01/18/23 16:44:31.033
Jan 18 16:44:31.033: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 01/18/23 16:44:31.033
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 18 16:44:31.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1185" for this suite. 01/18/23 16:44:31.055
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":359,"skipped":6637,"failed":0}
------------------------------
• [4.154 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:44:26.906
    Jan 18 16:44:26.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename container-runtime 01/18/23 16:44:26.919
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:44:26.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:44:26.966
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 01/18/23 16:44:26.975
    STEP: wait for the container to reach Succeeded 01/18/23 16:44:26.989
    STEP: get the container status 01/18/23 16:44:31.029
    STEP: the container should be terminated 01/18/23 16:44:31.033
    STEP: the termination message should be set 01/18/23 16:44:31.033
    Jan 18 16:44:31.033: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 01/18/23 16:44:31.033
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 18 16:44:31.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-1185" for this suite. 01/18/23 16:44:31.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:44:31.063
Jan 18 16:44:31.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
STEP: Building a namespace api object, basename configmap 01/18/23 16:44:31.064
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:44:31.084
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:44:31.089
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-277/configmap-test-47714ba4-b1c9-4210-8f52-2c48bccdf05a 01/18/23 16:44:31.094
STEP: Creating a pod to test consume configMaps 01/18/23 16:44:31.105
Jan 18 16:44:31.115: INFO: Waiting up to 5m0s for pod "pod-configmaps-19a68527-93a5-4337-bcd6-60fb9cb32fb5" in namespace "configmap-277" to be "Succeeded or Failed"
Jan 18 16:44:31.119: INFO: Pod "pod-configmaps-19a68527-93a5-4337-bcd6-60fb9cb32fb5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.439395ms
Jan 18 16:44:33.124: INFO: Pod "pod-configmaps-19a68527-93a5-4337-bcd6-60fb9cb32fb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008902012s
Jan 18 16:44:35.125: INFO: Pod "pod-configmaps-19a68527-93a5-4337-bcd6-60fb9cb32fb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009604442s
STEP: Saw pod success 01/18/23 16:44:35.125
Jan 18 16:44:35.125: INFO: Pod "pod-configmaps-19a68527-93a5-4337-bcd6-60fb9cb32fb5" satisfied condition "Succeeded or Failed"
Jan 18 16:44:35.130: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-19a68527-93a5-4337-bcd6-60fb9cb32fb5 container env-test: <nil>
STEP: delete the pod 01/18/23 16:44:35.139
Jan 18 16:44:35.169: INFO: Waiting for pod pod-configmaps-19a68527-93a5-4337-bcd6-60fb9cb32fb5 to disappear
Jan 18 16:44:35.175: INFO: Pod pod-configmaps-19a68527-93a5-4337-bcd6-60fb9cb32fb5 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 16:44:35.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-277" for this suite. 01/18/23 16:44:35.181
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":360,"skipped":6655,"failed":0}
------------------------------
• [4.127 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:44:31.063
    Jan 18 16:44:31.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3957318277
    STEP: Building a namespace api object, basename configmap 01/18/23 16:44:31.064
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:44:31.084
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:44:31.089
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-277/configmap-test-47714ba4-b1c9-4210-8f52-2c48bccdf05a 01/18/23 16:44:31.094
    STEP: Creating a pod to test consume configMaps 01/18/23 16:44:31.105
    Jan 18 16:44:31.115: INFO: Waiting up to 5m0s for pod "pod-configmaps-19a68527-93a5-4337-bcd6-60fb9cb32fb5" in namespace "configmap-277" to be "Succeeded or Failed"
    Jan 18 16:44:31.119: INFO: Pod "pod-configmaps-19a68527-93a5-4337-bcd6-60fb9cb32fb5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.439395ms
    Jan 18 16:44:33.124: INFO: Pod "pod-configmaps-19a68527-93a5-4337-bcd6-60fb9cb32fb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008902012s
    Jan 18 16:44:35.125: INFO: Pod "pod-configmaps-19a68527-93a5-4337-bcd6-60fb9cb32fb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009604442s
    STEP: Saw pod success 01/18/23 16:44:35.125
    Jan 18 16:44:35.125: INFO: Pod "pod-configmaps-19a68527-93a5-4337-bcd6-60fb9cb32fb5" satisfied condition "Succeeded or Failed"
    Jan 18 16:44:35.130: INFO: Trying to get logs from node v1-25-1-18760-w2 pod pod-configmaps-19a68527-93a5-4337-bcd6-60fb9cb32fb5 container env-test: <nil>
    STEP: delete the pod 01/18/23 16:44:35.139
    Jan 18 16:44:35.169: INFO: Waiting for pod pod-configmaps-19a68527-93a5-4337-bcd6-60fb9cb32fb5 to disappear
    Jan 18 16:44:35.175: INFO: Pod pod-configmaps-19a68527-93a5-4337-bcd6-60fb9cb32fb5 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 16:44:35.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-277" for this suite. 01/18/23 16:44:35.181
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":360,"skipped":6707,"failed":0}
Jan 18 16:44:35.204: INFO: Running AfterSuite actions on all nodes
Jan 18 16:44:35.204: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Jan 18 16:44:35.204: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Jan 18 16:44:35.204: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Jan 18 16:44:35.204: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jan 18 16:44:35.204: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jan 18 16:44:35.204: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jan 18 16:44:35.204: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Jan 18 16:44:35.205: INFO: Running AfterSuite actions on node 1
Jan 18 16:44:35.205: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.001 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jan 18 16:44:35.204: INFO: Running AfterSuite actions on all nodes
    Jan 18 16:44:35.204: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Jan 18 16:44:35.204: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Jan 18 16:44:35.204: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Jan 18 16:44:35.204: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Jan 18 16:44:35.204: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Jan 18 16:44:35.204: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Jan 18 16:44:35.204: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jan 18 16:44:35.205: INFO: Running AfterSuite actions on node 1
    Jan 18 16:44:35.205: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.113 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 360 of 7067 Specs in 5901.719 seconds
SUCCESS! -- 360 Passed | 0 Failed | 0 Pending | 6707 Skipped
PASS

Ginkgo ran 1 suite in 1h38m22.292782537s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

